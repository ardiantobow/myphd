{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardie85/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2024-04-15 00:36:15,252 [torchrl][INFO] check_env_specs succeeded!\n",
      "episode_reward_mean = -0.6058009266853333:  10%|█         | 6000/60000 [00:04<00:36, 1480.69it/s]"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'max_steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 332\u001b[0m\n\u001b[1;32m    329\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 332\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 323\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m visualization_handler \u001b[38;5;241m=\u001b[39m VisualizationHandler(gif_path)\n\u001b[1;32m    302\u001b[0m ppo_trainer \u001b[38;5;241m=\u001b[39m PPOTrainer(\n\u001b[1;32m    303\u001b[0m     policy\u001b[38;5;241m=\u001b[39mpolicy,\n\u001b[1;32m    304\u001b[0m     critic\u001b[38;5;241m=\u001b[39mcritic,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m     visualization_handler\u001b[38;5;241m=\u001b[39mvisualization_handler,\n\u001b[1;32m    321\u001b[0m )\n\u001b[0;32m--> 323\u001b[0m episode_reward_mean_list \u001b[38;5;241m=\u001b[39m \u001b[43mppo_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(episode_reward_mean_list)\n\u001b[1;32m    326\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining iterations\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 123\u001b[0m, in \u001b[0;36mPPOTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes_per_batch)\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisualization_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualization_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_and_save_gif\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m pbar\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m episode_reward_mean_list\n",
      "Cell \u001b[0;32mIn[1], line 141\u001b[0m, in \u001b[0;36mVisualizationHandler.render_and_save_gif\u001b[0;34m(self, env, policy)\u001b[0m\n\u001b[1;32m    137\u001b[0m     env\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    140\u001b[0m     env\u001b[38;5;241m.\u001b[39mrollout(\n\u001b[0;32m--> 141\u001b[0m         max_steps\u001b[38;5;241m=\u001b[39m\u001b[43mmax_steps\u001b[49m,\n\u001b[1;32m    142\u001b[0m         policy\u001b[38;5;241m=\u001b[39mpolicy,\n\u001b[1;32m    143\u001b[0m         callback\u001b[38;5;241m=\u001b[39mrendering_callback,\n\u001b[1;32m    144\u001b[0m         auto_cast_to_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    145\u001b[0m         break_when_any_done\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    146\u001b[0m     )\n\u001b[1;32m    148\u001b[0m env\u001b[38;5;241m.\u001b[39mframes \u001b[38;5;241m=\u001b[39m [PILImage\u001b[38;5;241m.\u001b[39mfromarray(frame) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(frame, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m frame \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m env\u001b[38;5;241m.\u001b[39mframes]\n\u001b[1;32m    150\u001b[0m frames \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mframes\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_steps' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode_reward_mean = -0.6058009266853333:  10%|█         | 6000/60000 [00:19<00:36, 1480.69it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import multiprocessing\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor, TanhNormal\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import PIL.Image as PILImage\n",
    "import io\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "\n",
    "class PPOTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy,\n",
    "        critic,\n",
    "        replay_buffer,\n",
    "        optimizer,\n",
    "        loss_module,\n",
    "        collector,\n",
    "        num_epochs,\n",
    "        minibatch_size,\n",
    "        frames_per_batch,\n",
    "        max_grad_norm,\n",
    "        device,\n",
    "        total_frames,\n",
    "        gamma,\n",
    "        lmbda,\n",
    "        clip_epsilon,\n",
    "        entropy_eps,\n",
    "        env,\n",
    "        visualization_handler=None,\n",
    "    ):\n",
    "        self.policy = policy\n",
    "        self.critic = critic\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_module = loss_module\n",
    "        self.collector = collector\n",
    "        self.num_epochs = num_epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.frames_per_batch = frames_per_batch\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.device = device\n",
    "        self.total_frames = total_frames\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.entropy_eps = entropy_eps\n",
    "        self.env = env\n",
    "        self.visualization_handler = visualization_handler\n",
    "\n",
    "    def train(self, max_steps):\n",
    "        pbar = tqdm(total=self.total_frames, desc=\"episode_reward_mean = 0\")\n",
    "        episode_reward_mean_list = []\n",
    "\n",
    "        for tensordict_data in self.collector:\n",
    "            tensordict_data.set(\n",
    "                (\"next\", \"agents\", \"done\"),\n",
    "                tensordict_data.get((\"next\", \"done\"))\n",
    "                .unsqueeze(-1)\n",
    "                .expand(tensordict_data.get_item_shape((\"next\", self.env.reward_key))),\n",
    "            )\n",
    "            tensordict_data.set(\n",
    "                (\"next\", \"agents\", \"terminated\"),\n",
    "                tensordict_data.get((\"next\", \"terminated\"))\n",
    "                .unsqueeze(-1)\n",
    "                .expand(tensordict_data.get_item_shape((\"next\", self.env.reward_key))),\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.loss_module.value_estimator(\n",
    "                    tensordict_data,\n",
    "                    params=self.loss_module.critic_network_params,\n",
    "                    target_params=self.loss_module.target_critic_network_params,\n",
    "                )\n",
    "\n",
    "            data_view = tensordict_data.reshape(-1)\n",
    "            self.replay_buffer.extend(data_view)\n",
    "\n",
    "            for _ in range(self.num_epochs):\n",
    "                for _ in range(self.frames_per_batch // self.minibatch_size):\n",
    "                    subdata = self.replay_buffer.sample()\n",
    "                    loss_vals = self.loss_module(subdata)\n",
    "\n",
    "                    loss_value = (\n",
    "                        loss_vals[\"loss_objective\"]\n",
    "                        + loss_vals[\"loss_critic\"]\n",
    "                        + loss_vals[\"loss_entropy\"]\n",
    "                    )\n",
    "\n",
    "                    loss_value.backward()\n",
    "\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.loss_module.parameters(), self.max_grad_norm\n",
    "                    )\n",
    "\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "\n",
    "            self.collector.update_policy_weights_()\n",
    "\n",
    "            done = tensordict_data.get((\"next\", \"agents\", \"done\"))\n",
    "            episode_reward_mean = (\n",
    "                tensordict_data.get((\"next\", \"agents\", \"episode_reward\"))[done].mean().item()\n",
    "            )\n",
    "            episode_reward_mean_list.append(episode_reward_mean)\n",
    "            pbar.set_description(f\"episode_reward_mean = {episode_reward_mean}\", refresh=True)\n",
    "            pbar.update(self.frames_per_batch)\n",
    "\n",
    "            if self.visualization_handler is not None:\n",
    "                self.visualization_handler.render_and_save_gif(self.env, self.policy, max_steps)\n",
    "\n",
    "        pbar.close()\n",
    "        return episode_reward_mean_list\n",
    "\n",
    "class VisualizationHandler:\n",
    "    def __init__(self, gif_path):\n",
    "        self.gif_path = gif_path\n",
    "\n",
    "    def render_and_save_gif(self, env, policy, max_steps):\n",
    "        env.frames = []\n",
    "\n",
    "        def rendering_callback(env, td):\n",
    "            frame = env.render(mode=\"rgb_array\")\n",
    "            env.frames.append(frame)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            env.rollout(\n",
    "                max_steps=max_steps,\n",
    "                policy=policy,\n",
    "                callback=rendering_callback,\n",
    "                auto_cast_to_device=True,\n",
    "                break_when_any_done=False,\n",
    "            )\n",
    "\n",
    "        env.frames = [PILImage.fromarray(frame) if isinstance(frame, np.ndarray) else frame for frame in env.frames]\n",
    "\n",
    "        frames = env.frames\n",
    "        frames[0].save(\n",
    "            self.gif_path,\n",
    "            save_all=True,\n",
    "            append_images=frames[1:],\n",
    "            duration=300,\n",
    "            loop=0,\n",
    "        )\n",
    "\n",
    "        display(Image(self.gif_path))\n",
    "\n",
    "def main():\n",
    "    torch.manual_seed(0)\n",
    "    is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "    device = (\n",
    "        torch.device(0)\n",
    "        if torch.cuda.is_available() and not is_fork\n",
    "        else torch.device(\"cpu\")\n",
    "    )\n",
    "    vmas_device = device\n",
    "\n",
    "    # Sampling\n",
    "    frames_per_batch = 6_000\n",
    "    n_iters = 10\n",
    "    total_frames = frames_per_batch * n_iters\n",
    "\n",
    "    # Training\n",
    "    num_epochs = 1\n",
    "    minibatch_size = 400\n",
    "    lr = 0.0003\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    # PPO\n",
    "    clip_epsilon = 0.2\n",
    "    gamma = 0.9\n",
    "    lmbda = 0.9\n",
    "    entropy_eps = 1e-4\n",
    "\n",
    "    max_steps = 100\n",
    "    num_vmas_envs = frames_per_batch // max_steps\n",
    "    scenario_name = \"navigation\"\n",
    "    n_agents = 3\n",
    "\n",
    "    env = VmasEnv(\n",
    "        scenario=scenario_name,\n",
    "        num_envs=num_vmas_envs,\n",
    "        continuous_actions=True,\n",
    "        max_steps=max_steps,\n",
    "        device=device,\n",
    "        n_agents=n_agents,\n",
    "    )\n",
    "\n",
    "    env = TransformedEnv(\n",
    "        env,\n",
    "        RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
    "    )\n",
    "\n",
    "    check_env_specs(env)\n",
    "\n",
    "    policy_net = torch.nn.Sequential(\n",
    "        MultiAgentMLP(\n",
    "            n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "            n_agent_outputs=2 * env.action_spec.shape[-1],\n",
    "            n_agents=env.n_agents,\n",
    "            centralised=False,\n",
    "            share_params=True,\n",
    "            device=device,\n",
    "            depth=2,\n",
    "            num_cells=256,\n",
    "            activation_class=torch.nn.Tanh,\n",
    "        ),\n",
    "        NormalParamExtractor(),\n",
    "    )\n",
    "\n",
    "    policy_module = TensorDictModule(\n",
    "        policy_net,\n",
    "        in_keys=[(\"agents\", \"observation\")],\n",
    "        out_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
    "    )\n",
    "\n",
    "    policy = ProbabilisticActor(\n",
    "        module=policy_module,\n",
    "        spec=env.unbatched_action_spec,\n",
    "        in_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
    "        out_keys=[env.action_key],\n",
    "        distribution_class=TanhNormal,\n",
    "        distribution_kwargs={\n",
    "            \"min\": env.unbatched_action_spec[env.action_key].space.low,\n",
    "            \"max\": env.unbatched_action_spec[env.action_key].space.high,\n",
    "        },\n",
    "        return_log_prob=True,\n",
    "        log_prob_key=(\"agents\", \"sample_log_prob\"),\n",
    "    )\n",
    "\n",
    "    critic_net = MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "        n_agent_outputs=1,\n",
    "        n_agents=env.n_agents,\n",
    "        centralised=False,\n",
    "        share_params=True,\n",
    "        device=device,\n",
    "        depth=2,\n",
    "        num_cells=256,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    )\n",
    "\n",
    "    critic = TensorDictModule(\n",
    "        module=critic_net,\n",
    "        in_keys=[(\"agents\", \"observation\")],\n",
    "        out_keys=[(\"agents\", \"state_value\")],\n",
    "    )\n",
    "\n",
    "    collector = SyncDataCollector(\n",
    "        env,\n",
    "        policy,\n",
    "        device=vmas_device,\n",
    "        storing_device=device,\n",
    "        frames_per_batch=frames_per_batch,\n",
    "        total_frames=total_frames,\n",
    "    )\n",
    "\n",
    "    replay_buffer = ReplayBuffer(\n",
    "        storage=LazyTensorStorage(frames_per_batch, device=device),\n",
    "        sampler=SamplerWithoutReplacement(),\n",
    "        batch_size=minibatch_size,\n",
    "    )\n",
    "\n",
    "    loss_module = ClipPPOLoss(\n",
    "        actor_network=policy,\n",
    "        critic_network=critic,\n",
    "        clip_epsilon=clip_epsilon,\n",
    "        entropy_coef=entropy_eps,\n",
    "        normalize_advantage=False,\n",
    "    )\n",
    "    loss_module.set_keys(\n",
    "        reward=env.reward_key,\n",
    "        action=env.action_key,\n",
    "        sample_log_prob=(\"agents\", \"sample_log_prob\"),\n",
    "        value=(\"agents\", \"state_value\"),\n",
    "        done=(\"agents\", \"done\"),\n",
    "        terminated=(\"agents\", \"terminated\"),\n",
    "    )\n",
    "\n",
    "    loss_module.make_value_estimator(\n",
    "        ValueEstimators.GAE, gamma=gamma, lmbda=lmbda\n",
    "    )\n",
    "\n",
    "    optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "\n",
    "    gif_path = f\"{scenario_name}.gif\"\n",
    "    visualization_handler = VisualizationHandler(gif_path)\n",
    "\n",
    "    ppo_trainer = PPOTrainer(\n",
    "        policy=policy,\n",
    "        critic=critic,\n",
    "        replay_buffer=replay_buffer,\n",
    "        optimizer=optim,\n",
    "        loss_module=loss_module,\n",
    "        collector=collector,\n",
    "        num_epochs=num_epochs,\n",
    "        minibatch_size=minibatch_size,\n",
    "        frames_per_batch=frames_per_batch,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        device=device,\n",
    "        total_frames=total_frames,\n",
    "        gamma=gamma,\n",
    "        lmbda=lmbda,\n",
    "        clip_epsilon=clip_epsilon,\n",
    "        entropy_eps=entropy_eps,\n",
    "        env=env,\n",
    "        visualization_handler=visualization_handler,\n",
    "    )\n",
    "\n",
    "    episode_reward_mean_list = ppo_trainer.train(max_steps)\n",
    "\n",
    "    plt.plot(episode_reward_mean_list)\n",
    "    plt.xlabel(\"Training iterations\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Episode reward mean\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
