{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardie85/PHD/Research/code/.venv/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- device used: cuda\n",
      "Step 1 of Episode 0\n",
      "action type of agent 0: problem solver\n",
      "action type of agent 1: problem solver\n",
      "action taken: [tensor([[[1.2000e-01, 4.0000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [9.9000e+01, 9.9000e+01, 1.2000e-01, 4.0000e-02, 0.0000e+00,\n",
      "          0.0000e+00]]], device='cuda:0', dtype=torch.float64), tensor([[[2.0000e-01, 4.0000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [9.9000e+01, 9.9000e+01, 2.0000e-01, 4.0000e-02, 0.0000e+00,\n",
      "          0.0000e+00]]], device='cuda:0', dtype=torch.float64)]\n",
      "physical action execution in environment tensor([[0.1200, 0.0400]], device='cuda:0', dtype=torch.float64)\n",
      "physical action execution in environment tensor([[0.2000, 0.0400]], device='cuda:0', dtype=torch.float64)\n",
      "physical action stored in CB: tensor([0.1200, 0.0400], device='cuda:0', dtype=torch.float64)\n",
      "physical action stored in CB: tensor([0.2000, 0.0400], device='cuda:0', dtype=torch.float64)\n",
      "Step 2 of Episode 0\n",
      "action type of agent 0: problem solver\n",
      "action type of agent 1: problem solver\n",
      "action taken: [tensor([[[1.2000e-01, 5.0000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [9.9000e+01, 9.9000e+01, 1.2000e-01, 5.0000e-02, 0.0000e+00,\n",
      "          0.0000e+00]]], device='cuda:0', dtype=torch.float64), tensor([[[2.0000e-01, 4.0000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [9.9000e+01, 9.9000e+01, 2.0000e-01, 4.0000e-02, 0.0000e+00,\n",
      "          0.0000e+00]]], device='cuda:0', dtype=torch.float64)]\n",
      "physical action execution in environment tensor([[0.1200, 0.0500]], device='cuda:0', dtype=torch.float64)\n",
      "physical action execution in environment tensor([[0.2000, 0.0400]], device='cuda:0', dtype=torch.float64)\n",
      "physical action stored in CB: tensor([0.1200, 0.0500], device='cuda:0', dtype=torch.float64)\n",
      "physical action stored in CB: tensor([0.2000, 0.0400], device='cuda:0', dtype=torch.float64)\n",
      "Step 3 of Episode 0\n",
      "action type of agent 0: problem solver\n",
      "action type of agent 1: problem solver\n",
      "action taken: [tensor([[[1.2000e-01, 5.0000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [9.9000e+01, 9.9000e+01, 1.2000e-01, 5.0000e-02, 0.0000e+00,\n",
      "          0.0000e+00]]], device='cuda:0', dtype=torch.float64), tensor([[[2.0000e-01, 4.0000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [9.9000e+01, 9.9000e+01, 2.0000e-01, 4.0000e-02, 0.0000e+00,\n",
      "          0.0000e+00]]], device='cuda:0', dtype=torch.float64)]\n",
      "physical action execution in environment tensor([[0.1200, 0.0500]], device='cuda:0', dtype=torch.float64)\n",
      "physical action execution in environment tensor([[0.2000, 0.0400]], device='cuda:0', dtype=torch.float64)\n",
      "physical action stored in CB: tensor([0.1200, 0.0500], device='cuda:0', dtype=torch.float64)\n",
      "physical action stored in CB: tensor([0.2000, 0.0400], device='cuda:0', dtype=torch.float64)\n",
      "Success hit of each agent at the end of episode 0:\n",
      "Agent 0: 0%\n",
      "Agent 1: 0%\n",
      "Episode 0: Failed\n",
      "case - problem: (4, 4)\n",
      "case - solution: tensor([0.1200, 0.0400], device='cuda:0', dtype=torch.float64)\n",
      "case - reward: tensor([0.0002], device='cuda:0')\n",
      "case - trust_value: 1\n",
      "case - problem then: (4, 4)\n",
      "case - solution then: [0.12, 0.04]\n",
      "case - reward then: 0.0001558065414428711\n",
      "case - trust_value then: 1.0\n",
      "case - problem: (4, 4)\n",
      "case - solution: tensor([0.1200, 0.0500], device='cuda:0', dtype=torch.float64)\n",
      "case - reward: tensor([0.0002], device='cuda:0')\n",
      "case - trust_value: 1\n",
      "case - problem then: (4, 4)\n",
      "case - solution then: [0.12, 0.05]\n",
      "case - reward then: 0.00024378299713134766\n",
      "case - trust_value then: 1.0\n",
      "case - problem: (4, 4)\n",
      "case - solution: tensor([0.1200, 0.0500], device='cuda:0', dtype=torch.float64)\n",
      "case - reward: tensor([0.0003], device='cuda:0')\n",
      "case - trust_value: 1\n",
      "case - problem then: (4, 4)\n",
      "case - solution then: [0.12, 0.05]\n",
      "case - reward then: 0.0002906322479248047\n",
      "case - trust_value then: 1.0\n",
      "Temporary case base saved successfully.\n",
      "Case base saved successfully.\n",
      "case - problem: (4, 5)\n",
      "case - solution: tensor([0.2000, 0.0400], device='cuda:0', dtype=torch.float64)\n",
      "case - reward: tensor([-0.0010], device='cuda:0')\n",
      "case - trust_value: 1\n",
      "case - problem then: (4, 5)\n",
      "case - solution then: [0.2, 0.04]\n",
      "case - reward then: -0.0009672641754150391\n",
      "case - trust_value then: 1.0\n",
      "case - problem: (4, 5)\n",
      "case - solution: tensor([0.2000, 0.0400], device='cuda:0', dtype=torch.float64)\n",
      "case - reward: tensor([-0.0019], device='cuda:0')\n",
      "case - trust_value: 1\n",
      "case - problem then: (4, 5)\n",
      "case - solution then: [0.2, 0.04]\n",
      "case - reward then: -0.001936793327331543\n",
      "case - trust_value then: 1.0\n",
      "case - problem: (4, 5)\n",
      "case - solution: tensor([0.2000, 0.0400], device='cuda:0', dtype=torch.float64)\n",
      "case - reward: tensor([-0.0027], device='cuda:0')\n",
      "case - trust_value: 1\n",
      "case - problem then: (4, 5)\n",
      "case - solution then: [0.2, 0.04]\n",
      "case - reward then: -0.002668023109436035\n",
      "case - trust_value then: 1.0\n",
      "Temporary case base saved successfully.\n",
      "Case base saved successfully.\n",
      "Step 1 of Episode 1\n",
      "action type of agent 0: problem solver\n",
      "action type of agent 1: problem solver\n",
      "action taken: [tensor([[[1.2000e-01, 4.0000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [9.9000e+01, 9.9000e+01, 1.2000e-01, 4.0000e-02, 0.0000e+00,\n",
      "          0.0000e+00]]], device='cuda:0', dtype=torch.float64), tensor([[[2.0000e-01, 4.0000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [9.9000e+01, 9.9000e+01, 2.0000e-01, 4.0000e-02, 0.0000e+00,\n",
      "          0.0000e+00]]], device='cuda:0', dtype=torch.float64)]\n",
      "physical action execution in environment tensor([[0.1200, 0.0400]], device='cuda:0', dtype=torch.float64)\n",
      "physical action execution in environment tensor([[0.2000, 0.0400]], device='cuda:0', dtype=torch.float64)\n",
      "physical action stored in CB: tensor([0.1200, 0.0400], device='cuda:0', dtype=torch.float64)\n",
      "physical action stored in CB: tensor([0.2000, 0.0400], device='cuda:0', dtype=torch.float64)\n",
      "Step 2 of Episode 1\n",
      "action type of agent 0: problem solver\n",
      "action type of agent 1: problem solver\n",
      "action taken: [tensor([[[1.2000e-01, 5.0000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [9.9000e+01, 9.9000e+01, 1.2000e-01, 5.0000e-02, 0.0000e+00,\n",
      "          0.0000e+00]]], device='cuda:0', dtype=torch.float64), tensor([[[2.0000e-01, 4.0000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [9.9000e+01, 9.9000e+01, 2.0000e-01, 4.0000e-02, 0.0000e+00,\n",
      "          0.0000e+00]]], device='cuda:0', dtype=torch.float64)]\n",
      "physical action execution in environment tensor([[0.1200, 0.0500]], device='cuda:0', dtype=torch.float64)\n",
      "physical action execution in environment tensor([[0.2000, 0.0400]], device='cuda:0', dtype=torch.float64)\n",
      "physical action stored in CB: tensor([0.1200, 0.0500], device='cuda:0', dtype=torch.float64)\n",
      "physical action stored in CB: tensor([0.2000, 0.0400], device='cuda:0', dtype=torch.float64)\n",
      "Step 3 of Episode 1\n",
      "action type of agent 0: problem solver\n",
      "action type of agent 1: problem solver\n",
      "action taken: [tensor([[[1.2000e-01, 5.0000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [9.9000e+01, 9.9000e+01, 1.2000e-01, 5.0000e-02, 0.0000e+00,\n",
      "          0.0000e+00]]], device='cuda:0', dtype=torch.float64), tensor([[[2.0000e-01, 4.0000e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00],\n",
      "         [9.9000e+01, 9.9000e+01, 2.0000e-01, 4.0000e-02, 0.0000e+00,\n",
      "          0.0000e+00]]], device='cuda:0', dtype=torch.float64)]\n",
      "physical action execution in environment tensor([[0.1200, 0.0500]], device='cuda:0', dtype=torch.float64)\n",
      "physical action execution in environment tensor([[0.2000, 0.0400]], device='cuda:0', dtype=torch.float64)\n",
      "physical action stored in CB: tensor([0.1200, 0.0500], device='cuda:0', dtype=torch.float64)\n",
      "physical action stored in CB: tensor([0.2000, 0.0400], device='cuda:0', dtype=torch.float64)\n",
      "Success hit of each agent at the end of episode 1:\n",
      "Agent 0: 0%\n",
      "Agent 1: 0%\n",
      "Episode 1: Failed\n",
      "case - problem: (4, 4)\n",
      "case - solution: tensor([0.1200, 0.0400], device='cuda:0', dtype=torch.float64)\n",
      "case - reward: tensor([0.0002], device='cuda:0')\n",
      "case - trust_value: 1\n",
      "case - problem then: (4, 4)\n",
      "case - solution then: [0.12, 0.04]\n",
      "case - reward then: 0.0001558065414428711\n",
      "case - trust_value then: 1.0\n",
      "case - problem: (4, 4)\n",
      "case - solution: tensor([0.1200, 0.0500], device='cuda:0', dtype=torch.float64)\n",
      "case - reward: tensor([0.0002], device='cuda:0')\n",
      "case - trust_value: 1\n",
      "case - problem then: (4, 4)\n",
      "case - solution then: [0.12, 0.05]\n",
      "case - reward then: 0.00024378299713134766\n",
      "case - trust_value then: 1.0\n",
      "case - problem: (4, 4)\n",
      "case - solution: tensor([0.1200, 0.0500], device='cuda:0', dtype=torch.float64)\n",
      "case - reward: tensor([0.0003], device='cuda:0')\n",
      "case - trust_value: 1\n",
      "case - problem then: (4, 4)\n",
      "case - solution then: [0.12, 0.05]\n",
      "case - reward then: 0.0002906322479248047\n",
      "case - trust_value then: 1.0\n",
      "Temporary case base saved successfully.\n",
      "Case base saved successfully.\n",
      "case - problem: (4, 5)\n",
      "case - solution: tensor([0.2000, 0.0400], device='cuda:0', dtype=torch.float64)\n",
      "case - reward: tensor([-0.0010], device='cuda:0')\n",
      "case - trust_value: 1\n",
      "case - problem then: (4, 5)\n",
      "case - solution then: [0.2, 0.04]\n",
      "case - reward then: -0.0009672641754150391\n",
      "case - trust_value then: 1.0\n",
      "case - problem: (4, 5)\n",
      "case - solution: tensor([0.2000, 0.0400], device='cuda:0', dtype=torch.float64)\n",
      "case - reward: tensor([-0.0019], device='cuda:0')\n",
      "case - trust_value: 1\n",
      "case - problem then: (4, 5)\n",
      "case - solution then: [0.2, 0.04]\n",
      "case - reward then: -0.001936793327331543\n",
      "case - trust_value then: 1.0\n",
      "case - problem: (4, 5)\n",
      "case - solution: tensor([0.2000, 0.0400], device='cuda:0', dtype=torch.float64)\n",
      "case - reward: tensor([-0.0027], device='cuda:0')\n",
      "case - trust_value: 1\n",
      "case - problem then: (4, 5)\n",
      "case - solution then: [0.2, 0.04]\n",
      "case - reward then: -0.002668023109436035\n",
      "case - trust_value then: 1.0\n",
      "Temporary case base saved successfully.\n",
      "Case base saved successfully.\n",
      "------------------------------------------\n",
      "Temporary case base saved successfully.\n",
      "Case base saved successfully.\n",
      "Temporary case base saved successfully.\n",
      "Case base saved successfully.\n",
      "Overall success percentage for all agents = 0.0%\n",
      "It took: 0.877321720123291s for 6 steps across 2 episodes of 1 parallel environments on device cuda for navigation_comm scenario.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file navigation_comm.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"navigation_comm.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import ast\n",
    "from vmas import make_env\n",
    "from vmas.simulator.core import Agent\n",
    "from vmas.simulator.scenario import BaseScenario\n",
    "from typing import Union\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "from IPython.display import HTML, display as ipython_display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.spaces import Discrete\n",
    "import math\n",
    "from collections import deque\n",
    "from collections import defaultdict \n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        # print (f\"input dim type: {type(input_dim)}\")\n",
    "        # print (f\"action dim type: {type(action_dim)}\")\n",
    "\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        policy_dist = torch.tanh(self.actor(x))\n",
    "        rounded_policy_dist = self.round_actions(policy_dist, decimal_places=2)\n",
    "        \n",
    "        return rounded_policy_dist, value\n",
    "\n",
    "    def round_actions(self, actions, decimal_places=3):\n",
    "        scale_factor = 10 ** decimal_places\n",
    "        return torch.round(actions * scale_factor) / scale_factor\n",
    "\n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, env, agent_id, state_dim, action_dim, device, alpha=0.1, gamma=0.99, epsilon=0.2, K_epochs=4, batch_size=64, communication_weight=0.5):\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.K_epoch = K_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.q_table = {}\n",
    "        self.env = env\n",
    "        self.agent_id = agent_id\n",
    "        self.communication_weight = communication_weight  # Weight parameter for incorporating messages\n",
    "        self.device = device\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=alpha)\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "    \n",
    "    def get_action_discrete(self, agent, env, agent_id, agent_obs):\n",
    "        agent_obs_cpu = agent_obs[:6].cpu().numpy()  # Transfer only the required slice to CPU\n",
    "        agent_obs = tuple(np.round(agent_obs_cpu, decimals=5))  # Round the observation\n",
    "\n",
    "        if agent_obs not in self.q_table:\n",
    "            self.q_table[agent_obs] = np.zeros(self.env.action_space[self.agent_id].n)\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Select a random action\n",
    "            action = np.random.randint(env.action_space[self.agent_id].n)\n",
    "        else:\n",
    "            # Select the action with the highest Q-value\n",
    "            action = np.argmax(self.q_table[agent_obs])\n",
    "        \n",
    "        return (action,)  # Return as a tuple\n",
    "    \n",
    "    def update_action_discrete(self,  agent, env, agent_id, obs, action, reward, next_obs):\n",
    "        obs_key = tuple(np.round(obs.cpu().numpy(), decimals=5))  # Only transfer to CPU when necessary\n",
    "        next_obs_key = tuple(np.round(next_obs.cpu().numpy(), decimals=5))\n",
    "        action = int(action.item())  # Convert tensor to Python scalar\n",
    "\n",
    "        # print (f\"reward obtained = {reward}\")\n",
    "\n",
    "        if isinstance(self.env.action_space[self.agent_id], Discrete):\n",
    "            action_space_size = self.env.action_space[self.agent_id].n\n",
    "        else:\n",
    "            raise ValueError(\"This Q-learning implementation requires a discrete action space.\")\n",
    "\n",
    "        if obs_key not in self.q_table:\n",
    "            self.q_table[obs_key] = np.zeros(action_space_size)\n",
    "\n",
    "        if next_obs_key not in self.q_table:\n",
    "            self.q_table[next_obs_key] = np.zeros(action_space_size)\n",
    "\n",
    "        best_next_action = np.argmax(self.q_table[next_obs_key])\n",
    "        td_target = reward + self.gamma * self.q_table[next_obs_key][best_next_action]\n",
    "\n",
    "        td_error = td_target - self.q_table[obs_key][action]\n",
    "        self.q_table[obs_key][action] += self.alpha * td_error\n",
    "\n",
    "        print(f\"Agent {self.agent_id} - Updated Q-table for obs {obs_key}, action {action}, reward {reward}, next_obs {next_obs_key}\")\n",
    "\n",
    "    \n",
    "    def print_q_table(self):\n",
    "        print(f\"Q-table for Agent {self.agent_id}:\")\n",
    "        for state, actions in self.q_table.items():\n",
    "            print(f\"  State: {state}\")\n",
    "            for action, q_value in enumerate(actions):\n",
    "                print(f\"    Action: {action}, Q-value: {q_value:.5f}\")\n",
    "        print(f\"End of Q-table for Agent {self.agent_id}\\n\")\n",
    "\n",
    "\n",
    "    def get_action_continuous(self, agent, env, agent_id, agent_obs, device):\n",
    "        state = torch.Tensor(agent_obs[:6]).to(device) if not isinstance(agent_obs[:6], torch.Tensor) else agent_obs[:6].to(device)\n",
    "        state = state.float()  # Ensure the state is float32\n",
    "        with torch.no_grad():\n",
    "            policy_dist, _ = self.policy_old(state)\n",
    "        action = policy_dist.cpu().numpy()\n",
    "\n",
    "        # print(f\"action generated from NN: {action}\")\n",
    "        \n",
    "        return action\n",
    "\n",
    "\n",
    "    def update_action_continuous(self, agent_obs, action, reward, next_obs, done):\n",
    "        agent_obs = torch.tensor(agent_obs[:6], dtype=torch.float32).to(self.device)\n",
    "        next_obs = torch.tensor(next_obs[:6], dtype=torch.float32).to(self.device)\n",
    "        action = torch.tensor(action, dtype=torch.float32).to(self.device)\n",
    "        # done = torch.tensor(done, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # print (f\"observation received for update: {agent_obs}\")\n",
    "        # print (f\"action received for update: {agent_obs}\")\n",
    "        # print (f\"reward received for update: {reward}\")\n",
    "        # print (f\"next observation received for update: {next_obs}\")\n",
    "        # print (f\"done received for update: {done}\")\n",
    "\n",
    "        # Calculate advantage\n",
    "        _, value = self.policy(agent_obs)\n",
    "        _, next_value = self.policy(next_obs)\n",
    "        # target_value = reward + self.gamma * next_value * (1 - done)\n",
    "        target_value = reward + self.gamma * next_value * (~done)\n",
    "        advantage = (target_value - value).detach()\n",
    "\n",
    "        # Calculate the log probability of the action under the current policy\n",
    "        mu, sigma = self.policy(agent_obs)\n",
    "        sigma = torch.exp(sigma) \n",
    "        # print(f\"mu: {mu}\")\n",
    "        # print(f\"sigma: {sigma}\")\n",
    "        dist = torch.distributions.Normal(mu, sigma)\n",
    "        \n",
    "        log_prob = dist.log_prob(action).sum()\n",
    "\n",
    "        # Calculate the log probability of the action under the old policy\n",
    "        with torch.no_grad():\n",
    "            mu_old, sigma_old = self.policy_old(agent_obs)\n",
    "            sigma_old = torch.exp(sigma_old) \n",
    "            dist_old = torch.distributions.Normal(mu_old, sigma_old)\n",
    "            old_log_prob = dist_old.log_prob(action).sum()\n",
    "\n",
    "        # Policy ratio\n",
    "        ratio = torch.exp(log_prob - old_log_prob)\n",
    "\n",
    "        # PPO objective with clipping\n",
    "        surrogate1 = ratio * advantage\n",
    "        surrogate2 = torch.clamp(ratio, 1.0 - self.epsilon, 1.0 + self.epsilon) * advantage\n",
    "        policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "        # Value function loss\n",
    "        value_loss = nn.MSELoss()(value, target_value.detach())\n",
    "\n",
    "        # Total loss\n",
    "        loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "        # Update the actor-critic model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # print(f\"Agent {self.agent_id} - PPO update: policy loss = {policy_loss.item()}, value loss = {value_loss.item()}\")\n",
    "\n",
    "    def print_model_parameters(self):\n",
    "        print(f\"Model parameters for Agent {self.agent_id}:\")\n",
    "        for name, param in self.actor_critic.named_parameters():\n",
    "            print(f\"  {name}: {param.data.numpy()}\")\n",
    "        print(f\"End of model parameters for Agent {self.agent_id}\\n\")\n",
    "\n",
    "\n",
    "class Case:\n",
    "    added_states = set()  # Class attribute to store states already added to the case base\n",
    "\n",
    "    def __init__(self, problem, solution, reward,  trust_value=1):\n",
    "        # self.problem = problem if isinstance(problem, list) else ast.literal_eval(problem)  # Convert problem to numpy array\n",
    "        self.problem = problem\n",
    "        self.solution = solution\n",
    "        self.reward = reward\n",
    "        self.trust_value = trust_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)\n",
    "        state2 = np.atleast_1d(state2)\n",
    "        CNDMaxDist = 6  # Maximum distance between two nodes in the-0.9, 0.7, 0.0, 0.0, 0.0, -0.9) CND based on EOPRA reference\n",
    "        v = state1.size  # Total number of objects the agent can perceive\n",
    "        DistQ = np.sum([Case.dist_q(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def dist_q(X1, X2):\n",
    "        return np.min(np.abs(X1 - X2))\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(agent, env, state, case_base, threshold=0.1):\n",
    "        \n",
    "        # print (f\"state retrieved for similiarity check: {state}\")\n",
    "        # Convert the state to numpy if it's a tensor\n",
    "        if isinstance(state, torch.Tensor):\n",
    "            state = state.cpu().numpy()\n",
    "\n",
    "        # Slice the physical observations\n",
    "        # physical_obs = state[:6]\n",
    "\n",
    "        # if not agent.silent:\n",
    "        #     comm_obs = state[6:]\n",
    "        #     # Convert comm_obs to a numpy array if it is a tensor\n",
    "        #     if isinstance(comm_obs, torch.Tensor):\n",
    "        #         comm_obs = comm_obs.cpu().numpy()\n",
    "\n",
    "        # print(f\"physical_obs = {physical_obs}\")\n",
    "\n",
    "        # Ensure the state is in a list format to avoid issues with ast.literal_eval\n",
    "        state_list = state.tolist() if isinstance(state, np.ndarray) else state\n",
    "        state_str = json.dumps(state_list)  # Convert list to a JSON string for ast.literal_eval\n",
    "\n",
    "        # Use ast.literal_eval safely to convert the string back to a list\n",
    "        state = ast.literal_eval(state_str)\n",
    "\n",
    "        similarities = {}\n",
    "        for case in case_base:\n",
    "            problem_numeric = np.array(case.problem, dtype=float)\n",
    "            state_numeric = np.array(state, dtype=float)\n",
    "            \n",
    "            print(f\"state received = {state_numeric}\")\n",
    "            print(f\"case received = {problem_numeric}\")\n",
    "            print(\"---------\")\n",
    "           \n",
    "            similarities[case] = Case.sim_q(state_numeric, problem_numeric)  # Compare state with the problem part of the case\n",
    "\n",
    "        sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if sorted_similarities:\n",
    "            most_similar_case = sorted_similarities[0][0] if sorted_similarities[0][1] >= threshold else None\n",
    "        else:\n",
    "            most_similar_case = None\n",
    "\n",
    "        return most_similar_case\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(c, temporary_case_base):\n",
    "        temporary_case_base.append(c)\n",
    "\n",
    "    @staticmethod\n",
    "    def revise(case_base, temporary_case_base, successfull_task):\n",
    "        for case in temporary_case_base:\n",
    "            if successfull_task and case in case_base:\n",
    "                case.trust_value += 0.1  # Increment trust value if the episode ended successfully and the case is in the case base\n",
    "            elif not successfull_task and case in case_base:\n",
    "                case.trust_value -= 0.1  # Decrement trust value if the episode ended unsuccessfully and the case is in the case base\n",
    "            case.trust_value = max(0, min(case.trust_value, 1))  # Ensure trust value is within [0, 1]\n",
    "\n",
    "    # @staticmethod\n",
    "    # def retain(case_base, temporary_case_base, successfull_task, threshold=0.7):\n",
    "    #     if successfull_task:\n",
    "    #         # Iterate through the temporary case base to find the last occurrence of each unique state\n",
    "    #         for case in reversed(temporary_case_base):\n",
    "    #             state = tuple(np.atleast_1d(case.problem))\n",
    "    #             # Check if the state is already in the case base or has been added previously\n",
    "    #             if state not in Case.added_states:\n",
    "    #                 # Add the case to the case base if the state is new\n",
    "    #                 case_base.append(case)\n",
    "    #                 Case.added_states.add(state)\n",
    "    #             else:\n",
    "    #                 # Find the index of the existing case in the case base\n",
    "    #                 existing_index = next((i for i, c in enumerate(case_base) if tuple(np.atleast_1d(c.problem)) == state), None)\n",
    "    #                 if existing_index is not None:\n",
    "    #                     # Get the existing case from the case base\n",
    "    #                     existing_case = case_base[existing_index]\n",
    "    #                     # Update the trust value of the existing case with the new value from the revise step\n",
    "    #                     existing_case.trust_value = case.trust_value\n",
    "\n",
    "    #     # Filter case_base based on trust_value\n",
    "    #     case_base = [case for case in case_base if case.trust_value >= threshold]\n",
    "    #     return case_base\n",
    "\n",
    "    @staticmethod\n",
    "    def retain(case_base, temporary_case_base, successfull_task, threshold=0.7):\n",
    "        if successfull_task:\n",
    "            # Aggregation dictionaries\n",
    "            aggregation_dict = defaultdict(lambda: {'x_velocity_sum': 0, 'y_velocity_sum': 0, 'reward_sum': 0, 'count': 0})\n",
    "            \n",
    "            # Process temporary case base to aggregate velocities and rewards\n",
    "            for case in temporary_case_base:\n",
    "                state = tuple(np.atleast_1d(case.problem))\n",
    "                aggregation_dict[state]['x_velocity_sum'] += case.x_velocity\n",
    "                aggregation_dict[state]['y_velocity_sum'] += case.y_velocity\n",
    "                aggregation_dict[state]['reward_sum'] += case.reward\n",
    "                aggregation_dict[state]['count'] += 1\n",
    "            \n",
    "            # Create aggregated cases\n",
    "            for state, values in aggregation_dict.items():\n",
    "                # Use the aggregated sums for x-velocity, y-velocity, and reward\n",
    "                aggregated_case = Case()\n",
    "                aggregated_case.problem = state\n",
    "                aggregated_case.x_velocity = values['x_velocity_sum']\n",
    "                aggregated_case.y_velocity = values['y_velocity_sum']\n",
    "                aggregated_case.reward = values['reward_sum']\n",
    "                aggregated_case.trust_value = np.nan  # Default value, should be updated if needed\n",
    "                \n",
    "                # Add aggregated case to the case base if it's new\n",
    "                if state not in Case.added_states:\n",
    "                    case_base.append(aggregated_case)\n",
    "                    Case.added_states.add(state)\n",
    "                else:\n",
    "                    # Update existing cases with aggregated values\n",
    "                    existing_index = next((i for i, c in enumerate(case_base) if tuple(np.atleast_1d(c.problem)) == state), None)\n",
    "                    if existing_index is not None:\n",
    "                        existing_case = case_base[existing_index]\n",
    "                        existing_case.x_velocity = aggregated_case.x_velocity\n",
    "                        existing_case.y_velocity = aggregated_case.y_velocity\n",
    "                        existing_case.reward = aggregated_case.reward\n",
    "                        existing_case.trust_value = np.nan  # Default value, should be updated if needed\n",
    "\n",
    "        # Filter case_base based on trust_value\n",
    "        case_base = [case for case in case_base if case.trust_value >= threshold]\n",
    "        return case_base\n",
    "\n",
    "\n",
    "class QCBRLVmasRunner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        render: bool,\n",
    "        num_envs: int,\n",
    "        num_episodes: int,\n",
    "        max_steps_per_episode: int,\n",
    "        device: str,\n",
    "        scenario: Union[str, BaseScenario],\n",
    "        continuous_actions: bool,\n",
    "        random_action: bool,\n",
    "        n_agents: int,\n",
    "        obs_discrete: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.render = render\n",
    "        self.num_envs = num_envs\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "        self.device = device\n",
    "        self.scenario = scenario\n",
    "        self.continuous_actions = continuous_actions\n",
    "        self.random_action = random_action\n",
    "        self.obs_discrete = obs_discrete\n",
    "        self.kwargs = kwargs\n",
    "        self.frame_list = []  \n",
    "        self.problem_solver_agents = []\n",
    "        self.rewards_history = []  \n",
    "        self.action_counts = {i: {} for i in range(n_agents)}  \n",
    "        self.agent_rewards_history = {i: [] for i in range(n_agents)}\n",
    "        self.successful_episodes_individual = {i: 0 for i in range(n_agents)}  # Track successful episodes for each agent\n",
    "        self.successful_episodes_all_agents = 0  # Track episodes where all agents succeed\n",
    "        self.case_base = {i: [] for i in range(n_agents)}  # Separate case base for each agent\n",
    "        self.temporary_case_base = {i: [] for i in range(n_agents)}  # Separate temporary case base for each agent\n",
    "\n",
    "    \n",
    "\n",
    "    def get_distance_category(self, distance):\n",
    "        \n",
    "        if distance == 0:\n",
    "            distance_qualitative = 0 #at\n",
    "        elif distance < 0.1:\n",
    "            distance_qualitative = 1 #very close\n",
    "        elif distance < 0.25:\n",
    "            distance_qualitative = 2 #close\n",
    "        elif distance < 0.5:\n",
    "            distance_qualitative = 3 #far\n",
    "        elif distance < 75:\n",
    "            distance_qualitative = 4 #far #very far\n",
    "        else:\n",
    "            distance_qualitative = 5 #farthest\n",
    "        \n",
    "        return distance_qualitative\n",
    "        \n",
    "    def get_direction_category(self, agent_x, agent_y, target_x, target_y):\n",
    "        dx = target_x - agent_x\n",
    "        dy = target_y - agent_y\n",
    "\n",
    "        if dy > 0:\n",
    "            if dx > 0:\n",
    "                direction_qualitative = 1 #top right\n",
    "            elif dx < 0:\n",
    "                direction_qualitative = 2 #top left\n",
    "            else:\n",
    "                direction_qualitative = 3 #top\n",
    "        elif dy < 0:\n",
    "            if dx > 0:\n",
    "                direction_qualitative = 4 #bottom right\n",
    "            elif dx < 0:\n",
    "                direction_qualitative = 5 #bottom left\n",
    "            else:\n",
    "                direction_qualitative = 6 #bottom\n",
    "        else:\n",
    "            if dx > 0:\n",
    "                direction_qualitative = 7 #right\n",
    "            elif dx < 0:\n",
    "                direction_qualitative = 8 #left\n",
    "            else:\n",
    "                direction_qualitative = 0 #at\n",
    "            \n",
    "        return direction_qualitative\n",
    "\n",
    "    def _get_qualitative_position(self, env, observation):\n",
    "\n",
    "        pos_x = observation[0:1]\n",
    "        pos_y = observation[1:2]\n",
    "        vel = observation[2:4]\n",
    "        goal_x = observation[4:5]\n",
    "        goal_y = observation[5:6]    \n",
    "        comms_data = observation[6:13]\n",
    "        sensor_data = observation[13:]\n",
    "\n",
    "        distance = math.sqrt((goal_x - pos_x)**2 + (goal_y - pos_y)**2) #Eucledian distance\n",
    "        # print(f\"distance: {distance}\")\n",
    "        distance_category = self.get_distance_category(distance)\n",
    "        direction_category = self.get_direction_category(pos_x, pos_y, goal_x, goal_y)\n",
    "      \n",
    "        return distance_category, direction_category\n",
    "\n",
    "\n",
    "    def _get_action(self, agent, env, agent_id, obs_qualitative, obs_quantitative, device):\n",
    "\n",
    "        # print(f\"obs_qualitative: {obs_qualitative}\")\n",
    "        # print(f\"obs_quantitative: {obs_quantitative}\")\n",
    "\n",
    "        physical_action=[]\n",
    "        comm_action_length = 4\n",
    "\n",
    "        case = Case.retrieve(agent, env, obs_qualitative, self.case_base[agent_id], threshold=0.1)\n",
    "                    \n",
    "        if case:\n",
    "            physical_action = case.solution\n",
    "            print(f\"action type of agent {agent_id}: case base\")\n",
    "        \n",
    "        else:    \n",
    "            \n",
    "            physical_obs_quantitative = obs_quantitative[0:6]\n",
    "\n",
    "            if self.continuous_actions:\n",
    "                physical_action = self.problem_solver_agents[agent_id].get_action_continuous(agent, env, agent_id, physical_obs_quantitative, device)\n",
    "                physical_action_tensor = torch.tensor(physical_action, device=self.device)\n",
    "                # physical_action = (0.1, -0.5)\n",
    "               \n",
    "            else:\n",
    "                physical_action = self.problem_solver_agents[agent_id].get_action_discrete(agent, env, agent_id, physical_obs_quantitative, device)\n",
    "            \n",
    "           \n",
    "        physical_action_tensor = torch.tensor(physical_action, device=self.device)\n",
    "        zero_tensor = torch.zeros(comm_action_length, dtype=torch.float64, device=self.device)\n",
    "        physical_action_tensor_padded = torch.cat((physical_action_tensor, zero_tensor))\n",
    "\n",
    "        # print(f\"physical action: {physical_action}\")\n",
    "        # print(f\"physical action Tensor: {physical_action_tensor}\")\n",
    "        # print (f\"shape of physical action: {physical_action_tensor.shape}\")\n",
    "            \n",
    "        if agent.silent:\n",
    "            action = physical_action_tensor_padded\n",
    "        else:\n",
    "            if not case:\n",
    "                obs_qualitative_tensor = torch.tensor([float(99), float(99)], device=self.device)\n",
    "                reward_tensor = torch.tensor([float(0)], device=self.device)\n",
    "                trust_value_tensor = torch.tensor([float(0)], device=self.device)\n",
    "            else:  \n",
    "                obs_qualitative_tensor = torch.tensor(case.problem, device=self.device)\n",
    "                reward_tensor = torch.tensor(case.reward, device=self.device)\n",
    "                trust_value_tensor = torch.tensor(case.trust_value, device=self.device)\n",
    "\n",
    "            comm_action_tensor = torch.cat([obs_qualitative_tensor, physical_action_tensor, reward_tensor, trust_value_tensor], dim=0)\n",
    "            action = torch.stack((physical_action_tensor_padded, comm_action_tensor)).unsqueeze(0)\n",
    "\n",
    "                # print(f\"Comm action Tensor: {comm_action_tensor}\")\n",
    "                # print (f\"shape of Comm action: {comm_action_tensor.shape}\")\n",
    "            \n",
    "            print(f\"action type of agent {agent_id}: problem solver\")\n",
    "\n",
    "        # else:\n",
    "        #     obs_qualitative_tensor = torch.tensor(obs_qualitative, device=self.device)\n",
    "        #     reward_tensor = torch.tensor([2.4], device=self.device)\n",
    "        #     trust_value_tensor = torch.tensor([1], device=self.device)\n",
    "\n",
    "        #     comm_action_tensor = torch.cat([obs_qualitative_tensor, physical_action_tensor, reward_tensor, trust_value_tensor], dim=0)\n",
    "        #     action = torch.stack((physical_action_tensor_padded, comm_action_tensor)).unsqueeze(0)\n",
    "\n",
    "\n",
    "        # print(f\"action delivered from the function: {action}\")\n",
    "        # print (f\"shape of action: {action.shape}\")\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def save_case_base(self, agent_id):\n",
    "        filename = f\"cases/case_base_{agent_id}.json\"\n",
    "        case_base_data = []\n",
    "        for case in self.case_base[agent_id]:\n",
    "            problem = case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem\n",
    "            \n",
    "            if torch.is_tensor(case.solution):\n",
    "                solution = case.solution.tolist() if case.solution.numel() > 1 else int(case.solution.item())\n",
    "            else:\n",
    "                solution = int(case.solution)\n",
    "            \n",
    "            if torch.is_tensor(case.reward):\n",
    "                reward = case.reward.tolist() if case.reward.numel() > 1 else float(case.reward.item())\n",
    "            else:\n",
    "                reward = float(case.reward)\n",
    "\n",
    "            if torch.is_tensor(case.trust_value):\n",
    "                trust_value = case.trust_value.tolist() if case.trust_value.numel() > 1 else float(case.trust_value.item())\n",
    "            else:\n",
    "                trust_value = float(case.trust_value)\n",
    "            \n",
    "            case_base_data.append({\n",
    "                \"problem\": problem,\n",
    "                \"solution\": solution,\n",
    "                \"reward\": reward,\n",
    "                \"trust_value\": trust_value\n",
    "            })\n",
    "        \n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "        print(\"Case base saved successfully.\")\n",
    "\n",
    "    def save_case_base_eps(self, agent_id, eps):\n",
    "        filename = f\"cases/case_base_{agent_id}_{eps}.json\"\n",
    "        case_base_data = []\n",
    "        for case in self.case_base[agent_id]:\n",
    "            problem = case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem\n",
    "            \n",
    "            if torch.is_tensor(case.solution):\n",
    "                solution = case.solution.tolist() if case.solution.numel() > 1 else int(case.solution.item())\n",
    "            else:\n",
    "                solution = int(case.solution)\n",
    "            \n",
    "            if torch.is_tensor(case.reward):\n",
    "                reward = case.reward.tolist() if case.reward.numel() > 1 else float(case.reward.item())\n",
    "            else:\n",
    "                reward = float(case.reward)\n",
    "            \n",
    "            if torch.is_tensor(case.trust_value):\n",
    "                trust_value = case.trust_value.tolist() if case.trust_value.numel() > 1 else float(case.trust_value.item())\n",
    "            else:\n",
    "                trust_value = float(case.trust_value)\n",
    "            \n",
    "            case_base_data.append({\n",
    "                \"problem\": problem,\n",
    "                \"solution\": solution,\n",
    "                \"reward\": reward,\n",
    "                \"trust_value\": trust_value\n",
    "            })\n",
    "        \n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "        print(\"Case base saved successfully.\")\n",
    "\n",
    "\n",
    "    def save_case_base_temporary(self, agent_id,):\n",
    "        filename = f\"cases/case_base_temporary_{agent_id}.json\"\n",
    "        case_base_data = []\n",
    "        for case in self.temporary_case_base[agent_id]:\n",
    "\n",
    "            problem = case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem\n",
    "            \n",
    "            if torch.is_tensor(case.solution):\n",
    "                solution = case.solution.tolist() if case.solution.numel() > 1 else int(case.solution.item())\n",
    "            else:\n",
    "                solution = int(case.solution)\n",
    "\n",
    "            if torch.is_tensor(case.reward):\n",
    "                reward = case.reward.tolist() if case.reward.numel() > 1 else float(case.reward.item())\n",
    "            else:\n",
    "                reward = float(case.reward)\n",
    "            \n",
    "            if torch.is_tensor(case.trust_value):\n",
    "                trust_value = case.trust_value.tolist() if case.trust_value.numel() > 1 else float(case.trust_value.item())\n",
    "            else:\n",
    "                trust_value = float(case.trust_value)\n",
    "            \n",
    "            case_base_data.append({\n",
    "                \"problem\": problem,\n",
    "                \"solution\": solution,\n",
    "                \"reward\": reward,\n",
    "                \"trust_value\": trust_value\n",
    "            })\n",
    "        \n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "        print(\"Temporary case base saved successfully.\")\n",
    "\n",
    "    def save_case_base_temporary_eps(self, agent_id, eps):\n",
    "        filename = f\"cases/case_base_temporary_{agent_id}_{eps}.json\"\n",
    "        case_base_data = []\n",
    "        for case in self.temporary_case_base[agent_id]:\n",
    "\n",
    "            print (f\"case - problem: {case.problem}\")\n",
    "            print (f\"case - solution: {case.solution}\")\n",
    "            print (f\"case - reward: {case.reward}\")\n",
    "            print (f\"case - trust_value: {case.trust_value}\")\n",
    "\n",
    "\n",
    "            problem = case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem\n",
    "            \n",
    "            precision = 4  # Adjust the precision as needed\n",
    "\n",
    "            if torch.is_tensor(case.solution):\n",
    "                solution = case.solution.cpu().numpy().tolist() if case.solution.numel() > 1 else case.solution.item()\n",
    "                solution = [round(val, precision) for val in solution] if isinstance(solution, list) else round(solution, precision)\n",
    "            else:\n",
    "                solution = round(float(case.solution), precision)\n",
    "\n",
    "            \n",
    "            if torch.is_tensor(case.reward):\n",
    "                reward = case.reward.tolist() if case.reward.numel() > 1 else float(case.reward.item())\n",
    "            else:\n",
    "                reward = float(case.reward)\n",
    "            \n",
    "            if torch.is_tensor(case.trust_value):\n",
    "                trust_value = case.trust_value.tolist() if case.trust_value.numel() > 1 else float(case.trust_value.item())\n",
    "            else:\n",
    "                trust_value = float(case.trust_value)\n",
    "            \n",
    "            print (f\"case - problem then: {problem}\")\n",
    "            print (f\"case - solution then: {solution}\")\n",
    "            print (f\"case - reward then: {reward}\")\n",
    "            print (f\"case - trust_value then: {trust_value}\")\n",
    "\n",
    "\n",
    "\n",
    "            case_base_data.append({\n",
    "                \"problem\": problem,\n",
    "                \"solution\": solution,\n",
    "                \"reward\": reward,\n",
    "                \"trust_value\": trust_value\n",
    "            })\n",
    "        \n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "        print(\"Temporary case base saved successfully.\")\n",
    "\n",
    "        \n",
    "    def load_case_base(self, agent_id):\n",
    "        filename = f\"cases/case_base_{agent_id}.json\"\n",
    "        try:\n",
    "            with open(filename, 'r') as file:\n",
    "                case_base_data = json.load(file)\n",
    "            self.case_base[agent_id] = [Case(problem=np.array(case[\"problem\"]) if isinstance(case[\"problem\"], list) else case[\"problem\"],\n",
    "                                            solution=case[\"solution\"],reward=case[\"reward\"],\n",
    "                                            trust_value=case[\"trust_value\"]) for case in case_base_data]\n",
    "        except FileNotFoundError:\n",
    "            self.case_base[agent_id] = []\n",
    "\n",
    "    \n",
    "    def generate_gif(self, scenario_name):\n",
    "        fps = 25\n",
    "        clip = ImageSequenceClip(self.frame_list, fps=fps)\n",
    "        clip.write_gif(f'{scenario_name}.gif', fps=fps)\n",
    "        return HTML(f'<img src=\"{scenario_name}.gif\">')\n",
    "\n",
    "    def plot_action_distribution(self):\n",
    "        num_agents = len(self.action_counts)\n",
    "\n",
    "        for agent_id, counts in self.action_counts.items():\n",
    "            unique_actions, action_counts = np.unique(list(counts.values()), return_counts=True)\n",
    "            action_dict = dict(zip(unique_actions, action_counts))\n",
    "            plt.bar(action_dict.keys(), action_dict.values(), label=f'Agent {agent_id}', alpha=0.7)\n",
    "\n",
    "        plt.xlabel('Action')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Action Distribution for Each Agent')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_rewards_history(self):\n",
    "        num_agents = len(self.agent_rewards_history)\n",
    "\n",
    "        for agent_id, rewards in self.agent_rewards_history.items():\n",
    "            plt.plot(range(1, self.num_episodes + 1), rewards, label=f'Agent {agent_id}')\n",
    "\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Total Reward per Episode for Each Agent')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def run_vmas_env(self):\n",
    "        scenario_name = self.scenario if isinstance(self.scenario, str) else self.scenario.__class__.__name__\n",
    "\n",
    "        env = make_env(\n",
    "            scenario=self.scenario,\n",
    "            num_envs=self.num_envs,\n",
    "            device=self.device,\n",
    "            continuous_actions=self.continuous_actions,\n",
    "            **self.kwargs\n",
    "        )\n",
    "        \n",
    "        print (f\"--- device used: {self.device}\")\n",
    "\n",
    "        states = env.reset()\n",
    "        # print(f\"states first agent after reset: {states}\")\n",
    "        # print(f\"states first agent shape after reset: {states[0].shape}\")\n",
    "        first_agent_state = states[0][0:6]\n",
    "        state_dim = first_agent_state.shape[0]\n",
    "        action_dim = len(env.action_space)\n",
    "        # print(f\"states after reset: {states}\")\n",
    "        # print(f\"state dim: {state_dim}\")\n",
    "        # print(f\"action dim: {action_dim}\")\n",
    "        \n",
    "        for agent_id, agent in enumerate(env.agents):\n",
    "            self.problem_solver_agents.append(ProblemSolver(env, agent_id, state_dim, action_dim, self.device, communication_weight=0.5))\n",
    "\n",
    "        init_time = time.time()\n",
    "        total_steps = 0\n",
    "        is_episode_success = False\n",
    "\n",
    "        for episode in range(self.num_episodes):\n",
    "            # print(f\"Episode {episode}\")\n",
    "            obs_cont = env.reset()\n",
    "            # print(f\"observation continous: {obs_cont}\")\n",
    "\n",
    "            dones = torch.tensor([False] * self.num_envs, device=self.device)\n",
    "            step = 0\n",
    "            \n",
    "            episode_rewards = {i: 0 for i in range(len(self.problem_solver_agents))}\n",
    "            episode_dones_counts = {i: 0 for i in range(len(env.agents))}  # Track dones counts for each agent\n",
    "           \n",
    "            \n",
    "            self.temporary_case_base = {i: [] for i in range(len(env.agents))}\n",
    "            \n",
    "            while not torch.all(dones).item() and step < self.max_steps_per_episode:\n",
    "                step += 1\n",
    "                total_steps += 1\n",
    "                print(f\"Step {step} of Episode {episode}\")\n",
    "\n",
    "                actions = []\n",
    "               \n",
    "\n",
    "                for agent_id, agent in enumerate(env.agents):\n",
    "                    # print(f\"agent {agent_id}\")\n",
    "                    # print(f\"agent {agent}\")\n",
    "                    if self.obs_discrete:\n",
    "                        obs_qualitative = self._get_qualitative_position(env, obs_cont[agent_id])\n",
    "                        \n",
    "                    # print(f\"observation continuous agent{agent_id} = {obs_cont[agent_id]}\")\n",
    "                    # print(f\"observation qualitative agent{agent_id} = {obs_qualitative}\")\n",
    "\n",
    "                    action = self._get_action(agent, env, agent_id, obs_qualitative, obs_cont[agent_id], self.device)\n",
    "                    \n",
    "                    actions.append(action)\n",
    "\n",
    "\n",
    "                print(f\"action taken: {actions}\")\n",
    "                # print(f\"actions agent {agent_id} shapes: {actions[agent_id].shape}\")\n",
    "                # print(f\"actions size from environment: {env.get_agent_action_size(agent)}\")\n",
    "\n",
    "                next_obs_cont, rewards, dones, info = env.step(actions)\n",
    "                # print(f\"next obs first agents = {next_obs_cont[0]}\")\n",
    "                # print(f\"next obs first agents shape = {next_obs_cont[0].shape}\")\n",
    "                # print(f\"reward all agents = {rewards}\")\n",
    "                \n",
    "\n",
    "                for agent_id, agent in enumerate(env.agents):\n",
    "                    if self.obs_discrete:\n",
    "                        obs_qualitative = self._get_qualitative_position(env, obs_cont[agent_id])\n",
    "                        discrete_next_obs = self._get_qualitative_position(env, next_obs_cont[agent_id])\n",
    "\n",
    "                    problem = obs_qualitative\n",
    "                    physical_action = actions[agent_id][0,0,0:2]\n",
    "                    reward = rewards[agent_id]\n",
    "                    done = dones[0][agent_id]\n",
    "\n",
    "                    print(f\"physical action stored in CB: {physical_action}\")\n",
    "\n",
    "                    new_case = Case(problem, physical_action, reward)\n",
    "                    Case.reuse(new_case, self.temporary_case_base[agent_id])\n",
    "\n",
    "                    # print(f\"observation before pass: {obs_cont[agent_id]}\")\n",
    "                    # print(f\"physical action before pass: {physical_action}\")\n",
    "                    # print(f\"reward before pass: {reward}\")\n",
    "                    # print(f\"next observation before pass: {next_obs_cont[agent_id]}\")\n",
    "                    # print(f\"dones before pass: {dones}\")\n",
    "                    # print(f\"done before pass: {done}\")\n",
    "\n",
    "                    if self.continuous_actions:\n",
    "\n",
    "                        self.problem_solver_agents[agent_id].update_action_continuous( \n",
    "                            obs_cont[agent_id], physical_action, reward, next_obs_cont[agent_id], done\n",
    "                        )\n",
    "                    else:\n",
    "                        self.problem_solver_agents[agent_id].update_action_discrete(agent, env, agent_id,\n",
    "                             obs_cont[agent_id], physical_action, rewards[agent_id].item(), next_obs_cont[agent_id]\n",
    "                        )\n",
    "                    # Accumulate rewards for each agent within the episode\n",
    "                    episode_rewards[agent_id] += rewards[agent_id].item()\n",
    "                    \n",
    "                    # print (f\"dones: {dones[0]}\")\n",
    "                    if (dones[0][agent_id]):  # Increment individual agent's done count\n",
    "                        episode_dones_counts[agent_id] += 1\n",
    "                \n",
    "                # done = dones\n",
    "                obs_cont = next_obs_cont\n",
    "\n",
    "                                \n",
    "                # print(f\"dones status for all agents = {done}\")\n",
    "                # print(\"--------------------\")\n",
    "\n",
    "                if self.render:\n",
    "                    frame = env.render(\n",
    "                        mode=\"rgb_array\",\n",
    "                        agent_index_focus=None,\n",
    "                    )\n",
    "                    self.frame_list.append(frame)\n",
    "\n",
    "                # print(\"-----------------\")\n",
    "            \n",
    "            # Update rewards history after each episode\n",
    "            for agent_id, total_reward in episode_rewards.items():\n",
    "                self.agent_rewards_history[agent_id].append(total_reward)\n",
    "\n",
    "\n",
    "            # Update success based on individual agent's done status\n",
    "            print(f\"Success hit of each agent at the end of episode {episode}:\")\n",
    "            for agent_id in range(len(env.agents)):\n",
    "                # print(f\"done status for agent {agent_id}: {dones[0][agent_id]}\")\n",
    "                # print(f\"dones status for agent {agent_id} v2: {dones[0, agent_id]}\")\n",
    "                # self.successful_episodes_individual[agent_id].append(episode_dones_counts[agent_id])  # Calculate success rate for the agent\n",
    "                print(f\"Agent {agent_id}: {episode_dones_counts[agent_id]}%\")\n",
    "            \n",
    "            # Update success based on all agents' done status\n",
    "            if torch.all(dones).item():\n",
    "                self.successful_episodes_all_agents += 1\n",
    "                print(f\"Episode {episode}: Success\")\n",
    "            else:\n",
    "                print(f\"Episode {episode}: Failed\")\n",
    "            \n",
    "            for agent_id in range(len(env.agents)):\n",
    "                \n",
    "                Case.revise(self.case_base[agent_id], self.temporary_case_base[agent_id], dones[0][agent_id])\n",
    "                self.case_base[agent_id] = Case.retain(\n",
    "                    self.case_base[agent_id], self.temporary_case_base[agent_id], dones[0][agent_id]\n",
    "                )\n",
    "\n",
    "                # for case in self.temporary_case_base[agent_id]:\n",
    "                #     print(f\"Step {step} -- Problem Stored in Temp CB: {case.problem}, Solution Stored in Temp CB: {case.solution}\")\n",
    "\n",
    "                # for case in self.case_base[agent_id]:\n",
    "                #     print(f\"Step {step} -- Problem Stored in CB: {case.problem}, Solution Stored in CB: {case.solution}\")\n",
    "\n",
    "                self.save_case_base_temporary_eps(agent_id, episode)  # Save temporary case base after each episode\n",
    "                self.save_case_base_eps(agent_id, episode)  # Save case base after each episode\n",
    "            \n",
    "            torch.cuda.empty_cache()  # Free up unused memory\n",
    "\n",
    "        print(\"------------------------------------------\")\n",
    "        \n",
    "        for agent_id, agent in enumerate(env.agents):\n",
    "            self.save_case_base_temporary(agent_id)  # Save temporary case base after training\n",
    "            self.save_case_base(agent_id)  # Save case base after training\n",
    "\n",
    "        # Print final success percentages for each agent-0.9, 0.7, 0.0, 0.0, 0.0, -0.9)\n",
    "        \n",
    "        # Print overall success percentage for all agents\n",
    "        overall_success_percentage = self.successful_episodes_all_agents / self.num_episodes * 100\n",
    "        print(f\"Overall success percentage for all agents = {overall_success_percentage}%\")\n",
    "\n",
    "\n",
    "\n",
    "        total_time = time.time() - init_time\n",
    "        print(\n",
    "            f\"It took: {total_time}s for {total_steps} steps across {self.num_episodes} episodes of {self.num_envs} parallel environments on device {self.device} \"\n",
    "            f\"for {scenario_name} scenario.\"\n",
    "        )\n",
    "\n",
    "        # success_percentage = (self.successful_episodes / self.num_episodes) * 100\n",
    "        # print(f\"Percentage of successful episodes: {success_percentage}%\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scenario_name = \"navigation_comm\"\n",
    "    use_cuda = True\n",
    "\n",
    "    env_runner = QCBRLVmasRunner( \n",
    "        render=True,\n",
    "        num_envs=1,\n",
    "        num_episodes=2,\n",
    "        max_steps_per_episode=3,\n",
    "        device = torch.device(\"cuda\" if use_cuda else \"cpu\"),\n",
    "        scenario=scenario_name,\n",
    "        continuous_actions=True,\n",
    "        random_action=False,\n",
    "        n_agents=2,\n",
    "        obs_discrete=True,\n",
    "        agents_with_same_goal=2,\n",
    "        collisions=False,\n",
    "        shared_rew=False,\n",
    "    )\n",
    "\n",
    "    env_runner.run_vmas_env()\n",
    "    # for agent in env_runner.problem_solver_agents:\n",
    "    #     agent.print_q_table()\n",
    "    env_runner.plot_rewards_history()\n",
    "\n",
    "    ipython_display(env_runner.generate_gif(scenario_name))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
