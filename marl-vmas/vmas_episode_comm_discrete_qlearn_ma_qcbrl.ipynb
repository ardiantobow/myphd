{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n",
      "Step 1 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.0000],\n",
      "       device='cuda:0')\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.0000],\n",
      "       device='cuda:0')\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9000,  0.8925,  0.0000, -0.1000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8925,  0.9075, -0.1000,  0.1000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 2 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9000,  0.8925,  0.0000, -0.1000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8925,  0.9075, -0.1000,  0.1000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9000,  0.8850,  0.0000, -0.0750,  0.0000, -0.8000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8850,  0.9150, -0.0750,  0.0750,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 3 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9000,  0.8850,  0.0000, -0.0750,  0.0000, -0.8000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8850,  0.9150, -0.0750,  0.0750,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9075,  0.8794, -0.1000, -0.0563,  0.0000, -0.8000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8794,  0.9131, -0.0563, -0.0437,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 4 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9075,  0.8794, -0.1000, -0.0563,  0.0000, -0.8000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8794,  0.9131, -0.0563, -0.0437,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9150,  0.8752, -0.0750, -0.0422,  0.0000, -0.8000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8752,  0.9023, -0.0422, -0.1328,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 5 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9150,  0.8752, -0.0750, -0.0422,  0.0000, -0.8000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8752,  0.9023, -0.0422, -0.1328,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9206,  0.8720, -0.0563, -0.0316,  0.0000, -0.8000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8720,  0.8849, -0.0316, -0.1996,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 6 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9206,  0.8720, -0.0563, -0.0316,  0.0000, -0.8000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8720,  0.8849, -0.0316, -0.1996,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9248,  0.8696, -0.0422, -0.0237,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8696,  0.8699, -0.0237, -0.1497,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 7 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9248,  0.8696, -0.0422, -0.0237,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8696,  0.8699, -0.0237, -0.1497,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9280,  0.8678, -0.0316, -0.0178,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8678,  0.8587, -0.0178, -0.1123,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 8 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9280,  0.8678, -0.0316, -0.0178,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8678,  0.8587, -0.0178, -0.1123,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9304,  0.8665, -0.0237, -0.0133,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8665,  0.8503, -0.0133, -0.0842,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 9 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9304,  0.8665, -0.0237, -0.0133,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8665,  0.8503, -0.0133, -0.0842,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9322,  0.8655, -0.0178, -0.0100,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8655,  0.8439, -0.0100, -0.0632,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 10 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9322,  0.8655, -0.0178, -0.0100,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8655,  0.8439, -0.0100, -0.0632,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9335,  0.8648, -0.0133, -0.0075,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8648,  0.8392, -0.0075, -0.0474,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 11 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9335,  0.8648, -0.0133, -0.0075,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8648,  0.8392, -0.0075, -0.0474,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9345,  0.8642, -0.0100, -0.0056,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8642,  0.8357, -0.0056, -0.0355,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 12 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9345,  0.8642, -0.0100, -0.0056,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8642,  0.8357, -0.0056, -0.0355,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9352,  0.8638, -0.0075, -0.0042,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8638,  0.8330, -0.0042, -0.0266,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 13 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9352,  0.8638, -0.0075, -0.0042,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8638,  0.8330, -0.0042, -0.0266,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9358,  0.8635, -0.0056, -0.0032,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8635,  0.8310, -0.0032, -0.0200,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 14 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9358,  0.8635, -0.0056, -0.0032,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8635,  0.8310, -0.0032, -0.0200,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9362,  0.8632, -0.0042, -0.0024,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8632,  0.8295, -0.0024, -0.0150,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 15 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9362,  0.8632, -0.0042, -0.0024,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8632,  0.8295, -0.0024, -0.0150,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9365,  0.8630, -0.0032, -0.0018,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8630,  0.8284, -0.0018, -0.0112,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 16 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9365,  0.8630, -0.0032, -0.0018,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8630,  0.8284, -0.0018, -0.0112,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  7.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.3679e-01,  8.6290e-01, -2.3757e-03, -1.3363e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  7.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8704,  0.8200,  0.0987, -0.1084,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 17 of Episode 0\n",
      "observation continuous agent0 = tensor([-9.3679e-01,  8.6290e-01, -2.3757e-03, -1.3363e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  7.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8704,  0.8200,  0.0987, -0.1084,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9370,  0.8628, -0.0018, -0.0010,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8778,  0.8119,  0.0740, -0.0813,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 18 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9370,  0.8628, -0.0018, -0.0010,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8778,  0.8119,  0.0740, -0.0813,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.3710e-01,  8.6273e-01, -1.3363e-03, -7.5169e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8834,  0.8058,  0.0555, -0.0610,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 19 of Episode 0\n",
      "observation continuous agent0 = tensor([-9.3710e-01,  8.6273e-01, -1.3363e-03, -7.5169e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8834,  0.8058,  0.0555, -0.0610,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.3720e-01,  8.6267e-01, -1.0023e-03, -5.6377e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8875,  0.8012,  0.0416, -0.0457,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 20 of Episode 0\n",
      "observation continuous agent0 = tensor([-9.3720e-01,  8.6267e-01, -1.0023e-03, -5.6377e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8875,  0.8012,  0.0416, -0.0457,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.3727e-01,  8.6263e-01, -7.5169e-04, -4.2283e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8906,  0.7978,  0.0312, -0.0343,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 21 of Episode 0\n",
      "observation continuous agent0 = tensor([-9.3727e-01,  8.6263e-01, -7.5169e-04, -4.2283e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8906,  0.7978,  0.0312, -0.0343,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.3733e-01,  8.6259e-01, -5.6377e-04, -3.1712e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8930,  0.7952,  0.0234, -0.0257,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 22 of Episode 0\n",
      "observation continuous agent0 = tensor([-9.3733e-01,  8.6259e-01, -5.6377e-04, -3.1712e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8930,  0.7952,  0.0234, -0.0257,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.3737e-01,  8.6257e-01, -4.2283e-04, -2.3784e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8872,  0.7858, -0.0824, -0.1193,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 23 of Episode 0\n",
      "observation continuous agent0 = tensor([-9.3737e-01,  8.6257e-01, -4.2283e-04, -2.3784e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8872,  0.7858, -0.0824, -0.1193,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.3741e-01,  8.6255e-01, -3.1712e-04, -1.7838e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8735,  0.7693, -0.1618, -0.1895,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 24 of Episode 0\n",
      "observation continuous agent0 = tensor([-9.3741e-01,  8.6255e-01, -3.1712e-04, -1.7838e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8735,  0.7693, -0.1618, -0.1895,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.3743e-01,  8.6254e-01, -2.3784e-04, -1.3379e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8539,  0.7476, -0.2214, -0.2421,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 25 of Episode 0\n",
      "observation continuous agent0 = tensor([-9.3743e-01,  8.6254e-01, -2.3784e-04, -1.3379e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8539,  0.7476, -0.2214, -0.2421,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.3745e-01,  8.6253e-01, -1.7838e-04, -1.0034e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8298,  0.7220, -0.2660, -0.2816,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 26 of Episode 0\n",
      "observation continuous agent0 = tensor([-9.3745e-01,  8.6253e-01, -1.7838e-04, -1.0034e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8298,  0.7220, -0.2660, -0.2816,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.3746e-01,  8.6252e-01, -1.3379e-04, -7.5254e-05,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8024,  0.6934, -0.2995, -0.3112,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 27 of Episode 0\n",
      "observation continuous agent0 = tensor([-9.3746e-01,  8.6252e-01, -1.3379e-04, -7.5254e-05,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8024,  0.6934, -0.2995, -0.3112,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.3747e-01,  8.6252e-01, -1.0034e-04, -5.6441e-05,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  6.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7799,  0.6700, -0.2246, -0.2334,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 28 of Episode 0\n",
      "observation continuous agent0 = tensor([-9.3747e-01,  8.6252e-01, -1.0034e-04, -5.6441e-05,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  6.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7799,  0.6700, -0.2246, -0.2334,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.3748e-01,  8.6251e-01, -7.5254e-05, -4.2331e-05,  0.0000e+00,\n",
      "        -8.0000e-01,  7.0000e-01,  6.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7630,  0.6525, -0.1685, -0.1750,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 29 of Episode 0\n",
      "observation continuous agent0 = tensor([-9.3748e-01,  8.6251e-01, -7.5254e-05, -4.2331e-05,  0.0000e+00,\n",
      "        -8.0000e-01,  7.0000e-01,  6.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7630,  0.6525, -0.1685, -0.1750,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9300,  0.8700,  0.0999,  0.1000,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7504,  0.6394, -0.1264, -0.1313,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 30 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9300,  0.8700,  0.0999,  0.1000,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7504,  0.6394, -0.1264, -0.1313,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9150,  0.8775,  0.1750,  0.0750,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7409,  0.6295, -0.0948, -0.0985,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 31 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9150,  0.8775,  0.1750,  0.0750,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 2.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  2.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7409,  0.6295, -0.0948, -0.0985,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9019,  0.8906,  0.1312,  0.1562,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7338,  0.6222, -0.0711, -0.0738,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 32 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.9019,  0.8906,  0.1312,  0.1562,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7338,  0.6222, -0.0711, -0.0738,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8920,  0.9023,  0.0984,  0.1172,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7285,  0.6166, -0.0533, -0.0554,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 33 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8920,  0.9023,  0.0984,  0.1172,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7285,  0.6166, -0.0533, -0.0554,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8846,  0.9111,  0.0738,  0.0879,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7245,  0.6125, -0.0400, -0.0415,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 34 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8846,  0.9111,  0.0738,  0.0879,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7245,  0.6125, -0.0400, -0.0415,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  7.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8716,  0.9252,  0.1554,  0.1659,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7290,  0.6018,  0.0700, -0.1312,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 35 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8716,  0.9252,  0.1554,  0.1659,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7290,  0.6018,  0.0700, -0.1312,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8600,  0.9377,  0.1165,  0.1244,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7342,  0.5920,  0.0525, -0.0984,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 36 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8600,  0.9377,  0.1165,  0.1244,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7342,  0.5920,  0.0525, -0.0984,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8512,  0.9470,  0.0874,  0.0933,  0.0000, -0.8000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7382,  0.5846,  0.0394, -0.0738,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 37 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8512,  0.9470,  0.0874,  0.0933,  0.0000, -0.8000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7382,  0.5846,  0.0394, -0.0738,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8447,  0.9540,  0.0655,  0.0700,  0.0000, -0.8000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7411,  0.5791,  0.0295, -0.0553,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 38 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8447,  0.9540,  0.0655,  0.0700,  0.0000, -0.8000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7411,  0.5791,  0.0295, -0.0553,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8397,  0.9593,  0.0492,  0.0525,  0.0000, -0.8000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7434,  0.5749,  0.0222, -0.0415,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 39 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8397,  0.9593,  0.0492,  0.0525,  0.0000, -0.8000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7434,  0.5749,  0.0222, -0.0415,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8361,  0.9632,  0.0369,  0.0394,  0.0000, -0.8000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7375,  0.5643, -0.0834, -0.1311,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 40 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8361,  0.9632,  0.0369,  0.0394,  0.0000, -0.8000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7375,  0.5643, -0.0834, -0.1311,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8333,  0.9661,  0.0277,  0.0295,  0.0000, -0.8000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7238,  0.5470, -0.1625, -0.1983,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 41 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8333,  0.9661,  0.0277,  0.0295,  0.0000, -0.8000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7238,  0.5470, -0.1625, -0.1983,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8312,  0.9684,  0.0207,  0.0221,  0.0000, -0.8000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7041,  0.5246, -0.2219, -0.2488,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 42 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8312,  0.9684,  0.0207,  0.0221,  0.0000, -0.8000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7041,  0.5246, -0.2219, -0.2488,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8297,  0.9700,  0.0156,  0.0166,  0.0000, -0.8000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6799,  0.4985, -0.2664, -0.2866,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 43 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8297,  0.9700,  0.0156,  0.0166,  0.0000, -0.8000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6799,  0.4985, -0.2664, -0.2866,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8285,  0.9713,  0.0117,  0.0125,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6599,  0.4770, -0.1998, -0.2149,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 44 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8285,  0.9713,  0.0117,  0.0125,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6599,  0.4770, -0.1998, -0.2149,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8276,  0.9722,  0.0087,  0.0093,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6450,  0.4609, -0.1499, -0.1612,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 45 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8276,  0.9722,  0.0087,  0.0093,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6450,  0.4609, -0.1499, -0.1612,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8270,  0.9729,  0.0066,  0.0070,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6337,  0.4488, -0.1124, -0.1209,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 46 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8270,  0.9729,  0.0066,  0.0070,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6337,  0.4488, -0.1124, -0.1209,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.2648e-01,  9.7342e-01,  4.9212e-03,  5.2554e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  4.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  6.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6328,  0.4397,  0.0157, -0.0907,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 47 of Episode 0\n",
      "observation continuous agent0 = tensor([-8.2648e-01,  9.7342e-01,  4.9212e-03,  5.2554e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  4.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  6.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6328,  0.4397,  0.0157, -0.0907,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8261,  0.9663,  0.0037, -0.0961,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6340,  0.4329,  0.0118, -0.0680,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 48 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8261,  0.9663,  0.0037, -0.0961,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6340,  0.4329,  0.0118, -0.0680,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.2583e-01,  9.5911e-01,  2.7682e-03, -7.2044e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  4.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6424,  0.4353,  0.1088,  0.0490,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 49 of Episode 0\n",
      "observation continuous agent0 = tensor([-8.2583e-01,  9.5911e-01,  2.7682e-03, -7.2044e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  4.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6424,  0.4353,  0.1088,  0.0490,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8256,  0.9537,  0.0021, -0.0540,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6505,  0.4390,  0.0816,  0.0367,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 50 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8256,  0.9537,  0.0021, -0.0540,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6505,  0.4390,  0.0816,  0.0367,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8255,  0.9497,  0.0016, -0.0405,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6566,  0.4417,  0.0612,  0.0276,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 51 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8255,  0.9497,  0.0016, -0.0405,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6566,  0.4417,  0.0612,  0.0276,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8254,  0.9466,  0.0012, -0.0304,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6612,  0.4438,  0.0459,  0.0207,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 52 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8254,  0.9466,  0.0012, -0.0304,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6612,  0.4438,  0.0459,  0.0207,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.2526e-01,  9.4434e-01,  8.7586e-04, -2.2795e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  4.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6647,  0.4453,  0.0344,  0.0155,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 53 of Episode 0\n",
      "observation continuous agent0 = tensor([-8.2526e-01,  9.4434e-01,  8.7586e-04, -2.2795e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  4.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6647,  0.4453,  0.0344,  0.0155,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.2520e-01,  9.4263e-01,  6.5690e-04, -1.7096e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  4.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6673,  0.4465,  0.0258,  0.0116,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 54 of Episode 0\n",
      "observation continuous agent0 = tensor([-8.2520e-01,  9.4263e-01,  6.5690e-04, -1.7096e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  4.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6673,  0.4465,  0.0258,  0.0116,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.2515e-01,  9.4135e-01,  4.9267e-04, -1.2822e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  4.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6692,  0.4474,  0.0194,  0.0087,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 55 of Episode 0\n",
      "observation continuous agent0 = tensor([-8.2515e-01,  9.4135e-01,  4.9267e-04, -1.2822e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  4.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6692,  0.4474,  0.0194,  0.0087,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.2511e-01,  9.4038e-01,  3.6950e-04, -9.6167e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  4.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  6.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6781,  0.4480,  0.1145,  0.0065,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 56 of Episode 0\n",
      "observation continuous agent0 = tensor([-8.2511e-01,  9.4038e-01,  3.6950e-04, -9.6167e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  4.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  6.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6781,  0.4480,  0.1145,  0.0065,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.2508e-01,  9.3966e-01,  2.7713e-04, -7.2125e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  4.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  1.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6867,  0.4410,  0.0859, -0.0951,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 57 of Episode 0\n",
      "observation continuous agent0 = tensor([-8.2508e-01,  9.3966e-01,  2.7713e-04, -7.2125e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  4.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  1.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6867,  0.4410,  0.0859, -0.0951,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 2.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  2.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8326,  0.9466, -0.0998,  0.0946,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6932,  0.4414,  0.0644,  0.0287,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 58 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8326,  0.9466, -0.0998,  0.0946,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6932,  0.4414,  0.0644,  0.0287,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8400,  0.9537, -0.0748,  0.0709,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6980,  0.4435,  0.0483,  0.0215,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 59 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8400,  0.9537, -0.0748,  0.0709,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6980,  0.4435,  0.0483,  0.0215,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8457,  0.9590, -0.0561,  0.0532,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7016,  0.4452,  0.0362,  0.0161,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 60 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8457,  0.9590, -0.0561,  0.0532,  0.0000, -0.8000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7016,  0.4452,  0.0362,  0.0161,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8424,  0.9705,  0.0579,  0.1399,  0.0000, -0.8000,  0.7000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7043,  0.4464,  0.0272,  0.0121,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 61 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8424,  0.9705,  0.0579,  0.1399,  0.0000, -0.8000,  0.7000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7043,  0.4464,  0.0272,  0.0121,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8380,  0.9810,  0.0434,  0.1049,  0.0000, -0.8000,  0.7000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7064,  0.4398,  0.0204, -0.0909,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 62 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8380,  0.9810,  0.0434,  0.1049,  0.0000, -0.8000,  0.7000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7064,  0.4398,  0.0204, -0.0909,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8348,  0.9889,  0.0326,  0.0787,  0.0000, -0.8000,  0.7000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7079,  0.4255,  0.0153, -0.1682,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 63 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8348,  0.9889,  0.0326,  0.0787,  0.0000, -0.8000,  0.7000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7079,  0.4255,  0.0153, -0.1682,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8323,  0.9948,  0.0244,  0.0590,  0.0000, -0.8000,  0.7000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7091,  0.4053,  0.0115, -0.2261,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 64 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8323,  0.9948,  0.0244,  0.0590,  0.0000, -0.8000,  0.7000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7091,  0.4053,  0.0115, -0.2261,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8305,  0.9992,  0.0183,  0.0443,  0.0000, -0.8000,  0.7000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7099,  0.3809,  0.0086, -0.2696,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 65 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8305,  0.9992,  0.0183,  0.0443,  0.0000, -0.8000,  0.7000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7099,  0.3809,  0.0086, -0.2696,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8291,  1.0025,  0.0137,  0.0332,  0.0000, -0.8000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7106,  0.3607,  0.0064, -0.2022,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 66 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8291,  1.0025,  0.0137,  0.0332,  0.0000, -0.8000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7106,  0.3607,  0.0064, -0.2022,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8206,  1.0050,  0.1103,  0.0249,  0.0000, -0.8000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.1105e-01,  3.4550e-01,  4.8371e-03, -1.5166e-01,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  6.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 67 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8206,  1.0050,  0.1103,  0.0249,  0.0000, -0.8000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.1105e-01,  3.4550e-01,  4.8371e-03, -1.5166e-01,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  6.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8123,  1.0069,  0.0827,  0.0187,  0.0000, -0.8000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7114,  0.3341,  0.0036, -0.1137,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 68 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8123,  1.0069,  0.0827,  0.0187,  0.0000, -0.8000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7114,  0.3341,  0.0036, -0.1137,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8061,  1.0083,  0.0620,  0.0140,  0.0000, -0.8000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7117,  0.3256,  0.0027, -0.0853,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 69 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8061,  1.0083,  0.0620,  0.0140,  0.0000, -0.8000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7117,  0.3256,  0.0027, -0.0853,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8015,  1.0093,  0.0465,  0.0105,  0.0000, -0.8000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7119,  0.3192,  0.0020, -0.0640,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 70 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8015,  1.0093,  0.0465,  0.0105,  0.0000, -0.8000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7119,  0.3192,  0.0020, -0.0640,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8055,  1.0101, -0.0651,  0.0079,  0.0000, -0.8000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7045,  0.3144, -0.0985, -0.0480,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 71 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8055,  1.0101, -0.0651,  0.0079,  0.0000, -0.8000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7045,  0.3144, -0.0985, -0.0480,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8104,  1.0107, -0.0488,  0.0059,  0.0000, -0.8000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6972,  0.3108, -0.0739, -0.0360,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 72 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8104,  1.0107, -0.0488,  0.0059,  0.0000, -0.8000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6972,  0.3108, -0.0739, -0.0360,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8140,  1.0112, -0.0366,  0.0044,  0.0000, -0.8000,  0.6000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6916,  0.3081, -0.0554, -0.0270,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 73 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8140,  1.0112, -0.0366,  0.0044,  0.0000, -0.8000,  0.6000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6916,  0.3081, -0.0554, -0.0270,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8168,  1.0115, -0.0275,  0.0033,  0.0000, -0.8000,  0.6000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6875,  0.3061, -0.0415, -0.0202,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 74 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8168,  1.0115, -0.0275,  0.0033,  0.0000, -0.8000,  0.6000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6875,  0.3061, -0.0415, -0.0202,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8188,  1.0118, -0.0206,  0.0025,  0.0000, -0.8000,  0.6000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6843,  0.3046, -0.0312, -0.0152,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 75 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8188,  1.0118, -0.0206,  0.0025,  0.0000, -0.8000,  0.6000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6843,  0.3046, -0.0312, -0.0152,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.2037e-01,  1.0119e+00, -1.5448e-02,  1.8696e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  3.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6745,  0.2959, -0.1234, -0.1114,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 76 of Episode 0\n",
      "observation continuous agent0 = tensor([-8.2037e-01,  1.0119e+00, -1.5448e-02,  1.8696e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  3.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6745,  0.2959, -0.1234, -0.1114,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.2152e-01,  1.0121e+00, -1.1586e-02,  1.4022e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6578,  0.2876, -0.1925, -0.0835,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 77 of Episode 0\n",
      "observation continuous agent0 = tensor([-8.2152e-01,  1.0121e+00, -1.1586e-02,  1.4022e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6578,  0.2876, -0.1925, -0.0835,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.2239e-01,  1.0122e+00, -8.6896e-03,  1.0517e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6358,  0.2813, -0.2444, -0.0627,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 78 of Episode 0\n",
      "observation continuous agent0 = tensor([-8.2239e-01,  1.0122e+00, -8.6896e-03,  1.0517e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6358,  0.2813, -0.2444, -0.0627,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.2305e-01,  1.0123e+00, -6.5172e-03,  7.8875e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6100,  0.2766, -0.2833, -0.0470,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 79 of Episode 0\n",
      "observation continuous agent0 = tensor([-8.2305e-01,  1.0123e+00, -6.5172e-03,  7.8875e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6100,  0.2766, -0.2833, -0.0470,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.2353e-01,  1.0123e+00, -4.8879e-03,  5.9156e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.5812,  0.2731, -0.3125, -0.0352,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 80 of Episode 0\n",
      "observation continuous agent0 = tensor([-8.2353e-01,  1.0123e+00, -4.8879e-03,  5.9156e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  6.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.5812,  0.2731, -0.3125, -0.0352,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.5000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.2390e-01,  1.0124e+00, -3.6659e-03,  4.4367e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  5.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.5578,  0.2704, -0.2344, -0.0264,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 81 of Episode 0\n",
      "observation continuous agent0 = tensor([-8.2390e-01,  1.0124e+00, -3.6659e-03,  4.4367e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  5.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.5578,  0.2704, -0.2344, -0.0264,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.5000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.2418e-01,  1.0124e+00, -2.7495e-03,  3.3275e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  5.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.5402,  0.2684, -0.1758, -0.0198,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 82 of Episode 0\n",
      "observation continuous agent0 = tensor([-8.2418e-01,  1.0124e+00, -2.7495e-03,  3.3275e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  5.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  7.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.5402,  0.2684, -0.1758, -0.0198,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.5000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8169,  1.0049,  0.0979, -0.0998,  0.0000, -0.8000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.5270,  0.2670, -0.1318, -0.0149,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 83 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8169,  1.0049,  0.0979, -0.0998,  0.0000, -0.8000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.5270,  0.2670, -0.1318, -0.0149,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.5000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8095,  0.9974,  0.0735, -0.0748,  0.0000, -0.8000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.5172,  0.2658, -0.0989, -0.0112,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 84 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8095,  0.9974,  0.0735, -0.0748,  0.0000, -0.8000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.5172,  0.2658, -0.0989, -0.0112,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.5000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8040,  0.9918,  0.0551, -0.0561,  0.0000, -0.8000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.5097,  0.2650, -0.0742, -0.0084,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 85 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.8040,  0.9918,  0.0551, -0.0561,  0.0000, -0.8000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.5097,  0.2650, -0.0742, -0.0084,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.5000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7999,  0.9876,  0.0413, -0.0421,  0.0000, -0.8000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.5042,  0.2644, -0.0556, -0.0063,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 86 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.7999,  0.9876,  0.0413, -0.0421,  0.0000, -0.8000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.5042,  0.2644, -0.0556, -0.0063,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.5000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7893,  0.9920,  0.1310,  0.0684,  0.0000, -0.8000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 5.0001e-01,  2.6391e-01, -4.1710e-02, -4.7044e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -8.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 87 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.7893,  0.9920,  0.1310,  0.0684,  0.0000, -0.8000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 5.0001e-01,  2.6391e-01, -4.1710e-02, -4.7044e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -8.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.5000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7795,  0.9971,  0.0982,  0.0513,  0.0000, -0.8000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.4969,  0.2636, -0.0313, -0.0035,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 88 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.7795,  0.9971,  0.0982,  0.0513,  0.0000, -0.8000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.5000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.4969,  0.2636, -0.0313, -0.0035,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7721,  1.0010,  0.0737,  0.0385,  0.0000, -0.8000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.4945,  0.2633, -0.0235, -0.0026,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 89 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.7721,  1.0010,  0.0737,  0.0385,  0.0000, -0.8000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.4945,  0.2633, -0.0235, -0.0026,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7666,  1.0038,  0.0553,  0.0289,  0.0000, -0.8000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.4928,  0.2631, -0.0176, -0.0020,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 90 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.7666,  1.0038,  0.0553,  0.0289,  0.0000, -0.8000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.4928,  0.2631, -0.0176, -0.0020,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7624,  1.0060,  0.0414,  0.0217,  0.0000, -0.8000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.4915,  0.2629, -0.0132, -0.0015,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 91 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.7624,  1.0060,  0.0414,  0.0217,  0.0000, -0.8000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.4915,  0.2629, -0.0132, -0.0015,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7593,  1.0076,  0.0311,  0.0162,  0.0000, -0.8000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.4905,  0.2628, -0.0099, -0.0011,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 92 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.7593,  1.0076,  0.0311,  0.0162,  0.0000, -0.8000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.4905,  0.2628, -0.0099, -0.0011,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7570,  1.0088,  0.0233,  0.0122,  0.0000, -0.8000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 4.8973e-01,  2.6275e-01, -7.4235e-03, -8.3729e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -8.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 93 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.7570,  1.0088,  0.0233,  0.0122,  0.0000, -0.8000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 4.8973e-01,  2.6275e-01, -7.4235e-03, -8.3729e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -8.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7552,  1.0098,  0.0175,  0.0091,  0.0000, -0.8000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.4817,  0.2552, -0.1056, -0.1006,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 94 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.7552,  1.0098,  0.0175,  0.0091,  0.0000, -0.8000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.4817,  0.2552, -0.1056, -0.1006,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7539,  1.0104,  0.0131,  0.0069,  0.0000, -0.8000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.4663,  0.2401, -0.1792, -0.1755,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 95 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.7539,  1.0104,  0.0131,  0.0069,  0.0000, -0.8000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.4663,  0.2401, -0.1792, -0.1755,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7530,  1.0110,  0.0098,  0.0051,  0.0000, -0.8000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.4453,  0.2195, -0.2344, -0.2316,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 96 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.7530,  1.0110,  0.0098,  0.0051,  0.0000, -0.8000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.4453,  0.2195, -0.2344, -0.2316,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-7.5221e-01,  1.0113e+00,  7.3764e-03,  3.8540e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  4.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.4202,  0.1946, -0.2758, -0.2737,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 97 of Episode 0\n",
      "observation continuous agent0 = tensor([-7.5221e-01,  1.0113e+00,  7.3764e-03,  3.8540e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  4.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.4202,  0.1946, -0.2758, -0.2737,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.4000,  0.1000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4000,  0.1000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7517,  1.0116,  0.0055,  0.0029,  0.0000, -0.8000,  0.4000,  0.1000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3996,  0.1741, -0.2068, -0.2053,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 98 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.7517,  1.0116,  0.0055,  0.0029,  0.0000, -0.8000,  0.4000,  0.1000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.4000,  0.1000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3996,  0.1741, -0.2068, -0.2053,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.1000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.1000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7587,  1.0043, -0.0959, -0.0978,  0.0000, -0.8000,  0.3000,  0.1000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3840,  0.1587, -0.1551, -0.1540,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 99 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.7587,  1.0043, -0.0959, -0.0978,  0.0000, -0.8000,  0.3000,  0.1000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.1000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3840,  0.1587, -0.1551, -0.1540,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.1000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.1000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7659,  0.9970, -0.0719, -0.0734,  0.0000, -0.8000,  0.3000,  0.1000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3724,  0.1471, -0.1163, -0.1155,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 100 of Episode 0\n",
      "observation continuous agent0 = tensor([-0.7659,  0.9970, -0.0719, -0.0734,  0.0000, -0.8000,  0.3000,  0.1000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.1000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3724,  0.1471, -0.1163, -0.1155,  0.0000, -0.8000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.1000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.1000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7713,  0.9915, -0.0539, -0.0550,  0.0000, -0.8000,  0.3000,  0.1000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3637,  0.1385, -0.0873, -0.0866,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "done status for agent 0: False\n",
      "done status for agent 1: False\n",
      "Success percentage of each agent at the end of episode 0:\n",
      "Agent 0: 0.0%\n",
      "Agent 1: 0.0%\n",
      "Overall success percentage up to episode 0: 0.0%\n",
      "Episode 1\n",
      "Step 1 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 2 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 3 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9075,  0.9000, -0.1000,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 4 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9075,  0.9000, -0.1000,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 2.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  2.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9150,  0.9000, -0.0750,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9075,  0.0000,  0.1000,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 5 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9150,  0.9000, -0.0750,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9075,  0.0000,  0.1000,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9206,  0.9000, -0.0563,  0.0000,  0.0000, -0.8000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9075,  0.0000, -0.0250,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 6 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9206,  0.9000, -0.0563,  0.0000,  0.0000, -0.8000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9075,  0.0000, -0.0250,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9248,  0.9000, -0.0422,  0.0000,  0.0000, -0.8000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.8981,  0.0000, -0.1188,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 7 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9248,  0.9000, -0.0422,  0.0000,  0.0000, -0.8000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.8981,  0.0000, -0.1188,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  7.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9280,  0.9000, -0.0316,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9075,  0.8817,  0.1000, -0.1891,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 8 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9280,  0.9000, -0.0316,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9075,  0.8817,  0.1000, -0.1891,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9304,  0.9000, -0.0237,  0.0000,  0.0000, -0.8000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9150,  0.8675,  0.0750, -0.1418,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 9 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9304,  0.9000, -0.0237,  0.0000,  0.0000, -0.8000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9150,  0.8675,  0.0750, -0.1418,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9322,  0.9000, -0.0178,  0.0000,  0.0000, -0.8000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9131,  0.8494, -0.0437, -0.2063,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 10 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9322,  0.9000, -0.0178,  0.0000,  0.0000, -0.8000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9131,  0.8494, -0.0437, -0.2063,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9335,  0.9000, -0.0133,  0.0000,  0.0000, -0.8000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9023,  0.8264, -0.1328, -0.2548,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 11 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9335,  0.9000, -0.0133,  0.0000,  0.0000, -0.8000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9023,  0.8264, -0.1328, -0.2548,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9345,  0.9000, -0.0100,  0.0000,  0.0000, -0.8000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8849,  0.7998, -0.1996, -0.2911,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 12 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9345,  0.9000, -0.0100,  0.0000,  0.0000, -0.8000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8849,  0.7998, -0.1996, -0.2911,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9427,  0.9000, -0.1075,  0.0000,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8624,  0.7705, -0.2497, -0.3183,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 13 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9427,  0.9000, -0.1075,  0.0000,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8624,  0.7705, -0.2497, -0.3183,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9508,  0.9000, -0.0806,  0.0000,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8362,  0.7391, -0.2873, -0.3387,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 14 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9508,  0.9000, -0.0806,  0.0000,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8362,  0.7391, -0.2873, -0.3387,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9569,  0.9000, -0.0605,  0.0000,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8071,  0.7062, -0.3155, -0.3540,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 15 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9569,  0.9000, -0.0605,  0.0000,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8071,  0.7062, -0.3155, -0.3540,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9614,  0.9000, -0.0454,  0.0000,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7760,  0.6722, -0.3366, -0.3655,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 16 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9614,  0.9000, -0.0454,  0.0000,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7760,  0.6722, -0.3366, -0.3655,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9648,  0.9000, -0.0340,  0.0000,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7507,  0.6447, -0.2524, -0.2742,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 17 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9648,  0.9000, -0.0340,  0.0000,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7507,  0.6447, -0.2524, -0.2742,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9673,  0.9000, -0.0255,  0.0000,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7318,  0.6242, -0.1893, -0.2056,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 18 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9673,  0.9000, -0.0255,  0.0000,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7318,  0.6242, -0.1893, -0.2056,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9768,  0.8925, -0.1191, -0.1000,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7176,  0.6088, -0.1420, -0.1542,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 19 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9768,  0.8925, -0.1191, -0.1000,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7176,  0.6088, -0.1420, -0.1542,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9857,  0.8850, -0.0894, -0.0750,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7070,  0.5972, -0.1065, -0.1157,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 20 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9857,  0.8850, -0.0894, -0.0750,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7070,  0.5972, -0.1065, -0.1157,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9924,  0.8794, -0.0670, -0.0563,  0.0000, -0.8000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6915,  0.5810, -0.1799, -0.1867,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 21 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9924,  0.8794, -0.0670, -0.0563,  0.0000, -0.8000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6915,  0.5810, -0.1799, -0.1867,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0049,  0.8827, -0.1503,  0.0578,  0.0000, -0.8000,  0.6000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6705,  0.5595, -0.2349, -0.2401,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 22 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0049,  0.8827, -0.1503,  0.0578,  0.0000, -0.8000,  0.6000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6705,  0.5595, -0.2349, -0.2401,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0162,  0.8870, -0.1127,  0.0434,  0.0000, -0.8000,  0.6000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6454,  0.5340, -0.2762, -0.2800,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 23 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0162,  0.8870, -0.1127,  0.0434,  0.0000, -0.8000,  0.6000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6454,  0.5340, -0.2762, -0.2800,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0246,  0.8902, -0.0845,  0.0325,  0.0000, -0.8000,  0.6000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.6171,  0.5205, -0.3071, -0.1100,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 24 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0246,  0.8902, -0.0845,  0.0325,  0.0000, -0.8000,  0.6000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.6171,  0.5205, -0.3071, -0.1100,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.6000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0310,  0.8927, -0.0634,  0.0244,  0.0000, -0.8000,  0.6000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.5866,  0.5048, -0.3304, -0.1825,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 25 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0310,  0.8927, -0.0634,  0.0244,  0.0000, -0.8000,  0.6000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.6000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.5866,  0.5048, -0.3304, -0.1825,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.5000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5000,  0.5000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0357,  0.8945, -0.0475,  0.0183,  0.0000, -0.8000,  0.5000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.5618,  0.4911, -0.2478, -0.1369,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 26 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0357,  0.8945, -0.0475,  0.0183,  0.0000, -0.8000,  0.5000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.5000,  0.5000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.5618,  0.4911, -0.2478, -0.1369,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.5000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0393,  0.8959, -0.0357,  0.0137,  0.0000, -0.8000,  0.5000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.5432,  0.4808, -0.1858, -0.1027,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 27 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0393,  0.8959, -0.0357,  0.0137,  0.0000, -0.8000,  0.5000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.5000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.5432,  0.4808, -0.1858, -0.1027,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.5000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0420,  0.8969, -0.0267,  0.0103,  0.0000, -0.8000,  0.5000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.5218,  0.4656, -0.2394, -0.1770,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 28 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0420,  0.8969, -0.0267,  0.0103,  0.0000, -0.8000,  0.5000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.5000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.5218,  0.4656, -0.2394, -0.1770,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.5000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.5000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0365,  0.9052,  0.0799,  0.1077,  0.0000, -0.8000,  0.5000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.4964,  0.4448, -0.2795, -0.2328,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 29 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0365,  0.9052,  0.0799,  0.1077,  0.0000, -0.8000,  0.5000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.5000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.4964,  0.4448, -0.2795, -0.2328,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.4000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0305,  0.9133,  0.0600,  0.0808,  0.0000, -0.8000,  0.4000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.4679,  0.4199, -0.3096, -0.2746,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 30 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0305,  0.9133,  0.0600,  0.0808,  0.0000, -0.8000,  0.4000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.4000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.4679,  0.4199, -0.3096, -0.2746,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.4000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4000,  0.4000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0260,  0.9193,  0.0450,  0.0606,  0.0000, -0.8000,  0.4000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.4372,  0.3918, -0.3322, -0.3059,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 31 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0260,  0.9193,  0.0450,  0.0606,  0.0000, -0.8000,  0.4000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.4000,  0.4000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.4372,  0.3918, -0.3322, -0.3059,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.4000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0226,  0.9239,  0.0337,  0.0454,  0.0000, -0.8000,  0.4000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.4048,  0.3763, -0.3492, -0.1294,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 32 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0226,  0.9239,  0.0337,  0.0454,  0.0000, -0.8000,  0.4000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.4000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.4048,  0.3763, -0.3492, -0.1294,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.4000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0201,  0.9273,  0.0253,  0.0341,  0.0000, -0.8000,  0.4000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3711,  0.3741, -0.3619,  0.0029,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 33 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0201,  0.9273,  0.0253,  0.0341,  0.0000, -0.8000,  0.4000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.4000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3711,  0.3741, -0.3619,  0.0029,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0182,  0.9298,  0.0190,  0.0256,  0.0000, -0.8000,  0.3000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3439,  0.3743, -0.2714,  0.0022,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 34 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0182,  0.9298,  0.0190,  0.0256,  0.0000, -0.8000,  0.3000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3439,  0.3743, -0.2714,  0.0022,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0168,  0.9317,  0.0142,  0.0192,  0.0000, -0.8000,  0.3000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3236,  0.3670, -0.2036, -0.0984,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 35 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0168,  0.9317,  0.0142,  0.0192,  0.0000, -0.8000,  0.3000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3236,  0.3670, -0.2036, -0.0984,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0157,  0.9332,  0.0107,  0.0144,  0.0000, -0.8000,  0.3000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3083,  0.3521, -0.1527, -0.1738,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 36 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0157,  0.9332,  0.0107,  0.0144,  0.0000, -0.8000,  0.3000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3083,  0.3521, -0.1527, -0.1738,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0149,  0.9343,  0.0080,  0.0108,  0.0000, -0.8000,  0.3000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.2969,  0.3316, -0.1145, -0.2303,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 37 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0149,  0.9343,  0.0080,  0.0108,  0.0000, -0.8000,  0.3000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.2969,  0.3316, -0.1145, -0.2303,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.2000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0068,  0.9351,  0.1060,  0.0081,  0.0000, -0.8000,  0.2000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.2883,  0.3143, -0.0859, -0.1727,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 38 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0068,  0.9351,  0.1060,  0.0081,  0.0000, -0.8000,  0.2000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.2000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.2883,  0.3143, -0.0859, -0.1727,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.2000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9989,  0.9357,  0.0795,  0.0061,  0.0000, -0.8000,  0.2000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.2818,  0.3014, -0.0644, -0.1296,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 39 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9989,  0.9357,  0.0795,  0.0061,  0.0000, -0.8000,  0.2000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.2000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.2818,  0.3014, -0.0644, -0.1296,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.2000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 2.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  2.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9929,  0.9286,  0.0596, -0.0955,  0.0000, -0.8000,  0.2000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.2770,  0.2992, -0.0483,  0.0028,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 40 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9929,  0.9286,  0.0596, -0.0955,  0.0000, -0.8000,  0.2000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.2000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.2770,  0.2992, -0.0483,  0.0028,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.2000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9884,  0.9215,  0.0447, -0.0716,  0.0000, -0.8000,  0.2000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.2734,  0.2994, -0.0362,  0.0021,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 41 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9884,  0.9215,  0.0447, -0.0716,  0.0000, -0.8000,  0.2000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.2000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.2734,  0.2994, -0.0362,  0.0021,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.2000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9851,  0.9161,  0.0335, -0.0537,  0.0000, -0.8000,  0.2000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.2707,  0.2995, -0.0272,  0.0016,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 42 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9851,  0.9161,  0.0335, -0.0537,  0.0000, -0.8000,  0.2000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.2000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.2707,  0.2995, -0.0272,  0.0016,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.2000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9825,  0.9121,  0.0252, -0.0403,  0.0000, -0.8000,  0.2000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.2686,  0.2996, -0.0204,  0.0012,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 43 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9825,  0.9121,  0.0252, -0.0403,  0.0000, -0.8000,  0.2000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.2000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.2686,  0.2996, -0.0204,  0.0012,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.2000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9882,  0.9091, -0.0811, -0.0302,  0.0000, -0.8000,  0.2000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 2.6709e-01,  2.9973e-01, -1.5284e-02,  8.9574e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -1.0000e+00,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 44 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9882,  0.9091, -0.0811, -0.0302,  0.0000, -0.8000,  0.2000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.2000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 2.6709e-01,  2.9973e-01, -1.5284e-02,  8.9574e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -1.0000e+00,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.2000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9942,  0.9068, -0.0609, -0.0227,  0.0000, -0.8000,  0.2000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 2.7344e-01,  2.9980e-01,  8.8537e-02,  6.7180e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -1.0000e+00,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 45 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9942,  0.9068, -0.0609, -0.0227,  0.0000, -0.8000,  0.2000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.2000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 2.7344e-01,  2.9980e-01,  8.8537e-02,  6.7180e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -1.0000e+00,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.2000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9988,  0.9051, -0.0456, -0.0170,  0.0000, -0.8000,  0.2000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.2876,  0.3073,  0.1664,  0.1005,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 46 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9988,  0.9051, -0.0456, -0.0170,  0.0000, -0.8000,  0.2000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.2000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.2876,  0.3073,  0.1664,  0.1005,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.2000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.2000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0022,  0.9038, -0.0342, -0.0127,  0.0000, -0.8000,  0.2000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3001,  0.3149,  0.1248,  0.0754,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 47 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0022,  0.9038, -0.0342, -0.0127,  0.0000, -0.8000,  0.2000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.2000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3001,  0.3149,  0.1248,  0.0754,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0048,  0.9029, -0.0257, -0.0096,  0.0000, -0.8000,  0.3000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3094,  0.3130,  0.0936, -0.0435,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 48 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0048,  0.9029, -0.0257, -0.0096,  0.0000, -0.8000,  0.3000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3094,  0.3130,  0.0936, -0.0435,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0067,  0.9021, -0.0193, -0.0072,  0.0000, -0.8000,  0.3000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3164,  0.3023,  0.0702, -0.1326,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 49 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0067,  0.9021, -0.0193, -0.0072,  0.0000, -0.8000,  0.3000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3164,  0.3023,  0.0702, -0.1326,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.3000,  0.0000,  0.0000,  0.0000, -0.9000,  7.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0082e+00,  9.0161e-01, -1.4440e-02, -5.3752e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  3.0000e-01,  3.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  7.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3292,  0.2848,  0.1527, -0.1994,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 50 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0082e+00,  9.0161e-01, -1.4440e-02, -5.3752e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  3.0000e-01,  3.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  7.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.3000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3292,  0.2848,  0.1527, -0.1994,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0093,  0.9012, -0.0108, -0.0040,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3407,  0.2699,  0.1145, -0.1496,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 51 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0093,  0.9012, -0.0108, -0.0040,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3407,  0.2699,  0.1145, -0.1496,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0101,  0.9009, -0.0081, -0.0030,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3492,  0.2587,  0.0859, -0.1122,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 52 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0101,  0.9009, -0.0081, -0.0030,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3492,  0.2587,  0.0859, -0.1122,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0107,  0.9007, -0.0061, -0.0023,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3557,  0.2502,  0.0644, -0.0841,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 53 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0107,  0.9007, -0.0061, -0.0023,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3557,  0.2502,  0.0644, -0.0841,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0111,  0.9005, -0.0046, -0.0017,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3605,  0.2439,  0.0483, -0.0631,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 54 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0111,  0.9005, -0.0046, -0.0017,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3605,  0.2439,  0.0483, -0.0631,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0115,  0.9004, -0.0034, -0.0013,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3641,  0.2392,  0.0362, -0.0473,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 55 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0115,  0.9004, -0.0034, -0.0013,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3641,  0.2392,  0.0362, -0.0473,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0117e+00,  9.0029e-01, -2.5700e-03, -9.5666e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  3.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3668,  0.2356,  0.0272, -0.0355,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 56 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0117e+00,  9.0029e-01, -2.5700e-03, -9.5666e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  3.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3668,  0.2356,  0.0272, -0.0355,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0119e+00,  9.0022e-01, -1.9275e-03, -7.1750e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  3.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3689,  0.2330,  0.0204, -0.0266,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 57 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0119e+00,  9.0022e-01, -1.9275e-03, -7.1750e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  3.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3689,  0.2330,  0.0204, -0.0266,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0121e+00,  9.0016e-01, -1.4456e-03, -5.3812e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  3.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3704,  0.2310,  0.0153, -0.0200,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 58 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0121e+00,  9.0016e-01, -1.4456e-03, -5.3812e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  3.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3704,  0.2310,  0.0153, -0.0200,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0047e+00,  9.0012e-01,  9.8916e-02, -4.0359e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  3.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3716,  0.2295,  0.0115, -0.0150,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 59 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0047e+00,  9.0012e-01,  9.8916e-02, -4.0359e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  3.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3716,  0.2295,  0.0115, -0.0150,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.9726e-01,  9.0009e-01,  7.4187e-02, -3.0269e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  3.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3724,  0.2284,  0.0086, -0.0112,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 60 of Episode 1\n",
      "observation continuous agent0 = tensor([-9.9726e-01,  9.0009e-01,  7.4187e-02, -3.0269e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  3.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3724,  0.2284,  0.0086, -0.0112,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.9169e-01,  9.0007e-01,  5.5640e-02, -2.2702e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  3.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3731,  0.2275,  0.0064, -0.0084,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 61 of Episode 1\n",
      "observation continuous agent0 = tensor([-9.9169e-01,  9.0007e-01,  5.5640e-02, -2.2702e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  3.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3731,  0.2275,  0.0064, -0.0084,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.8752e-01,  9.0005e-01,  4.1730e-02, -1.7027e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  3.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3735,  0.2269,  0.0048, -0.0063,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 62 of Episode 1\n",
      "observation continuous agent0 = tensor([-9.8752e-01,  9.0005e-01,  4.1730e-02, -1.7027e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  3.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3735,  0.2269,  0.0048, -0.0063,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9919,  0.9075, -0.0687,  0.0999,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 3.7391e-01,  2.2642e-01,  3.6266e-03, -4.7384e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -1.0000e+00,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  5.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 63 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9919,  0.9075, -0.0687,  0.0999,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 3.7391e-01,  2.2642e-01,  3.6266e-03, -4.7384e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -1.0000e+00,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  5.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9970,  0.9150, -0.0515,  0.0749,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3742,  0.2261,  0.0027, -0.0036,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 64 of Episode 1\n",
      "observation continuous agent0 = tensor([-0.9970,  0.9150, -0.0515,  0.0749,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3742,  0.2261,  0.0027, -0.0036,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0009,  0.9206, -0.0386,  0.0562,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3744,  0.2183,  0.0020, -0.1027,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 65 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0009,  0.9206, -0.0386,  0.0562,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3744,  0.2183,  0.0020, -0.1027,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0038,  0.9249, -0.0290,  0.0421,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3745,  0.2106,  0.0015, -0.0770,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 66 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0038,  0.9249, -0.0290,  0.0421,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3745,  0.2106,  0.0015, -0.0770,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0060,  0.9280, -0.0217,  0.0316,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3822,  0.2123,  0.1011,  0.0423,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 67 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0060,  0.9280, -0.0217,  0.0316,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3822,  0.2123,  0.1011,  0.0423,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0076,  0.9304, -0.0163,  0.0237,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3897,  0.2155,  0.0759,  0.0317,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 68 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0076,  0.9304, -0.0163,  0.0237,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3897,  0.2155,  0.0759,  0.0317,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0088,  0.9322, -0.0122,  0.0178,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3879,  0.2179, -0.0431,  0.0238,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 69 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0088,  0.9322, -0.0122,  0.0178,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3879,  0.2179, -0.0431,  0.0238,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0172,  0.9410, -0.1092,  0.1133,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3922,  0.2197,  0.0677,  0.0178,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 70 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0172,  0.9410, -0.1092,  0.1133,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3922,  0.2197,  0.0677,  0.0178,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0254,  0.9495, -0.0819,  0.0850,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3973,  0.2210,  0.0508,  0.0134,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 71 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0254,  0.9495, -0.0819,  0.0850,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3973,  0.2210,  0.0508,  0.0134,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0316,  0.9559, -0.0614,  0.0637,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.4011,  0.2220,  0.0381,  0.0100,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 72 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0316,  0.9559, -0.0614,  0.0637,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.4011,  0.2220,  0.0381,  0.0100,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0362,  0.9607, -0.0461,  0.0478,  0.0000, -0.8000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3964,  0.2152, -0.0715, -0.0925,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 73 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0362,  0.9607, -0.0461,  0.0478,  0.0000, -0.8000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.4000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3964,  0.2152, -0.0715, -0.0925,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0396,  0.9642, -0.0345,  0.0359,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3911,  0.2083, -0.0536, -0.0694,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 74 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0396,  0.9642, -0.0345,  0.0359,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3911,  0.2083, -0.0536, -0.0694,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0422,  0.9669, -0.0259,  0.0269,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3871,  0.2031, -0.0402, -0.0520,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 75 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0422,  0.9669, -0.0259,  0.0269,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3871,  0.2031, -0.0402, -0.0520,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0442,  0.9689, -0.0194,  0.0202,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3840,  0.1992, -0.0301, -0.0390,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 76 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0442,  0.9689, -0.0194,  0.0202,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3840,  0.1992, -0.0301, -0.0390,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.1000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.1000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0456,  0.9705, -0.0146,  0.0151,  0.0000, -0.8000,  0.3000,  0.1000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3743,  0.2038, -0.1226,  0.0707,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 77 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0456,  0.9705, -0.0146,  0.0151,  0.0000, -0.8000,  0.3000,  0.1000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.1000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3743,  0.2038, -0.1226,  0.0707,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0467,  0.9716, -0.0109,  0.0113,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3651,  0.2091, -0.0920,  0.0531,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 78 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0467,  0.9716, -0.0109,  0.0113,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3651,  0.2091, -0.0920,  0.0531,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0475,  0.9724, -0.0082,  0.0085,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3582,  0.2131, -0.0690,  0.0398,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 79 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0475,  0.9724, -0.0082,  0.0085,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3582,  0.2131, -0.0690,  0.0398,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0482,  0.9731, -0.0061,  0.0064,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3530,  0.2160, -0.0517,  0.0298,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 80 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0482,  0.9731, -0.0061,  0.0064,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3530,  0.2160, -0.0517,  0.0298,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 2.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  2.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0486,  0.9736, -0.0046,  0.0048,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3491,  0.2258, -0.0388,  0.1224,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 81 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0486,  0.9736, -0.0046,  0.0048,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3491,  0.2258, -0.0388,  0.1224,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0415,  0.9814,  0.0965,  0.1036,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3462,  0.2350, -0.0291,  0.0918,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 82 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0415,  0.9814,  0.0965,  0.1036,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3462,  0.2350, -0.0291,  0.0918,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0342,  0.9892,  0.0724,  0.0777,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3440,  0.2418, -0.0218,  0.0688,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 83 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0342,  0.9892,  0.0724,  0.0777,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3440,  0.2418, -0.0218,  0.0688,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0288,  0.9950,  0.0543,  0.0583,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3424,  0.2470, -0.0164,  0.0516,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 84 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0288,  0.9950,  0.0543,  0.0583,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3424,  0.2470, -0.0164,  0.0516,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0247,  0.9994,  0.0407,  0.0437,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3412,  0.2509, -0.0123,  0.0387,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 85 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0247,  0.9994,  0.0407,  0.0437,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3412,  0.2509, -0.0123,  0.0387,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0292,  1.0102, -0.0695,  0.1328,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3403,  0.2538, -0.0092,  0.0290,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 86 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0292,  1.0102, -0.0695,  0.1328,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3403,  0.2538, -0.0092,  0.0290,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0344,  1.0201, -0.0521,  0.0996,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3396,  0.2560, -0.0069,  0.0218,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 87 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0344,  1.0201, -0.0521,  0.0996,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3396,  0.2560, -0.0069,  0.0218,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0383,  1.0276, -0.0391,  0.0747,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3391,  0.2576, -0.0052,  0.0163,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 88 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0383,  1.0276, -0.0391,  0.0747,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3391,  0.2576, -0.0052,  0.0163,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0412,  1.0332, -0.0293,  0.0560,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3387,  0.2588, -0.0039,  0.0123,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 89 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0412,  1.0332, -0.0293,  0.0560,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3387,  0.2588, -0.0039,  0.0123,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0434,  1.0374, -0.0220,  0.0420,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3384,  0.2597, -0.0029,  0.0092,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 90 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0434,  1.0374, -0.0220,  0.0420,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3384,  0.2597, -0.0029,  0.0092,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0451,  1.0405, -0.0165,  0.0315,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3382,  0.2604, -0.0022,  0.0069,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 91 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0451,  1.0405, -0.0165,  0.0315,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3382,  0.2604, -0.0022,  0.0069,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0463,  1.0429, -0.0124,  0.0236,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3380,  0.2609, -0.0016,  0.0052,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 92 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0463,  1.0429, -0.0124,  0.0236,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3380,  0.2609, -0.0016,  0.0052,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0472,  1.0447, -0.0093,  0.0177,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3379,  0.2613, -0.0012,  0.0039,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 93 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0472,  1.0447, -0.0093,  0.0177,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3379,  0.2613, -0.0012,  0.0039,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0479e+00,  1.0460e+00, -6.9532e-03,  1.3293e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  3.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3453,  0.2691,  0.0991,  0.1029,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 94 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0479e+00,  1.0460e+00, -6.9532e-03,  1.3293e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  3.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3453,  0.2691,  0.0991,  0.1029,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0484,  1.0470, -0.0052,  0.0100,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3527,  0.2768,  0.0743,  0.0772,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 95 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0484,  1.0470, -0.0052,  0.0100,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3527,  0.2768,  0.0743,  0.0772,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0488,  1.0478, -0.0039,  0.0075,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3583,  0.2826,  0.0557,  0.0579,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 96 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0488,  1.0478, -0.0039,  0.0075,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3583,  0.2826,  0.0557,  0.0579,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0491,  1.0483, -0.0029,  0.0056,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3625,  0.2795,  0.0418, -0.0566,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 97 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0491,  1.0483, -0.0029,  0.0056,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3625,  0.2795,  0.0418, -0.0566,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0493,  1.0487, -0.0022,  0.0042,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3656,  0.2677,  0.0313, -0.1424,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 98 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0493,  1.0487, -0.0022,  0.0042,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3656,  0.2677,  0.0313, -0.1424,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0495,  1.0491, -0.0017,  0.0032,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3679,  0.2495,  0.0235, -0.2068,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 99 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0495,  1.0491, -0.0017,  0.0032,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3679,  0.2495,  0.0235, -0.2068,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0496,  1.0493, -0.0012,  0.0024,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3697,  0.2265,  0.0176, -0.2551,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 100 of Episode 1\n",
      "observation continuous agent0 = tensor([-1.0496,  1.0493, -0.0012,  0.0024,  0.0000, -0.8000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.3000,  0.2000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.3697,  0.2265,  0.0176, -0.2551,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.3000,  0.2000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-1.0497e+00,  1.0495e+00, -9.2814e-04,  1.7744e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  3.0000e-01,  2.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  1.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.3710,  0.1999,  0.0132, -0.2913,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "done status for agent 0: False\n",
      "done status for agent 1: False\n",
      "Success percentage of each agent at the end of episode 1:\n",
      "Agent 0: 0.0%\n",
      "Agent 1: 0.0%\n",
      "Overall success percentage up to episode 1: 0.0%\n",
      "Episode 2\n",
      "Step 1 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9075,  0.9000, -0.1000,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 2 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.9075,  0.9000, -0.1000,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9150,  0.9000, -0.0750,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 3 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.9150,  0.9000, -0.0750,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9206,  0.9000, -0.0563,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 4 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.9206,  0.9000, -0.0563,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9248,  0.9000, -0.0422,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 5 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.9248,  0.9000, -0.0422,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9205,  0.9000,  0.0684,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8925,  0.9075, -0.1000,  0.1000,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 6 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.9205,  0.9000,  0.0684,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8925,  0.9075, -0.1000,  0.1000,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9154,  0.9000,  0.0513,  0.0000,  0.0000, -0.8000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8850,  0.9075, -0.0750, -0.0250,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 7 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.9154,  0.9000,  0.0513,  0.0000,  0.0000, -0.8000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8850,  0.9075, -0.0750, -0.0250,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9115,  0.9000,  0.0385,  0.0000,  0.0000, -0.8000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8794,  0.8981, -0.0563, -0.1188,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 8 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.9115,  0.9000,  0.0385,  0.0000,  0.0000, -0.8000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8794,  0.8981, -0.0563, -0.1188,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9087,  0.9000,  0.0288,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8752,  0.8892, -0.0422, -0.0891,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 9 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.9087,  0.9000,  0.0288,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8752,  0.8892, -0.0422, -0.0891,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8990,  0.9000,  0.1216,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8720,  0.8825, -0.0316, -0.0668,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 10 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.8990,  0.9000,  0.1216,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8720,  0.8825, -0.0316, -0.0668,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8824,  0.9000,  0.1912,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8696,  0.8775, -0.0237, -0.0501,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 11 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.8824,  0.9000,  0.1912,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8696,  0.8775, -0.0237, -0.0501,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8605,  0.9000,  0.2434,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8753,  0.8738,  0.0822, -0.0376,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 12 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.8605,  0.9000,  0.2434,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8753,  0.8738,  0.0822, -0.0376,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8423,  0.9000,  0.1826,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8815,  0.8710,  0.0617, -0.0282,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 13 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.8423,  0.9000,  0.1826,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8815,  0.8710,  0.0617, -0.0282,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8286,  0.9000,  0.1369,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8861,  0.8688,  0.0462, -0.0211,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 14 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.8286,  0.9000,  0.1369,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8861,  0.8688,  0.0462, -0.0211,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8183,  0.9000,  0.1027,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8896,  0.8673,  0.0347, -0.0159,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 15 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.8183,  0.9000,  0.1027,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8896,  0.8673,  0.0347, -0.0159,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8106,  0.9000,  0.0770,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8922,  0.8661,  0.0260, -0.0119,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 16 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.8106,  0.9000,  0.0770,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8922,  0.8661,  0.0260, -0.0119,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8048,  0.9000,  0.0578,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8941,  0.8652,  0.0195, -0.0089,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 17 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.8048,  0.9000,  0.0578,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8941,  0.8652,  0.0195, -0.0089,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8005,  0.9000,  0.0433,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8956,  0.8645,  0.0146, -0.0067,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 18 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.8005,  0.9000,  0.0433,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8956,  0.8645,  0.0146, -0.0067,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7897,  0.9075,  0.1325,  0.1000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 8.9671e-01,  8.6400e-01,  1.0973e-02, -5.0154e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 19 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.7897,  0.9075,  0.1325,  0.1000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 8.9671e-01,  8.6400e-01,  1.0973e-02, -5.0154e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7798,  0.9150,  0.0994,  0.0750,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8975,  0.8636,  0.0082, -0.0038,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 20 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.7798,  0.9150,  0.0994,  0.0750,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8975,  0.8636,  0.0082, -0.0038,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7649,  0.9206,  0.1745,  0.0563,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 8.9815e-01,  8.6335e-01,  6.1721e-03, -2.8212e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -8.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  6.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 21 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.7649,  0.9206,  0.1745,  0.0563,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 8.9815e-01,  8.6335e-01,  6.1721e-03, -2.8212e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -8.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  6.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7518,  0.9248,  0.1309,  0.0422,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8986,  0.8631,  0.0046, -0.0021,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 22 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.7518,  0.9248,  0.1309,  0.0422,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8986,  0.8631,  0.0046, -0.0021,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7420,  0.9280,  0.0982,  0.0316,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8990,  0.8630,  0.0035, -0.0016,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 23 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.7420,  0.9280,  0.0982,  0.0316,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8990,  0.8630,  0.0035, -0.0016,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7346,  0.9304,  0.0736,  0.0237,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8917,  0.8554, -0.0974, -0.1012,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 24 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.7346,  0.9304,  0.0736,  0.0237,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8917,  0.8554, -0.0974, -0.1012,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7291,  0.9322,  0.0552,  0.0178,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8919,  0.8478,  0.0270, -0.0759,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 25 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.7291,  0.9322,  0.0552,  0.0178,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8919,  0.8478,  0.0270, -0.0759,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7249,  0.9335,  0.0414,  0.0133,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8939,  0.8421,  0.0202, -0.0569,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 26 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.7249,  0.9335,  0.0414,  0.0133,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8939,  0.8421,  0.0202, -0.0569,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7218,  0.9345,  0.0311,  0.0100,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8955,  0.8378,  0.0152, -0.0427,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 27 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.7218,  0.9345,  0.0311,  0.0100,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8955,  0.8378,  0.0152, -0.0427,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7195,  0.9352,  0.0233,  0.0075,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8966,  0.8346,  0.0114, -0.0320,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 28 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.7195,  0.9352,  0.0233,  0.0075,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 2.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  2.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8966,  0.8346,  0.0114, -0.0320,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7177,  0.9433,  0.0175,  0.1056,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8974,  0.8322,  0.0085, -0.0240,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 29 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.7177,  0.9433,  0.0175,  0.1056,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8974,  0.8322,  0.0085, -0.0240,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7164,  0.9512,  0.0131,  0.0792,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8981,  0.8304,  0.0064, -0.0180,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 30 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.7164,  0.9512,  0.0131,  0.0792,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8981,  0.8304,  0.0064, -0.0180,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7154,  0.9572,  0.0098,  0.0594,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8986,  0.8291,  0.0048, -0.0135,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 31 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.7154,  0.9572,  0.0098,  0.0594,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8986,  0.8291,  0.0048, -0.0135,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7147,  0.9616,  0.0074,  0.0446,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8989,  0.8280,  0.0036, -0.0101,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 32 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.7147,  0.9616,  0.0074,  0.0446,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  7.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8989,  0.8280,  0.0036, -0.0101,  0.0000, -0.8000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.7067,  0.9575,  0.1055, -0.0666,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 8.9919e-01,  8.2728e-01,  2.6983e-03, -7.5978e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -8.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  7.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 33 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.7067,  0.9575,  0.1055, -0.0666,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  7.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 8.9919e-01,  8.2728e-01,  2.6983e-03, -7.5978e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -8.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  7.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6912,  0.9450,  0.1791, -0.1499,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 8.9939e-01,  8.2671e-01,  2.0237e-03, -5.6984e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -8.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  7.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 34 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6912,  0.9450,  0.1791, -0.1499,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 8.9939e-01,  8.2671e-01,  2.0237e-03, -5.6984e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -8.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  7.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6778,  0.9337,  0.1344, -0.1124,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8995,  0.8263,  0.0015, -0.0043,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 35 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6778,  0.9337,  0.1344, -0.1124,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8995,  0.8263,  0.0015, -0.0043,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6602,  0.9328,  0.2008,  0.0157,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 8.9966e-01,  8.2596e-01,  1.1384e-03, -3.2053e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -7.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 36 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6602,  0.9328,  0.2008,  0.0157,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 8.9966e-01,  8.2596e-01,  1.1384e-03, -3.2053e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -7.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6452,  0.9340,  0.1506,  0.0117,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 8.9974e-01,  8.2572e-01,  8.5377e-04, -2.4040e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -7.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 37 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6452,  0.9340,  0.1506,  0.0117,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 8.9974e-01,  8.2572e-01,  8.5377e-04, -2.4040e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -7.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6414,  0.9424,  0.0129,  0.1088,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 8.9231e-01,  8.2554e-01, -9.9360e-02, -1.8030e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -7.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  5.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 38 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6414,  0.9424,  0.0129,  0.1088,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 8.9231e-01,  8.2554e-01, -9.9360e-02, -1.8030e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -7.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  5.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6404,  0.9505,  0.0097,  0.0816,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8924,  0.8329,  0.0255,  0.0986,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 39 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6404,  0.9505,  0.0097,  0.0816,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8924,  0.8329,  0.0255,  0.0986,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6397,  0.9566,  0.0073,  0.0612,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8943,  0.8403,  0.0191,  0.0740,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 40 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6397,  0.9566,  0.0073,  0.0612,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8943,  0.8403,  0.0191,  0.0740,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6391,  0.9612,  0.0055,  0.0459,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8957,  0.8459,  0.0143,  0.0555,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 41 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6391,  0.9612,  0.0055,  0.0459,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8957,  0.8459,  0.0143,  0.0555,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6387,  0.9647,  0.0041,  0.0344,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8968,  0.8500,  0.0107,  0.0416,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 42 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6387,  0.9647,  0.0041,  0.0344,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8968,  0.8500,  0.0107,  0.0416,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-6.3842e-01,  9.6725e-01,  3.0691e-03,  2.5821e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  6.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9051,  0.8531,  0.1081,  0.0312,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 43 of Episode 2\n",
      "observation continuous agent0 = tensor([-6.3842e-01,  9.6725e-01,  3.0691e-03,  2.5821e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  6.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9051,  0.8531,  0.1081,  0.0312,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-6.3819e-01,  9.6919e-01,  2.3018e-03,  1.9366e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9057,  0.8480, -0.0190, -0.0766,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 44 of Episode 2\n",
      "observation continuous agent0 = tensor([-6.3819e-01,  9.6919e-01,  2.3018e-03,  1.9366e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9057,  0.8480, -0.0190, -0.0766,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-6.3802e-01,  9.7064e-01,  1.7263e-03,  1.4524e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8968,  0.8347, -0.1142, -0.1574,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 45 of Episode 2\n",
      "observation continuous agent0 = tensor([-6.3802e-01,  9.7064e-01,  1.7263e-03,  1.4524e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8968,  0.8347, -0.1142, -0.1574,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6379,  0.9717,  0.0013,  0.0109,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8882,  0.8229, -0.0857, -0.1181,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 46 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6379,  0.9717,  0.0013,  0.0109,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8882,  0.8229, -0.0857, -0.1181,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-6.3779e-01,  9.7255e-01,  9.7107e-04,  8.1700e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8818,  0.8141, -0.0642, -0.0886,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 47 of Episode 2\n",
      "observation continuous agent0 = tensor([-6.3779e-01,  9.7255e-01,  9.7107e-04,  8.1700e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8818,  0.8141, -0.0642, -0.0886,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-6.3772e-01,  9.7316e-01,  7.2830e-04,  6.1275e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  5.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8695,  0.8149, -0.1482,  0.0336,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 48 of Episode 2\n",
      "observation continuous agent0 = tensor([-6.3772e-01,  9.7316e-01,  7.2830e-04,  6.1275e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  5.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8695,  0.8149, -0.1482,  0.0336,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-6.3766e-01,  9.7362e-01,  5.4623e-04,  4.5956e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8508,  0.8174, -0.2111,  0.0252,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 49 of Episode 2\n",
      "observation continuous agent0 = tensor([-6.3766e-01,  9.7362e-01,  5.4623e-04,  4.5956e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8508,  0.8174, -0.2111,  0.0252,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-6.3762e-01,  9.7397e-01,  4.0967e-04,  3.4467e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8350,  0.8193, -0.1584,  0.0189,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 50 of Episode 2\n",
      "observation continuous agent0 = tensor([-6.3762e-01,  9.7397e-01,  4.0967e-04,  3.4467e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8350,  0.8193, -0.1584,  0.0189,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-6.3759e-01,  9.7422e-01,  3.0725e-04,  2.5850e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8231,  0.8207, -0.1188,  0.0142,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 51 of Episode 2\n",
      "observation continuous agent0 = tensor([-6.3759e-01,  9.7422e-01,  3.0725e-04,  2.5850e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8231,  0.8207, -0.1188,  0.0142,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-6.3757e-01,  9.7442e-01,  2.3044e-04,  1.9388e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  5.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8067,  0.8293, -0.1891,  0.1106,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 52 of Episode 2\n",
      "observation continuous agent0 = tensor([-6.3757e-01,  9.7442e-01,  2.3044e-04,  1.9388e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  5.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8067,  0.8293, -0.1891,  0.1106,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-6.3755e-01,  9.7456e-01,  1.7283e-04,  1.4541e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7925,  0.8376, -0.1418,  0.0830,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 53 of Episode 2\n",
      "observation continuous agent0 = tensor([-6.3755e-01,  9.7456e-01,  1.7283e-04,  1.4541e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7925,  0.8376, -0.1418,  0.0830,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6300,  0.9747,  0.1001,  0.0011,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7819,  0.8438, -0.1064,  0.0622,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 54 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6300,  0.9747,  0.1001,  0.0011,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7819,  0.8438, -0.1064,  0.0622,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-6.2253e-01,  9.7475e-01,  7.5097e-02,  8.1792e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  7.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  1.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7739,  0.8410, -0.0798, -0.0533,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 55 of Episode 2\n",
      "observation continuous agent0 = tensor([-6.2253e-01,  9.7475e-01,  7.5097e-02,  8.1792e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  7.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  1.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7739,  0.8410, -0.0798, -0.0533,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 2.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  2.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6169,  0.9673,  0.0563, -0.0994,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7679,  0.8445, -0.0598,  0.0600,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 56 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6169,  0.9673,  0.0563, -0.0994,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7679,  0.8445, -0.0598,  0.0600,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6202,  0.9674, -0.0578,  0.0255,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7635,  0.8415, -0.0449, -0.0550,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 57 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6202,  0.9674, -0.0578,  0.0255,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7635,  0.8415, -0.0449, -0.0550,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6245,  0.9693, -0.0433,  0.0191,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7601,  0.8299, -0.0337, -0.1412,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 58 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6245,  0.9693, -0.0433,  0.0191,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7601,  0.8299, -0.0337, -0.1412,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6278,  0.9707, -0.0325,  0.0143,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7576,  0.8118, -0.0252, -0.2059,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 59 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6278,  0.9707, -0.0325,  0.0143,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7576,  0.8118, -0.0252, -0.2059,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6302,  0.9718, -0.0244,  0.0107,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7557,  0.7888, -0.0189, -0.2545,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 60 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6302,  0.9718, -0.0244,  0.0107,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7557,  0.7888, -0.0189, -0.2545,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6320,  0.9726, -0.0183,  0.0081,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7543,  0.7698, -0.0142, -0.1908,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 61 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6320,  0.9726, -0.0183,  0.0081,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7543,  0.7698, -0.0142, -0.1908,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 2.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  2.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6334,  0.9732, -0.0137,  0.0060,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7532,  0.7629, -0.0106, -0.0431,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 62 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6334,  0.9732, -0.0137,  0.0060,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7532,  0.7629, -0.0106, -0.0431,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6419,  0.9661, -0.1103, -0.0955,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7524,  0.7597, -0.0080, -0.0323,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 63 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6419,  0.9661, -0.1103, -0.0955,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7524,  0.7597, -0.0080, -0.0323,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6502,  0.9590, -0.0827, -0.0716,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7518,  0.7573, -0.0060, -0.0243,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 64 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6502,  0.9590, -0.0827, -0.0716,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7518,  0.7573, -0.0060, -0.0243,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6564,  0.9536, -0.0620, -0.0537,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7438,  0.7630, -0.1045,  0.0818,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 65 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6564,  0.9536, -0.0620, -0.0537,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7438,  0.7630, -0.1045,  0.0818,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6610,  0.9496, -0.0465, -0.0403,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7360,  0.7691, -0.0784,  0.0614,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 66 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6610,  0.9496, -0.0465, -0.0403,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7360,  0.7691, -0.0784,  0.0614,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6645,  0.9466, -0.0349, -0.0302,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7301,  0.7737, -0.0588,  0.0460,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 67 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6645,  0.9466, -0.0349, -0.0302,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7301,  0.7737, -0.0588,  0.0460,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6596,  0.9518,  0.0738,  0.0773,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7332,  0.7771,  0.0559,  0.0345,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 68 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6596,  0.9518,  0.0738,  0.0773,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7332,  0.7771,  0.0559,  0.0345,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6541,  0.9576,  0.0554,  0.0580,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7374,  0.7797,  0.0419,  0.0259,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 69 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6541,  0.9576,  0.0554,  0.0580,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7374,  0.7797,  0.0419,  0.0259,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6500,  0.9619,  0.0415,  0.0435,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7406,  0.7817,  0.0315,  0.0194,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 70 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6500,  0.9619,  0.0415,  0.0435,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7406,  0.7817,  0.0315,  0.0194,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6468,  0.9652,  0.0311,  0.0326,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7429,  0.7831,  0.0236,  0.0146,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 71 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6468,  0.9652,  0.0311,  0.0326,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7429,  0.7831,  0.0236,  0.0146,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6445,  0.9677,  0.0234,  0.0245,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7447,  0.7842,  0.0177,  0.0109,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 72 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6445,  0.9677,  0.0234,  0.0245,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7447,  0.7842,  0.0177,  0.0109,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 2.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  2.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6428,  0.9695,  0.0175,  0.0184,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7460,  0.7925,  0.0133,  0.1082,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 73 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6428,  0.9695,  0.0175,  0.0184,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7460,  0.7925,  0.0133,  0.1082,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6414,  0.9709,  0.0131,  0.0138,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7395,  0.7932, -0.0900, -0.0189,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 74 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6414,  0.9709,  0.0131,  0.0138,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7395,  0.7932, -0.0900, -0.0189,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6405,  0.9719,  0.0099,  0.0103,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7328,  0.7917, -0.0675, -0.0141,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 75 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6405,  0.9719,  0.0099,  0.0103,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7328,  0.7917, -0.0675, -0.0141,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6397,  0.9727,  0.0074,  0.0077,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7277,  0.7907, -0.0507, -0.0106,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 76 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6397,  0.9727,  0.0074,  0.0077,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7277,  0.7907, -0.0507, -0.0106,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6392,  0.9733,  0.0055,  0.0058,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7239,  0.7899, -0.0380, -0.0080,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 77 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6392,  0.9733,  0.0055,  0.0058,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7239,  0.7899, -0.0380, -0.0080,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6387,  0.9737,  0.0042,  0.0044,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7210,  0.7893, -0.0285, -0.0060,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 78 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6387,  0.9737,  0.0042,  0.0044,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7210,  0.7893, -0.0285, -0.0060,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6459,  0.9665, -0.0969, -0.0967,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7264,  0.7963,  0.0786,  0.0955,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 79 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6459,  0.9665, -0.0969, -0.0967,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7264,  0.7963,  0.0786,  0.0955,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6532,  0.9593, -0.0727, -0.0725,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7323,  0.8035,  0.0590,  0.0716,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 80 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6532,  0.9593, -0.0727, -0.0725,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7323,  0.8035,  0.0590,  0.0716,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6587,  0.9538, -0.0545, -0.0544,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7367,  0.8014,  0.0442, -0.0463,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 81 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6587,  0.9538, -0.0545, -0.0544,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7367,  0.8014,  0.0442, -0.0463,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6627,  0.9497, -0.0409, -0.0408,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7400,  0.7904,  0.0332, -0.1347,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 82 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6627,  0.9497, -0.0409, -0.0408,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7400,  0.7904,  0.0332, -0.1347,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6583,  0.9542,  0.0693,  0.0694,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7425,  0.7803,  0.0249, -0.1010,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 83 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6583,  0.9542,  0.0693,  0.0694,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7425,  0.7803,  0.0249, -0.1010,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  7.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6531,  0.9594,  0.0520,  0.0520,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7519,  0.7652,  0.1187, -0.1758,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 84 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6531,  0.9594,  0.0520,  0.0520,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7519,  0.7652,  0.1187, -0.1758,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6492,  0.9633,  0.0390,  0.0390,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7608,  0.7520,  0.0890, -0.1318,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 85 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6492,  0.9633,  0.0390,  0.0390,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7608,  0.7520,  0.0890, -0.1318,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6463,  0.9662,  0.0293,  0.0293,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7675,  0.7422,  0.0667, -0.0989,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 86 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6463,  0.9662,  0.0293,  0.0293,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7675,  0.7422,  0.0667, -0.0989,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6441,  0.9684,  0.0219,  0.0220,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7725,  0.7347,  0.0501, -0.0742,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 87 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6441,  0.9684,  0.0219,  0.0220,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7725,  0.7347,  0.0501, -0.0742,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6424,  0.9701,  0.0165,  0.0165,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7762,  0.7292,  0.0375, -0.0556,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 88 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6424,  0.9701,  0.0165,  0.0165,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7762,  0.7292,  0.0375, -0.0556,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6412,  0.9713,  0.0123,  0.0124,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7791,  0.7250,  0.0282, -0.0417,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 89 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6412,  0.9713,  0.0123,  0.0124,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7791,  0.7250,  0.0282, -0.0417,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6403,  0.9722,  0.0093,  0.0093,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7812,  0.7219,  0.0211, -0.0313,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 90 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6403,  0.9722,  0.0093,  0.0093,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7812,  0.7219,  0.0211, -0.0313,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6396,  0.9729,  0.0069,  0.0069,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7902,  0.7195,  0.1158, -0.0235,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 91 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6396,  0.9729,  0.0069,  0.0069,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7902,  0.7195,  0.1158, -0.0235,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6391,  0.9734,  0.0052,  0.0052,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7989,  0.7178,  0.0869, -0.0176,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 92 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6391,  0.9734,  0.0052,  0.0052,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  7.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7989,  0.7178,  0.0869, -0.0176,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6312,  0.9663,  0.1039, -0.0961,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8055,  0.7165,  0.0652, -0.0132,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 93 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6312,  0.9663,  0.1039, -0.0961,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8055,  0.7165,  0.0652, -0.0132,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6234,  0.9591,  0.0779, -0.0721,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8028,  0.7155, -0.0511, -0.0099,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 94 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6234,  0.9591,  0.0779, -0.0721,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8028,  0.7155, -0.0511, -0.0099,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  7.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6175,  0.9537,  0.0584, -0.0541,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8065,  0.7072,  0.0617, -0.1074,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 95 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6175,  0.9537,  0.0584, -0.0541,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8065,  0.7072,  0.0617, -0.1074,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6132,  0.9497,  0.0438, -0.0405,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8036,  0.6917, -0.0538, -0.1806,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 96 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6132,  0.9497,  0.0438, -0.0405,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8036,  0.6917, -0.0538, -0.1806,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6099,  0.9466,  0.0329, -0.0304,  0.0000, -0.8000,  0.8000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7996,  0.6781, -0.0403, -0.1354,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 97 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6099,  0.9466,  0.0329, -0.0304,  0.0000, -0.8000,  0.8000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7996,  0.6781, -0.0403, -0.1354,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6074,  0.9443,  0.0247, -0.0228,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7966,  0.6680, -0.0302, -0.1016,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 98 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6074,  0.9443,  0.0247, -0.0228,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7966,  0.6680, -0.0302, -0.1016,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6055,  0.9426,  0.0185, -0.0171,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8018,  0.6679,  0.0773,  0.0238,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 99 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6055,  0.9426,  0.0185, -0.0171,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8018,  0.6679,  0.0773,  0.0238,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6042,  0.9413,  0.0139, -0.0128,  0.0000, -0.8000,  0.8000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8076,  0.6696,  0.0580,  0.0179,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 100 of Episode 2\n",
      "observation continuous agent0 = tensor([-0.6042,  0.9413,  0.0139, -0.0128,  0.0000, -0.8000,  0.8000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8076,  0.6696,  0.0580,  0.0179,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.6031,  0.9329,  0.0104, -0.1096,  0.0000, -0.8000,  0.8000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8120,  0.6710,  0.0435,  0.0134,  0.0000, -0.8000, -0.7000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "done status for agent 0: False\n",
      "done status for agent 1: False\n",
      "Success percentage of each agent at the end of episode 2:\n",
      "Agent 0: 0.0%\n",
      "Agent 1: 0.0%\n",
      "Overall success percentage up to episode 2: 0.0%\n",
      "Episode 3\n",
      "Step 1 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 2 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 3 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 4 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8925,  0.9000,  0.1000,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.8925,  0.0000, -0.1000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 5 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8925,  0.9000,  0.1000,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.8925,  0.0000, -0.1000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8850,  0.9000,  0.0750,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.8850,  0.0000, -0.0750,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 6 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8850,  0.9000,  0.0750,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.8850,  0.0000, -0.0750,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8794,  0.9000,  0.0563,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.8794,  0.0000, -0.0563,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 7 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8794,  0.9000,  0.0563,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.8794,  0.0000, -0.0563,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8752,  0.9000,  0.0422,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.8752,  0.0000, -0.0422,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 8 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8752,  0.9000,  0.0422,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.8752,  0.0000, -0.0422,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8720,  0.9000,  0.0316,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.8720,  0.0000, -0.0316,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 9 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8720,  0.9000,  0.0316,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.8720,  0.0000, -0.0316,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8696,  0.9000,  0.0237,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.8696,  0.0000, -0.0237,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 10 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8696,  0.9000,  0.0237,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.8696,  0.0000, -0.0237,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8678,  0.9000,  0.0178,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9075,  0.8678,  0.1000, -0.0178,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 11 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8678,  0.9000,  0.0178,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9075,  0.8678,  0.1000, -0.0178,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8665,  0.9000,  0.0133,  0.0000,  0.0000, -0.8000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9075,  0.8590, -0.0250, -0.1133,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 12 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8665,  0.9000,  0.0133,  0.0000,  0.0000, -0.8000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9075,  0.8590, -0.0250, -0.1133,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8655,  0.9000,  0.0100,  0.0000,  0.0000, -0.8000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8981,  0.8430, -0.1188, -0.1850,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 13 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8655,  0.9000,  0.0100,  0.0000,  0.0000, -0.8000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8981,  0.8430, -0.1188, -0.1850,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8648,  0.9000,  0.0075,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8892,  0.8291, -0.0891, -0.1388,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 14 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8648,  0.9000,  0.0075,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8892,  0.8291, -0.0891, -0.1388,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8642,  0.9000,  0.0056,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8825,  0.8187, -0.0668, -0.1041,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 15 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8642,  0.9000,  0.0056,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8825,  0.8187, -0.0668, -0.1041,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8638,  0.9000,  0.0042,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8775,  0.8109, -0.0501, -0.0781,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 16 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8638,  0.9000,  0.0042,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8775,  0.8109, -0.0501, -0.0781,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8635,  0.9000,  0.0032,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8738,  0.8051, -0.0376, -0.0585,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 17 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8635,  0.9000,  0.0032,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8738,  0.8051, -0.0376, -0.0585,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.6321e-01,  9.0000e-01,  2.3757e-03,  0.0000e+00,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  5.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8635,  0.8082, -0.1282,  0.0561,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 18 of Episode 3\n",
      "observation continuous agent0 = tensor([-8.6321e-01,  9.0000e-01,  2.3757e-03,  0.0000e+00,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  5.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8635,  0.8082, -0.1282,  0.0561,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8630,  0.9000,  0.0018,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8538,  0.8124, -0.0961,  0.0421,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 19 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8630,  0.9000,  0.0018,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 2.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  2.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8538,  0.8124, -0.0961,  0.0421,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.6290e-01,  9.0750e-01,  1.3363e-03,  1.0000e-01,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8391,  0.8155, -0.1721,  0.0316,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 20 of Episode 3\n",
      "observation continuous agent0 = tensor([-8.6290e-01,  9.0750e-01,  1.3363e-03,  1.0000e-01,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8391,  0.8155, -0.1721,  0.0316,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8628,  0.9150,  0.0010,  0.0750,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8262,  0.8179, -0.1291,  0.0237,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 21 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8628,  0.9150,  0.0010,  0.0750,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8262,  0.8179, -0.1291,  0.0237,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.6273e-01,  9.2063e-01,  7.5169e-04,  5.6250e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8165,  0.8197, -0.0968,  0.0177,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 22 of Episode 3\n",
      "observation continuous agent0 = tensor([-8.6273e-01,  9.2063e-01,  7.5169e-04,  5.6250e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8165,  0.8197, -0.0968,  0.0177,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8702,  0.9323, -0.0994,  0.1422,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8093,  0.8210, -0.0726,  0.0133,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 23 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8702,  0.9323, -0.0994,  0.1422,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8093,  0.8210, -0.0726,  0.0133,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8776,  0.9430, -0.0746,  0.1066,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8038,  0.8220, -0.0545,  0.0100,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 24 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8776,  0.9430, -0.0746,  0.1066,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8038,  0.8220, -0.0545,  0.0100,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8832,  0.9510, -0.0559,  0.0800,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7998,  0.8228, -0.0408,  0.0075,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 25 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8832,  0.9510, -0.0559,  0.0800,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7998,  0.8228, -0.0408,  0.0075,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8874,  0.9570, -0.0419,  0.0600,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7967,  0.8158, -0.0306, -0.0944,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 26 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8874,  0.9570, -0.0419,  0.0600,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7967,  0.8158, -0.0306, -0.0944,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 2.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  2.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8906,  0.9615, -0.0315,  0.0450,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7944,  0.8162, -0.0230,  0.0292,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 27 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8906,  0.9615, -0.0315,  0.0450,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7944,  0.8162, -0.0230,  0.0292,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8929,  0.9649, -0.0236,  0.0337,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7927,  0.8109, -0.0172, -0.0781,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 28 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8929,  0.9649, -0.0236,  0.0337,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7927,  0.8109, -0.0172, -0.0781,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9022,  0.9749, -0.1177,  0.1253,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7914,  0.7976, -0.0129, -0.1586,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 29 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9022,  0.9749, -0.1177,  0.1253,  0.0000, -0.8000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7914,  0.7976, -0.0129, -0.1586,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9110,  0.9843, -0.0883,  0.0940,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7904,  0.7857, -0.0097, -0.1189,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 30 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9110,  0.9843, -0.0883,  0.0940,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7904,  0.7857, -0.0097, -0.1189,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9176,  0.9914, -0.0662,  0.0705,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7897,  0.7768, -0.0073, -0.0892,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 31 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9176,  0.9914, -0.0662,  0.0705,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7897,  0.7768, -0.0073, -0.0892,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9151,  0.9966,  0.0503,  0.0529,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.8914e-01,  7.7007e-01, -5.4515e-03, -6.6896e-02,  0.0000e+00,\n",
      "        -8.0000e-01, -1.0000e+00,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  6.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 32 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9151,  0.9966,  0.0503,  0.0529,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.8914e-01,  7.7007e-01, -5.4515e-03, -6.6896e-02,  0.0000e+00,\n",
      "        -8.0000e-01, -1.0000e+00,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  6.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9113,  1.0006,  0.0378,  0.0396,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7887,  0.7651, -0.0041, -0.0502,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 33 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9113,  1.0006,  0.0378,  0.0396,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7887,  0.7651, -0.0041, -0.0502,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9085,  1.0036,  0.0283,  0.0297,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7884,  0.7613, -0.0031, -0.0376,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 34 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9085,  1.0036,  0.0283,  0.0297,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7884,  0.7613, -0.0031, -0.0376,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9064,  1.0058,  0.0212,  0.0223,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7882,  0.7585, -0.0023, -0.0282,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 35 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9064,  1.0058,  0.0212,  0.0223,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7882,  0.7585, -0.0023, -0.0282,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9048,  1.0000,  0.0159, -0.0833,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7880,  0.7563, -0.0017, -0.0212,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 36 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9048,  1.0000,  0.0159, -0.0833,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7880,  0.7563, -0.0017, -0.0212,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9036,  0.9937,  0.0119, -0.0625,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7879,  0.7548, -0.0013, -0.0159,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 37 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9036,  0.9937,  0.0119, -0.0625,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7879,  0.7548, -0.0013, -0.0159,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9027,  0.9891,  0.0090, -0.0468,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.8779e-01,  7.5357e-01, -9.7026e-04, -1.1906e-02,  0.0000e+00,\n",
      "        -8.0000e-01, -1.0000e+00,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 38 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9027,  0.9891,  0.0090, -0.0468,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.8779e-01,  7.5357e-01, -9.7026e-04, -1.1906e-02,  0.0000e+00,\n",
      "        -8.0000e-01, -1.0000e+00,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  7.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.0202e-01,  9.8554e-01,  6.7204e-03, -3.5131e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  7.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  7.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7952,  0.7452,  0.0993, -0.1089,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 39 of Episode 3\n",
      "observation continuous agent0 = tensor([-9.0202e-01,  9.8554e-01,  6.7204e-03, -3.5131e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  7.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  7.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7952,  0.7452,  0.0993, -0.1089,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9015,  0.9829,  0.0050, -0.0263,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8027,  0.7370,  0.0745, -0.0817,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 40 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9015,  0.9829,  0.0050, -0.0263,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8027,  0.7370,  0.0745, -0.0817,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.0113e-01,  9.8093e-01,  3.7802e-03, -1.9761e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8007,  0.7234, -0.0442, -0.1613,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 41 of Episode 3\n",
      "observation continuous agent0 = tensor([-9.0113e-01,  9.8093e-01,  3.7802e-03, -1.9761e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8007,  0.7234, -0.0442, -0.1613,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.0085e-01,  9.7195e-01,  2.8352e-03, -1.1482e-01,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7899,  0.7038, -0.1331, -0.2210,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 42 of Episode 3\n",
      "observation continuous agent0 = tensor([-9.0085e-01,  9.7195e-01,  2.8352e-03, -1.1482e-01,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7899,  0.7038, -0.1331, -0.2210,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9006,  0.9633,  0.0021, -0.0861,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7800,  0.6872, -0.0998, -0.1657,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 43 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9006,  0.9633,  0.0021, -0.0861,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7800,  0.6872, -0.0998, -0.1657,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9005,  0.9569,  0.0016, -0.0646,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7725,  0.6748, -0.0749, -0.1243,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 44 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9005,  0.9569,  0.0016, -0.0646,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7725,  0.6748, -0.0749, -0.1243,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9004,  0.9520,  0.0012, -0.0484,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7668,  0.6655, -0.0562, -0.0932,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 45 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9004,  0.9520,  0.0012, -0.0484,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7668,  0.6655, -0.0562, -0.0932,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.0027e-01,  9.4840e-01,  8.9707e-04, -3.6330e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  7.0000e-01,  6.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7626,  0.6585, -0.0421, -0.0699,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 46 of Episode 3\n",
      "observation continuous agent0 = tensor([-9.0027e-01,  9.4840e-01,  8.9707e-04, -3.6330e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  7.0000e-01,  6.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7626,  0.6585, -0.0421, -0.0699,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.0020e-01,  9.4567e-01,  6.7280e-04, -2.7248e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  7.0000e-01,  6.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7670,  0.6607,  0.0684,  0.0476,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 47 of Episode 3\n",
      "observation continuous agent0 = tensor([-9.0020e-01,  9.4567e-01,  6.7280e-04, -2.7248e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  7.0000e-01,  6.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7670,  0.6607,  0.0684,  0.0476,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8927,  0.9511,  0.1005,  0.0796,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7721,  0.6643,  0.0513,  0.0357,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 48 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8927,  0.9511,  0.1005,  0.0796,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7721,  0.6643,  0.0513,  0.0357,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8851,  0.9571,  0.0754,  0.0597,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7760,  0.6670,  0.0385,  0.0268,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 49 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8851,  0.9571,  0.0754,  0.0597,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7760,  0.6670,  0.0385,  0.0268,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8795,  0.9616,  0.0565,  0.0448,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7788,  0.6690,  0.0289,  0.0201,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 50 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8795,  0.9616,  0.0565,  0.0448,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7788,  0.6690,  0.0289,  0.0201,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8752,  0.9649,  0.0424,  0.0336,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7810,  0.6705,  0.0216,  0.0151,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 51 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8752,  0.9649,  0.0424,  0.0336,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7810,  0.6705,  0.0216,  0.0151,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8720,  0.9674,  0.0318,  0.0252,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7826,  0.6716,  0.0162,  0.0113,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 52 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8720,  0.9674,  0.0318,  0.0252,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7826,  0.6716,  0.0162,  0.0113,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8697,  0.9693,  0.0239,  0.0189,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7838,  0.6725,  0.0122,  0.0085,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 53 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8697,  0.9693,  0.0239,  0.0189,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7838,  0.6725,  0.0122,  0.0085,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8679,  0.9708,  0.0179,  0.0142,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7848,  0.6731,  0.0091,  0.0063,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 54 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8679,  0.9708,  0.0179,  0.0142,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7848,  0.6731,  0.0091,  0.0063,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8590,  0.9718,  0.1134,  0.0106,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7779,  0.6811, -0.0932,  0.1048,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 55 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8590,  0.9718,  0.1134,  0.0106,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7779,  0.6811, -0.0932,  0.1048,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8505,  0.9726,  0.0851,  0.0080,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7710,  0.6889, -0.0699,  0.0786,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 56 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8505,  0.9726,  0.0851,  0.0080,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7710,  0.6889, -0.0699,  0.0786,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8441,  0.9732,  0.0638,  0.0060,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7657,  0.6948, -0.0524,  0.0589,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 57 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8441,  0.9732,  0.0638,  0.0060,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7657,  0.6948, -0.0524,  0.0589,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8469,  0.9812, -0.0522,  0.1045,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7618,  0.6992, -0.0393,  0.0442,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 58 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8469,  0.9812, -0.0522,  0.1045,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7618,  0.6992, -0.0393,  0.0442,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8508,  0.9890, -0.0391,  0.0784,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7588,  0.7026, -0.0295,  0.0331,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 59 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8508,  0.9890, -0.0391,  0.0784,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7588,  0.7026, -0.0295,  0.0331,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8537,  0.9949, -0.0293,  0.0588,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7641,  0.7125,  0.0779,  0.1249,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 60 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8537,  0.9949, -0.0293,  0.0588,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7641,  0.7125,  0.0779,  0.1249,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8559,  0.9993, -0.0220,  0.0441,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7700,  0.7219,  0.0584,  0.0936,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 61 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8559,  0.9993, -0.0220,  0.0441,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7700,  0.7219,  0.0584,  0.0936,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8575,  1.0026, -0.0165,  0.0331,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7744,  0.7289,  0.0438,  0.0702,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 62 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8575,  1.0026, -0.0165,  0.0331,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7744,  0.7289,  0.0438,  0.0702,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8588,  1.0051, -0.0124,  0.0248,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7776,  0.7342,  0.0329,  0.0527,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 63 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8588,  1.0051, -0.0124,  0.0248,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7776,  0.7342,  0.0329,  0.0527,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8597,  1.0069, -0.0093,  0.0186,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7801,  0.7381,  0.0246,  0.0395,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 64 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8597,  1.0069, -0.0093,  0.0186,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7801,  0.7381,  0.0246,  0.0395,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8604,  1.0083, -0.0070,  0.0139,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7820,  0.7411,  0.0185,  0.0296,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 65 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8604,  1.0083, -0.0070,  0.0139,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7820,  0.7411,  0.0185,  0.0296,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8609,  1.0094, -0.0052,  0.0105,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7833,  0.7433,  0.0139,  0.0222,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 66 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8609,  1.0094, -0.0052,  0.0105,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7833,  0.7433,  0.0139,  0.0222,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8613,  1.0101, -0.0039,  0.0078,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7844,  0.7450,  0.0104,  0.0167,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 67 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8613,  1.0101, -0.0039,  0.0078,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7844,  0.7450,  0.0104,  0.0167,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8541,  1.0182,  0.0971,  0.1059,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.8516e-01,  7.4625e-01,  7.7983e-03,  1.2500e-02,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 68 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8541,  1.0182,  0.0971,  0.1059,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.8516e-01,  7.4625e-01,  7.7983e-03,  1.2500e-02,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8468,  1.0262,  0.0728,  0.0794,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7857,  0.7472,  0.0058,  0.0094,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 69 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8468,  1.0262,  0.0728,  0.0794,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7857,  0.7472,  0.0058,  0.0094,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8414,  1.0321,  0.0546,  0.0596,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7862,  0.7479,  0.0044,  0.0070,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 70 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8414,  1.0321,  0.0546,  0.0596,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7862,  0.7479,  0.0044,  0.0070,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8373,  1.0366,  0.0409,  0.0447,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7865,  0.7484,  0.0033,  0.0053,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 71 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8373,  1.0366,  0.0409,  0.0447,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7865,  0.7484,  0.0033,  0.0053,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8342,  1.0399,  0.0307,  0.0335,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7868,  0.7488,  0.0025,  0.0040,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 72 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8342,  1.0399,  0.0307,  0.0335,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7868,  0.7488,  0.0025,  0.0040,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8319,  1.0425,  0.0230,  0.0251,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7869,  0.7491,  0.0019,  0.0030,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 73 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8319,  1.0425,  0.0230,  0.0251,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7869,  0.7491,  0.0019,  0.0030,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8302,  1.0443,  0.0173,  0.0188,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7796,  0.7493, -0.0986,  0.0022,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 74 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8302,  1.0443,  0.0173,  0.0188,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7796,  0.7493, -0.0986,  0.0022,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8289,  1.0458,  0.0130,  0.0141,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7722,  0.7495, -0.0740,  0.0017,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 75 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8289,  1.0458,  0.0130,  0.0141,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7722,  0.7495, -0.0740,  0.0017,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8354,  1.0543, -0.0903,  0.1106,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.6664e-01,  7.4962e-01, -5.5469e-02,  1.2514e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  5.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 76 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8354,  1.0543, -0.0903,  0.1106,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.6664e-01,  7.4962e-01, -5.5469e-02,  1.2514e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  5.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8422,  1.0626, -0.0677,  0.0830,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.6248e-01,  7.4972e-01, -4.1602e-02,  9.3857e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 77 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8422,  1.0626, -0.0677,  0.0830,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.6248e-01,  7.4972e-01, -4.1602e-02,  9.3857e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8473,  1.0688, -0.0508,  0.0622,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7669,  0.7573,  0.0688,  0.1007,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 78 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8473,  1.0688, -0.0508,  0.0622,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7669,  0.7573,  0.0688,  0.1007,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8511,  1.0735, -0.0381,  0.0467,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7645,  0.7573, -0.0484, -0.0245,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 79 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8511,  1.0735, -0.0381,  0.0467,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7645,  0.7573, -0.0484, -0.0245,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8539,  1.0770, -0.0286,  0.0350,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7609,  0.7555, -0.0363, -0.0184,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 80 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8539,  1.0770, -0.0286,  0.0350,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7609,  0.7555, -0.0363, -0.0184,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8561,  1.0796, -0.0214,  0.0262,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7582,  0.7541, -0.0272, -0.0138,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 81 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8561,  1.0796, -0.0214,  0.0262,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7582,  0.7541, -0.0272, -0.0138,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8577,  1.0816, -0.0161,  0.0197,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7561,  0.7531, -0.0204, -0.0103,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 82 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8577,  1.0816, -0.0161,  0.0197,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7561,  0.7531, -0.0204, -0.0103,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8589,  1.0831, -0.0121,  0.0148,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7546,  0.7523, -0.0153, -0.0077,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 83 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8589,  1.0831, -0.0121,  0.0148,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7546,  0.7523, -0.0153, -0.0077,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8598,  1.0842, -0.0090,  0.0111,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7534,  0.7517, -0.0115, -0.0058,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 84 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8598,  1.0842, -0.0090,  0.0111,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7534,  0.7517, -0.0115, -0.0058,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8605,  1.0850, -0.0068,  0.0083,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7526,  0.7513, -0.0086, -0.0044,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 85 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8605,  1.0850, -0.0068,  0.0083,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7526,  0.7513, -0.0086, -0.0044,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8610,  1.0856, -0.0051,  0.0062,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7519,  0.7510, -0.0065, -0.0033,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 86 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8610,  1.0856, -0.0051,  0.0062,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7519,  0.7510, -0.0065, -0.0033,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8689,  1.0786, -0.1038, -0.0953,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.5145e-01,  7.5074e-01, -4.8456e-03, -2.4500e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 87 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8689,  1.0786, -0.1038, -0.0953,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.5145e-01,  7.5074e-01, -4.8456e-03, -2.4500e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8766,  1.0714, -0.0779, -0.0715,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7511,  0.7506, -0.0036, -0.0018,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 88 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8766,  1.0714, -0.0779, -0.0715,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7511,  0.7506, -0.0036, -0.0018,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8825,  1.0661, -0.0584, -0.0536,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7508,  0.7504, -0.0027, -0.0014,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 89 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8825,  1.0661, -0.0584, -0.0536,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7508,  0.7504, -0.0027, -0.0014,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8869,  1.0621, -0.0438, -0.0402,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7506,  0.7503, -0.0020, -0.0010,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 90 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8869,  1.0621, -0.0438, -0.0402,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7506,  0.7503, -0.0020, -0.0010,  0.0000, -0.8000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8901,  1.0590, -0.0328, -0.0302,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.5046e-01,  7.5023e-01, -1.5332e-03, -7.7519e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 91 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8901,  1.0590, -0.0328, -0.0302,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.5046e-01,  7.5023e-01, -1.5332e-03, -7.7519e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8926,  1.0568, -0.0246, -0.0226,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.5034e-01,  7.5017e-01, -1.1499e-03, -5.8139e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 92 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8926,  1.0568, -0.0246, -0.0226,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.5034e-01,  7.5017e-01, -1.1499e-03, -5.8139e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8945,  1.0551, -0.0185, -0.0170,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.5026e-01,  7.5013e-01, -8.6241e-04, -4.3604e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 93 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.8945,  1.0551, -0.0185, -0.0170,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.5026e-01,  7.5013e-01, -8.6241e-04, -4.3604e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9033,  1.0538, -0.1139, -0.0127,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.5019e-01,  7.5010e-01, -6.4681e-04, -3.2703e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 94 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9033,  1.0538, -0.1139, -0.0127,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.5019e-01,  7.5010e-01, -6.4681e-04, -3.2703e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9119,  1.0529, -0.0854, -0.0095,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.5015e-01,  7.5007e-01, -4.8510e-04, -2.4527e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -1.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 95 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9119,  1.0529, -0.0854, -0.0095,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.5015e-01,  7.5007e-01, -4.8510e-04, -2.4527e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -1.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9183,  1.0521, -0.0640, -0.0072,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.5011e-01,  7.5006e-01, -3.6383e-04, -1.8396e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -1.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 96 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9183,  1.0521, -0.0640, -0.0072,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.5011e-01,  7.5006e-01, -3.6383e-04, -1.8396e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -1.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9156,  1.0591,  0.0520,  0.0946,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.5008e-01,  7.5004e-01, -2.7287e-04, -1.3797e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -1.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 97 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9156,  1.0591,  0.0520,  0.0946,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.5008e-01,  7.5004e-01, -2.7287e-04, -1.3797e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -1.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9117,  1.0662,  0.0390,  0.0710,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.5006e-01,  7.5003e-01, -2.0465e-04, -1.0347e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -1.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 98 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9117,  1.0662,  0.0390,  0.0710,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.5006e-01,  7.5003e-01, -2.0465e-04, -1.0347e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -1.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9088,  1.0715,  0.0292,  0.0532,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7575,  0.7575,  0.0998,  0.0999,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 99 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9088,  1.0715,  0.0292,  0.0532,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7575,  0.7575,  0.0998,  0.0999,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9066,  1.0755,  0.0219,  0.0399,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7650,  0.7650,  0.0749,  0.0749,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 100 of Episode 3\n",
      "observation continuous agent0 = tensor([-0.9066,  1.0755,  0.0219,  0.0399,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7650,  0.7650,  0.0749,  0.0749,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9049,  1.0785,  0.0164,  0.0299,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7707,  0.7706,  0.0562,  0.0562,  0.0000, -0.8000, -1.0000,  1.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "done status for agent 0: False\n",
      "done status for agent 1: False\n",
      "Success percentage of each agent at the end of episode 3:\n",
      "Agent 0: 0.0%\n",
      "Agent 1: 0.0%\n",
      "Overall success percentage up to episode 3: 0.0%\n",
      "Episode 4\n",
      "Step 1 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 2 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  7.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8925,  0.8925,  0.1000, -0.1000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 3 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8925,  0.8925,  0.1000, -0.1000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8850,  0.8850,  0.0750, -0.0750,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 4 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8850,  0.8850,  0.0750, -0.0750,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8869,  0.8794, -0.0437, -0.0563,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 5 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8869,  0.8794, -0.0437, -0.0563,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8902,  0.8752, -0.0328, -0.0422,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 6 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8902,  0.8752, -0.0328, -0.0422,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8926,  0.8720, -0.0246, -0.0316,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 7 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8926,  0.8720, -0.0246, -0.0316,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8945,  0.8696, -0.0185, -0.0237,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 8 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8945,  0.8696, -0.0185, -0.0237,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8958,  0.8678, -0.0138, -0.0178,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 9 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8958,  0.8678, -0.0138, -0.0178,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8969,  0.8665, -0.0104, -0.0133,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 10 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8969,  0.8665, -0.0104, -0.0133,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8977,  0.8655, -0.0078, -0.0100,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 11 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8977,  0.8655, -0.0078, -0.0100,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8982,  0.8648, -0.0058, -0.0075,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 12 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8982,  0.8648, -0.0058, -0.0075,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8987,  0.8642, -0.0044, -0.0056,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 13 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8987,  0.8642, -0.0044, -0.0056,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8990,  0.8638, -0.0033, -0.0042,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 14 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8990,  0.8638, -0.0033, -0.0042,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8993,  0.8635, -0.0025, -0.0032,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 15 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8993,  0.8635, -0.0025, -0.0032,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8994,  0.8632, -0.0018, -0.0024,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 16 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8994,  0.8632, -0.0018, -0.0024,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8996,  0.8630, -0.0014, -0.0018,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 17 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8996,  0.8630, -0.0014, -0.0018,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8997,  0.8629, -0.0010, -0.0013,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 18 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8997,  0.8629, -0.0010, -0.0013,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.9977e-01,  8.6280e-01, -7.7954e-04, -1.0023e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8925,  0.9000, -0.1000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 19 of Episode 4\n",
      "observation continuous agent0 = tensor([-8.9977e-01,  8.6280e-01, -7.7954e-04, -1.0023e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8925,  0.9000, -0.1000,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.9982e-01,  8.6273e-01, -5.8465e-04, -7.5169e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8850,  0.9000, -0.0750,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 20 of Episode 4\n",
      "observation continuous agent0 = tensor([-8.9982e-01,  8.6273e-01, -5.8465e-04, -7.5169e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8850,  0.9000, -0.0750,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.9987e-01,  8.6267e-01, -4.3849e-04, -5.6377e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8794,  0.9000, -0.0563,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 21 of Episode 4\n",
      "observation continuous agent0 = tensor([-8.9987e-01,  8.6267e-01, -4.3849e-04, -5.6377e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8794,  0.9000, -0.0563,  0.0000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.9990e-01,  8.6263e-01, -3.2887e-04, -4.2283e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8827,  0.9075,  0.0578,  0.1000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 22 of Episode 4\n",
      "observation continuous agent0 = tensor([-8.9990e-01,  8.6263e-01, -3.2887e-04, -4.2283e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8827,  0.9075,  0.0578,  0.1000,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.9993e-01,  8.6259e-01, -2.4665e-04, -3.1712e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  1.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8870,  0.9075,  0.0434, -0.0250,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 23 of Episode 4\n",
      "observation continuous agent0 = tensor([-8.9993e-01,  8.6259e-01, -2.4665e-04, -3.1712e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  1.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8870,  0.9075,  0.0434, -0.0250,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  1.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.9994e-01,  8.6257e-01, -1.8499e-04, -2.3784e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  1.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8902,  0.8981,  0.0325, -0.1188,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 24 of Episode 4\n",
      "observation continuous agent0 = tensor([-8.9994e-01,  8.6257e-01, -1.8499e-04, -2.3784e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  1.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8902,  0.8981,  0.0325, -0.1188,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.9996e-01,  8.6255e-01, -1.3874e-04, -1.7838e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  6.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9002,  0.8892,  0.1244, -0.0891,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 25 of Episode 4\n",
      "observation continuous agent0 = tensor([-8.9996e-01,  8.6255e-01, -1.3874e-04, -1.7838e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  6.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9002,  0.8892,  0.1244, -0.0891,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.9997e-01,  8.6254e-01, -1.0406e-04, -1.3379e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  9.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.9020,  0.8750, -0.0067, -0.1668,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 26 of Episode 4\n",
      "observation continuous agent0 = tensor([-8.9997e-01,  8.6254e-01, -1.0406e-04, -1.3379e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  9.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.9020,  0.8750, -0.0067, -0.1668,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.9998e-01,  8.6253e-01, -7.8042e-05, -1.0034e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  9.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8940,  0.8550, -0.1050, -0.2251,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 27 of Episode 4\n",
      "observation continuous agent0 = tensor([-8.9998e-01,  8.6253e-01, -7.8042e-05, -1.0034e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  9.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  4.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8940,  0.8550, -0.1050, -0.2251,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.9998e-01,  8.6252e-01, -5.8531e-05, -7.5254e-05,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8861,  0.8381, -0.0788, -0.1688,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 28 of Episode 4\n",
      "observation continuous agent0 = tensor([-8.9998e-01,  8.6252e-01, -5.8531e-05, -7.5254e-05,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8861,  0.8381, -0.0788, -0.1688,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.9999e-01,  8.6252e-01, -4.3898e-05, -5.6441e-05,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8802,  0.8255, -0.0591, -0.1266,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 29 of Episode 4\n",
      "observation continuous agent0 = tensor([-8.9999e-01,  8.6252e-01, -4.3898e-05, -5.6441e-05,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8802,  0.8255, -0.0591, -0.1266,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.9999e-01,  8.6251e-01, -3.2924e-05, -4.2331e-05,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8758,  0.8160, -0.0443, -0.0950,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 30 of Episode 4\n",
      "observation continuous agent0 = tensor([-8.9999e-01,  8.6251e-01, -3.2924e-05, -4.2331e-05,  0.0000e+00,\n",
      "        -8.0000e-01,  8.0000e-01,  8.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8758,  0.8160, -0.0443, -0.0950,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9075,  0.8550, -0.1000, -0.1000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8725,  0.8089, -0.0332, -0.0712,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 31 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9075,  0.8550, -0.1000, -0.1000,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8725,  0.8089, -0.0332, -0.0712,  0.0000, -0.8000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9150,  0.8475, -0.0750, -0.0750,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8700,  0.8035, -0.0249, -0.0534,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 32 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9150,  0.8475, -0.0750, -0.0750,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8700,  0.8035, -0.0249, -0.0534,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9206,  0.8419, -0.0563, -0.0563,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8681,  0.7995, -0.0187, -0.0401,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 33 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9206,  0.8419, -0.0563, -0.0563,  0.0000, -0.8000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8681,  0.7995, -0.0187, -0.0401,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9248,  0.8377, -0.0422, -0.0422,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8592,  0.7890, -0.1140, -0.1300,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 34 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9248,  0.8377, -0.0422, -0.0422,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8592,  0.7890, -0.1140, -0.1300,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9280,  0.8345, -0.0316, -0.0317,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8432,  0.7718, -0.1855, -0.1975,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 35 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9280,  0.8345, -0.0316, -0.0317,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8432,  0.7718, -0.1855, -0.1975,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 2.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  2.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9304,  0.8321, -0.0237, -0.0237,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8292,  0.7644, -0.1391, -0.0482,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 36 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9304,  0.8321, -0.0237, -0.0237,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8292,  0.7644, -0.1391, -0.0482,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9322,  0.8303, -0.0178, -0.0178,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8188,  0.7608, -0.1044, -0.0361,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 37 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9322,  0.8303, -0.0178, -0.0178,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8188,  0.7608, -0.1044, -0.0361,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9335,  0.8290, -0.0134, -0.0134,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.8035,  0.7506, -0.1783, -0.1271,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 38 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9335,  0.8290, -0.0134, -0.0134,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.8035,  0.7506, -0.1783, -0.1271,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 4.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.8000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  4.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9345,  0.8280, -0.0100, -0.0100,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7826,  0.7336, -0.2337, -0.1953,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 39 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9345,  0.8280, -0.0100, -0.0100,  0.0000, -0.8000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7826,  0.7336, -0.2337, -0.1953,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9352,  0.8273, -0.0075, -0.0075,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7651,  0.7189, -0.1753, -0.1465,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 40 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9352,  0.8273, -0.0075, -0.0075,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 2.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  2.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7651,  0.7189, -0.1753, -0.1465,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9358,  0.8342, -0.0056,  0.0944,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7519,  0.7080, -0.1315, -0.1099,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 41 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9358,  0.8342, -0.0056,  0.0944,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7519,  0.7080, -0.1315, -0.1099,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9362,  0.8413, -0.0042,  0.0708,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7421,  0.6997, -0.0986, -0.0824,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 42 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9362,  0.8413, -0.0042,  0.0708,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7421,  0.6997, -0.0986, -0.0824,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9365,  0.8466, -0.0032,  0.0531,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7347,  0.6935, -0.0739, -0.0618,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 43 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9365,  0.8466, -0.0032,  0.0531,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7347,  0.6935, -0.0739, -0.0618,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9368,  0.8506, -0.0024,  0.0398,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7291,  0.6889, -0.0555, -0.0463,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 44 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9368,  0.8506, -0.0024,  0.0398,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  7.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7291,  0.6889, -0.0555, -0.0463,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9295,  0.8460,  0.0982, -0.0701,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7250,  0.6854, -0.0416, -0.0348,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 45 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9295,  0.8460,  0.0982, -0.0701,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7250,  0.6854, -0.0416, -0.0348,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9221,  0.8408,  0.0737, -0.0526,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7219,  0.6828, -0.0312, -0.0261,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 46 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9221,  0.8408,  0.0737, -0.0526,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7219,  0.6828, -0.0312, -0.0261,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9166,  0.8368,  0.0552, -0.0395,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7195,  0.6809, -0.0234, -0.0196,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 47 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9166,  0.8368,  0.0552, -0.0395,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7195,  0.6809, -0.0234, -0.0196,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9124,  0.8339,  0.0414, -0.0296,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7178,  0.6794, -0.0175, -0.0147,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 48 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9124,  0.8339,  0.0414, -0.0296,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7178,  0.6794, -0.0175, -0.0147,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9093,  0.8317,  0.0311, -0.0222,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7164,  0.6783, -0.0132, -0.0110,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 49 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9093,  0.8317,  0.0311, -0.0222,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 2.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  2.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7164,  0.6783, -0.0132, -0.0110,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9070,  0.8375,  0.0233,  0.0834,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7155,  0.6775, -0.0099, -0.0082,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 50 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9070,  0.8375,  0.0233,  0.0834,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7155,  0.6775, -0.0099, -0.0082,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9052,  0.8437,  0.0175,  0.0625,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7222,  0.6769,  0.0926, -0.0062,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 51 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9052,  0.8437,  0.0175,  0.0625,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7222,  0.6769,  0.0926, -0.0062,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9039,  0.8484,  0.0131,  0.0469,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7217,  0.6839, -0.0306,  0.0954,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 52 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9039,  0.8484,  0.0131,  0.0469,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7217,  0.6839, -0.0306,  0.0954,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9029,  0.8519,  0.0098,  0.0352,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7194,  0.6910, -0.0229,  0.0715,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 53 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9029,  0.8519,  0.0098,  0.0352,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7194,  0.6910, -0.0229,  0.0715,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9022,  0.8546,  0.0074,  0.0264,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7177,  0.6964, -0.0172,  0.0536,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 54 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9022,  0.8546,  0.0074,  0.0264,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7177,  0.6964, -0.0172,  0.0536,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9017,  0.8566,  0.0055,  0.0198,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7164,  0.7004, -0.0129,  0.0402,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 55 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9017,  0.8566,  0.0055,  0.0198,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7164,  0.7004, -0.0129,  0.0402,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-9.0124e-01,  8.5805e-01,  4.1482e-03,  1.4835e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  7.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  6.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7229,  0.7034,  0.0903,  0.0302,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 56 of Episode 4\n",
      "observation continuous agent0 = tensor([-9.0124e-01,  8.5805e-01,  4.1482e-03,  1.4835e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  7.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  6.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7229,  0.7034,  0.0903,  0.0302,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9084,  0.8667, -0.0969,  0.1111,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7297,  0.7057,  0.0677,  0.0226,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 57 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9084,  0.8667, -0.0969,  0.1111,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7297,  0.7057,  0.0677,  0.0226,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9157,  0.8750, -0.0727,  0.0833,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7348,  0.7074,  0.0508,  0.0170,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 58 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9157,  0.8750, -0.0727,  0.0833,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7348,  0.7074,  0.0508,  0.0170,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9211,  0.8812, -0.0545,  0.0625,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7311,  0.7087, -0.0619,  0.0127,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 59 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9211,  0.8812, -0.0545,  0.0625,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7311,  0.7087, -0.0619,  0.0127,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9252,  0.8859, -0.0409,  0.0469,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7264,  0.7096, -0.0464,  0.0095,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 60 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9252,  0.8859, -0.0409,  0.0469,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7264,  0.7096, -0.0464,  0.0095,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9283,  0.8895, -0.0307,  0.0352,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7229,  0.7104, -0.0348,  0.0072,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 61 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9283,  0.8895, -0.0307,  0.0352,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7229,  0.7104, -0.0348,  0.0072,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9306,  0.8921, -0.0230,  0.0264,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7203,  0.7109, -0.0261,  0.0054,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 62 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9306,  0.8921, -0.0230,  0.0264,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7203,  0.7109, -0.0261,  0.0054,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  7.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9323,  0.8941, -0.0172,  0.0198,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7259,  0.7038,  0.0804, -0.0960,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 63 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9323,  0.8941, -0.0172,  0.0198,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7259,  0.7038,  0.0804, -0.0960,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9336,  0.8955, -0.0129,  0.0148,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7319,  0.6966,  0.0603, -0.0720,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 64 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9336,  0.8955, -0.0129,  0.0148,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7319,  0.6966,  0.0603, -0.0720,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 2.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  2.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9346,  0.8967, -0.0097,  0.0111,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7364,  0.6987,  0.0452,  0.0460,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 65 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9346,  0.8967, -0.0097,  0.0111,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7364,  0.6987,  0.0452,  0.0460,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.6000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9278,  0.9050,  0.0927,  0.1083,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7398,  0.7021,  0.0339,  0.0345,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 66 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9278,  0.9050,  0.0927,  0.1083,  0.0000, -0.8000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.6000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7398,  0.7021,  0.0339,  0.0345,  0.0000, -0.8000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9209,  0.9131,  0.0695,  0.0813,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7424,  0.7047,  0.0254,  0.0259,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 67 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9209,  0.9131,  0.0695,  0.0813,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7424,  0.7047,  0.0254,  0.0259,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9156,  0.9192,  0.0522,  0.0609,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7443,  0.7067,  0.0191,  0.0194,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 68 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9156,  0.9192,  0.0522,  0.0609,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7443,  0.7067,  0.0191,  0.0194,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9117,  0.9238,  0.0391,  0.0457,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7457,  0.7081,  0.0143,  0.0146,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 69 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9117,  0.9238,  0.0391,  0.0457,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  7.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7457,  0.7081,  0.0143,  0.0146,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.9013,  0.9197,  0.1293, -0.0657,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7468,  0.7092,  0.0107,  0.0109,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 70 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.9013,  0.9197,  0.1293, -0.0657,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.0000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7468,  0.7092,  0.0107,  0.0109,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8916,  0.9148,  0.0970, -0.0493,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7476,  0.7100,  0.0081,  0.0082,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 71 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8916,  0.9148,  0.0970, -0.0493,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 6.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  6.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7476,  0.7100,  0.0081,  0.0082,  0.0000, -0.8000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8768,  0.9111,  0.1728, -0.0370,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7482,  0.7107,  0.0060,  0.0061,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 72 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8768,  0.9111,  0.1728, -0.0370,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 5.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  5.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7482,  0.7107,  0.0060,  0.0061,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8714,  0.9158,  0.0296,  0.0723,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.4864e-01,  7.1112e-01,  4.5286e-03,  4.6067e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  5.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 73 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8714,  0.9158,  0.0296,  0.0723,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.4864e-01,  7.1112e-01,  4.5286e-03,  4.6067e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  5.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8692,  0.9212,  0.0222,  0.0542,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7490,  0.7115,  0.0034,  0.0035,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 74 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8692,  0.9212,  0.0222,  0.0542,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7490,  0.7115,  0.0034,  0.0035,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8675,  0.9253,  0.0166,  0.0407,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7492,  0.7117,  0.0025,  0.0026,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 75 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8675,  0.9253,  0.0166,  0.0407,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 3.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  3.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7492,  0.7117,  0.0025,  0.0026,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8737,  0.9284, -0.0875,  0.0305,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.4943e-01,  7.1192e-01,  1.9105e-03,  1.9435e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 76 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8737,  0.9284, -0.0875,  0.0305,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.4943e-01,  7.1192e-01,  1.9105e-03,  1.9435e-03,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  3.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8803,  0.9306, -0.0656,  0.0229,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7496,  0.7121,  0.0014,  0.0015,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 77 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8803,  0.9306, -0.0656,  0.0229,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7496,  0.7121,  0.0014,  0.0015,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8852,  0.9324, -0.0492,  0.0172,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7497,  0.7122,  0.0011,  0.0011,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 78 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8852,  0.9324, -0.0492,  0.0172,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7497,  0.7122,  0.0011,  0.0011,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8889,  0.9336, -0.0369,  0.0129,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.4976e-01,  7.1225e-01,  8.0599e-04,  8.1990e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 79 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8889,  0.9336, -0.0369,  0.0129,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.4976e-01,  7.1225e-01,  8.0599e-04,  8.1990e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8917,  0.9346, -0.0277,  0.0096,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.4982e-01,  7.1232e-01,  6.0449e-04,  6.1493e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 80 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8917,  0.9346, -0.0277,  0.0096,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.4982e-01,  7.1232e-01,  6.0449e-04,  6.1493e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8938,  0.9353, -0.0208,  0.0072,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.4986e-01,  7.1236e-01,  4.5337e-04,  4.6119e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 81 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8938,  0.9353, -0.0208,  0.0072,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.4986e-01,  7.1236e-01,  4.5337e-04,  4.6119e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8953,  0.9359, -0.0156,  0.0054,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.4990e-01,  7.1240e-01,  3.4003e-04,  3.4590e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 82 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8953,  0.9359, -0.0156,  0.0054,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.4990e-01,  7.1240e-01,  3.4003e-04,  3.4590e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8965,  0.9363, -0.0117,  0.0041,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.4992e-01,  7.1242e-01,  2.5502e-04,  2.5942e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 83 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8965,  0.9363, -0.0117,  0.0041,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.4992e-01,  7.1242e-01,  2.5502e-04,  2.5942e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8974,  0.9366, -0.0088,  0.0031,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.4994e-01,  7.1244e-01,  1.9127e-04,  1.9457e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 84 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8974,  0.9366, -0.0088,  0.0031,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.4994e-01,  7.1244e-01,  1.9127e-04,  1.9457e-04,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 2.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  2.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8980,  0.9368, -0.0066,  0.0023,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.4996e-01,  7.1996e-01,  1.4345e-04,  1.0015e-01,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 85 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8980,  0.9368, -0.0066,  0.0023,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.4996e-01,  7.1996e-01,  1.4345e-04,  1.0015e-01,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8985,  0.9370, -0.0049,  0.0017,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.4997e-01,  7.2747e-01,  1.0759e-04,  7.5109e-02,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 86 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8985,  0.9370, -0.0049,  0.0017,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.4997e-01,  7.2747e-01,  1.0759e-04,  7.5109e-02,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8989,  0.9371, -0.0037,  0.0013,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.4998e-01,  7.3310e-01,  8.0690e-05,  5.6332e-02,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 87 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8989,  0.9371, -0.0037,  0.0013,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.4998e-01,  7.3310e-01,  8.0690e-05,  5.6332e-02,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8992,  0.9372, -0.0028,  0.0010,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.4998e-01,  7.3733e-01,  6.0518e-05,  4.2249e-02,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 88 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8992,  0.9372, -0.0028,  0.0010,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.4998e-01,  7.3733e-01,  6.0518e-05,  4.2249e-02,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.9938e-01,  9.3728e-01, -2.0794e-03,  7.2438e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  7.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 7.4999e-01,  7.4049e-01,  4.5388e-05,  3.1687e-02,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 89 of Episode 4\n",
      "observation continuous agent0 = tensor([-8.9938e-01,  9.3728e-01, -2.0794e-03,  7.2438e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  7.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 7.4999e-01,  7.4049e-01,  4.5388e-05,  3.1687e-02,  0.0000e+00,\n",
      "        -8.0000e-01, -9.0000e-01,  9.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.9953e-01,  9.3734e-01, -1.5596e-03,  5.4328e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  7.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7575,  0.7504,  0.1000,  0.1238,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 90 of Episode 4\n",
      "observation continuous agent0 = tensor([-8.9953e-01,  9.3734e-01, -1.5596e-03,  5.4328e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  7.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  8.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7575,  0.7504,  0.1000,  0.1238,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-8.9965e-01,  9.3738e-01, -1.1697e-03,  4.0746e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  7.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7650,  0.7597,  0.0750,  0.0928,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 91 of Episode 4\n",
      "observation continuous agent0 = tensor([-8.9965e-01,  9.3738e-01, -1.1697e-03,  4.0746e-04,  0.0000e+00,\n",
      "        -8.0000e-01,  7.0000e-01,  7.0000e-01,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -9.0000e-01,  0.0000e+00], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 8.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7650,  0.7597,  0.0750,  0.0928,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8922,  0.9449,  0.0991,  0.1003,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7706,  0.7666,  0.0563,  0.0696,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 92 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8922,  0.9449,  0.0991,  0.1003,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7706,  0.7666,  0.0563,  0.0696,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8848,  0.9524,  0.0743,  0.0752,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7748,  0.7718,  0.0422,  0.0522,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 93 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8848,  0.9524,  0.0743,  0.0752,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  7.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7748,  0.7718,  0.0422,  0.0522,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8717,  0.9506,  0.1558, -0.0436,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7780,  0.7758,  0.0317,  0.0392,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 94 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8717,  0.9506,  0.1558, -0.0436,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7780,  0.7758,  0.0317,  0.0392,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8600,  0.9473,  0.1168, -0.0327,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7804,  0.7787,  0.0237,  0.0294,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 95 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8600,  0.9473,  0.1168, -0.0327,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7804,  0.7787,  0.0237,  0.0294,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8513,  0.9449,  0.0876, -0.0245,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7822,  0.7809,  0.0178,  0.0220,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 96 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8513,  0.9449,  0.0876, -0.0245,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7822,  0.7809,  0.0178,  0.0220,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8447,  0.9430,  0.0657, -0.0184,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7835,  0.7825,  0.0134,  0.0165,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 97 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8447,  0.9430,  0.0657, -0.0184,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7835,  0.7825,  0.0134,  0.0165,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8398,  0.9416,  0.0493, -0.0138,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7845,  0.7838,  0.0100,  0.0124,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 98 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8398,  0.9416,  0.0493, -0.0138,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 7.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  7.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7845,  0.7838,  0.0100,  0.0124,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8286,  0.9331,  0.1370, -0.1103,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7852,  0.7847,  0.0075,  0.0093,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 99 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8286,  0.9331,  0.1370, -0.1103,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7852,  0.7847,  0.0075,  0.0093,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8183,  0.9248,  0.1027, -0.0828,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7858,  0.7854,  0.0056,  0.0070,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "Step 100 of Episode 4\n",
      "observation continuous agent0 = tensor([-0.8183,  0.9248,  0.1027, -0.0828,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent0 = tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "observation continuous agent1 = tensor([ 0.7858,  0.7854,  0.0056,  0.0070,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "observation discrete agent1 = tensor([ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "action from problem solver = tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.7000,  0.7000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "next obs all agents = [tensor([-0.8106,  0.9186,  0.0770, -0.0621,  0.0000, -0.8000,  0.7000,  0.7000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64), tensor([ 0.7862,  0.7859,  0.0042,  0.0052,  0.0000, -0.8000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)]\n",
      "-----------------\n",
      "done status for agent 0: False\n",
      "done status for agent 1: False\n",
      "Success percentage of each agent at the end of episode 4:\n",
      "Agent 0: 0.0%\n",
      "Agent 1: 0.0%\n",
      "Overall success percentage up to episode 4: 0.0%\n",
      "Temporary case base saved successfully.\n",
      "Case base saved successfully.\n",
      "Temporary case base saved successfully.\n",
      "Case base saved successfully.\n",
      "It took: 5.451704502105713s for 500 steps across 5 episodes of 1 parallel environments on device cuda for navigation_comm scenario.\n",
      "Percentage of successful episodes: 0.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import json\n",
    "import ast\n",
    "from vmas import make_env\n",
    "from vmas.simulator.core import Agent\n",
    "from vmas.simulator.scenario import BaseScenario\n",
    "from typing import Union\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "from IPython.display import HTML, display as ipython_display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.spaces import Discrete \n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, env, agent_id, alpha=0.1, gamma=0.99, epsilon=0.2, communication_weight=0.5):\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.q_table = {}\n",
    "        self.env = env\n",
    "        self.agent_id = agent_id\n",
    "        self.communication_weight = communication_weight  # Weight parameter for incorporating messages\n",
    "\n",
    "    def get_action(self, agent, env, agent_id, agent_obs):\n",
    "        agent_obs_cpu = agent_obs[:6].cpu().numpy()  # Transfer only the required slice to CPU\n",
    "        agent_obs = tuple(np.round(agent_obs_cpu, decimals=5))  # Round the observation\n",
    "\n",
    "        if agent_obs not in self.q_table:\n",
    "            self.q_table[agent_obs] = np.zeros(self.env.action_space[self.agent_id].n)\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Select a random action\n",
    "            action = np.random.randint(env.action_space[self.agent_id].n)\n",
    "        else:\n",
    "            # Select the action with the highest Q-value\n",
    "            action = np.argmax(self.q_table[agent_obs])\n",
    "        \n",
    "        return (action,)  # Return as a tuple\n",
    "\n",
    "    def update_q_table(self, obs, action, reward, next_obs):\n",
    "        obs_key = tuple(np.round(obs.cpu().numpy(), decimals=5))  # Only transfer to CPU when necessary\n",
    "        next_obs_key = tuple(np.round(next_obs.cpu().numpy(), decimals=5))\n",
    "        action = int(action.item())  # Convert tensor to Python scalar\n",
    "\n",
    "        if isinstance(self.env.action_space[self.agent_id], Discrete):\n",
    "            action_space_size = self.env.action_space[self.agent_id].n\n",
    "        else:\n",
    "            raise ValueError(\"This Q-learning implementation requires a discrete action space.\")\n",
    "\n",
    "        if obs_key not in self.q_table:\n",
    "            self.q_table[obs_key] = np.zeros(action_space_size)\n",
    "\n",
    "        if next_obs_key not in self.q_table:\n",
    "            self.q_table[next_obs_key] = np.zeros(action_space_size)\n",
    "\n",
    "        best_next_action = np.argmax(self.q_table[next_obs_key])\n",
    "        td_target = reward + self.gamma * self.q_table[next_obs_key][best_next_action]\n",
    "\n",
    "        td_error = td_target - self.q_table[obs_key][action]\n",
    "        self.q_table[obs_key][action] += self.alpha * td_error\n",
    "    \n",
    "    def print_q_table(self):\n",
    "        print(f\"Q-table for Agent {self.agent_id}:\")\n",
    "        for state, actions in self.q_table.items():\n",
    "            print(f\"  State: {state}\")\n",
    "            for action, q_value in enumerate(actions):\n",
    "                print(f\"    Action: {action}, Q-value: {q_value:.5f}\")\n",
    "        print(f\"End of Q-table for Agent {self.agent_id}\\n\")\n",
    "\n",
    "class Case:\n",
    "    added_states = set()  # Class attribute to store states already added to the case base\n",
    "\n",
    "    def __init__(self, problem, solution, trust_value=1):\n",
    "        self.problem = problem if isinstance(problem, list) else ast.literal_eval(problem)  # Convert problem to numpy array\n",
    "        self.solution = solution\n",
    "        self.trust_value = trust_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)\n",
    "        state2 = np.atleast_1d(state2)\n",
    "        CNDMaxDist = 6  # Maximum distance between two nodes in the CND based on EOPRA reference\n",
    "        v = state1.size  # Total number of objects the agent can perceive\n",
    "        DistQ = np.sum([Case.dist_q(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def dist_q(X1, X2):\n",
    "        return np.min(np.abs(X1 - X2))\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(agent, env, state, case_base, threshold=0.1):\n",
    "\n",
    "        # Convert the state to numpy if it's a tensor\n",
    "        if isinstance(state, torch.Tensor):\n",
    "            state = state.cpu().numpy()\n",
    "\n",
    "        # Slice the physical observations\n",
    "        physical_obs = state[:6]\n",
    "\n",
    "        if not agent.silent:\n",
    "            comm_obs = state[6:]\n",
    "            # Convert comm_obs to a numpy array if it is a tensor\n",
    "            if isinstance(comm_obs, torch.Tensor):\n",
    "                comm_obs = comm_obs.cpu().numpy()\n",
    "\n",
    "        # print(f\"physical_obs = {physical_obs}\")\n",
    "\n",
    "        # Ensure the state is in a list format to avoid issues with ast.literal_eval\n",
    "        state_list = state.tolist() if isinstance(state, np.ndarray) else state\n",
    "        state_str = json.dumps(state_list)  # Convert list to a JSON string for ast.literal_eval\n",
    "\n",
    "        # Use ast.literal_eval safely to convert the string back to a list\n",
    "        state = ast.literal_eval(state_str)\n",
    "\n",
    "        similarities = {}\n",
    "        for case in case_base:\n",
    "            problem_numeric = np.array(case.problem, dtype=float)\n",
    "            state_numeric = np.array(state, dtype=float)\n",
    "            \n",
    "            # print(f\"state received = {state_numeric}\")\n",
    "            # print(f\"case received = {problem_numeric}\")\n",
    "            # print(\"---------\")\n",
    "           \n",
    "            similarities[case] = Case.sim_q(state_numeric, problem_numeric)  # Compare state with the problem part of the case\n",
    "\n",
    "        sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if sorted_similarities:\n",
    "            most_similar_case = sorted_similarities[0][0] if sorted_similarities[0][1] >= threshold else None\n",
    "        else:\n",
    "            most_similar_case = None\n",
    "\n",
    "        return most_similar_case\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(c, temporary_case_base):\n",
    "        temporary_case_base.append(c)\n",
    "\n",
    "    @staticmethod\n",
    "    def revise(case_base, temporary_case_base, successfull_task):\n",
    "        for case in temporary_case_base:\n",
    "            if successfull_task and case in case_base:\n",
    "                case.trust_value += 0.1  # Increment trust value if the episode ended successfully and the case is in the case base\n",
    "            elif not successfull_task and case in case_base:\n",
    "                case.trust_value -= 0.1  # Decrement trust value if the episode ended unsuccessfully and the case is in the case base\n",
    "            case.trust_value = max(0, min(case.trust_value, 1))  # Ensure trust value is within [0, 1]\n",
    "\n",
    "    @staticmethod\n",
    "    def retain(case_base, temporary_case_base, successfull_task, threshold=0.7):\n",
    "        if successfull_task:\n",
    "            # Iterate through the temporary case base to find the last occurrence of each unique state\n",
    "            for case in reversed(temporary_case_base):\n",
    "                state = tuple(np.atleast_1d(case.problem))\n",
    "                # Check if the state is already in the case base or has been added previously\n",
    "                if state not in Case.added_states:\n",
    "                    # Add the case to the case base if the state is new\n",
    "                    case_base.append(case)\n",
    "                    Case.added_states.add(state)\n",
    "                else:\n",
    "                    # Find the index of the existing case in the case base\n",
    "                    existing_index = next((i for i, c in enumerate(case_base) if tuple(np.atleast_1d(c.problem)) == state), None)\n",
    "                    if existing_index is not None:\n",
    "                        # Get the existing case from the case base\n",
    "                        existing_case = case_base[existing_index]\n",
    "                        # Update the trust value of the existing case with the new value from the revise step\n",
    "                        existing_case.trust_value = case.trust_value\n",
    "\n",
    "        # Filter case_base based on trust_value\n",
    "        case_base = [case for case in case_base if case.trust_value >= threshold]\n",
    "        return case_base\n",
    "\n",
    "\n",
    "class QCBRLVmasRunner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        render: bool,\n",
    "        num_envs: int,\n",
    "        num_episodes: int,\n",
    "        max_steps_per_episode: int,\n",
    "        device: str,\n",
    "        scenario: Union[str, BaseScenario],\n",
    "        continuous_actions: bool,\n",
    "        random_action: bool,\n",
    "        n_agents: int,\n",
    "        obs_discrete: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.render = render\n",
    "        self.num_envs = num_envs\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "        self.device = device\n",
    "        self.scenario = scenario\n",
    "        self.continuous_actions = continuous_actions\n",
    "        self.random_action = random_action\n",
    "        self.obs_discrete = obs_discrete\n",
    "        self.kwargs = kwargs\n",
    "        self.frame_list = []  \n",
    "        self.problem_solver_agents = []\n",
    "        self.rewards_history = []  \n",
    "        self.action_counts = {i: {} for i in range(n_agents)}  \n",
    "        self.agent_rewards_history = {i: [] for i in range(n_agents)}\n",
    "        self.successful_episodes = 0\n",
    "        self.case_base = {i: [] for i in range(n_agents)}  # Separate case base for each agent\n",
    "        self.temporary_case_base = {i: [] for i in range(n_agents)}  # Separate temporary case base for each agent\n",
    "\n",
    "    def discretize(self, data, bins):\n",
    "        bins = np.array(bins)\n",
    "        if np.isscalar(data):\n",
    "            data = np.array([data])\n",
    "        bin_indices = np.digitize(data, bins) - 1  # np.digitize returns indices starting from 1\n",
    "        bin_indices = np.clip(bin_indices, 0, len(bins) - 1)  # Ensure indices are within the valid range\n",
    "        bin_values = bins[bin_indices]\n",
    "        bin_values = np.round(bin_values, 2)  # Round the bin values to two decimal places\n",
    "        return bin_indices, bin_values\n",
    "\n",
    "    def discretize_tensor_slice(self, tensor_slice, bins):\n",
    "        tensor_np = tensor_slice.cpu().numpy()  # Convert to numpy for easier handling\n",
    "        indices, values = self.discretize(tensor_np, bins)\n",
    "        indices = torch.tensor(indices, device=tensor_slice.device)\n",
    "        values = torch.tensor(values, device=tensor_slice.device)\n",
    "        return indices, values\n",
    "\n",
    "    def _get_deterministic_obs(self, env, observation):\n",
    "        pos_bins = np.linspace(-1, 1, num=21)\n",
    "        vel_bins = np.linspace(0, 0, num=21)\n",
    "        lidar_bins = np.linspace(0, 1, num=11)\n",
    "\n",
    "        pos = observation[0:2]\n",
    "        vel = observation[2:4]\n",
    "        goal_pose = observation[4:6]\n",
    "        comms_data = observation[6:13]\n",
    "        sensor_data = observation[13:]\n",
    "\n",
    "        discrete_pos_indices, discrete_pos_values = self.discretize_tensor_slice(pos, pos_bins)\n",
    "        discrete_vel_indices, discrete_vel_values = self.discretize_tensor_slice(vel, vel_bins)\n",
    "        discrete_goal_pose_indices, discrete_goal_pose_values = self.discretize_tensor_slice(goal_pose, pos_bins)\n",
    "        discrete_sensor_data_indices, discrete_sensor_data_values = self.discretize_tensor_slice(sensor_data, lidar_bins)\n",
    "\n",
    "        concatenated_tensor_values = torch.cat(\n",
    "            [discrete_pos_values, discrete_vel_values, discrete_goal_pose_values, comms_data, discrete_sensor_data_values],\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        return concatenated_tensor_values\n",
    "\n",
    "    def _get_deterministic_action(self, agent: Agent, env, agent_id, agent_obs):\n",
    "        if self.continuous_actions:\n",
    "            if agent.silent:\n",
    "                action = torch.tensor([[-1, 0.5]], device=env.device)\n",
    "            else:\n",
    "                if agent_id == 0:\n",
    "                    action = torch.tensor([[-1, 0.5, 2]], device=env.device)\n",
    "                else:\n",
    "                    action = torch.tensor([[-1, 0.5, 1]], device=env.device)\n",
    "        else:\n",
    "            physical_obs = agent_obs[0:6]\n",
    "\n",
    "            if not agent.silent:\n",
    "                comm_obs = agent_obs[6:]\n",
    "            \n",
    "            physical_action = self.problem_solver_agents[agent_id].get_action(agent, env, agent_id, physical_obs)\n",
    "            physical_action_tensor = torch.tensor(physical_action, device=self.device)\n",
    "\n",
    "            if agent.silent:\n",
    "                action = physical_action_tensor\n",
    "            else:\n",
    "                physical_obs_tensor = torch.tensor(physical_obs, device=self.device)\n",
    "                comm_action_tensor = torch.cat([physical_obs_tensor, physical_action_tensor], dim=0) \n",
    "\n",
    "                zero_tensor = torch.zeros(6, dtype=torch.float64, device=self.device)\n",
    "                first_row = torch.cat((physical_action_tensor, zero_tensor))\n",
    "                action = torch.stack((first_row, comm_action_tensor)).unsqueeze(0)\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def save_case_base(self, agent_id):\n",
    "        filename = f\"case_base_{agent_id}.json\"\n",
    "        case_base_data = []\n",
    "        for case in self.case_base[agent_id]:\n",
    "            problem = case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem\n",
    "            \n",
    "            if torch.is_tensor(case.solution):\n",
    "                solution = case.solution.tolist() if case.solution.numel() > 1 else int(case.solution.item())\n",
    "            else:\n",
    "                solution = int(case.solution)\n",
    "            \n",
    "            if torch.is_tensor(case.trust_value):\n",
    "                trust_value = case.trust_value.tolist() if case.trust_value.numel() > 1 else float(case.trust_value.item())\n",
    "            else:\n",
    "                trust_value = float(case.trust_value)\n",
    "            \n",
    "            case_base_data.append({\n",
    "                \"problem\": problem,\n",
    "                \"solution\": solution,\n",
    "                \"trust_value\": trust_value\n",
    "            })\n",
    "        \n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "        print(\"Case base saved successfully.\")\n",
    "\n",
    "\n",
    "    def save_case_base_temporary(self, agent_id):\n",
    "        filename = f\"case_base_temporary_{agent_id}.json\"\n",
    "        case_base_data = []\n",
    "        for case in self.temporary_case_base[agent_id]:\n",
    "            problem = case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem\n",
    "            \n",
    "            if torch.is_tensor(case.solution):\n",
    "                solution = case.solution.tolist() if case.solution.numel() > 1 else int(case.solution.item())\n",
    "            else:\n",
    "                solution = int(case.solution)\n",
    "            \n",
    "            if torch.is_tensor(case.trust_value):\n",
    "                trust_value = case.trust_value.tolist() if case.trust_value.numel() > 1 else float(case.trust_value.item())\n",
    "            else:\n",
    "                trust_value = float(case.trust_value)\n",
    "            \n",
    "            case_base_data.append({\n",
    "                \"problem\": problem,\n",
    "                \"solution\": solution,\n",
    "                \"trust_value\": trust_value\n",
    "            })\n",
    "        \n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "        print(\"Temporary case base saved successfully.\")\n",
    "\n",
    "        \n",
    "    def load_case_base(self, agent_id):\n",
    "        filename = f\"case_base_{agent_id}.json\"\n",
    "        try:\n",
    "            with open(filename, 'r') as file:\n",
    "                case_base_data = json.load(file)\n",
    "            self.case_base[agent_id] = [Case(problem=np.array(case[\"problem\"]) if isinstance(case[\"problem\"], list) else case[\"problem\"],\n",
    "                                            solution=case[\"solution\"],\n",
    "                                            trust_value=case[\"trust_value\"]) for case in case_base_data]\n",
    "        except FileNotFoundError:\n",
    "            self.case_base[agent_id] = []\n",
    "\n",
    "\n",
    "    def generate_gif(self, scenario_name):\n",
    "        fps = 25\n",
    "        clip = ImageSequenceClip(self.frame_list, fps=fps)\n",
    "        clip.write_gif(f'{scenario_name}.gif', fps=fps)\n",
    "        return HTML(f'<img src=\"{scenario_name}.gif\">')\n",
    "\n",
    "    def plot_action_distribution(self):\n",
    "        num_agents = len(self.action_counts)\n",
    "\n",
    "        for agent_id, counts in self.action_counts.items():\n",
    "            unique_actions, action_counts = np.unique(list(counts.values()), return_counts=True)\n",
    "            action_dict = dict(zip(unique_actions, action_counts))\n",
    "            plt.bar(action_dict.keys(), action_dict.values(), label=f'Agent {agent_id}', alpha=0.7)\n",
    "\n",
    "        plt.xlabel('Action')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Action Distribution for Each Agent')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_rewards_history(self):\n",
    "        num_agents = len(self.agent_rewards_history)\n",
    "\n",
    "        for agent_id, rewards in self.agent_rewards_history.items():\n",
    "            plt.plot(range(1, self.num_episodes + 1), rewards, label=f'Agent {agent_id}')\n",
    "\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Total Reward per Episode for Each Agent')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def run_vmas_env(self):\n",
    "        scenario_name = self.scenario if isinstance(self.scenario, str) else self.scenario.__class__.__name__\n",
    "\n",
    "        env = make_env(\n",
    "            scenario=self.scenario,\n",
    "            num_envs=self.num_envs,\n",
    "            device=self.device,\n",
    "            continuous_actions=self.continuous_actions,\n",
    "            **self.kwargs\n",
    "        )\n",
    "        \n",
    "        for agent_id, agent in enumerate(env.agents):\n",
    "            self.problem_solver_agents.append(ProblemSolver(env, agent_id, communication_weight=0.5))\n",
    "\n",
    "        init_time = time.time()\n",
    "        total_steps = 0\n",
    "        is_episode_success = False\n",
    "\n",
    "        for episode in range(self.num_episodes):\n",
    "            print(f\"Episode {episode}\")\n",
    "            obs_cont = env.reset()\n",
    "\n",
    "            done = torch.tensor([False] * self.num_envs, device=self.device)\n",
    "            step = 0\n",
    "            \n",
    "            # Initialize rewards for each agent for the current episode\n",
    "            episode_rewards = {i: 0 for i in range(len(self.problem_solver_agents))}\n",
    "            \n",
    "            self.temporary_case_base = {i: [] for i in range(len(env.agents))}\n",
    "            \n",
    "            while not torch.all(done).item() and step < self.max_steps_per_episode:\n",
    "                step += 1\n",
    "                total_steps += 1\n",
    "                print(f\"Step {step} of Episode {episode}\")\n",
    "\n",
    "                actions = []\n",
    "               \n",
    "\n",
    "                for i, agent in enumerate(env.agents):\n",
    "                    if self.obs_discrete:\n",
    "                        discrete_obs = self._get_deterministic_obs(env, obs_cont[i])\n",
    "                        \n",
    "                    # print(f\"observation continuous agent{i} = {obs_cont[i]}\")\n",
    "                    # print(f\"observation discrete agent{i} = {discrete_obs}\")\n",
    "\n",
    "                    case = Case.retrieve(agent, env, discrete_obs[0:6], self.case_base[i], threshold=0.1)\n",
    "                    \n",
    "                    if case:\n",
    "                        action = case.solution\n",
    "                        Case.reuse(case, self.temporary_case_base[i])\n",
    "                        print(f\"action from case base = {action}\")\n",
    "                    else:\n",
    "                        if self.random_action:\n",
    "                            action = env.get_random_action(agent)\n",
    "                        else:\n",
    "                            action = self._get_deterministic_action(agent, env, i, discrete_obs)\n",
    "                        print(f\"action from problem solver = {action}\")\n",
    "\n",
    "                    problem = discrete_obs[0:6].cpu().numpy().tolist()\n",
    "                    new_case = Case(problem, action)\n",
    "                    self.temporary_case_base[i].append(new_case)\n",
    "\n",
    "                    actions.append(action)\n",
    "                \n",
    "                next_obs_cont, rews, dones, info = env.step(actions)\n",
    "                # print(f\"next obs all agents = {next_obs_cont}\")\n",
    "                # print(f\"reward all agents = {rews}\")\n",
    "                \n",
    "                done = dones\n",
    "                # print(f\"dones status for all agents = {done}\")\n",
    "                # print(\"--------------------\")\n",
    "\n",
    "                for i, agent in enumerate(env.agents):\n",
    "                    if self.obs_discrete:\n",
    "                        discrete_next_obs = self._get_deterministic_obs(env, next_obs_cont[i])\n",
    "                        \n",
    "                    physical_obs_for_update = discrete_obs[0:6]\n",
    "                    physical_nextobs_for_update = discrete_next_obs[0:6]\n",
    "                    physical_actions_for_update = actions[i][0, 0, 0].unsqueeze(0)\n",
    "\n",
    "                    self.problem_solver_agents[i].update_q_table(physical_obs_for_update, physical_actions_for_update, rews[i].item(), physical_nextobs_for_update)\n",
    "\n",
    "                    # Accumulate rewards for each agent within the episode\n",
    "                    episode_rewards[i] += rews[i].item()\n",
    "\n",
    "                obs_cont = next_obs_cont\n",
    "\n",
    "                if self.render:\n",
    "                    frame = env.render(\n",
    "                        mode=\"rgb_array\",\n",
    "                        agent_index_focus=None,\n",
    "                    )\n",
    "                    self.frame_list.append(frame)\n",
    "\n",
    "                print(\"-----------------\")\n",
    "            # Update rewards history after each episode\n",
    "            for agent_id, total_reward in episode_rewards.items():\n",
    "                self.agent_rewards_history[agent_id].append(total_reward)\n",
    "\n",
    "            # Check if the episode was successful\n",
    "            if torch.all(done):\n",
    "                self.successful_episodes += 1\n",
    "                is_episode_success = True\n",
    "\n",
    "            for i in range(len(env.agents)):\n",
    "                print(f\"done status for agent {i}: {done[0][i]}\")\n",
    "                \n",
    "                # Case.revise(self.case_base[i], self.temporary_case_base[i], torch.any(done).item())\n",
    "                # self.case_base[i] = Case.retain(\n",
    "                #     self.case_base[i], self.temporary_case_base[i], torch.any(done).item()\n",
    "                # )\n",
    "\n",
    "                Case.revise(self.case_base[i], self.temporary_case_base[i], done[0][i])\n",
    "                self.case_base[i] = Case.retain(\n",
    "                    self.case_base[i], self.temporary_case_base[i], done[0][i]\n",
    "                )\n",
    "\n",
    "                # for case in self.temporary_case_base[i]:\n",
    "                #     print(f\"Step {step} -- Problem Stored in Temp CB: {case.problem}, Solution Stored in Temp CB: {case.solution}\")\n",
    "\n",
    "                # for case in self.case_base[i]:\n",
    "                #     print(f\"Step {step} -- Problem Stored in CB: {case.problem}, Solution Stored in CB: {case.solution}\")\n",
    "\n",
    "\n",
    "            torch.cuda.empty_cache()  # Free up unused memory\n",
    "\n",
    "            # Calculate success percentages\n",
    "            success_percents = [torch.mean(done[:, i].float()).item() * 100 for i in range(len(env.agents))]\n",
    "            overall_success_percent = (self.successful_episodes / (episode + 1)) * 100\n",
    "\n",
    "            print(f\"Success percentage of each agent at the end of episode {episode}:\")\n",
    "            for i, percent in enumerate(success_percents):\n",
    "                print(f\"Agent {i}: {percent}%\")\n",
    "\n",
    "            print(f\"Overall success percentage up to episode {episode}: {overall_success_percent}%\")\n",
    "\n",
    "        \n",
    "        for agent_id, agent in enumerate(env.agents):\n",
    "            self.save_case_base_temporary(agent_id)  # Save temporary case base after training\n",
    "            self.save_case_base(agent_id)  # Save case base after training\n",
    "\n",
    "        total_time = time.time() - init_time\n",
    "        print(\n",
    "            f\"It took: {total_time}s for {total_steps} steps across {self.num_episodes} episodes of {self.num_envs} parallel environments on device {self.device} \"\n",
    "            f\"for {scenario_name} scenario.\"\n",
    "        )\n",
    "\n",
    "        # success_percentage = (self.successful_episodes / self.num_episodes) * 100\n",
    "        # print(f\"Percentage of successful episodes: {success_percentage}%\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scenario_name = \"navigation_comm\"\n",
    "    use_cuda = True\n",
    "\n",
    "    env_runner = QCBRLVmasRunner( \n",
    "        render=True,\n",
    "        num_envs=1,\n",
    "        num_episodes=30,\n",
    "        max_steps_per_episode=300,\n",
    "        device = torch.device(\"cuda\" if use_cuda else \"cpu\"),\n",
    "        scenario=scenario_name,\n",
    "        continuous_actions=False,\n",
    "        random_action=False,\n",
    "        n_agents=2,\n",
    "        obs_discrete=True,\n",
    "        agents_with_same_goal=2,\n",
    "        collisions=False,\n",
    "        shared_rew=False,\n",
    "    )\n",
    "\n",
    "    env_runner.run_vmas_env()\n",
    "    # for agent in env_runner.problem_solver_agents:\n",
    "    #     agent.print_q_table()\n",
    "    env_runner.plot_rewards_history()\n",
    "\n",
    "    # ipython_display(env_runner.generate_gif(scenario_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
