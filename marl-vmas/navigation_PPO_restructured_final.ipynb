{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardie85/PHD/Research/code/.venv/lib/python3.10/site-packages/vmas/scenarios/navigation_comm.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  comms.append(torch.tensor(other_agent.state.c).unsqueeze(0).unsqueeze(0))  # Ensure it's 3D\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs-shape= torch.Size([1, 1, 60, 6])\n",
      "comm-obs-shape= torch.Size([2, 1, 1, 60, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 4 and 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 272\u001b[0m\n\u001b[1;32m    269\u001b[0m     ppo\u001b[38;5;241m.\u001b[39mevaluate(max_steps)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 272\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 263\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    260\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fork \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# Initialize PPO\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m ppo \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscenario_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_agents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_per_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Train PPO\u001b[39;00m\n\u001b[1;32m    266\u001b[0m ppo\u001b[38;5;241m.\u001b[39mtrain(n_iters, minibatch_size, num_epochs, max_grad_norm, lr)\n",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[0;34m(self, scenario_name, n_agents, max_steps, frames_per_batch, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Setup environment\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Setup policy and value networks\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_policy_network()\n",
      "Cell \u001b[0;32mIn[1], line 42\u001b[0m, in \u001b[0;36mPPO.setup_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     41\u001b[0m     num_vmas_envs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes_per_batch \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_steps\n\u001b[0;32m---> 42\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43mVmasEnv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscenario\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscenario_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_vmas_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontinuous_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_agents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_agents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     env \u001b[38;5;241m=\u001b[39m TransformedEnv(\n\u001b[1;32m     51\u001b[0m         env,\n\u001b[1;32m     52\u001b[0m         RewardSum(in_keys\u001b[38;5;241m=\u001b[39m[env\u001b[38;5;241m.\u001b[39mreward_key], out_keys\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magents\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[1;32m     53\u001b[0m     )\n\u001b[1;32m     54\u001b[0m     check_env_specs(env)\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torchrl/envs/common.py:158\u001b[0m, in \u001b[0;36m_EnvPostInit.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 158\u001b[0m     instance: EnvBase \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# we create the done spec by adding a done/terminated entry if one is missing\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     instance\u001b[38;5;241m.\u001b[39m_create_done_specs()\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torchrl/envs/libs/vmas.py:762\u001b[0m, in \u001b[0;36mVmasEnv.__init__\u001b[0;34m(self, scenario, num_envs, continuous_actions, max_steps, categorical_actions, seed, group_map, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _has_vmas:\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    759\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvmas python package was not found. Please install this dependency. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    760\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMore info: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgit_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    761\u001b[0m     )\n\u001b[0;32m--> 762\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscenario\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinuous_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontinuous_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torchrl/envs/libs/vmas.py:245\u001b[0m, in \u001b[0;36mVmasWrapper.__init__\u001b[0;34m(self, env, categorical_actions, group_map, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_map \u001b[38;5;241m=\u001b[39m group_map\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_actions \u001b[38;5;241m=\u001b[39m categorical_actions\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_done_after_reset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torchrl/envs/common.py:2819\u001b[0m, in \u001b[0;36m_EnvWrapper.__init__\u001b[0;34m(self, dtype, device, batch_size, allow_done_after_reset, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2817\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m   2818\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_kwargs(kwargs)\n\u001b[0;32m-> 2819\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_env\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# writes the self._env attribute\u001b[39;00m\n\u001b[1;32m   2820\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_specs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env)  \u001b[38;5;66;03m# writes the self._env attribute\u001b[39;00m\n\u001b[1;32m   2821\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torchrl/envs/libs/vmas.py:795\u001b[0m, in \u001b[0;36mVmasEnv._build_env\u001b[0;34m(self, scenario, num_envs, continuous_actions, max_steps, seed, **scenario_kwargs)\u001b[0m\n\u001b[1;32m    791\u001b[0m from_pixels \u001b[38;5;241m=\u001b[39m scenario_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_pixels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    792\u001b[0m pixels_only \u001b[38;5;241m=\u001b[39m scenario_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixels_only\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_build_env(\n\u001b[0;32m--> 795\u001b[0m     env\u001b[38;5;241m=\u001b[39m\u001b[43mvmas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_env\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscenario\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontinuous_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontinuous_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscenario_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    805\u001b[0m     pixels_only\u001b[38;5;241m=\u001b[39mpixels_only,\n\u001b[1;32m    806\u001b[0m     from_pixels\u001b[38;5;241m=\u001b[39mfrom_pixels,\n\u001b[1;32m    807\u001b[0m )\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/vmas/make_env.py:75\u001b[0m, in \u001b[0;36mmake_env\u001b[0;34m(scenario, num_envs, device, continuous_actions, wrapper, max_steps, seed, dict_spaces, multidiscrete_actions, clamp_actions, grad_enabled, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         scenario \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m     scenario \u001b[38;5;241m=\u001b[39m scenarios\u001b[38;5;241m.\u001b[39mload(scenario)\u001b[38;5;241m.\u001b[39mScenario()\n\u001b[0;32m---> 75\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mEnvironment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinuous_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontinuous_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdict_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultidiscrete_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultidiscrete_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclamp_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclamp_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_enabled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\u001b[38;5;241m.\u001b[39mget_env(env) \u001b[38;5;28;01mif\u001b[39;00m wrapper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m env\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/vmas/simulator/environment/environment.py:67\u001b[0m, in \u001b[0;36mEnvironment.__init__\u001b[0;34m(self, scenario, num_envs, device, max_steps, continuous_actions, seed, dict_spaces, multidiscrete_actions, clamp_actions, grad_enabled, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclamp_action \u001b[38;5;241m=\u001b[39m clamp_actions\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_enabled \u001b[38;5;241m=\u001b[39m grad_enabled\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# configure spaces\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultidiscrete_actions \u001b[38;5;241m=\u001b[39m multidiscrete_actions\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/vmas/simulator/environment/environment.py:98\u001b[0m, in \u001b[0;36mEnvironment.reset\u001b[0;34m(self, seed, return_observations, return_info, return_dones)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscenario\u001b[38;5;241m.\u001b[39menv_reset_world_at(env_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 98\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_from_scenario\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_observations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_observations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_infos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_rewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_dones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dones\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/vmas/simulator/environment/environment.py:162\u001b[0m, in \u001b[0;36mEnvironment.get_from_scenario\u001b[0;34m(self, get_observations, get_rewards, get_infos, get_dones, dict_agent_names)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_observations:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents:\n\u001b[1;32m    161\u001b[0m         observation \u001b[38;5;241m=\u001b[39m TorchUtils\u001b[38;5;241m.\u001b[39mrecursive_clone(\n\u001b[0;32m--> 162\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscenario\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m         )\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m dict_agent_names:\n\u001b[1;32m    165\u001b[0m             obs\u001b[38;5;241m.\u001b[39mupdate({agent\u001b[38;5;241m.\u001b[39mname: observation})\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/vmas/scenarios/navigation_comm.py:275\u001b[0m, in \u001b[0;36mScenario.observation\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs-shape= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomm-obs-shape= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomms_obs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)    \n\u001b[0;32m--> 275\u001b[0m     combined_obs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomms_obs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# Verify combined_obs size\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined-obs-value= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombined_obs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 4 and 5"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import multiprocessing\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor, TanhNormal\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "import os\n",
    "import PIL.Image as PILImage\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, scenario_name, n_agents, max_steps, frames_per_batch, device):\n",
    "        self.scenario_name = scenario_name\n",
    "        self.n_agents = n_agents\n",
    "        self.max_steps = max_steps\n",
    "        self.frames_per_batch = frames_per_batch\n",
    "        self.device = device\n",
    "\n",
    "        # Setup environment\n",
    "        self.env = self.setup_environment()\n",
    "\n",
    "        # Setup policy and value networks\n",
    "        self.policy = self.setup_policy_network()\n",
    "        self.critic = self.setup_value_network()\n",
    "\n",
    "        # PPO Loss setup\n",
    "        self.loss_module = self.setup_ppo_loss()\n",
    "\n",
    "    def setup_environment(self):\n",
    "        num_vmas_envs = self.frames_per_batch // self.max_steps\n",
    "        env = VmasEnv(\n",
    "            scenario=self.scenario_name,\n",
    "            num_envs=num_vmas_envs,\n",
    "            continuous_actions=True,\n",
    "            max_steps=self.max_steps,\n",
    "            device=self.device,\n",
    "            n_agents=self.n_agents,\n",
    "        )\n",
    "        env = TransformedEnv(\n",
    "            env,\n",
    "            RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")])\n",
    "        )\n",
    "        check_env_specs(env)\n",
    "        return env\n",
    "\n",
    "    def setup_policy_network(self):\n",
    "        policy_net = torch.nn.Sequential(\n",
    "            MultiAgentMLP(\n",
    "                n_agent_inputs=self.env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "                n_agent_outputs=2 * self.env.action_spec.shape[-1],\n",
    "                n_agents=self.env.n_agents,\n",
    "                centralised=False,\n",
    "                share_params=True,\n",
    "                device=self.device,\n",
    "                depth=2,\n",
    "                num_cells=256,\n",
    "                activation_class=torch.nn.Tanh,\n",
    "            ),\n",
    "            NormalParamExtractor(),\n",
    "        )\n",
    "\n",
    "        policy_module = TensorDictModule(\n",
    "            policy_net,\n",
    "            in_keys=[(\"agents\", \"observation\")],\n",
    "            out_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
    "        )\n",
    "\n",
    "        policy = ProbabilisticActor(\n",
    "            module=policy_module,\n",
    "            spec=self.env.unbatched_action_spec,\n",
    "            in_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
    "            out_keys=[self.env.action_key],\n",
    "            distribution_class=TanhNormal,\n",
    "            distribution_kwargs={\n",
    "                \"min\": self.env.unbatched_action_spec[self.env.action_key].space.low,\n",
    "                \"max\": self.env.unbatched_action_spec[self.env.action_key].space.high,\n",
    "            },\n",
    "            return_log_prob=True,\n",
    "            log_prob_key=(\"agents\", \"sample_log_prob\"),\n",
    "        )\n",
    "\n",
    "        return policy\n",
    "\n",
    "    def setup_value_network(self):\n",
    "        critic_net = MultiAgentMLP(\n",
    "            n_agent_inputs=self.env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "            n_agent_outputs=1,\n",
    "            n_agents=self.env.n_agents,\n",
    "            centralised=True,\n",
    "            share_params=True,\n",
    "            device=self.device,\n",
    "            depth=2,\n",
    "            num_cells=256,\n",
    "            activation_class=torch.nn.Tanh,\n",
    "        )\n",
    "\n",
    "        critic = TensorDictModule(\n",
    "            module=critic_net,\n",
    "            in_keys=[(\"agents\", \"observation\")],\n",
    "            out_keys=[(\"agents\", \"state_value\")],\n",
    "        )\n",
    "\n",
    "        return critic\n",
    "\n",
    "    def setup_ppo_loss(self, clip_epsilon=0.2, entropy_eps=1e-4, gamma=0.9, lmbda=0.9):\n",
    "        loss_module = ClipPPOLoss(\n",
    "            actor_network=self.policy,\n",
    "            critic_network=self.critic,\n",
    "            clip_epsilon=clip_epsilon,\n",
    "            entropy_coef=entropy_eps,\n",
    "            normalize_advantage=False,\n",
    "        )\n",
    "\n",
    "        loss_module.set_keys(\n",
    "            reward=self.env.reward_key,\n",
    "            action=self.env.action_key,\n",
    "            sample_log_prob=(\"agents\", \"sample_log_prob\"),\n",
    "            value=(\"agents\", \"state_value\"),\n",
    "            done=(\"agents\", \"done\"),\n",
    "            terminated=(\"agents\", \"terminated\"),\n",
    "        )\n",
    "\n",
    "        loss_module.make_value_estimator(ValueEstimators.GAE, gamma=gamma, lmbda=lmbda)\n",
    "\n",
    "        return loss_module\n",
    "\n",
    "    def train(self, n_iters, minibatch_size, num_epochs, max_grad_norm, lr):\n",
    "        collector = SyncDataCollector(\n",
    "            self.env,\n",
    "            self.policy,\n",
    "            device=self.device,\n",
    "            storing_device=self.device,\n",
    "            frames_per_batch=self.frames_per_batch,\n",
    "            total_frames=self.frames_per_batch * n_iters,\n",
    "        )\n",
    "\n",
    "        replay_buffer = ReplayBuffer(\n",
    "            storage=LazyTensorStorage(self.frames_per_batch, device=self.device),\n",
    "            sampler=SamplerWithoutReplacement(),\n",
    "            batch_size=minibatch_size,\n",
    "        )\n",
    "\n",
    "        optim = torch.optim.Adam(self.loss_module.parameters(), lr)\n",
    "\n",
    "        pbar = tqdm(total=n_iters, desc=\"episode_reward_mean = 0\")\n",
    "        episode_reward_mean_list = []\n",
    "\n",
    "        for tensordict_data in collector:\n",
    "            tensordict_data.set(\n",
    "                (\"next\", \"agents\", \"done\"),\n",
    "                tensordict_data.get((\"next\", \"done\"))\n",
    "                .unsqueeze(-1)\n",
    "                .expand(tensordict_data.get_item_shape((\"next\", self.env.reward_key))),\n",
    "            )\n",
    "            tensordict_data.set(\n",
    "                (\"next\", \"agents\", \"terminated\"),\n",
    "                tensordict_data.get((\"next\", \"terminated\"))\n",
    "                .unsqueeze(-1)\n",
    "                .expand(tensordict_data.get_item_shape((\"next\", self.env.reward_key))),\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.loss_module.value_estimator(\n",
    "                    tensordict_data,\n",
    "                    params=self.loss_module.critic_network_params,\n",
    "                    target_params=self.loss_module.target_critic_network_params,\n",
    "                )\n",
    "\n",
    "            data_view = tensordict_data.reshape(-1)\n",
    "            replay_buffer.extend(data_view)\n",
    "\n",
    "            for _ in range(num_epochs):\n",
    "                for _ in range(self.frames_per_batch // minibatch_size):\n",
    "                    subdata = replay_buffer.sample()\n",
    "                    loss_vals = self.loss_module(subdata)\n",
    "\n",
    "                    loss_value = (\n",
    "                        loss_vals[\"loss_objective\"]\n",
    "                        + loss_vals[\"loss_critic\"]\n",
    "                        + loss_vals[\"loss_entropy\"]\n",
    "                    )\n",
    "\n",
    "                    loss_value.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.loss_module.parameters(), max_grad_norm)\n",
    "                    optim.step()\n",
    "                    optim.zero_grad()\n",
    "\n",
    "            collector.update_policy_weights_()\n",
    "\n",
    "            # Logging\n",
    "            done = tensordict_data.get((\"next\", \"agents\", \"done\"))\n",
    "            episode_reward_mean = (\n",
    "                tensordict_data.get((\"next\", \"agents\", \"episode_reward\"))[done].mean().item()\n",
    "            )\n",
    "            episode_reward_mean_list.append(episode_reward_mean)\n",
    "            pbar.set_description(f\"episode_reward_mean = {episode_reward_mean}\", refresh=False)\n",
    "            pbar.update()\n",
    "\n",
    "        plt.plot(episode_reward_mean_list)\n",
    "        plt.xlabel(\"Training iterations\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(\"Episode reward mean\")\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate(self, max_steps):\n",
    "        self.env.frames = []\n",
    "\n",
    "        def rendering_callback(env, td):\n",
    "            frame = env.render(mode=\"rgb_array\")\n",
    "            self.env.frames.append(frame)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.env.rollout(\n",
    "                max_steps=max_steps,\n",
    "                policy=self.policy,\n",
    "                callback=rendering_callback,\n",
    "                auto_cast_to_device=True,\n",
    "                break_when_any_done=False,\n",
    "            )\n",
    "\n",
    "        self.env.frames = [PILImage.fromarray(frame) if isinstance(frame, np.ndarray) else frame for frame in self.env.frames]\n",
    "\n",
    "        gif_path = f\"{self.scenario_name}.gif\"\n",
    "        frames = self.env.frames\n",
    "        frames[0].save(\n",
    "            gif_path,\n",
    "            save_all=True,\n",
    "            append_images=frames[1:],\n",
    "            duration=300,\n",
    "            loop=0,\n",
    "        )\n",
    "        display(Image(gif_path))\n",
    "\n",
    "def main():\n",
    "    # Configurations\n",
    "    scenario_name = \"navigation\"\n",
    "    n_agents = 3\n",
    "    max_steps = 100\n",
    "    frames_per_batch = 6_000\n",
    "    n_iters = 10\n",
    "    minibatch_size = 400\n",
    "    num_epochs = 1\n",
    "    lr = 0.0003\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    # Device setup\n",
    "    torch.manual_seed(0)\n",
    "    is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "    device = torch.device(0) if torch.cuda.is_available() and not is_fork else torch.device(\"cpu\")\n",
    "\n",
    "    # Initialize PPO\n",
    "    ppo = PPO(scenario_name, n_agents, max_steps, frames_per_batch, device)\n",
    "\n",
    "    # Train PPO\n",
    "    ppo.train(n_iters, minibatch_size, num_epochs, max_grad_norm, lr)\n",
    "\n",
    "    # Evaluation\n",
    "    ppo.evaluate(max_steps)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
