{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Tuple(Box(-1.0, 1.0, (2,), float32), Box(-1.0, 1.0, (2,), float32), Box(-1.0, 1.0, (2,), float32))\n",
      "Observation space: Tuple(Box(-inf, inf, (18,), float32), Box(-inf, inf, (18,), float32), Box(-inf, inf, (18,), float32))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 270\u001b[0m\n\u001b[1;32m    267\u001b[0m num_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([env\u001b[38;5;241m.\u001b[39maction_space[i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space))]) \u001b[38;5;66;03m# Sum of actions for all agents\u001b[39;00m\n\u001b[1;32m    268\u001b[0m agent \u001b[38;5;241m=\u001b[39m QCBRL(num_actions, env)\n\u001b[0;32m--> 270\u001b[0m rewards, success_rate, memory_usage, gpu_memory_usage \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m agent\u001b[38;5;241m.\u001b[39mdisplay_success_rate(success_rate)\n\u001b[1;32m    272\u001b[0m agent\u001b[38;5;241m.\u001b[39mplot_rewards(rewards)\n",
      "Cell \u001b[0;32mIn[24], line 164\u001b[0m, in \u001b[0;36mQCBRL.run\u001b[0;34m(self, episodes, max_steps, alpha, gamma, epsilon, render)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m    163\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake_action(state, epsilon)\n\u001b[0;32m--> 164\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m c \u001b[38;5;241m=\u001b[39m Case(state, action)\n\u001b[1;32m    167\u001b[0m Case\u001b[38;5;241m.\u001b[39mreuse(c, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporary_case_base)\n",
      "Cell \u001b[0;32mIn[24], line 31\u001b[0m, in \u001b[0;36mVMASWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 31\u001b[0m     state, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state, reward, done, info\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torchrl/envs/common.py:1406\u001b[0m, in \u001b[0;36mEnvBase.step\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Makes a step in the environment.\u001b[39;00m\n\u001b[1;32m   1389\u001b[0m \n\u001b[1;32m   1390\u001b[0m \u001b[38;5;124;03mStep accepts a single argument, tensordict, which usually carries an 'action' key which indicates the action\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1403\u001b[0m \n\u001b[1;32m   1404\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;66;03m# sanity check\u001b[39;00m\n\u001b[0;32m-> 1406\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assert_tensordict_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1407\u001b[0m next_preset \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1409\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step(tensordict)\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torchrl/envs/common.py:2175\u001b[0m, in \u001b[0;36mEnvBase._assert_tensordict_shape\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m   2172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_assert_tensordict_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensordict: TensorDictBase) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2174\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_locked \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m!=\u001b[39m ()\n\u001b[0;32m-> 2175\u001b[0m     ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mtensordict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[1;32m   2176\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2177\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a tensordict with shape==env.batch_size, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2178\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensordict\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2179\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'batch_size'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import pynvml\n",
    "from collections import Counter\n",
    "from gym.envs.registration import register\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from gym import spaces\n",
    "\n",
    "class VMASWrapper:\n",
    "    def __init__(self, scenario_name, num_agents, max_steps):\n",
    "        self.env = VmasEnv(\n",
    "            scenario=scenario_name,\n",
    "            num_envs=1,\n",
    "            continuous_actions=True,\n",
    "            max_steps=max_steps,\n",
    "            device=\"cpu\",  # Assuming CPU for now, adjust as needed\n",
    "            n_agents=num_agents,\n",
    "        )\n",
    "\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "\n",
    "        print(\"Action space:\", self.action_space)  # Debugging\n",
    "        print(\"Observation space:\", self.observation_space)  # Debugging\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, num_actions, env, lambda_=0.9):\n",
    "        self.num_actions = num_actions  # Add this line to store num_actions\n",
    "        num_observations = sum([np.prod(space.shape) if isinstance(space, spaces.Box) else space.n for space in env.observation_space])\n",
    "        self.Q_values = np.zeros((num_observations, num_actions))\n",
    "        self.eligibility_traces = np.zeros_like(self.Q_values)\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            return np.random.choice(self.num_actions)  # Random action\n",
    "        else:\n",
    "            return np.argmax(self.Q_values[state])  # Greedy action\n",
    "\n",
    "    def update_Q(self, state, action, reward, next_state, next_action, alpha, gamma):\n",
    "        # Calculate TD error\n",
    "        td_error = reward + gamma * self.Q_values[next_state, next_action] - self.Q_values[state, action]\n",
    "        \n",
    "        # Update eligibility trace\n",
    "        self.eligibility_traces *= gamma * self.lambda_\n",
    "        self.eligibility_traces[state, action] += 1\n",
    "        \n",
    "        # Update Q-values\n",
    "        self.Q_values += alpha * td_error * self.eligibility_traces\n",
    "\n",
    "class Case:\n",
    "    added_states = set()  # Class attribute to store states already added to the case base\n",
    "\n",
    "    def __init__(self, problem, solution, trust_value=1):\n",
    "        self.problem = np.array(problem)  # Convert problem to numpy array\n",
    "        self.solution = solution\n",
    "        self.trust_value = trust_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)  # Ensure state1 is at least 1-dimensional\n",
    "        state2 = np.atleast_1d(state2)  # Ensure state2 is at least 1-dimensional\n",
    "        CNDMaxDist = 6  # Maximum distance between two nodes in the CND\n",
    "        v = state1.size  # Total number of objects the agent can perceive\n",
    "        DistQ = np.sum([Case.Dmin_phi(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def Dmin_phi(X1, X2):\n",
    "        return np.max(np.abs(X1 - X2))\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(state, case_base, threshold=0.2):\n",
    "        similarities = {}\n",
    "        for case in case_base:\n",
    "            similarity = Case.sim_q(state, case.problem)\n",
    "            if similarity >= threshold:\n",
    "                return case  # Return the first case with similarity above the threshold\n",
    "            \n",
    "        return None  # Return None if no case meets the similarity threshold\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(c, temporary_case_base):\n",
    "        temporary_case_base.append(c)\n",
    "\n",
    "    @staticmethod\n",
    "    def revise(case_base, temporary_case_base, successful_episodes):\n",
    "        for case in temporary_case_base:\n",
    "            if successful_episodes and case in case_base:\n",
    "                case.trust_value += 0.1\n",
    "            elif not successful_episodes and case in case_base:\n",
    "                case.trust_value -= 0.1\n",
    "            case.trust_value = max(0, min(case.trust_value,1))\n",
    "\n",
    "    @staticmethod\n",
    "    def retain(case_base, temporary_case_base, successful_episodes, threshold=0):\n",
    "        if successful_episodes:\n",
    "            for case in reversed(temporary_case_base):\n",
    "                state = tuple(np.atleast_1d(case.problem))\n",
    "                if state not in Case.added_states:\n",
    "                    case_base.append(case)\n",
    "                    Case.added_states.add(state)\n",
    "            \n",
    "            filtered_case_base = []\n",
    "            for case in case_base:\n",
    "                if case.trust_value >= threshold:\n",
    "                    filtered_case_base.append(case)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            return filtered_case_base\n",
    "        else:\n",
    "            return case_base\n",
    "\n",
    "class QCBRL:\n",
    "    def __init__(self, num_actions, env):\n",
    "        self.num_actions = num_actions\n",
    "        self.env = env\n",
    "        self.problem_solver = ProblemSolver(num_actions, env)  # Initialize ProblemSolver\n",
    "        self.case_base = []\n",
    "        self.temporary_case_base = []\n",
    "\n",
    "    def run(self, episodes=100, max_steps=100, alpha=0.1, gamma=0.9, epsilon=0.1, render=False):\n",
    "        rewards = []\n",
    "        memory_usage = []\n",
    "        gpu_memory_usage = []\n",
    "        successful_episodes = False\n",
    "        num_successful_episodes = 0\n",
    "\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            self.temporary_case_base = []\n",
    "            \n",
    "            for _ in range(max_steps):\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                action = self.take_action(state, epsilon)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                c = Case(state, action)\n",
    "                Case.reuse(c, self.temporary_case_base)\n",
    "\n",
    "                next_action = self.take_action(next_state, epsilon)\n",
    "                self.problem_solver.update_Q(state, action, reward, next_state, next_action, alpha, gamma)\n",
    "\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    successful_episodes = reward > 0\n",
    "                    break\n",
    "                \n",
    "            if episode_reward > 0:\n",
    "                num_successful_episodes += 1\n",
    "\n",
    "            rewards.append(episode_reward)\n",
    "            print(f\"Episode {episode + 1}, Total Reward: {episode_reward}\")\n",
    "\n",
    "            Case.revise(self.case_base, self.temporary_case_base, successful_episodes)\n",
    "            self.case_base = Case.retain(self.case_base, self.temporary_case_base, successful_episodes)\n",
    "            \n",
    "            memory_usage.append(psutil.virtual_memory().percent)\n",
    "            gpu_memory_usage.append(pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1024**2)\n",
    "\n",
    "        self.save_case_base_temporary()\n",
    "        self.save_case_base()\n",
    "\n",
    "        success_rate = (num_successful_episodes / episodes) * 100\n",
    "\n",
    "        return rewards, success_rate, memory_usage, gpu_memory_usage\n",
    "\n",
    "    def take_action(self, state, epsilon):\n",
    "        # if state in self.problem_solver.Q_values:  # Access Q_values from ProblemSolver\n",
    "        #     similar_solution = Case.retrieve(state, self.case_base)\n",
    "        #     if similar_solution is not None:\n",
    "        #         action = similar_solution.solution\n",
    "        #     else:\n",
    "        #         action = self.problem_solver.choose_action(state, epsilon)\n",
    "        # else:\n",
    "        #     # State not found in Q_values, return random action\n",
    "        #     action = np.random.choice(self.num_actions)\n",
    "        \n",
    "        action = np.random.choice(self.num_actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save_case_base_temporary(self):\n",
    "        filename = \"case_base_temporary.json\"\n",
    "        case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else int(case.problem), \n",
    "                        \"solution\": int(case.solution), \n",
    "                        \"trust_value\": int(case.trust_value)} for case in self.temporary_case_base]\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "        print(\"Temporary case base saved successfully.\")\n",
    "\n",
    "    def save_case_base(self):\n",
    "        filename = \"case_base.json\"\n",
    "        case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else int(case.problem), \n",
    "                        \"solution\": int(case.solution), \n",
    "                        \"trust_value\": int(case.trust_value)} for case in self.case_base]\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "            print(\"Case base saved successfully.\")\n",
    "        \n",
    "    def load_case_base(self):\n",
    "        filename = \"case_base.json\"\n",
    "        try:\n",
    "            with open(filename, 'r') as file:\n",
    "                case_base_data = json.load(file)\n",
    "                self.case_base = [Case(np.array(case[\"problem\"]), case[\"solution\"], case[\"trust_value\"]) for case in case_base_data]\n",
    "                print(\"Case base loaded successfully.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Case base file not found. Starting with an empty case base.\")\n",
    "\n",
    "    def display_success_rate(self, success_rate):\n",
    "        print(f\"Success rate: {success_rate}%\")\n",
    "\n",
    "    def plot_rewards(self, rewards):\n",
    "        plt.plot(rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Rewards over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show() \n",
    "\n",
    "    def plot_resources(self, memory_usage, gpu_memory_usage):\n",
    "        plt.plot(memory_usage, label='Memory (%)')\n",
    "        plt.plot(gpu_memory_usage, label='GPU Memory (MB)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Resource Usage')\n",
    "        plt.title('Resource Usage over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = VMASWrapper(scenario_name=\"navigation\", num_agents=3, max_steps=100)\n",
    "    num_actions = sum([env.action_space[i].shape[0] for i in range(len(env.action_space))]) # Sum of actions for all agents\n",
    "    agent = QCBRL(num_actions, env)\n",
    "\n",
    "    rewards, success_rate, memory_usage, gpu_memory_usage = agent.run(episodes=1000, max_steps=1000, alpha=0.1, gamma=0.9, epsilon=0.1)\n",
    "    agent.display_success_rate(success_rate)\n",
    "    agent.plot_rewards(rewards)\n",
    "    agent.plot_resources(memory_usage, gpu_memory_usage)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
