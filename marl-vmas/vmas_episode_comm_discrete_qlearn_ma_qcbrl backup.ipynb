{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 324\u001b[0m\n\u001b[1;32m    306\u001b[0m use_cuda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    308\u001b[0m env_runner \u001b[38;5;241m=\u001b[39m QCBRL(\n\u001b[1;32m    309\u001b[0m     render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m     num_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    321\u001b[0m     shared_rew\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    322\u001b[0m )\n\u001b[0;32m--> 324\u001b[0m \u001b[43menv_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_vmas_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# for agent in env_runner.problem_solver_agents:\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m#     agent.print_q_table()\u001b[39;00m\n\u001b[1;32m    327\u001b[0m env_runner\u001b[38;5;241m.\u001b[39mplot_rewards_history()\n",
      "Cell \u001b[0;32mIn[8], line 231\u001b[0m, in \u001b[0;36mQCBRL.run_vmas_env\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_episodes):\n\u001b[1;32m    230\u001b[0m     observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 231\u001b[0m     observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_deterministic_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[8], line 198\u001b[0m, in \u001b[0;36mQCBRL._get_deterministic_obs\u001b[0;34m(self, env, observation)\u001b[0m\n\u001b[1;32m    195\u001b[0m dpos_bins \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1.5\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    196\u001b[0m dvel_bins \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m--> 198\u001b[0m num_envs, num_agents, obs_size \u001b[38;5;241m=\u001b[39m \u001b[43mobservation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()\n\u001b[1;32m    199\u001b[0m discretized_obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((num_envs, num_agents, obs_size), device\u001b[38;5;241m=\u001b[39mobservation\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_envs):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import json\n",
    "import ast\n",
    "from vmas import make_env\n",
    "from vmas.simulator.core import Agent\n",
    "from vmas.simulator.scenario import BaseScenario\n",
    "from typing import Union\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "from IPython.display import HTML, display as ipython_display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.spaces import Discrete \n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, env, agent_id, alpha=0.1, gamma=0.99, epsilon=0.2, communication_weight=0.5):\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.q_table = {}\n",
    "        self.env = env\n",
    "        self.agent_id = agent_id\n",
    "        self.communication_weight = communication_weight  # Weight parameter for incorporating messages\n",
    "\n",
    "    def get_action(self, agent, env, agent_id, agent_obs):\n",
    "        agent_obs_cpu = agent_obs[:6].cpu().numpy()  # Transfer only the required slice to CPU\n",
    "        agent_obs = tuple(np.round(agent_obs_cpu, decimals=5))  # Round the observation\n",
    "\n",
    "        if agent_obs not in self.q_table:\n",
    "            self.q_table[agent_obs] = np.zeros(self.env.action_space[self.agent_id].n)\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Select a random action\n",
    "            action = np.random.randint(env.action_space[self.agent_id].n)\n",
    "        else:\n",
    "            # Select the action with the highest Q-value\n",
    "            action = np.argmax(self.q_table[agent_obs])\n",
    "        \n",
    "        return (action,)  # Return as a tuple\n",
    "\n",
    "    def update_q_table(self, obs, action, reward, next_obs):\n",
    "        obs_key = tuple(np.round(obs.cpu().numpy(), decimals=5))  # Only transfer to CPU when necessary\n",
    "        next_obs_key = tuple(np.round(next_obs.cpu().numpy(), decimals=5))\n",
    "        action = int(action.item())  # Convert tensor to Python scalar\n",
    "\n",
    "        if isinstance(self.env.action_space[self.agent_id], Discrete):\n",
    "            action_space_size = self.env.action_space[self.agent_id].n\n",
    "        else:\n",
    "            raise ValueError(\"This Q-learning implementation requires a discrete action space.\")\n",
    "\n",
    "        if obs_key not in self.q_table:\n",
    "            self.q_table[obs_key] = np.zeros(action_space_size)\n",
    "\n",
    "        if next_obs_key not in self.q_table:\n",
    "            self.q_table[next_obs_key] = np.zeros(action_space_size)\n",
    "\n",
    "        best_next_action = np.argmax(self.q_table[next_obs_key])\n",
    "        td_target = reward + self.gamma * self.q_table[next_obs_key][best_next_action]\n",
    "\n",
    "        td_error = td_target - self.q_table[obs_key][action]\n",
    "        self.q_table[obs_key][action] += self.alpha * td_error\n",
    "\n",
    "class Case:\n",
    "    added_states = set()  # Class attribute to store states already added to the case base\n",
    "\n",
    "    def __init__(self, problem, solution, trust_value=1):\n",
    "        self.problem = problem if isinstance(problem, list) else ast.literal_eval(problem)  # Convert problem to numpy array\n",
    "        self.solution = solution\n",
    "        self.trust_value = trust_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)\n",
    "        state2 = np.atleast_1d(state2)\n",
    "        CNDMaxDist = 6  # Maximum distance between two nodes in the CND based on EOPRA reference\n",
    "        v = state1.size  # Total number of objects the agent can perceive\n",
    "        DistQ = np.sum([Case.dist_q(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def dist_q(X1, X2):\n",
    "        return np.min(np.abs(X1 - X2))\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(state, case_base, threshold=0.1):\n",
    "        state = ast.literal_eval(state)\n",
    "\n",
    "        similarities = {}\n",
    "        for case in case_base:\n",
    "            problem_numeric = np.array(case.problem, dtype=float)\n",
    "            state_numeric = np.array(state, dtype=float)\n",
    "            similarities[case] = Case.sim_q(state_numeric, problem_numeric)  # Compare state with the problem part of the case\n",
    "        \n",
    "        sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if sorted_similarities:\n",
    "            most_similar_case = sorted_similarities[0][0] if sorted_similarities[0][1] >= threshold else None\n",
    "        else:\n",
    "            most_similar_case = None\n",
    "        \n",
    "        return most_similar_case\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(c, temporary_case_base):\n",
    "        temporary_case_base.append(c)\n",
    "\n",
    "    @staticmethod\n",
    "    def revise(case_base, temporary_case_base, successful_episodes):\n",
    "        for case in temporary_case_base:\n",
    "            if successful_episodes and case in case_base:\n",
    "                case.trust_value += 0.1  # Increment trust value if the episode ended successfully and the case is in the case base\n",
    "            elif not successful_episodes and case in case_base:\n",
    "                case.trust_value -= 0.1  # Decrement trust value if the episode ended unsuccessfully and the case is in the case base\n",
    "            case.trust_value = max(0, min(case.trust_value, 1))  # Ensure trust value is within [0, 1]\n",
    "\n",
    "    @staticmethod\n",
    "    def retain(case_base, temporary_case_base, successful_episodes, threshold=0.7):\n",
    "        if successful_episodes:\n",
    "            # Iterate through the temporary case base to find the last occurrence of each unique state\n",
    "            for case in reversed(temporary_case_base):\n",
    "                state = tuple(np.atleast_1d(case.problem))\n",
    "                # Check if the state is already in the case base or has been added previously\n",
    "                if state not in Case.added_states:\n",
    "                    # Add the case to the case base if the state is new\n",
    "                    case_base.append(case)\n",
    "                    Case.added_states.add(state)\n",
    "                else:\n",
    "                    # Find the index of the existing case in the case base\n",
    "                    existing_index = next((i for i, c in enumerate(case_base) if tuple(np.atleast_1d(c.problem)) == state), None)\n",
    "                    if existing_index is not None:\n",
    "                        # Get the existing case from the case base\n",
    "                        existing_case = case_base[existing_index]\n",
    "                        # Update the trust value of the existing case with the new value from the revise step\n",
    "                        existing_case.trust_value = case.trust_value\n",
    "\n",
    "        # Filter case_base based on trust_value\n",
    "        filtered_case_base = [case for case in case_base if case.trust_value >= threshold]\n",
    "        return filtered_case_base\n",
    "\n",
    "class QCBRL:\n",
    "    def __init__(\n",
    "        self,\n",
    "        render: bool,\n",
    "        num_envs: int,\n",
    "        num_episodes: int,\n",
    "        max_steps_per_episode: int,\n",
    "        device: str,\n",
    "        scenario: Union[str, BaseScenario],\n",
    "        continuous_actions: bool,\n",
    "        random_action: bool,\n",
    "        n_agents: int,\n",
    "        obs_discrete: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.render = render\n",
    "        self.num_envs = num_envs\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "        self.device = device\n",
    "        self.scenario = scenario\n",
    "        self.continuous_actions = continuous_actions\n",
    "        self.random_action = random_action\n",
    "        self.obs_discrete = obs_discrete\n",
    "        self.kwargs = kwargs\n",
    "        self.frame_list = []  \n",
    "        self.problem_solver_agents = []\n",
    "        self.rewards_history = []  \n",
    "        self.action_counts = {i: {} for i in range(n_agents)}  \n",
    "        self.agent_rewards_history = {i: [] for i in range(n_agents)}\n",
    "        self.successful_episodes = 0\n",
    "        self.case_base = {i: [] for i in range(n_agents)}  # Separate case base for each agent\n",
    "        self.temporary_case_base = {i: [] for i in range(n_agents)}  # Separate temporary case base for each agent\n",
    "\n",
    "    def discretize(self, data, bins):\n",
    "        bins = np.array(bins)\n",
    "        if np.isscalar(data):\n",
    "            data = np.array([data])\n",
    "        bin_indices = np.digitize(data, bins) - 1  # np.digitize returns indices starting from 1\n",
    "        bin_indices = np.clip(bin_indices, 0, len(bins) - 1)  # Ensure indices are within the valid range\n",
    "        bin_values = bins[bin_indices]\n",
    "        bin_values = np.round(bin_values, 2)  # Round the bin values to two decimal places\n",
    "        return bin_indices, bin_values\n",
    "\n",
    "    def discretize_tensor_slice(self, tensor_slice, bins):\n",
    "        tensor_np = tensor_slice.cpu().numpy()  # Convert to numpy for easier handling\n",
    "        indices, values = self.discretize(tensor_np, bins)\n",
    "        indices = torch.tensor(indices, device=tensor_slice.device)\n",
    "        values = torch.tensor(values, device=tensor_slice.device)\n",
    "        return indices, values\n",
    "\n",
    "    def _get_deterministic_obs(self, env, observation):\n",
    "        pos_bins = np.linspace(-1, 1, 10)\n",
    "        vel_bins = np.linspace(-1, 1, 10)\n",
    "        dpos_bins = np.linspace(0, 1.5, 10)\n",
    "        dvel_bins = np.linspace(0, 2, 10)\n",
    "\n",
    "        num_envs, num_agents, obs_size = observation.size()\n",
    "        discretized_obs = torch.zeros((num_envs, num_agents, obs_size), device=observation.device)\n",
    "\n",
    "        for i in range(num_envs):\n",
    "            for j in range(num_agents):\n",
    "                agent_obs = observation[i, j, :]\n",
    "                pos_indices, pos_values = self.discretize_tensor_slice(agent_obs[:2], pos_bins)\n",
    "                vel_indices, vel_values = self.discretize_tensor_slice(agent_obs[2:4], vel_bins)\n",
    "                dpos_indices, dpos_values = self.discretize_tensor_slice(agent_obs[4:6], dpos_bins)\n",
    "                dvel_indices, dvel_values = self.discretize_tensor_slice(agent_obs[6:8], dvel_bins)\n",
    "                discretized_obs[i, j, :2] = pos_values\n",
    "                discretized_obs[i, j, 2:4] = vel_values\n",
    "                discretized_obs[i, j, 4:6] = dpos_values\n",
    "                discretized_obs[i, j, 6:8] = dvel_values\n",
    "\n",
    "        return discretized_obs\n",
    "\n",
    "    def run_vmas_env(self):\n",
    "        env = make_env(\n",
    "            scenario=self.scenario,\n",
    "            num_envs=self.num_envs,\n",
    "            device=self.device,\n",
    "            continuous_actions=self.continuous_actions,\n",
    "            **self.kwargs\n",
    "        )\n",
    "        env.reset()\n",
    "        n_agents = len(env.agents)\n",
    "        self.problem_solver_agents = [\n",
    "            ProblemSolver(env, agent_id) for agent_id in range(n_agents)\n",
    "        ]\n",
    "\n",
    "        for episode in range(self.num_episodes):\n",
    "            observation = env.reset()\n",
    "            observation = self._get_deterministic_obs(env, observation)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            for step in range(self.max_steps_per_episode):\n",
    "                actions = {}\n",
    "                for i, agent in enumerate(env.possible_agents):\n",
    "                    agent_obs = observation[0, i, :].to(self.device)\n",
    "                    case = Case.retrieve(\n",
    "                        str(agent_obs.cpu().numpy()), self.case_base[i], threshold=0.1\n",
    "                    )\n",
    "                    if case:\n",
    "                        actions[agent] = case.solution\n",
    "                        Case.reuse(case, self.temporary_case_base[i])\n",
    "                    else:\n",
    "                        action = self.problem_solver_agents[i].get_action(\n",
    "                            agent, env, i, agent_obs\n",
    "                        )\n",
    "                        actions[agent] = action\n",
    "                        problem = agent_obs.cpu().numpy().tolist()\n",
    "                        new_case = Case(problem, action)\n",
    "                        self.temporary_case_base[i].append(new_case)\n",
    "                next_observation, reward, done, _ = env.step(actions)\n",
    "                next_observation = self._get_deterministic_obs(env, next_observation)\n",
    "                episode_reward += sum(reward.values())\n",
    "\n",
    "                for i, agent in enumerate(env.possible_agents):\n",
    "                    agent_obs = observation[0, i, :].to(self.device)\n",
    "                    next_agent_obs = next_observation[0, i, :].to(self.device)\n",
    "                    self.problem_solver_agents[i].update_q_table(\n",
    "                        agent_obs, torch.tensor(actions[agent]), reward[agent], next_agent_obs\n",
    "                    )\n",
    "\n",
    "                    action = actions[agent]\n",
    "                    if action in self.action_counts[i]:\n",
    "                        self.action_counts[i][action] += 1\n",
    "                    else:\n",
    "                        self.action_counts[i][action] = 1\n",
    "\n",
    "                observation = next_observation\n",
    "                if self.render:\n",
    "                    self.frame_list.append(env.render(mode=\"rgb_array\")[0])\n",
    "\n",
    "                if all(done.values()):\n",
    "                    break\n",
    "\n",
    "            self.rewards_history.append(episode_reward)\n",
    "            for i, agent in enumerate(env.possible_agents):\n",
    "                self.agent_rewards_history[i].append(reward[agent])\n",
    "\n",
    "            Case.revise(self.case_base, self.temporary_case_base, successful_episodes=True)\n",
    "            for i in range(n_agents):\n",
    "                self.case_base[i] = Case.retain(\n",
    "                    self.case_base[i], self.temporary_case_base[i], successful_episodes=True\n",
    "                )\n",
    "            self.temporary_case_base = {i: [] for i in range(n_agents)}\n",
    "\n",
    "        env.close()\n",
    "        return self.rewards_history, self.agent_rewards_history, self.action_counts\n",
    "\n",
    "    def generate_gif(self, scenario_name):\n",
    "        clip = ImageSequenceClip(self.frame_list, fps=20)\n",
    "        gif_path = f\"{scenario_name}.gif\"\n",
    "        clip.write_gif(gif_path, fps=20)\n",
    "        return gif_path\n",
    "\n",
    "    def plot_rewards_history(self):\n",
    "        plt.plot(self.rewards_history)\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Rewards\")\n",
    "        plt.title(\"Training Rewards Over Episodes\")\n",
    "        plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scenario_name = \"navigation_comm\"\n",
    "    use_cuda = True\n",
    "\n",
    "    env_runner = QCBRL(\n",
    "        render=True,\n",
    "        num_envs=1,\n",
    "        num_episodes=50,\n",
    "        max_steps_per_episode=200,\n",
    "        device=torch.device(\"cuda\" if use_cuda else \"cpu\"),\n",
    "        scenario=scenario_name,\n",
    "        continuous_actions=False,\n",
    "        random_action=False,\n",
    "        n_agents=2,\n",
    "        obs_discrete=True,\n",
    "        agents_with_same_goal=2,\n",
    "        collisions=False,\n",
    "        shared_rew=False,\n",
    "    )\n",
    "\n",
    "    env_runner.run_vmas_env()\n",
    "    # for agent in env_runner.problem_solver_agents:\n",
    "    #     agent.print_q_table()\n",
    "    env_runner.plot_rewards_history()\n",
    "\n",
    "    ipython_display(HTML(f'<img src=\"{env_runner.generate_gif(scenario_name)}\">'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
