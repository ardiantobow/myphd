{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 02:31:41,508 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 164\u001b[0m\n\u001b[1;32m    160\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 164\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 142\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m env \u001b[38;5;241m=\u001b[39m TransformedEnv(\n\u001b[1;32m    136\u001b[0m     env,\n\u001b[1;32m    137\u001b[0m     RewardSum(in_keys\u001b[38;5;241m=\u001b[39m[env\u001b[38;5;241m.\u001b[39mreward_key], out_keys\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magents\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m)]),\n\u001b[1;32m    138\u001b[0m )\n\u001b[1;32m    140\u001b[0m check_env_specs(env)\n\u001b[0;32m--> 142\u001b[0m sarsa_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSARSATrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframes_per_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon_decay_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon_decay_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon_decay_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m episode_rewards \u001b[38;5;241m=\u001b[39m sarsa_trainer\u001b[38;5;241m.\u001b[39mtrain(max_steps)\n\u001b[1;32m    156\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(episode_rewards)\n",
      "Cell \u001b[0;32mIn[9], line 35\u001b[0m, in \u001b[0;36mSARSATrainer.__init__\u001b[0;34m(self, num_epochs, frames_per_batch, device, total_frames, gamma, epsilon_start, epsilon_end, epsilon_decay_steps, env)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m env\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Initialize Q-table\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_actions(env\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_actions))\n",
      "Cell \u001b[0;32mIn[9], line 45\u001b[0m, in \u001b[0;36mSARSATrainer.get_num_states\u001b[0;34m(self, observation_space)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_states(space) \u001b[38;5;28;01mfor\u001b[39;00m space \u001b[38;5;129;01min\u001b[39;00m observation_space)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation_space\u001b[38;5;241m.\u001b[39mn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(observation_space, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import multiprocessing\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SARSATrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_epochs,\n",
    "        frames_per_batch,\n",
    "        device,\n",
    "        total_frames,\n",
    "        gamma,\n",
    "        epsilon_start,\n",
    "        epsilon_end,\n",
    "        epsilon_decay_steps,\n",
    "        env,\n",
    "    ):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.frames_per_batch = frames_per_batch\n",
    "        self.device = device\n",
    "        self.total_frames = total_frames\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.env = env\n",
    "\n",
    "        # Initialize Q-table\n",
    "        self.num_states = self.get_num_states(env.observation_space)\n",
    "        self.num_actions = self.get_num_actions(env.action_space)\n",
    "        self.q_values = np.zeros((self.num_states, self.num_actions))\n",
    "\n",
    "    def get_num_states(self, observation_space):\n",
    "        if observation_space is None:\n",
    "            return 0\n",
    "        elif isinstance(observation_space, tuple):\n",
    "            return sum(self.get_num_states(space) for space in observation_space)\n",
    "        else:\n",
    "            return observation_space.n if hasattr(observation_space, 'n') else observation_space.shape[0]\n",
    "\n",
    "    def get_num_actions(self, action_space):\n",
    "        if action_space is None:\n",
    "            return 0\n",
    "        elif isinstance(action_space, tuple):\n",
    "            return sum(self.get_num_actions(space) for space in action_space)\n",
    "        else:\n",
    "            return action_space.n if hasattr(action_space, 'n') else action_space.shape[0]\n",
    "\n",
    "\n",
    "    def train(self, max_steps):\n",
    "        pbar = tqdm(total=self.total_frames, desc=\"episode_reward_mean = 0\")\n",
    "        episode_reward_mean_list = []\n",
    "\n",
    "        epsilon_decay = np.linspace(self.epsilon_start, self.epsilon_end, self.epsilon_decay_steps)\n",
    "\n",
    "        for step, epsilon in zip(range(self.total_frames), epsilon_decay):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            # Select initial action using epsilon-greedy policy\n",
    "            action = self.select_action(state, epsilon)\n",
    "\n",
    "            while not done:\n",
    "                # Take action, observe next state and reward\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                # Select next action using epsilon-greedy policy\n",
    "                next_action = self.select_action(next_state, epsilon)\n",
    "\n",
    "                # Update Q-value based on SARSA algorithm\n",
    "                self.update_q_value(state, action, reward, next_state, next_action)\n",
    "\n",
    "                # Update state, action and episode reward\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                episode_reward += reward\n",
    "\n",
    "            episode_reward_mean_list.append(episode_reward)\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "        return episode_reward_mean_list\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        # Epsilon-greedy policy\n",
    "        if np.random.rand() < epsilon:\n",
    "            return self.env.action_space.sample()  # Random action\n",
    "        else:\n",
    "            return np.argmax(self.q_values[state])\n",
    "\n",
    "    def update_q_value(self, state, action, reward, next_state, next_action):\n",
    "        # SARSA update rule: Q(s, a) += alpha * (r + gamma * Q(s', a') - Q(s, a))\n",
    "        alpha = 0.1  # Learning rate\n",
    "        self.q_values[state, action] += alpha * (\n",
    "            reward + self.gamma * self.q_values[next_state, next_action] - self.q_values[state, action]\n",
    "        )\n",
    "\n",
    "\n",
    "def main():\n",
    "    torch.manual_seed(0)\n",
    "    is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "    device = (\n",
    "        torch.device(0)\n",
    "        if torch.cuda.is_available() and not is_fork\n",
    "        else torch.device(\"cpu\")\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    num_epochs = 1\n",
    "    gamma = 0.9\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_end = 0.1\n",
    "    epsilon_decay_steps = 100\n",
    "    max_steps = 100\n",
    "    num_vmas_envs = 1  # SARSA doesn't require parallel environments\n",
    "    scenario_name = \"navigation\"\n",
    "    n_agents = 3\n",
    "\n",
    "    env = VmasEnv(\n",
    "        scenario=scenario_name,\n",
    "        num_envs=num_vmas_envs,\n",
    "        continuous_actions=True,\n",
    "        max_steps=max_steps,\n",
    "        device=device,\n",
    "        n_agents=n_agents,\n",
    "    )\n",
    "\n",
    "    env = TransformedEnv(\n",
    "        env,\n",
    "        RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
    "    )\n",
    "\n",
    "    check_env_specs(env)\n",
    "\n",
    "    sarsa_trainer = SARSATrainer(\n",
    "        num_epochs=num_epochs,\n",
    "        frames_per_batch=max_steps,\n",
    "        device=device,\n",
    "        total_frames=epsilon_decay_steps,\n",
    "        gamma=gamma,\n",
    "        epsilon_start=epsilon_start,\n",
    "        epsilon_end=epsilon_end,\n",
    "        epsilon_decay_steps=epsilon_decay_steps,\n",
    "        env=env,\n",
    "    )\n",
    "\n",
    "    episode_rewards = sarsa_trainer.train(max_steps)\n",
    "\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Episode Reward\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
