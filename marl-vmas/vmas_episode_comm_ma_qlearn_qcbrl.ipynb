{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardie85/PHD/Research/code/.venv/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim type: <class 'int'>\n",
      "action dim type: <class 'int'>\n",
      "input dim type: <class 'int'>\n",
      "action dim type: <class 'int'>\n",
      "input dim type: <class 'int'>\n",
      "action dim type: <class 'int'>\n",
      "input dim type: <class 'int'>\n",
      "action dim type: <class 'int'>\n",
      "Step 1 of Episode 0\n",
      "action generated from NN: [ 0.1442 -0.025 ]\n",
      "action type of agent 0: problem solver\n",
      "action generated from NN: [ 0.0473 -0.1016]\n",
      "action type of agent 1: problem solver\n",
      "observation before pass: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.0000],\n",
      "       device='cuda:0')\n",
      "physical action before pass: tensor([ 0.1442, -0.0250], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0007], device='cuda:0')\n",
      "next observation before pass: tensor([-8.9892e-01,  8.9981e-01,  1.4420e-02, -2.5000e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  4.7300e-02, -1.0160e-01,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0007], device='cuda:0')\n",
      "next observation received for update: tensor([-0.8989,  0.8998,  0.0144, -0.0025,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.1442, -0.0250], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([1.0369], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 0 - PPO update: policy loss = -9.93087887763977e-05, value loss = 9.862235472724024e-09\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "observation before pass: tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.0000],\n",
      "       device='cuda:0')\n",
      "physical action before pass: tensor([ 0.0473, -0.1016], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0005], device='cuda:0')\n",
      "next observation before pass: tensor([ 9.0035e-01,  8.9924e-01,  4.7300e-03, -1.0160e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  1.4420e-01, -2.5000e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0005], device='cuda:0')\n",
      "next observation received for update: tensor([ 0.9004,  0.8992,  0.0047, -0.0102,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.0473, -0.1016], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([1.0920], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 1 - PPO update: policy loss = 0.0003646761178970337, value loss = 1.329886742951203e-07\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "Step 2 of Episode 0\n",
      "action generated from NN: [ 0.1436 -0.0255]\n",
      "action type of agent 0: problem solver\n",
      "action generated from NN: [ 0.0471 -0.102 ]\n",
      "action type of agent 1: problem solver\n",
      "observation before pass: tensor([-8.9892e-01,  8.9981e-01,  1.4420e-02, -2.5000e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  4.7300e-02, -1.0160e-01,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "physical action before pass: tensor([ 0.1436, -0.0255], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0013], device='cuda:0')\n",
      "next observation before pass: tensor([-8.9676e-01,  8.9943e-01,  2.5175e-02, -4.4250e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  4.7100e-02, -1.0200e-01,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([-0.8989,  0.8998,  0.0144, -0.0025,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([-0.8989,  0.8998,  0.0144, -0.0025,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0013], device='cuda:0')\n",
      "next observation received for update: tensor([-0.8968,  0.8994,  0.0252, -0.0044,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.1436, -0.0255], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([2.5388e-09], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 0 - PPO update: policy loss = -0.24706651270389557, value loss = 0.04239017516374588\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "observation before pass: tensor([ 9.0035e-01,  8.9924e-01,  4.7300e-03, -1.0160e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  1.4420e-01, -2.5000e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "physical action before pass: tensor([ 0.0471, -0.1020], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0010], device='cuda:0')\n",
      "next observation before pass: tensor([ 9.0106e-01,  8.9771e-01,  8.2575e-03, -1.7820e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  1.4360e-01, -2.5500e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([ 0.9004,  0.8992,  0.0047, -0.0102,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([ 0.9004,  0.8992,  0.0047, -0.0102,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0010], device='cuda:0')\n",
      "next observation received for update: tensor([ 0.9011,  0.8977,  0.0083, -0.0178,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.0471, -0.1020], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([2191023.5000], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 1 - PPO update: policy loss = 0.11774139851331711, value loss = 0.02166099287569523\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "Step 3 of Episode 0\n",
      "action generated from NN: [ 0.14299999 -0.0258    ]\n",
      "action type of agent 0: problem solver\n",
      "action generated from NN: [ 0.047  -0.1023]\n",
      "action type of agent 1: problem solver\n",
      "observation before pass: tensor([-8.9676e-01,  8.9943e-01,  2.5175e-02, -4.4250e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  4.7100e-02, -1.0200e-01,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "physical action before pass: tensor([ 0.1430, -0.0258], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0018], device='cuda:0')\n",
      "next observation before pass: tensor([-8.9380e-01,  8.9891e-01,  3.3181e-02, -5.8987e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  4.7000e-02, -1.0230e-01,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([-0.8968,  0.8994,  0.0252, -0.0044,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([-0.8968,  0.8994,  0.0252, -0.0044,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0018], device='cuda:0')\n",
      "next observation received for update: tensor([-0.8938,  0.8989,  0.0332, -0.0059,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.1430, -0.0258], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([0.2470], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 0 - PPO update: policy loss = -0.022109556943178177, value loss = 0.00033946699113585055\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "observation before pass: tensor([ 9.0106e-01,  8.9771e-01,  8.2575e-03, -1.7820e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  1.4360e-01, -2.5500e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "physical action before pass: tensor([ 0.0470, -0.1023], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0014], device='cuda:0')\n",
      "next observation before pass: tensor([ 9.0203e-01,  8.9561e-01,  1.0893e-02, -2.3595e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  1.4300e-01, -2.5800e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([ 0.9011,  0.8977,  0.0083, -0.0178,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([ 0.9011,  0.8977,  0.0083, -0.0178,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0014], device='cuda:0')\n",
      "next observation received for update: tensor([ 0.9020,  0.8956,  0.0109, -0.0236,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.0470, -0.1023], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([1.8206], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 1 - PPO update: policy loss = 0.004319572355598211, value loss = 2.915422919613775e-05\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "Step 4 of Episode 0\n",
      "action generated from NN: [ 0.1425 -0.026 ]\n",
      "action type of agent 0: problem solver\n",
      "action generated from NN: [ 0.0469 -0.1025]\n",
      "action type of agent 1: problem solver\n",
      "observation before pass: tensor([-8.9380e-01,  8.9891e-01,  3.3181e-02, -5.8987e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  4.7000e-02, -1.0230e-01,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "physical action before pass: tensor([ 0.1425, -0.0260], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0022], device='cuda:0')\n",
      "next observation before pass: tensor([-8.9024e-01,  8.9827e-01,  3.9136e-02, -7.0241e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  4.6900e-02, -1.0250e-01,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([-0.8938,  0.8989,  0.0332, -0.0059,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([-0.8938,  0.8989,  0.0332, -0.0059,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0022], device='cuda:0')\n",
      "next observation received for update: tensor([-0.8902,  0.8983,  0.0391, -0.0070,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.1425, -0.0260], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([2.6097], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 0 - PPO update: policy loss = 0.008028173819184303, value loss = 0.00010070557618746534\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "observation before pass: tensor([ 9.0203e-01,  8.9561e-01,  1.0893e-02, -2.3595e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  1.4300e-01, -2.5800e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "physical action before pass: tensor([ 0.0469, -0.1025], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0017], device='cuda:0')\n",
      "next observation before pass: tensor([ 9.0320e-01,  8.9307e-01,  1.2860e-02, -2.7946e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  1.4250e-01, -2.6000e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([ 0.9020,  0.8956,  0.0109, -0.0236,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([ 0.9020,  0.8956,  0.0109, -0.0236,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0017], device='cuda:0')\n",
      "next observation received for update: tensor([ 0.9032,  0.8931,  0.0129, -0.0279,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.0469, -0.1025], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([0.2779], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 1 - PPO update: policy loss = -0.021872950717806816, value loss = 0.000332240218995139\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "Step 5 of Episode 0\n",
      "action generated from NN: [ 0.14199999 -0.0261    ]\n",
      "action type of agent 0: problem solver\n",
      "action generated from NN: [ 0.0469 -0.1026]\n",
      "action type of agent 1: problem solver\n",
      "observation before pass: tensor([-8.9024e-01,  8.9827e-01,  3.9136e-02, -7.0241e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  4.6900e-02, -1.0250e-01,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "physical action before pass: tensor([ 0.1420, -0.0261], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0025], device='cuda:0')\n",
      "next observation before pass: tensor([-8.8624e-01,  8.9755e-01,  4.3552e-02, -7.8780e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  4.6900e-02, -1.0260e-01,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([-0.8902,  0.8983,  0.0391, -0.0070,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([-0.8902,  0.8983,  0.0391, -0.0070,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0025], device='cuda:0')\n",
      "next observation received for update: tensor([-0.8862,  0.8975,  0.0436, -0.0079,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.1420, -0.0261], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([2.4063], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 0 - PPO update: policy loss = 0.006494570057839155, value loss = 6.590536941075698e-05\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "observation before pass: tensor([ 9.0320e-01,  8.9307e-01,  1.2860e-02, -2.7946e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  1.4250e-01, -2.6000e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "physical action before pass: tensor([ 0.0469, -0.1026], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0019], device='cuda:0')\n",
      "next observation before pass: tensor([ 9.0452e-01,  8.9020e-01,  1.4335e-02, -3.1220e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  1.4200e-01, -2.6100e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([ 0.9032,  0.8931,  0.0129, -0.0279,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([ 0.9032,  0.8931,  0.0129, -0.0279,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0019], device='cuda:0')\n",
      "next observation received for update: tensor([ 0.9045,  0.8902,  0.0143, -0.0312,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.0469, -0.1026], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([0.5268], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 1 - PPO update: policy loss = -0.0130553487688303, value loss = 0.00011836258636321872\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "done status for agent 0: False\n",
      "done status for agent 1: False\n",
      "Success percentage of each agent at the end of episode 0:\n",
      "Agent 0: 0.0%\n",
      "Agent 1: 0.0%\n",
      "Overall success percentage for all agents up to episode 0: 0.0%\n",
      "done status for agent 0: False\n",
      "Temporary case base saved successfully.\n",
      "Case base saved successfully.\n",
      "done status for agent 1: False\n",
      "Temporary case base saved successfully.\n",
      "Case base saved successfully.\n",
      "Step 1 of Episode 1\n",
      "action generated from NN: [ 0.1442 -0.025 ]\n",
      "action type of agent 0: problem solver\n",
      "action generated from NN: [ 0.0473 -0.1016]\n",
      "action type of agent 1: problem solver\n",
      "observation before pass: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "physical action before pass: tensor([ 0.1442, -0.0250], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0007], device='cuda:0')\n",
      "next observation before pass: tensor([-8.9892e-01,  8.9981e-01,  1.4420e-02, -2.5000e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  4.7300e-02, -1.0160e-01,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0007], device='cuda:0')\n",
      "next observation received for update: tensor([-0.8989,  0.8998,  0.0144, -0.0025,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.1442, -0.0250], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([1.9359], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 0 - PPO update: policy loss = 0.008547306060791016, value loss = 0.00011415068729547784\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "observation before pass: tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "physical action before pass: tensor([ 0.0473, -0.1016], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0005], device='cuda:0')\n",
      "next observation before pass: tensor([ 9.0035e-01,  8.9924e-01,  4.7300e-03, -1.0160e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  1.4420e-01, -2.5000e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([ 0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0005], device='cuda:0')\n",
      "next observation received for update: tensor([ 0.9004,  0.8992,  0.0047, -0.0102,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.0473, -0.1016], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([0.8187], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 1 - PPO update: policy loss = -0.006062275264412165, value loss = 2.5521652787574567e-05\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "Step 2 of Episode 1\n",
      "action generated from NN: [ 0.1436 -0.0255]\n",
      "action type of agent 0: problem solver\n",
      "action generated from NN: [ 0.0471 -0.102 ]\n",
      "action type of agent 1: problem solver\n",
      "observation before pass: tensor([-8.9892e-01,  8.9981e-01,  1.4420e-02, -2.5000e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  4.7300e-02, -1.0160e-01,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "physical action before pass: tensor([ 0.1436, -0.0255], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0013], device='cuda:0')\n",
      "next observation before pass: tensor([-8.9676e-01,  8.9943e-01,  2.5175e-02, -4.4250e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  4.7100e-02, -1.0200e-01,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([-0.8989,  0.8998,  0.0144, -0.0025,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([-0.8989,  0.8998,  0.0144, -0.0025,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0013], device='cuda:0')\n",
      "next observation received for update: tensor([-0.8968,  0.8994,  0.0252, -0.0044,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.1436, -0.0255], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([1.4656], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 0 - PPO update: policy loss = 0.004459738731384277, value loss = 3.107698285020888e-05\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "observation before pass: tensor([ 9.0035e-01,  8.9924e-01,  4.7300e-03, -1.0160e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  1.4420e-01, -2.5000e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "physical action before pass: tensor([ 0.0471, -0.1020], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0010], device='cuda:0')\n",
      "next observation before pass: tensor([ 9.0106e-01,  8.9771e-01,  8.2575e-03, -1.7820e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  1.4360e-01, -2.5500e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([ 0.9004,  0.8992,  0.0047, -0.0102,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([ 0.9004,  0.8992,  0.0047, -0.0102,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0010], device='cuda:0')\n",
      "next observation received for update: tensor([ 0.9011,  0.8977,  0.0083, -0.0178,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.0471, -0.1020], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([1.1519], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 1 - PPO update: policy loss = 0.0006197758484631777, value loss = 4.757275462452526e-07\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "Step 3 of Episode 1\n",
      "action generated from NN: [ 0.14299999 -0.0258    ]\n",
      "action type of agent 0: problem solver\n",
      "action generated from NN: [ 0.047  -0.1023]\n",
      "action type of agent 1: problem solver\n",
      "observation before pass: tensor([-8.9676e-01,  8.9943e-01,  2.5175e-02, -4.4250e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  4.7100e-02, -1.0200e-01,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "physical action before pass: tensor([ 0.1430, -0.0258], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0018], device='cuda:0')\n",
      "next observation before pass: tensor([-8.9380e-01,  8.9891e-01,  3.3181e-02, -5.8987e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  4.7000e-02, -1.0230e-01,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([-0.8968,  0.8994,  0.0252, -0.0044,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([-0.8968,  0.8994,  0.0252, -0.0044,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0018], device='cuda:0')\n",
      "next observation received for update: tensor([-0.8938,  0.8989,  0.0332, -0.0059,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.1430, -0.0258], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([1.1699], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 0 - PPO update: policy loss = -9.604053775547072e-05, value loss = 1.4970362727240172e-08\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "observation before pass: tensor([ 9.0106e-01,  8.9771e-01,  8.2575e-03, -1.7820e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  1.4360e-01, -2.5500e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "physical action before pass: tensor([ 0.0470, -0.1023], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0014], device='cuda:0')\n",
      "next observation before pass: tensor([ 9.0203e-01,  8.9561e-01,  1.0893e-02, -2.3595e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  1.4300e-01, -2.5800e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([ 0.9011,  0.8977,  0.0083, -0.0178,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([ 0.9011,  0.8977,  0.0083, -0.0178,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0014], device='cuda:0')\n",
      "next observation received for update: tensor([ 0.9020,  0.8956,  0.0109, -0.0236,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.0470, -0.1023], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([1.5543], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 1 - PPO update: policy loss = 0.002806019736453891, value loss = 1.230272937391419e-05\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "Step 4 of Episode 1\n",
      "action generated from NN: [ 0.1425 -0.026 ]\n",
      "action type of agent 0: problem solver\n",
      "action generated from NN: [ 0.0469 -0.1025]\n",
      "action type of agent 1: problem solver\n",
      "observation before pass: tensor([-8.9380e-01,  8.9891e-01,  3.3181e-02, -5.8987e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  4.7000e-02, -1.0230e-01,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "physical action before pass: tensor([ 0.1425, -0.0260], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0022], device='cuda:0')\n",
      "next observation before pass: tensor([-8.9024e-01,  8.9827e-01,  3.9136e-02, -7.0241e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  4.6900e-02, -1.0250e-01,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([-0.8938,  0.8989,  0.0332, -0.0059,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([-0.8938,  0.8989,  0.0332, -0.0059,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0022], device='cuda:0')\n",
      "next observation received for update: tensor([-0.8902,  0.8983,  0.0391, -0.0070,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.1425, -0.0260], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([1.0029], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 0 - PPO update: policy loss = -0.0034281634725630283, value loss = 1.0301001566404011e-05\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "observation before pass: tensor([ 9.0203e-01,  8.9561e-01,  1.0893e-02, -2.3595e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  1.4300e-01, -2.5800e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "physical action before pass: tensor([ 0.0469, -0.1025], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0017], device='cuda:0')\n",
      "next observation before pass: tensor([ 9.0320e-01,  8.9307e-01,  1.2860e-02, -2.7946e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  1.4250e-01, -2.6000e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([ 0.9020,  0.8956,  0.0109, -0.0236,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([ 0.9020,  0.8956,  0.0109, -0.0236,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0017], device='cuda:0')\n",
      "next observation received for update: tensor([ 0.9032,  0.8931,  0.0129, -0.0279,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.0469, -0.1025], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([1.5142], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 1 - PPO update: policy loss = 0.0022751332726329565, value loss = 8.087860805972014e-06\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "Step 5 of Episode 1\n",
      "action generated from NN: [ 0.14199999 -0.0261    ]\n",
      "action type of agent 0: problem solver\n",
      "action generated from NN: [ 0.0469 -0.1026]\n",
      "action type of agent 1: problem solver\n",
      "observation before pass: tensor([-8.9024e-01,  8.9827e-01,  3.9136e-02, -7.0241e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  4.6900e-02, -1.0250e-01,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "physical action before pass: tensor([ 0.1420, -0.0261], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0025], device='cuda:0')\n",
      "next observation before pass: tensor([-8.8624e-01,  8.9755e-01,  4.3552e-02, -7.8780e-03,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  4.6900e-02, -1.0260e-01,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([-0.8902,  0.8983,  0.0391, -0.0070,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([-0.8902,  0.8983,  0.0391, -0.0070,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0025], device='cuda:0')\n",
      "next observation received for update: tensor([-0.8862,  0.8975,  0.0436, -0.0079,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.1420, -0.0261], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([0.8353], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 0 - PPO update: policy loss = -0.00674745487049222, value loss = 3.1616764317732304e-05\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "observation before pass: tensor([ 9.0320e-01,  8.9307e-01,  1.2860e-02, -2.7946e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  1.4250e-01, -2.6000e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "physical action before pass: tensor([ 0.0469, -0.1026], device='cuda:0', dtype=torch.float64)\n",
      "reward before pass: tensor([0.0019], device='cuda:0')\n",
      "next observation before pass: tensor([ 9.0452e-01,  8.9020e-01,  1.4335e-02, -3.1220e-02,  0.0000e+00,\n",
      "        -8.0000e-01,  9.9000e+01,  9.9000e+01,  1.4200e-01, -2.6100e-02,\n",
      "         0.0000e+00,  0.0000e+00], device='cuda:0', dtype=torch.float64)\n",
      "dones before pass: tensor([[False, False]], device='cuda:0')\n",
      "done before pass: False\n",
      "observation received for update: tensor([ 0.9032,  0.8931,  0.0129, -0.0279,  0.0000, -0.8000], device='cuda:0')\n",
      "action received for update: tensor([ 0.9032,  0.8931,  0.0129, -0.0279,  0.0000, -0.8000], device='cuda:0')\n",
      "reward received for update: tensor([0.0019], device='cuda:0')\n",
      "next observation received for update: tensor([ 0.9045,  0.8902,  0.0143, -0.0312,  0.0000, -0.8000], device='cuda:0')\n",
      "done received for update: False\n",
      "mu: tensor([ 0.0469, -0.1026], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "sigma: tensor([1.3442], device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Agent 1 - PPO update: policy loss = 0.0010021686321124434, value loss = 1.5692844499426428e-06\n",
      "dones: tensor([False, False], device='cuda:0')\n",
      "done status for agent 0: False\n",
      "done status for agent 1: False\n",
      "Success percentage of each agent at the end of episode 1:\n",
      "Agent 0: 0.0%\n",
      "Agent 1: 0.0%\n",
      "Overall success percentage for all agents up to episode 1: 0.0%\n",
      "done status for agent 0: False\n",
      "Temporary case base saved successfully.\n",
      "Case base saved successfully.\n",
      "done status for agent 1: False\n",
      "Temporary case base saved successfully.\n",
      "Case base saved successfully.\n",
      "Temporary case base saved successfully.\n",
      "Case base saved successfully.\n",
      "Temporary case base saved successfully.\n",
      "Case base saved successfully.\n",
      "Overall success percentage for all agents = 0.0%\n",
      "It took: 1.0512096881866455s for 10 steps across 2 episodes of 1 parallel environments on device cuda for navigation_comm scenario.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file navigation_comm.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"navigation_comm.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import ast\n",
    "from vmas import make_env\n",
    "from vmas.simulator.core import Agent\n",
    "from vmas.simulator.scenario import BaseScenario\n",
    "from typing import Union\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "from IPython.display import HTML, display as ipython_display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.spaces import Discrete\n",
    "import math\n",
    "from collections import deque \n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        # print (f\"input dim type: {type(input_dim)}\")\n",
    "        # print (f\"action dim type: {type(action_dim)}\")\n",
    "\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        policy_dist = torch.tanh(self.actor(x))\n",
    "        policy_dist = torch.round(policy_dist * 10000) / 10000\n",
    "        return policy_dist, value\n",
    "\n",
    "    \n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, env, agent_id, state_dim, action_dim, device, alpha=0.1, gamma=0.99, epsilon=0.2, K_epochs=4, batch_size=64, communication_weight=0.5):\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.K_epoch = K_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.q_table = {}\n",
    "        self.env = env\n",
    "        self.agent_id = agent_id\n",
    "        self.communication_weight = communication_weight  # Weight parameter for incorporating messages\n",
    "        self.device = device\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=alpha)\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "    \n",
    "    def get_action_discrete(self, agent, env, agent_id, agent_obs):\n",
    "        agent_obs_cpu = agent_obs[:6].cpu().numpy()  # Transfer only the required slice to CPU\n",
    "        agent_obs = tuple(np.round(agent_obs_cpu, decimals=5))  # Round the observation\n",
    "\n",
    "        if agent_obs not in self.q_table:\n",
    "            self.q_table[agent_obs] = np.zeros(self.env.action_space[self.agent_id].n)\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Select a random action\n",
    "            action = np.random.randint(env.action_space[self.agent_id].n)\n",
    "        else:\n",
    "            # Select the action with the highest Q-value\n",
    "            action = np.argmax(self.q_table[agent_obs])\n",
    "        \n",
    "        return (action,)  # Return as a tuple\n",
    "    \n",
    "    def update_action_discrete(self,  agent, env, agent_id, obs, action, reward, next_obs):\n",
    "        obs_key = tuple(np.round(obs.cpu().numpy(), decimals=5))  # Only transfer to CPU when necessary\n",
    "        next_obs_key = tuple(np.round(next_obs.cpu().numpy(), decimals=5))\n",
    "        action = int(action.item())  # Convert tensor to Python scalar\n",
    "\n",
    "        # print (f\"reward obtained = {reward}\")\n",
    "\n",
    "        if isinstance(self.env.action_space[self.agent_id], Discrete):\n",
    "            action_space_size = self.env.action_space[self.agent_id].n\n",
    "        else:\n",
    "            raise ValueError(\"This Q-learning implementation requires a discrete action space.\")\n",
    "\n",
    "        if obs_key not in self.q_table:\n",
    "            self.q_table[obs_key] = np.zeros(action_space_size)\n",
    "\n",
    "        if next_obs_key not in self.q_table:\n",
    "            self.q_table[next_obs_key] = np.zeros(action_space_size)\n",
    "\n",
    "        best_next_action = np.argmax(self.q_table[next_obs_key])\n",
    "        td_target = reward + self.gamma * self.q_table[next_obs_key][best_next_action]\n",
    "\n",
    "        td_error = td_target - self.q_table[obs_key][action]\n",
    "        self.q_table[obs_key][action] += self.alpha * td_error\n",
    "\n",
    "        print(f\"Agent {self.agent_id} - Updated Q-table for obs {obs_key}, action {action}, reward {reward}, next_obs {next_obs_key}\")\n",
    "\n",
    "    \n",
    "    def print_q_table(self):\n",
    "        print(f\"Q-table for Agent {self.agent_id}:\")\n",
    "        for state, actions in self.q_table.items():\n",
    "            print(f\"  State: {state}\")\n",
    "            for action, q_value in enumerate(actions):\n",
    "                print(f\"    Action: {action}, Q-value: {q_value:.5f}\")\n",
    "        print(f\"End of Q-table for Agent {self.agent_id}\\n\")\n",
    "\n",
    "\n",
    "    def get_action_continuous(self, agent, env, agent_id, agent_obs, device):\n",
    "        state = torch.Tensor(agent_obs[:6]).to(device) if not isinstance(agent_obs[:6], torch.Tensor) else agent_obs[:6].to(device)\n",
    "        state = state.float()  # Ensure the state is float32\n",
    "        with torch.no_grad():\n",
    "            policy_dist, _ = self.policy_old(state)\n",
    "        action = policy_dist.cpu().numpy()\n",
    "\n",
    "        print(f\"action generated from NN: {action}\")\n",
    "        \n",
    "        return action\n",
    "\n",
    "\n",
    "    def update_action_continuous(self, agent_obs, action, reward, next_obs, done):\n",
    "        agent_obs = torch.tensor(agent_obs[:6], dtype=torch.float32).to(self.device)\n",
    "        next_obs = torch.tensor(next_obs[:6], dtype=torch.float32).to(self.device)\n",
    "        action = torch.tensor(action, dtype=torch.float32).to(self.device)\n",
    "        # done = torch.tensor(done, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # print (f\"observation received for update: {agent_obs}\")\n",
    "        # print (f\"action received for update: {agent_obs}\")\n",
    "        # print (f\"reward received for update: {reward}\")\n",
    "        # print (f\"next observation received for update: {next_obs}\")\n",
    "        # print (f\"done received for update: {done}\")\n",
    "\n",
    "        # Calculate advantage\n",
    "        _, value = self.policy(agent_obs)\n",
    "        _, next_value = self.policy(next_obs)\n",
    "        # target_value = reward + self.gamma * next_value * (1 - done)\n",
    "        target_value = reward + self.gamma * next_value * (~done)\n",
    "        advantage = (target_value - value).detach()\n",
    "\n",
    "        # Calculate the log probability of the action under the current policy\n",
    "        mu, sigma = self.policy(agent_obs)\n",
    "        sigma = torch.exp(sigma) \n",
    "        # print(f\"mu: {mu}\")\n",
    "        # print(f\"sigma: {sigma}\")\n",
    "        dist = torch.distributions.Normal(mu, sigma)\n",
    "        \n",
    "        log_prob = dist.log_prob(action).sum()\n",
    "\n",
    "        # Calculate the log probability of the action under the old policy\n",
    "        with torch.no_grad():\n",
    "            mu_old, sigma_old = self.policy_old(agent_obs)\n",
    "            sigma_old = torch.exp(sigma_old) \n",
    "            dist_old = torch.distributions.Normal(mu_old, sigma_old)\n",
    "            old_log_prob = dist_old.log_prob(action).sum()\n",
    "\n",
    "        # Policy ratio\n",
    "        ratio = torch.exp(log_prob - old_log_prob)\n",
    "\n",
    "        # PPO objective with clipping\n",
    "        surrogate1 = ratio * advantage\n",
    "        surrogate2 = torch.clamp(ratio, 1.0 - self.epsilon, 1.0 + self.epsilon) * advantage\n",
    "        policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "        # Value function loss\n",
    "        value_loss = nn.MSELoss()(value, target_value.detach())\n",
    "\n",
    "        # Total loss\n",
    "        loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "        # Update the actor-critic model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        print(f\"Agent {self.agent_id} - PPO update: policy loss = {policy_loss.item()}, value loss = {value_loss.item()}\")\n",
    "\n",
    "    def print_model_parameters(self):\n",
    "        print(f\"Model parameters for Agent {self.agent_id}:\")\n",
    "        for name, param in self.actor_critic.named_parameters():\n",
    "            print(f\"  {name}: {param.data.numpy()}\")\n",
    "        print(f\"End of model parameters for Agent {self.agent_id}\\n\")\n",
    "\n",
    "\n",
    "class Case:\n",
    "    added_states = set()  # Class attribute to store states already added to the case base\n",
    "\n",
    "    def __init__(self, problem, solution, reward,  trust_value=1):\n",
    "        # self.problem = problem if isinstance(problem, list) else ast.literal_eval(problem)  # Convert problem to numpy array\n",
    "        self.problem = problem\n",
    "        self.solution = solution\n",
    "        self.reward = reward\n",
    "        self.trust_value = trust_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)\n",
    "        state2 = np.atleast_1d(state2)\n",
    "        CNDMaxDist = 6  # Maximum distance between two nodes in the-0.9, 0.7, 0.0, 0.0, 0.0, -0.9) CND based on EOPRA reference\n",
    "        v = state1.size  # Total number of objects the agent can perceive\n",
    "        DistQ = np.sum([Case.dist_q(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def dist_q(X1, X2):\n",
    "        return np.min(np.abs(X1 - X2))\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(agent, env, state, case_base, threshold=0.1):\n",
    "\n",
    "        # Convert the state to numpy if it's a tensor\n",
    "        if isinstance(state, torch.Tensor):\n",
    "            state = state.cpu().numpy()\n",
    "\n",
    "        # Slice the physical observations\n",
    "        physical_obs = state[:6]\n",
    "\n",
    "        if not agent.silent:\n",
    "            comm_obs = state[6:]\n",
    "            # Convert comm_obs to a numpy array if it is a tensor\n",
    "            if isinstance(comm_obs, torch.Tensor):\n",
    "                comm_obs = comm_obs.cpu().numpy()\n",
    "\n",
    "        # print(f\"physical_obs = {physical_obs}\")\n",
    "\n",
    "        # Ensure the state is in a list format to avoid issues with ast.literal_eval\n",
    "        state_list = state.tolist() if isinstance(state, np.ndarray) else state\n",
    "        state_str = json.dumps(state_list)  # Convert list to a JSON string for ast.literal_eval\n",
    "\n",
    "        # Use ast.literal_eval safely to convert the string back to a list\n",
    "        state = ast.literal_eval(state_str)\n",
    "\n",
    "        similarities = {}\n",
    "        for case in case_base:\n",
    "            problem_numeric = np.array(case.problem, dtype=float)\n",
    "            state_numeric = np.array(state, dtype=float)\n",
    "            \n",
    "            # print(f\"state received = {state_numeric}\")\n",
    "            # print(f\"case received = {problem_numeric}\")\n",
    "            # print(\"---------\")\n",
    "           \n",
    "            similarities[case] = Case.sim_q(state_numeric, problem_numeric)  # Compare state with the problem part of the case\n",
    "\n",
    "        sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if sorted_similarities:\n",
    "            most_similar_case = sorted_similarities[0][0] if sorted_similarities[0][1] >= threshold else None\n",
    "        else:\n",
    "            most_similar_case = None\n",
    "\n",
    "        return most_similar_case\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(c, temporary_case_base):\n",
    "        temporary_case_base.append(c)\n",
    "\n",
    "    @staticmethod\n",
    "    def revise(case_base, temporary_case_base, successfull_task):\n",
    "        for case in temporary_case_base:\n",
    "            if successfull_task and case in case_base:\n",
    "                case.trust_value += 0.1  # Increment trust value if the episode ended successfully and the case is in the case base\n",
    "            elif not successfull_task and case in case_base:\n",
    "                case.trust_value -= 0.1  # Decrement trust value if the episode ended unsuccessfully and the case is in the case base\n",
    "            case.trust_value = max(0, min(case.trust_value, 1))  # Ensure trust value is within [0, 1]\n",
    "\n",
    "    @staticmethod\n",
    "    def retain(case_base, temporary_case_base, successfull_task, threshold=0.7):\n",
    "        if successfull_task:\n",
    "            # Iterate through the temporary case base to find the last occurrence of each unique state\n",
    "            for case in reversed(temporary_case_base):\n",
    "                state = tuple(np.atleast_1d(case.problem))\n",
    "                # Check if the state is already in the case base or has been added previously\n",
    "                if state not in Case.added_states:\n",
    "                    # Add the case to the case base if the state is new\n",
    "                    case_base.append(case)\n",
    "                    Case.added_states.add(state)\n",
    "                else:\n",
    "                    # Find the index of the existing case in the case base\n",
    "                    existing_index = next((i for i, c in enumerate(case_base) if tuple(np.atleast_1d(c.problem)) == state), None)\n",
    "                    if existing_index is not None:\n",
    "                        # Get the existing case from the case base\n",
    "                        existing_case = case_base[existing_index]\n",
    "                        # Update the trust value of the existing case with the new value from the revise step\n",
    "                        existing_case.trust_value = case.trust_value\n",
    "\n",
    "        # Filter case_base based on trust_value\n",
    "        case_base = [case for case in case_base if case.trust_value >= threshold]\n",
    "        return case_base\n",
    "\n",
    "\n",
    "class QCBRLVmasRunner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        render: bool,\n",
    "        num_envs: int,\n",
    "        num_episodes: int,\n",
    "        max_steps_per_episode: int,\n",
    "        device: str,\n",
    "        scenario: Union[str, BaseScenario],\n",
    "        continuous_actions: bool,\n",
    "        random_action: bool,\n",
    "        n_agents: int,\n",
    "        obs_discrete: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.render = render\n",
    "        self.num_envs = num_envs\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "        self.device = device\n",
    "        self.scenario = scenario\n",
    "        self.continuous_actions = continuous_actions\n",
    "        self.random_action = random_action\n",
    "        self.obs_discrete = obs_discrete\n",
    "        self.kwargs = kwargs\n",
    "        self.frame_list = []  \n",
    "        self.problem_solver_agents = []\n",
    "        self.rewards_history = []  \n",
    "        self.action_counts = {i: {} for i in range(n_agents)}  \n",
    "        self.agent_rewards_history = {i: [] for i in range(n_agents)}\n",
    "        self.successful_episodes_individual = {i: 0 for i in range(n_agents)}  # Track successful episodes for each agent\n",
    "        self.successful_episodes_all_agents = 0  # Track episodes where all agents succeed\n",
    "        self.case_base = {i: [] for i in range(n_agents)}  # Separate case base for each agent\n",
    "        self.temporary_case_base = {i: [] for i in range(n_agents)}  # Separate temporary case base for each agent\n",
    "\n",
    "    \n",
    "\n",
    "    def get_distance_category(self, distance):\n",
    "        \n",
    "        if distance == 0:\n",
    "            distance_qualitative = 0 #at\n",
    "        elif distance < 0.1:\n",
    "            distance_qualitative = 1 #very close\n",
    "        elif distance < 0.25:\n",
    "            distance_qualitative = 2 #close\n",
    "        elif distance < 0.5:\n",
    "            distance_qualitative = 3 #far\n",
    "        elif distance < 75:\n",
    "            distance_qualitative = 4 #far #very far\n",
    "        else:\n",
    "            distance_qualitative = 5 #farthest\n",
    "        \n",
    "        return distance_qualitative\n",
    "        \n",
    "    def get_direction_category(self, agent_x, agent_y, target_x, target_y):\n",
    "        dx = target_x - agent_x\n",
    "        dy = target_y - agent_y\n",
    "\n",
    "        if dy > 0:\n",
    "            if dx > 0:\n",
    "                direction_qualitative = 1 #top right\n",
    "            elif dx < 0:\n",
    "                direction_qualitative = 2 #top left\n",
    "            else:\n",
    "                direction_qualitative = 3 #top\n",
    "        elif dy < 0:\n",
    "            if dx > 0:\n",
    "                direction_qualitative = 4 #bottom right\n",
    "            elif dx < 0:\n",
    "                direction_qualitative = 5 #bottom left\n",
    "            else:\n",
    "                direction_qualitative = 6 #bottom\n",
    "        else:\n",
    "            if dx > 0:\n",
    "                direction_qualitative = 7 #right\n",
    "            elif dx < 0:\n",
    "                direction_qualitative = 8 #left\n",
    "            else:\n",
    "                direction_qualitative = 0 #at\n",
    "            \n",
    "        return direction_qualitative\n",
    "\n",
    "    def _get_qualitative_position(self, env, observation):\n",
    "\n",
    "        pos_x = observation[0:1]\n",
    "        pos_y = observation[1:2]\n",
    "        vel = observation[2:4]\n",
    "        goal_x = observation[4:5]\n",
    "        goal_y = observation[5:6]    \n",
    "        comms_data = observation[6:13]\n",
    "        sensor_data = observation[13:]\n",
    "\n",
    "        distance = math.sqrt((goal_x - pos_x)**2 + (goal_y - pos_y)**2) #Eucledian distance\n",
    "        # print(f\"distance: {distance}\")\n",
    "        distance_category = self.get_distance_category(distance)\n",
    "        direction_category = self.get_direction_category(pos_x, pos_y, goal_x, goal_y)\n",
    "      \n",
    "        return distance_category, direction_category\n",
    "\n",
    "\n",
    "    def _get_action(self, agent, env, agent_id, obs_qualitative, obs_quantitative, device):\n",
    "\n",
    "        # print(f\"obs_qualitative: {obs_qualitative}\")\n",
    "        # print(f\"obs_quantitative: {obs_quantitative}\")\n",
    "\n",
    "        physical_action=[]\n",
    "        comm_action_length = 4\n",
    "\n",
    "        case = Case.retrieve(agent, env, obs_qualitative, self.case_base[agent_id], threshold=0.1)\n",
    "                    \n",
    "        if case:\n",
    "           \n",
    "            physical_action = case.solution\n",
    "            print(f\"action type of agent {agent_id}: case base\")\n",
    "        \n",
    "        else:    \n",
    "            \n",
    "            physical_obs_quantitative = obs_quantitative[0:6]\n",
    "\n",
    "            if self.continuous_actions:\n",
    "                physical_action = self.problem_solver_agents[agent_id].get_action_continuous(agent, env, agent_id, physical_obs_quantitative, device)\n",
    "                physical_action_tensor = torch.tensor(physical_action, device=self.device)\n",
    "                # physical_action = (0.1, -0.5)\n",
    "               \n",
    "            else:\n",
    "                physical_action = self.problem_solver_agents[agent_id].get_action_discrete(agent, env, agent_id, physical_obs_quantitative, device)\n",
    "            \n",
    "           \n",
    "        physical_action_tensor = torch.tensor(physical_action, device=self.device)\n",
    "        zero_tensor = torch.zeros(comm_action_length, dtype=torch.float64, device=self.device)\n",
    "        physical_action_tensor_padded = torch.cat((physical_action_tensor, zero_tensor))\n",
    "\n",
    "        # print(f\"physical action: {physical_action}\")\n",
    "        # print(f\"physical action Tensor: {physical_action_tensor}\")\n",
    "        # print (f\"shape of physical action: {physical_action_tensor.shape}\")\n",
    "            \n",
    "        if agent.silent:\n",
    "            action = physical_action_tensor_padded\n",
    "        else:\n",
    "            if not case:\n",
    "                obs_qualitative_tensor = torch.tensor([float(99), float(99)], device=self.device)\n",
    "                reward_tensor = torch.tensor([float(0)], device=self.device)\n",
    "                trust_value_tensor = torch.tensor([float(0)], device=self.device)\n",
    "            else:  \n",
    "                obs_qualitative_tensor = torch.tensor(case.problem, device=self.device)\n",
    "                reward_tensor = torch.tensor(case.reward, device=self.device)\n",
    "                trust_value_tensor = torch.tensor(case.trust_value, device=self.device)\n",
    "\n",
    "            comm_action_tensor = torch.cat([obs_qualitative_tensor, physical_action_tensor, reward_tensor, trust_value_tensor], dim=0)\n",
    "            action = torch.stack((physical_action_tensor_padded, comm_action_tensor)).unsqueeze(0)\n",
    "\n",
    "                # print(f\"Comm action Tensor: {comm_action_tensor}\")\n",
    "                # print (f\"shape of Comm action: {comm_action_tensor.shape}\")\n",
    "            \n",
    "            print(f\"action type of agent {agent_id}: problem solver\")\n",
    "\n",
    "        # else:\n",
    "        #     obs_qualitative_tensor = torch.tensor(obs_qualitative, device=self.device)\n",
    "        #     reward_tensor = torch.tensor([2.4], device=self.device)\n",
    "        #     trust_value_tensor = torch.tensor([1], device=self.device)\n",
    "\n",
    "        #     comm_action_tensor = torch.cat([obs_qualitative_tensor, physical_action_tensor, reward_tensor, trust_value_tensor], dim=0)\n",
    "        #     action = torch.stack((physical_action_tensor_padded, comm_action_tensor)).unsqueeze(0)\n",
    "\n",
    "\n",
    "        # print(f\"action delivered from the function: {action}\")\n",
    "        # print (f\"shape of action: {action.shape}\")\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def save_case_base(self, agent_id):\n",
    "        filename = f\"cases/case_base_{agent_id}.json\"\n",
    "        case_base_data = []\n",
    "        for case in self.case_base[agent_id]:\n",
    "            problem = case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem\n",
    "            \n",
    "            if torch.is_tensor(case.solution):\n",
    "                solution = case.solution.tolist() if case.solution.numel() > 1 else int(case.solution.item())\n",
    "            else:\n",
    "                solution = int(case.solution)\n",
    "            \n",
    "            if torch.is_tensor(case.trust_value):\n",
    "                trust_value = case.trust_value.tolist() if case.trust_value.numel() > 1 else float(case.trust_value.item())\n",
    "            else:\n",
    "                trust_value = float(case.trust_value)\n",
    "            \n",
    "            case_base_data.append({\n",
    "                \"problem\": problem,\n",
    "                \"solution\": solution,\n",
    "                \"trust_value\": trust_value\n",
    "            })\n",
    "        \n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "        print(\"Case base saved successfully.\")\n",
    "\n",
    "    def save_case_base_eps(self, agent_id, eps):\n",
    "        filename = f\"cases/case_base_{agent_id}_{eps}.json\"\n",
    "        case_base_data = []\n",
    "        for case in self.case_base[agent_id]:\n",
    "            problem = case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem\n",
    "            \n",
    "            if torch.is_tensor(case.solution):\n",
    "                solution = case.solution.tolist() if case.solution.numel() > 1 else int(case.solution.item())\n",
    "            else:\n",
    "                solution = int(case.solution)\n",
    "            \n",
    "            if torch.is_tensor(case.trust_value):\n",
    "                trust_value = case.trust_value.tolist() if case.trust_value.numel() > 1 else float(case.trust_value.item())\n",
    "            else:\n",
    "                trust_value = float(case.trust_value)\n",
    "            \n",
    "            case_base_data.append({\n",
    "                \"problem\": problem,\n",
    "                \"solution\": solution,\n",
    "                \"trust_value\": trust_value\n",
    "            })\n",
    "        \n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "        print(\"Case base saved successfully.\")\n",
    "\n",
    "\n",
    "    def save_case_base_temporary(self, agent_id,):\n",
    "        filename = f\"cases/case_base_temporary_{agent_id}.json\"\n",
    "        case_base_data = []\n",
    "        for case in self.temporary_case_base[agent_id]:\n",
    "            problem = case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem\n",
    "            \n",
    "            if torch.is_tensor(case.solution):\n",
    "                solution = case.solution.tolist() if case.solution.numel() > 1 else int(case.solution.item())\n",
    "            else:\n",
    "                solution = int(case.solution)\n",
    "            \n",
    "            if torch.is_tensor(case.trust_value):\n",
    "                trust_value = case.trust_value.tolist() if case.trust_value.numel() > 1 else float(case.trust_value.item())\n",
    "            else:\n",
    "                trust_value = float(case.trust_value)\n",
    "            \n",
    "            case_base_data.append({\n",
    "                \"problem\": problem,\n",
    "                \"solution\": solution,\n",
    "                \"trust_value\": trust_value\n",
    "            })\n",
    "        \n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "        print(\"Temporary case base saved successfully.\")\n",
    "\n",
    "    def save_case_base_temporary_eps(self, agent_id, eps):\n",
    "        filename = f\"cases/case_base_temporary_{agent_id}_{eps}.json\"\n",
    "        case_base_data = []\n",
    "        for case in self.temporary_case_base[agent_id]:\n",
    "            problem = case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem\n",
    "            \n",
    "            if torch.is_tensor(case.solution):\n",
    "                solution = case.solution.tolist() if case.solution.numel() > 1 else int(case.solution.item())\n",
    "            else:\n",
    "                solution = int(case.solution)\n",
    "            \n",
    "            if torch.is_tensor(case.trust_value):\n",
    "                trust_value = case.trust_value.tolist() if case.trust_value.numel() > 1 else float(case.trust_value.item())\n",
    "            else:\n",
    "                trust_value = float(case.trust_value)\n",
    "            \n",
    "            case_base_data.append({\n",
    "                \"problem\": problem,\n",
    "                \"solution\": solution,\n",
    "                \"trust_value\": trust_value\n",
    "            })\n",
    "        \n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "        print(\"Temporary case base saved successfully.\")\n",
    "\n",
    "        \n",
    "    def load_case_base(self, agent_id):\n",
    "        filename = f\"cases/case_base_{agent_id}.json\"\n",
    "        try:\n",
    "            with open(filename, 'r') as file:\n",
    "                case_base_data = json.load(file)\n",
    "            self.case_base[agent_id] = [Case(problem=np.array(case[\"problem\"]) if isinstance(case[\"problem\"], list) else case[\"problem\"],\n",
    "                                            solution=case[\"solution\"],\n",
    "                                            trust_value=case[\"trust_value\"]) for case in case_base_data]\n",
    "        except FileNotFoundError:\n",
    "            self.case_base[agent_id] = []\n",
    "\n",
    "    \n",
    "    def generate_gif(self, scenario_name):\n",
    "        fps = 25\n",
    "        clip = ImageSequenceClip(self.frame_list, fps=fps)\n",
    "        clip.write_gif(f'{scenario_name}.gif', fps=fps)\n",
    "        return HTML(f'<img src=\"{scenario_name}.gif\">')\n",
    "\n",
    "    def plot_action_distribution(self):\n",
    "        num_agents = len(self.action_counts)\n",
    "\n",
    "        for agent_id, counts in self.action_counts.items():\n",
    "            unique_actions, action_counts = np.unique(list(counts.values()), return_counts=True)\n",
    "            action_dict = dict(zip(unique_actions, action_counts))\n",
    "            plt.bar(action_dict.keys(), action_dict.values(), label=f'Agent {agent_id}', alpha=0.7)\n",
    "\n",
    "        plt.xlabel('Action')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Action Distribution for Each Agent')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_rewards_history(self):\n",
    "        num_agents = len(self.agent_rewards_history)\n",
    "\n",
    "        for agent_id, rewards in self.agent_rewards_history.items():\n",
    "            plt.plot(range(1, self.num_episodes + 1), rewards, label=f'Agent {agent_id}')\n",
    "\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Total Reward per Episode for Each Agent')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def run_vmas_env(self):\n",
    "        scenario_name = self.scenario if isinstance(self.scenario, str) else self.scenario.__class__.__name__\n",
    "\n",
    "        env = make_env(\n",
    "            scenario=self.scenario,\n",
    "            num_envs=self.num_envs,\n",
    "            device=self.device,\n",
    "            continuous_actions=self.continuous_actions,\n",
    "            **self.kwargs\n",
    "        )\n",
    "        \n",
    "        states = env.reset()\n",
    "        # print(f\"states first agent after reset: {states}\")\n",
    "        # print(f\"states first agent shape after reset: {states[0].shape}\")\n",
    "        first_agent_state = states[0][0:6]\n",
    "        state_dim = first_agent_state.shape[0]\n",
    "        action_dim = len(env.action_space)\n",
    "        # print(f\"states after reset: {states}\")\n",
    "        # print(f\"state dim: {state_dim}\")\n",
    "        # print(f\"action dim: {action_dim}\")\n",
    "        \n",
    "        for agent_id, agent in enumerate(env.agents):\n",
    "            self.problem_solver_agents.append(ProblemSolver(env, agent_id, state_dim, action_dim, self.device, communication_weight=0.5))\n",
    "\n",
    "        init_time = time.time()\n",
    "        total_steps = 0\n",
    "        is_episode_success = False\n",
    "\n",
    "        for episode in range(self.num_episodes):\n",
    "            # print(f\"Episode {episode}\")\n",
    "            obs_cont = env.reset()\n",
    "            # print(f\"observation continous: {obs_cont}\")\n",
    "\n",
    "            dones = torch.tensor([False] * self.num_envs, device=self.device)\n",
    "            step = 0\n",
    "            \n",
    "            episode_rewards = {i: 0 for i in range(len(self.problem_solver_agents))}\n",
    "            episode_dones_counts = {i: 0 for i in range(len(env.agents))}  # Track dones counts for each agent\n",
    "            \n",
    "            self.temporary_case_base = {i: [] for i in range(len(env.agents))}\n",
    "            \n",
    "            while not torch.all(dones).item() and step < self.max_steps_per_episode:\n",
    "                step += 1\n",
    "                total_steps += 1\n",
    "                print(f\"Step {step} of Episode {episode}\")\n",
    "\n",
    "                actions = []\n",
    "               \n",
    "\n",
    "                for agent_id, agent in enumerate(env.agents):\n",
    "                    # print(f\"agent {agent_id}\")\n",
    "                    # print(f\"agent {agent}\")\n",
    "                    if self.obs_discrete:\n",
    "                        obs_qualitative = self._get_qualitative_position(env, obs_cont[agent_id])\n",
    "                        \n",
    "                    # print(f\"observation continuous agent{agent_id} = {obs_cont[agent_id]}\")\n",
    "                    # print(f\"observation qualitative agent{agent_id} = {obs_qualitative}\")\n",
    "\n",
    "                    action = self._get_action(agent, env, agent_id, obs_qualitative, obs_cont[agent_id], self.device)\n",
    "                    \n",
    "                    actions.append(action)\n",
    "\n",
    "\n",
    "                # print(f\"action taken: {actions}\")\n",
    "                # print(f\"actions agent {agent_id} shapes: {actions[agent_id].shape}\")\n",
    "                # print(f\"actions size from environment: {env.get_agent_action_size(agent)}\")\n",
    "\n",
    "                next_obs_cont, rewards, dones, info = env.step(actions)\n",
    "                # print(f\"next obs first agents = {next_obs_cont[0]}\")\n",
    "                # print(f\"next obs first agents shape = {next_obs_cont[0].shape}\")\n",
    "                # print(f\"reward all agents = {rewards}\")\n",
    "                \n",
    "\n",
    "                for agent_id, agent in enumerate(env.agents):\n",
    "                    if self.obs_discrete:\n",
    "                        obs_qualitative = self._get_qualitative_position(env, obs_cont[agent_id])\n",
    "                        discrete_next_obs = self._get_qualitative_position(env, next_obs_cont[agent_id])\n",
    "\n",
    "                    problem = obs_qualitative\n",
    "                    physical_action = actions[agent_id][0,0,0:2]\n",
    "                    reward = rewards[agent_id]\n",
    "                    done = dones[0][agent_id]\n",
    "\n",
    "                    new_case = Case(problem, physical_action, reward)\n",
    "                    Case.reuse(new_case, self.temporary_case_base[agent_id])\n",
    "\n",
    "                    # print(f\"observation before pass: {obs_cont[agent_id]}\")\n",
    "                    # print(f\"physical action before pass: {physical_action}\")\n",
    "                    # print(f\"reward before pass: {reward}\")\n",
    "                    # print(f\"next observation before pass: {next_obs_cont[agent_id]}\")\n",
    "                    # print(f\"dones before pass: {dones}\")\n",
    "                    # print(f\"done before pass: {done}\")\n",
    "\n",
    "                    if self.continuous_actions:\n",
    "\n",
    "                        self.problem_solver_agents[agent_id].update_action_continuous( \n",
    "                            obs_cont[agent_id], physical_action, reward, next_obs_cont[agent_id], done\n",
    "                        )\n",
    "                    else:\n",
    "                        self.problem_solver_agents[agent_id].update_action_discrete(agent, env, agent_id,\n",
    "                             obs_cont[agent_id], physical_action, rewards[agent_id].item(), next_obs_cont[agent_id]\n",
    "                        )\n",
    "                    # Accumulate rewards for each agent within the episode\n",
    "                    episode_rewards[agent_id] += rewards[agent_id].item()\n",
    "                    \n",
    "                    print (f\"dones: {dones[0]}\")\n",
    "                    if (dones[0][agent_id]):  # Increment individual agent's done count\n",
    "                        episode_dones_counts[agent_id] += 1\n",
    "                \n",
    "                # done = dones\n",
    "                obs_cont = next_obs_cont\n",
    "\n",
    "                                \n",
    "                # print(f\"dones status for all agents = {done}\")\n",
    "                # print(\"--------------------\")\n",
    "\n",
    "                if self.render:\n",
    "                    frame = env.render(\n",
    "                        mode=\"rgb_array\",\n",
    "                        agent_index_focus=None,\n",
    "                    )\n",
    "                    self.frame_list.append(frame)\n",
    "\n",
    "                # print(\"-----------------\")\n",
    "            # Update rewards history after each episode\n",
    "            for agent_id, total_reward in episode_rewards.items():\n",
    "                self.agent_rewards_history[agent_id].append(total_reward)\n",
    "\n",
    "            # Update success based on individual agent's done status\n",
    "            for agent_id in range(len(env.agents)):\n",
    "                print(f\"done status for agent {agent_id}: {dones[0][agent_id]}\")\n",
    "                # print(f\"dones status for agent {agent_id} v2: {dones[0, agent_id]}\")\n",
    "                if dones[0][agent_id]:\n",
    "                    self.successful_episodes_individual[agent_id] += episode_dones_counts[agent_id] / step  # Calculate success rate for the agent\n",
    "\n",
    "            # Update success based on all agents' done status\n",
    "            if torch.all(dones).item():\n",
    "                self.successful_episodes_all_agents += 1\n",
    "            \n",
    "            # Calculate success percentages for each agent\n",
    "            success_percents = [self.successful_episodes_individual[agent_id] / (episode + 1) * 100 for agent_id in range(len(env.agents))]\n",
    "            overall_success_percent = self.successful_episodes_all_agents / (episode + 1) * 100\n",
    "\n",
    "            print(f\"Success percentage of each agent at the end of episode {episode}:\")\n",
    "            \n",
    "            for agent_id, percent in enumerate(success_percents):\n",
    "                print(f\"Agent {agent_id}: {percent}%\")\n",
    "\n",
    "            print(f\"Overall success percentage for all agents up to episode {episode}: {overall_success_percent}%\")\n",
    "\n",
    "\n",
    "            for agent_id in range(len(env.agents)):\n",
    "                print(f\"done status for agent {agent_id}: {dones[0][agent_id]}\")\n",
    "                \n",
    "                # Case.revise(self.case_base[agent_id], self.temporary_case_base[agent_id], torch.any(dones).item())\n",
    "                # self.case_base[agent_id] = Case.retain(\n",
    "                #     self.case_base[agent_id], self.temporary_case_base[agent_id], torch.any(dones).item()\n",
    "                # )\n",
    "\n",
    "                Case.revise(self.case_base[agent_id], self.temporary_case_base[agent_id], dones[0][agent_id])\n",
    "                self.case_base[agent_id] = Case.retain(\n",
    "                    self.case_base[agent_id], self.temporary_case_base[agent_id], dones[0][agent_id]\n",
    "                )\n",
    "\n",
    "                # for case in self.temporary_case_base[agent_id]:\n",
    "                #     print(f\"Step {step} -- Problem Stored in Temp CB: {case.problem}, Solution Stored in Temp CB: {case.solution}\")\n",
    "\n",
    "                # for case in self.case_base[agent_id]:\n",
    "                #     print(f\"Step {step} -- Problem Stored in CB: {case.problem}, Solution Stored in CB: {case.solution}\")\n",
    "\n",
    "                self.save_case_base_temporary_eps(agent_id, episode)  # Save temporary case base after each episode\n",
    "                self.save_case_base_eps(agent_id, episode)  # Save case base after each episode\n",
    "            \n",
    "            torch.cuda.empty_cache()  # Free up unused memory\n",
    "\n",
    "        \n",
    "        for agent_id, agent in enumerate(env.agents):\n",
    "            self.save_case_base_temporary(agent_id)  # Save temporary case base after training\n",
    "            self.save_case_base(agent_id)  # Save case base after training\n",
    "\n",
    "        # Print final success percentages for each agent-0.9, 0.7, 0.0, 0.0, 0.0, -0.9)\n",
    "        \n",
    "        # Print overall success percentage for all agents\n",
    "        overall_success_percentage = self.successful_episodes_all_agents / self.num_episodes * 100\n",
    "        print(f\"Overall success percentage for all agents = {overall_success_percentage}%\")\n",
    "\n",
    "\n",
    "\n",
    "        total_time = time.time() - init_time\n",
    "        print(\n",
    "            f\"It took: {total_time}s for {total_steps} steps across {self.num_episodes} episodes of {self.num_envs} parallel environments on device {self.device} \"\n",
    "            f\"for {scenario_name} scenario.\"\n",
    "        )\n",
    "\n",
    "        # success_percentage = (self.successful_episodes / self.num_episodes) * 100\n",
    "        # print(f\"Percentage of successful episodes: {success_percentage}%\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scenario_name = \"navigation_comm\"\n",
    "    use_cuda = True\n",
    "\n",
    "    env_runner = QCBRLVmasRunner( \n",
    "        render=True,\n",
    "        num_envs=1,\n",
    "        num_episodes=2,\n",
    "        max_steps_per_episode=5,\n",
    "        device = torch.device(\"cuda\" if use_cuda else \"cpu\"),\n",
    "        scenario=scenario_name,\n",
    "        continuous_actions=True,\n",
    "        random_action=False,\n",
    "        n_agents=2,\n",
    "        obs_discrete=True,\n",
    "        agents_with_same_goal=2,\n",
    "        collisions=False,\n",
    "        shared_rew=False,\n",
    "    )\n",
    "\n",
    "    env_runner.run_vmas_env()\n",
    "    # for agent in env_runner.problem_solver_agents:\n",
    "    #     agent.print_q_table()\n",
    "    env_runner.plot_rewards_history()\n",
    "\n",
    "    ipython_display(env_runner.generate_gif(scenario_name))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
