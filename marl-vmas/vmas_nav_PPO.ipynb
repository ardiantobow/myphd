{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "\n",
    "# Tensordict modules\n",
    "from torch import multiprocessing\n",
    "\n",
    "# Data collection\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "\n",
    "# Env\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "\n",
    "# Multi-agent network\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor, TanhNormal\n",
    "\n",
    "# Loss\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "\n",
    "# Utils\n",
    "torch.manual_seed(0)\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devices\n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "vmas_device = device  # The device where the simulator is run (VMAS can run on GPU)\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 6_000  # Number of team frames collected per training iteration\n",
    "n_iters = 10  # Number of sampling and training iterations\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 30  # Number of optimization steps per training iteration\n",
    "minibatch_size = 400  # Size of the mini-batches in each optimization step\n",
    "lr = 3e-4  # Learning rate\n",
    "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
    "\n",
    "# PPO\n",
    "clip_epsilon = 0.2  # clip value for PPO loss\n",
    "gamma = 0.9  # discount factor\n",
    "lmbda = 0.9  # lambda for generalised advantage estimation\n",
    "entropy_eps = 1e-4  # coefficient of the entropy term in the PPO loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 100  # Episode steps before done\n",
    "num_vmas_envs = (\n",
    "    frames_per_batch // max_steps\n",
    ")  # Number of vectorized envs. frames_per_batch should be divisible by this number\n",
    "scenario_name = \"navigation\"\n",
    "n_agents = 3\n",
    "\n",
    "env = VmasEnv(\n",
    "    scenario=scenario_name,\n",
    "    num_envs=num_vmas_envs,\n",
    "    continuous_actions=True,  # VMAS supports both continuous and discrete actions\n",
    "    max_steps=max_steps,\n",
    "    device=vmas_device,\n",
    "    # Scenario kwargs\n",
    "    n_agents=n_agents,  # These are custom kwargs that change for each VMAS scenario, see the VMAS repo to know more.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: CompositeSpec(\n",
      "    agents: CompositeSpec(\n",
      "        action: BoundedTensorSpec(\n",
      "            shape=torch.Size([60, 3, 2]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([60, 3, 2]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([60, 3, 2]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "            device=cuda:0,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous), device=cuda:0, shape=torch.Size([60, 3])), device=cuda:0, shape=torch.Size([60]))\n",
      "reward_spec: CompositeSpec(\n",
      "    agents: CompositeSpec(\n",
      "        reward: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([60, 3, 1]),\n",
      "            space=None,\n",
      "            device=cuda:0,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous), device=cuda:0, shape=torch.Size([60, 3])), device=cuda:0, shape=torch.Size([60]))\n",
      "done_spec: CompositeSpec(\n",
      "    done: DiscreteTensorSpec(\n",
      "        shape=torch.Size([60, 1]),\n",
      "        space=DiscreteBox(n=2),\n",
      "        device=cuda:0,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    terminated: DiscreteTensorSpec(\n",
      "        shape=torch.Size([60, 1]),\n",
      "        space=DiscreteBox(n=2),\n",
      "        device=cuda:0,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete), device=cuda:0, shape=torch.Size([60]))\n",
      "observation_spec: CompositeSpec(\n",
      "    agents: CompositeSpec(\n",
      "        observation: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([60, 3, 18]),\n",
      "            space=None,\n",
      "            device=cuda:0,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        info: CompositeSpec(\n",
      "            pos_rew: UnboundedContinuousTensorSpec(\n",
      "                shape=torch.Size([60, 3, 1]),\n",
      "                space=None,\n",
      "                device=cuda:0,\n",
      "                dtype=torch.float32,\n",
      "                domain=continuous),\n",
      "            final_rew: UnboundedContinuousTensorSpec(\n",
      "                shape=torch.Size([60, 3, 1]),\n",
      "                space=None,\n",
      "                device=cuda:0,\n",
      "                dtype=torch.float32,\n",
      "                domain=continuous),\n",
      "            agent_collisions: UnboundedContinuousTensorSpec(\n",
      "                shape=torch.Size([60, 3, 1]),\n",
      "                space=None,\n",
      "                device=cuda:0,\n",
      "                dtype=torch.float32,\n",
      "                domain=continuous), device=cuda:0, shape=torch.Size([60, 3])), device=cuda:0, shape=torch.Size([60, 3])), device=cuda:0, shape=torch.Size([60]))\n"
     ]
    }
   ],
   "source": [
    "print(\"action_spec:\", env.full_action_spec)\n",
    "print(\"reward_spec:\", env.full_reward_spec)\n",
    "print(\"done_spec:\", env.full_done_spec)\n",
    "print(\"observation_spec:\", env.observation_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_keys: [('agents', 'action')]\n",
      "reward_keys: [('agents', 'reward')]\n",
      "done_keys: ['done', 'terminated']\n"
     ]
    }
   ],
   "source": [
    "print(\"action_keys:\", env.action_keys)\n",
    "print(\"reward_keys:\", env.reward_keys)\n",
    "print(\"done_keys:\", env.done_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcheck_env_specs\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torchrl/envs/utils.py:471\u001b[0m, in \u001b[0;36mcheck_env_specs\u001b[0;34m(env, return_contiguous, check_dtype, seed)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m check_env_specs(\n\u001b[1;32m    467\u001b[0m             env, return_contiguous\u001b[38;5;241m=\u001b[39mreturn_contiguous, check_dtype\u001b[38;5;241m=\u001b[39mcheck_dtype\n\u001b[1;32m    468\u001b[0m         )\n\u001b[1;32m    470\u001b[0m fake_tensordict \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mfake_tensordict()\n\u001b[0;32m--> 471\u001b[0m real_tensordict \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_contiguous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_contiguous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_contiguous:\n\u001b[1;32m    474\u001b[0m     fake_tensordict \u001b[38;5;241m=\u001b[39m fake_tensordict\u001b[38;5;241m.\u001b[39munsqueeze(real_tensordict\u001b[38;5;241m.\u001b[39mbatch_dims \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torchrl/envs/common.py:2430\u001b[0m, in \u001b[0;36mEnvBase.rollout\u001b[0;34m(self, max_steps, policy, callback, auto_reset, auto_cast_to_device, break_when_any_done, return_contiguous, tensordict, out)\u001b[0m\n\u001b[1;32m   2420\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2421\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensordict\u001b[39m\u001b[38;5;124m\"\u001b[39m: tensordict,\n\u001b[1;32m   2422\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_cast_to_device\u001b[39m\u001b[38;5;124m\"\u001b[39m: auto_cast_to_device,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2427\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m: callback,\n\u001b[1;32m   2428\u001b[0m }\n\u001b[1;32m   2429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m break_when_any_done:\n\u001b[0;32m-> 2430\u001b[0m     tensordicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rollout_stop_early\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2432\u001b[0m     tensordicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rollout_nonstop(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torchrl/envs/common.py:2467\u001b[0m, in \u001b[0;36mEnvBase._rollout_stop_early\u001b[0;34m(self, tensordict, auto_cast_to_device, max_steps, policy, policy_device, env_device, callback)\u001b[0m\n\u001b[1;32m   2465\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2466\u001b[0m         tensordict\u001b[38;5;241m.\u001b[39mclear_device_()\n\u001b[0;32m-> 2467\u001b[0m tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2468\u001b[0m tensordicts\u001b[38;5;241m.\u001b[39mappend(tensordict\u001b[38;5;241m.\u001b[39mclone(\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m   2470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m max_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2471\u001b[0m     \u001b[38;5;66;03m# we don't truncated as one could potentially continue the run\u001b[39;00m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torchrl/envs/common.py:1409\u001b[0m, in \u001b[0;36mEnvBase.step\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_tensordict_shape(tensordict)\n\u001b[1;32m   1407\u001b[0m next_preset \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1409\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_proc_data(next_tensordict)\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1412\u001b[0m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torchrl/envs/transforms/transforms.py:738\u001b[0m, in \u001b[0;36mTransformedEnv._step\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    736\u001b[0m next_preset \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    737\u001b[0m tensordict_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39minv(tensordict)\n\u001b[0;32m--> 738\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     next_tensordict\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    744\u001b[0m         next_preset\u001b[38;5;241m.\u001b[39mexclude(\u001b[38;5;241m*\u001b[39mnext_tensordict\u001b[38;5;241m.\u001b[39mkeys(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    745\u001b[0m     )\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torchrl/envs/libs/vmas.py:541\u001b[0m, in \u001b[0;36mVmasWrapper._step\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    538\u001b[0m     action_list \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m group_action_list\n\u001b[1;32m    539\u001b[0m action \u001b[38;5;241m=\u001b[39m [action_list[agent_indices[i]] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_agents)]\n\u001b[0;32m--> 541\u001b[0m obs, rews, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_done(dones)\n\u001b[1;32m    545\u001b[0m source \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m\"\u001b[39m: dones, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminated\u001b[39m\u001b[38;5;124m\"\u001b[39m: dones\u001b[38;5;241m.\u001b[39mclone()}\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/vmas/simulator/environment/environment.py:257\u001b[0m, in \u001b[0;36mEnvironment.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# assert (\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m#     actions[i].shape[0] == self.num_envs\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m# ), f\"Actions used in input of env must be of len {self.num_envs}, got {actions[i].shape[0]}\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# set action for each agent\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents):\n\u001b[0;32m--> 257\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Scenarios can define a custom action processor. This step takes care also of scripted agents automatically\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39magents:\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/vmas/simulator/environment/environment.py:497\u001b[0m, in \u001b[0;36mEnvironment._set_action\u001b[0;34m(self, action, agent)\u001b[0m\n\u001b[1;32m    487\u001b[0m action_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuous_actions:\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;66;03m# physical_action = action[:, action_index : action_index + agent.action_size]\u001b[39;00m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;66;03m# print (f\"action received by the environment: {action}\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;66;03m# print (f\"physical action received by the environment: {action[0, 0, 0:2]}\")\u001b[39;00m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# print (f\"physical action shape received by the environment: {action[0, 0, 0:2].shape}\")\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     physical_action \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;66;03m# print (f\"action sequeezed by the environment: {physical_action}\")\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# print (f\"action sequeezed shape received by the environment: {physical_action[0:2].shape}\")\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# # print(f\"physical action detected in environment:{physical_action}\")\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     action_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39mdim_p\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout of three steps: TensorDict(\n",
      "    fields={\n",
      "        agents: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([60, 5, 3, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                episode_reward: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                info: TensorDict(\n",
      "                    fields={\n",
      "                        agent_collisions: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                        final_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                        pos_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "                    batch_size=torch.Size([60, 5, 3]),\n",
      "                    device=cuda:0,\n",
      "                    is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([60, 5, 3, 18]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "            batch_size=torch.Size([60, 5, 3]),\n",
      "            device=cuda:0,\n",
      "            is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([60, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                agents: TensorDict(\n",
      "                    fields={\n",
      "                        episode_reward: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                        info: TensorDict(\n",
      "                            fields={\n",
      "                                agent_collisions: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                                final_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                                pos_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "                            batch_size=torch.Size([60, 5, 3]),\n",
      "                            device=cuda:0,\n",
      "                            is_shared=True),\n",
      "                        observation: Tensor(shape=torch.Size([60, 5, 3, 18]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                        reward: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "                    batch_size=torch.Size([60, 5, 3]),\n",
      "                    device=cuda:0,\n",
      "                    is_shared=True),\n",
      "                done: Tensor(shape=torch.Size([60, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                terminated: Tensor(shape=torch.Size([60, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "            batch_size=torch.Size([60, 5]),\n",
      "            device=cuda:0,\n",
      "            is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([60, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([60, 5]),\n",
      "    device=cuda:0,\n",
      "    is_shared=True)\n",
      "Shape of the rollout TensorDict: torch.Size([60, 5])\n"
     ]
    }
   ],
   "source": [
    "n_rollout_steps = 5\n",
    "rollout = env.rollout(n_rollout_steps)\n",
    "print(\"rollout of three steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_parameters_policy = True\n",
    "\n",
    "policy_net = torch.nn.Sequential(\n",
    "    MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[\n",
    "            -1\n",
    "        ],  # n_obs_per_agent\n",
    "        n_agent_outputs=2 * env.action_spec.shape[-1],  # 2 * n_actions_per_agents\n",
    "        n_agents=env.n_agents,\n",
    "        centralised=False,  # the policies are decentralised (ie each agent will act from its observation)\n",
    "        share_params=share_parameters_policy,\n",
    "        device=device,\n",
    "        depth=2,\n",
    "        num_cells=256,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    ),\n",
    "    NormalParamExtractor(),  # this will just separate the last dimension into two outputs: a loc and a non-negative scale\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = TensorDictModule(\n",
    "    policy_net,\n",
    "    in_keys=[(\"agents\", \"observation\")],\n",
    "    out_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.unbatched_action_spec,\n",
    "    in_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
    "    out_keys=[env.action_key],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"min\": env.unbatched_action_spec[env.action_key].space.low,\n",
    "        \"max\": env.unbatched_action_spec[env.action_key].space.high,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    "    log_prob_key=(\"agents\", \"sample_log_prob\"),\n",
    ")  # we'll need the log-prob for the PPO loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_parameters_critic = True\n",
    "mappo = True  # IPPO if False\n",
    "\n",
    "critic_net = MultiAgentMLP(\n",
    "    n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "    n_agent_outputs=1,  # 1 value per agent\n",
    "    n_agents=env.n_agents,\n",
    "    centralised=mappo,\n",
    "    share_params=share_parameters_critic,\n",
    "    device=device,\n",
    "    depth=2,\n",
    "    num_cells=256,\n",
    "    activation_class=torch.nn.Tanh,\n",
    ")\n",
    "\n",
    "critic = TensorDictModule(\n",
    "    module=critic_net,\n",
    "    in_keys=[(\"agents\", \"observation\")],\n",
    "    out_keys=[(\"agents\", \"state_value\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running policy: TensorDict(\n",
      "    fields={\n",
      "        agents: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([60, 3, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                episode_reward: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                info: TensorDict(\n",
      "                    fields={\n",
      "                        agent_collisions: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                        final_rew: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                        pos_rew: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "                    batch_size=torch.Size([60, 3]),\n",
      "                    device=cuda:0,\n",
      "                    is_shared=True),\n",
      "                loc: Tensor(shape=torch.Size([60, 3, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([60, 3, 18]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                sample_log_prob: Tensor(shape=torch.Size([60, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                scale: Tensor(shape=torch.Size([60, 3, 2]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "            batch_size=torch.Size([60, 3]),\n",
      "            device=cuda:0,\n",
      "            is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([60, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([60, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([60]),\n",
      "    device=cuda:0,\n",
      "    is_shared=True)\n",
      "Running value: TensorDict(\n",
      "    fields={\n",
      "        agents: TensorDict(\n",
      "            fields={\n",
      "                episode_reward: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                info: TensorDict(\n",
      "                    fields={\n",
      "                        agent_collisions: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                        final_rew: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                        pos_rew: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "                    batch_size=torch.Size([60, 3]),\n",
      "                    device=cuda:0,\n",
      "                    is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([60, 3, 18]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                state_value: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "            batch_size=torch.Size([60, 3]),\n",
      "            device=cuda:0,\n",
      "            is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([60, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([60, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([60]),\n",
      "    device=cuda:0,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Running policy:\", policy(env.reset()))\n",
    "print(\"Running value:\", critic(env.reset()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    device=vmas_device,\n",
    "    storing_device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(\n",
    "        frames_per_batch, device=device\n",
    "    ),  # We store the frames_per_batch collected at each iteration\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    "    batch_size=minibatch_size,  # We will sample minibatches of this size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy,\n",
    "    critic_network=critic,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_coef=entropy_eps,\n",
    "    normalize_advantage=False,  # Important to avoid normalizing across the agent dimension\n",
    ")\n",
    "loss_module.set_keys(  # We have to tell the loss where to find the keys\n",
    "    reward=env.reward_key,\n",
    "    action=env.action_key,\n",
    "    sample_log_prob=(\"agents\", \"sample_log_prob\"),\n",
    "    value=(\"agents\", \"state_value\"),\n",
    "    # These last 2 keys will be expanded to match the reward shape\n",
    "    done=(\"agents\", \"done\"),\n",
    "    terminated=(\"agents\", \"terminated\"),\n",
    ")\n",
    "\n",
    "\n",
    "loss_module.make_value_estimator(\n",
    "    ValueEstimators.GAE, gamma=gamma, lmbda=lmbda\n",
    ")  # We build GAE\n",
    "GAE = loss_module.value_estimator\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode_reward_mean = 0.9121366739273071:  30%|███       | 3/10 [00:13<00:30,  4.41s/it] "
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=n_iters, desc=\"episode_reward_mean = 0\")\n",
    "\n",
    "episode_reward_mean_list = []\n",
    "for tensordict_data in collector:\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"agents\", \"done\"),\n",
    "        tensordict_data.get((\"next\", \"done\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "    )\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"agents\", \"terminated\"),\n",
    "        tensordict_data.get((\"next\", \"terminated\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "    )\n",
    "    # We need to expand the done and terminated to match the reward shape (this is expected by the value estimator)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        GAE(\n",
    "            tensordict_data,\n",
    "            params=loss_module.critic_network_params,\n",
    "            target_params=loss_module.target_critic_network_params,\n",
    "        )  # Compute GAE and add it to the data\n",
    "\n",
    "    data_view = tensordict_data.reshape(-1)  # Flatten the batch size to shuffle data\n",
    "    replay_buffer.extend(data_view)\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        for _ in range(frames_per_batch // minibatch_size):\n",
    "            subdata = replay_buffer.sample()\n",
    "            loss_vals = loss_module(subdata)\n",
    "\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            loss_value.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                loss_module.parameters(), max_grad_norm\n",
    "            )  # Optional\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    collector.update_policy_weights_()\n",
    "\n",
    "    # Logging\n",
    "    done = tensordict_data.get((\"next\", \"agents\", \"done\"))\n",
    "    episode_reward_mean = (\n",
    "        tensordict_data.get((\"next\", \"agents\", \"episode_reward\"))[done].mean().item()\n",
    "    )\n",
    "    episode_reward_mean_list.append(episode_reward_mean)\n",
    "    pbar.set_description(f\"episode_reward_mean = {episode_reward_mean}\", refresh=False)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_reward_mean_list)\n",
    "plt.xlabel(\"Training iterations\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Episode reward mean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#    env.rollout(\n",
    "#        max_steps=max_steps,\n",
    "#        policy=policy,\n",
    "#        callback=lambda env, _: env.render(),\n",
    "#        auto_cast_to_device=True,\n",
    "#        break_when_any_done=False,\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvirtualdisplay\n",
    "display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
    "display.start()\n",
    "from PIL import Image\n",
    "\n",
    "def rendering_callback(env, td):\n",
    "    env.frames.append(Image.fromarray(env.render(mode=\"rgb_array\")))\n",
    "env.frames = []\n",
    "with torch.no_grad():\n",
    "   env.rollout(\n",
    "       max_steps=max_steps,\n",
    "       policy=policy,\n",
    "       callback=rendering_callback,\n",
    "       auto_cast_to_device=True,\n",
    "       break_when_any_done=False,\n",
    "   )\n",
    "env.frames[0].save(\n",
    "    f\"{scenario_name}.gif\",\n",
    "    save_all=True,\n",
    "    append_images=env.frames[1:],\n",
    "   duration=3,\n",
    "   loop=0,\n",
    ")\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(open(f\"{scenario_name}.gif\", \"rb\").read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
