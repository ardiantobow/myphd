{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 21:39:26,799 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Tuple(Box(-inf, inf, (18,), float32), Box(-inf, inf, (18,), float32), Box(-inf, inf, (18,), float32))\n",
      "action_spec: CompositeSpec(\n",
      "    agents: CompositeSpec(\n",
      "        action: BoundedTensorSpec(\n",
      "            shape=torch.Size([60, 3, 2]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([60, 3, 2]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([60, 3, 2]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous), device=cpu, shape=torch.Size([60, 3])), device=cpu, shape=torch.Size([60]))\n",
      "reward_spec: CompositeSpec(\n",
      "    agents: CompositeSpec(\n",
      "        reward: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([60, 3, 1]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous), device=cpu, shape=torch.Size([60, 3])), device=cpu, shape=torch.Size([60]))\n",
      "done_spec: CompositeSpec(\n",
      "    done: DiscreteTensorSpec(\n",
      "        shape=torch.Size([60, 1]),\n",
      "        space=DiscreteBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    terminated: DiscreteTensorSpec(\n",
      "        shape=torch.Size([60, 1]),\n",
      "        space=DiscreteBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete), device=cpu, shape=torch.Size([60]))\n",
      "observation_spec: CompositeSpec(\n",
      "    agents: CompositeSpec(\n",
      "        observation: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([60, 3, 18]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        info: CompositeSpec(\n",
      "            pos_rew: UnboundedContinuousTensorSpec(\n",
      "                shape=torch.Size([60, 3, 1]),\n",
      "                space=None,\n",
      "                device=cpu,\n",
      "                dtype=torch.float32,\n",
      "                domain=continuous),\n",
      "            final_rew: UnboundedContinuousTensorSpec(\n",
      "                shape=torch.Size([60, 3, 1]),\n",
      "                space=None,\n",
      "                device=cpu,\n",
      "                dtype=torch.float32,\n",
      "                domain=continuous),\n",
      "            agent_collisions: UnboundedContinuousTensorSpec(\n",
      "                shape=torch.Size([60, 3, 1]),\n",
      "                space=None,\n",
      "                device=cpu,\n",
      "                dtype=torch.float32,\n",
      "                domain=continuous), device=cpu, shape=torch.Size([60, 3])), device=cpu, shape=torch.Size([60, 3])), device=cpu, shape=torch.Size([60]))\n",
      "action_keys: [('agents', 'action')]\n",
      "reward_keys: [('agents', 'reward')]\n",
      "done_keys: ['done', 'terminated']\n",
      "rollout of three steps: TensorDict(\n",
      "    fields={\n",
      "        agents: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([60, 5, 3, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                info: TensorDict(\n",
      "                    fields={\n",
      "                        agent_collisions: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        final_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        pos_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([60, 5, 3]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                observation: Tensor(shape=torch.Size([60, 5, 3, 18]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([60, 5, 3]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([60, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                agents: TensorDict(\n",
      "                    fields={\n",
      "                        episode_reward: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        info: TensorDict(\n",
      "                            fields={\n",
      "                                agent_collisions: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                                final_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                                pos_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                            batch_size=torch.Size([60, 5, 3]),\n",
      "                            device=cpu,\n",
      "                            is_shared=False),\n",
      "                        observation: Tensor(shape=torch.Size([60, 5, 3, 18]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        reward: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([60, 5, 3]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                done: Tensor(shape=torch.Size([60, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([60, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([60, 5]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([60, 5, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([60, 5]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "Shape of the rollout TensorDict: torch.Size([60, 5])\n",
      "Observation space shape: None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObservation space shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Initialize Q-network and target Q-network\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m q_net \u001b[38;5;241m=\u001b[39m \u001b[43mQNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    108\u001b[0m target_q_net \u001b[38;5;241m=\u001b[39m QNetwork(env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    109\u001b[0m target_q_net\u001b[38;5;241m.\u001b[39mload_state_dict(q_net\u001b[38;5;241m.\u001b[39mstate_dict())\n",
      "Cell \u001b[0;32mIn[8], line 92\u001b[0m, in \u001b[0;36mQNetwork.__init__\u001b[0;34m(self, obs_shape, action_shape)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_shape \u001b[38;5;241m=\u001b[39m obs_shape\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_shape \u001b[38;5;241m=\u001b[39m action_shape\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_net \u001b[38;5;241m=\u001b[39m MultiAgentMLP(\n\u001b[0;32m---> 92\u001b[0m     n_agent_inputs\u001b[38;5;241m=\u001b[39m\u001b[43mobs_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m,  \u001b[38;5;66;03m# Assuming obs_shape is (batch_size, obs_dim)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     n_agent_outputs\u001b[38;5;241m=\u001b[39maction_shape[\u001b[38;5;241m1\u001b[39m],  \u001b[38;5;66;03m# Assuming action_shape is (batch_size, num_actions)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     n_agents\u001b[38;5;241m=\u001b[39mobs_shape[\u001b[38;5;241m0\u001b[39m],  \u001b[38;5;66;03m# Number of agents\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     centralised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# Each agent has its own Q-values\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     97\u001b[0m     num_cells\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     98\u001b[0m     activation_class\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mReLU,  \u001b[38;5;66;03m# Using ReLU activation\u001b[39;00m\n\u001b[1;32m     99\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Torch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Tensordict modules\n",
    "from torch import multiprocessing\n",
    "\n",
    "# Data collection\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "\n",
    "# Env\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "\n",
    "# Multi-agent network\n",
    "from torchrl.modules import MultiAgentMLP\n",
    "\n",
    "# Utils\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Devices\n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = torch.device(0) if torch.cuda.is_available() and not is_fork else torch.device(\"cpu\")\n",
    "vmas_device = device  # The device where the simulator is run (VMAS can run on GPU)\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 6_000  # Number of team frames collected per training iteration\n",
    "n_iters = 10  # Number of sampling and training iterations\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 30  # Number of optimization steps per training iteration\n",
    "minibatch_size = 400  # Size of the mini-batches in each optimization step\n",
    "lr = 3e-4  # Learning rate\n",
    "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
    "\n",
    "# PPO\n",
    "clip_epsilon = 0.2  # clip value for PPO loss\n",
    "gamma = 0.9  # discount factor\n",
    "lmbda = 0.9  # lambda for generalised advantage estimation\n",
    "entropy_eps = 1e-4  # coefficient of the entropy term in the PPO loss\n",
    "\n",
    "max_steps = 100  # Episode steps before done\n",
    "num_vmas_envs = frames_per_batch // max_steps  # Number of vectorized envs. frames_per_batch should be divisible by this number\n",
    "scenario_name = \"navigation\"\n",
    "n_agents = 3\n",
    "\n",
    "env = VmasEnv(\n",
    "    scenario=scenario_name,\n",
    "    num_envs=num_vmas_envs,\n",
    "    continuous_actions=True,\n",
    "    max_steps=max_steps,\n",
    "    device=vmas_device,\n",
    "    n_agents=n_agents,\n",
    ")\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "\n",
    "\n",
    "print(\"action_spec:\", env.full_action_spec)\n",
    "print(\"reward_spec:\", env.full_reward_spec)\n",
    "print(\"done_spec:\", env.full_done_spec)\n",
    "print(\"observation_spec:\", env.observation_spec)\n",
    "\n",
    "print(\"action_keys:\", env.action_keys)\n",
    "print(\"reward_keys:\", env.reward_keys)\n",
    "print(\"done_keys:\", env.done_keys)\n",
    "\n",
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
    ")\n",
    "\n",
    "check_env_specs(env)\n",
    "\n",
    "n_rollout_steps = 5\n",
    "rollout = env.rollout(n_rollout_steps)\n",
    "print(\"rollout of three steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)\n",
    "\n",
    "# Define Q-network architecture\n",
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self, obs_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.obs_shape = obs_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.q_net = MultiAgentMLP(\n",
    "            n_agent_inputs=obs_shape[1],  # Assuming obs_shape is (batch_size, obs_dim)\n",
    "            n_agent_outputs=action_shape[1],  # Assuming action_shape is (batch_size, num_actions)\n",
    "            n_agents=obs_shape[0],  # Number of agents\n",
    "            centralised=False,  # Each agent has its own Q-values\n",
    "            depth=2,\n",
    "            num_cells=256,\n",
    "            activation_class=torch.nn.ReLU,  # Using ReLU activation\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.q_net(obs)\n",
    "\n",
    "print(\"Observation space shape:\", env.observation_space.shape)\n",
    "\n",
    "# Initialize Q-network and target Q-network\n",
    "q_net = QNetwork(env.observation_space.shape, env.action_space.shape).to(device)\n",
    "target_q_net = QNetwork(env.observation_space.shape, env.action_space.shape).to(device)\n",
    "target_q_net.load_state_dict(q_net.state_dict())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
