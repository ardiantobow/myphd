{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardie85/PHD/Research/code/.venv/lib/python3.10/site-packages/vmas/scenarios/navigation_comm.py:247: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  comms.append(torch.tensor(other_agent.state.c).unsqueeze(0).unsqueeze(0))  # Ensure it's 3D\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 4 and 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 267\u001b[0m\n\u001b[1;32m    264\u001b[0m     evaluate(env, policy, max_steps, scenario_name)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 267\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 251\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    248\u001b[0m max_grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# Environment setup\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43msetup_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscenario_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_agents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_per_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmas_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# Policy and Value Networks\u001b[39;00m\n\u001b[1;32m    254\u001b[0m policy \u001b[38;5;241m=\u001b[39m setup_policy_network(env, device)\n",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m, in \u001b[0;36msetup_environment\u001b[0;34m(scenario_name, n_agents, max_steps, frames_per_batch, vmas_device)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_environment\u001b[39m(scenario_name, n_agents, max_steps, frames_per_batch, vmas_device):\n\u001b[1;32m     29\u001b[0m     num_vmas_envs \u001b[38;5;241m=\u001b[39m frames_per_batch \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m max_steps\n\u001b[0;32m---> 31\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43mVmasEnv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscenario\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscenario_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_vmas_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontinuous_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmas_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_agents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_agents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     env \u001b[38;5;241m=\u001b[39m TransformedEnv(\n\u001b[1;32m     41\u001b[0m         env,\n\u001b[1;32m     42\u001b[0m         RewardSum(in_keys\u001b[38;5;241m=\u001b[39m[env\u001b[38;5;241m.\u001b[39mreward_key], out_keys\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magents\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     45\u001b[0m     check_env_specs(env)\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torchrl/envs/common.py:158\u001b[0m, in \u001b[0;36m_EnvPostInit.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 158\u001b[0m     instance: EnvBase \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# we create the done spec by adding a done/terminated entry if one is missing\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     instance\u001b[38;5;241m.\u001b[39m_create_done_specs()\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torchrl/envs/libs/vmas.py:762\u001b[0m, in \u001b[0;36mVmasEnv.__init__\u001b[0;34m(self, scenario, num_envs, continuous_actions, max_steps, categorical_actions, seed, group_map, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _has_vmas:\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    759\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvmas python package was not found. Please install this dependency. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    760\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMore info: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgit_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    761\u001b[0m     )\n\u001b[0;32m--> 762\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscenario\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinuous_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontinuous_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torchrl/envs/libs/vmas.py:245\u001b[0m, in \u001b[0;36mVmasWrapper.__init__\u001b[0;34m(self, env, categorical_actions, group_map, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_map \u001b[38;5;241m=\u001b[39m group_map\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_actions \u001b[38;5;241m=\u001b[39m categorical_actions\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_done_after_reset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torchrl/envs/common.py:2819\u001b[0m, in \u001b[0;36m_EnvWrapper.__init__\u001b[0;34m(self, dtype, device, batch_size, allow_done_after_reset, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2817\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m   2818\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_kwargs(kwargs)\n\u001b[0;32m-> 2819\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_env\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# writes the self._env attribute\u001b[39;00m\n\u001b[1;32m   2820\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_specs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env)  \u001b[38;5;66;03m# writes the self._env attribute\u001b[39;00m\n\u001b[1;32m   2821\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torchrl/envs/libs/vmas.py:795\u001b[0m, in \u001b[0;36mVmasEnv._build_env\u001b[0;34m(self, scenario, num_envs, continuous_actions, max_steps, seed, **scenario_kwargs)\u001b[0m\n\u001b[1;32m    791\u001b[0m from_pixels \u001b[38;5;241m=\u001b[39m scenario_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_pixels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    792\u001b[0m pixels_only \u001b[38;5;241m=\u001b[39m scenario_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixels_only\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_build_env(\n\u001b[0;32m--> 795\u001b[0m     env\u001b[38;5;241m=\u001b[39m\u001b[43mvmas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_env\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscenario\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontinuous_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontinuous_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscenario_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    805\u001b[0m     pixels_only\u001b[38;5;241m=\u001b[39mpixels_only,\n\u001b[1;32m    806\u001b[0m     from_pixels\u001b[38;5;241m=\u001b[39mfrom_pixels,\n\u001b[1;32m    807\u001b[0m )\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/vmas/make_env.py:75\u001b[0m, in \u001b[0;36mmake_env\u001b[0;34m(scenario, num_envs, device, continuous_actions, wrapper, max_steps, seed, dict_spaces, multidiscrete_actions, clamp_actions, grad_enabled, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         scenario \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m     scenario \u001b[38;5;241m=\u001b[39m scenarios\u001b[38;5;241m.\u001b[39mload(scenario)\u001b[38;5;241m.\u001b[39mScenario()\n\u001b[0;32m---> 75\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mEnvironment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinuous_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontinuous_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdict_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultidiscrete_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultidiscrete_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclamp_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclamp_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_enabled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\u001b[38;5;241m.\u001b[39mget_env(env) \u001b[38;5;28;01mif\u001b[39;00m wrapper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m env\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/vmas/simulator/environment/environment.py:67\u001b[0m, in \u001b[0;36mEnvironment.__init__\u001b[0;34m(self, scenario, num_envs, device, max_steps, continuous_actions, seed, dict_spaces, multidiscrete_actions, clamp_actions, grad_enabled, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclamp_action \u001b[38;5;241m=\u001b[39m clamp_actions\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_enabled \u001b[38;5;241m=\u001b[39m grad_enabled\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# configure spaces\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultidiscrete_actions \u001b[38;5;241m=\u001b[39m multidiscrete_actions\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/vmas/simulator/environment/environment.py:98\u001b[0m, in \u001b[0;36mEnvironment.reset\u001b[0;34m(self, seed, return_observations, return_info, return_dones)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscenario\u001b[38;5;241m.\u001b[39menv_reset_world_at(env_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 98\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_from_scenario\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_observations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_observations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_infos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_rewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_dones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dones\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/vmas/simulator/environment/environment.py:162\u001b[0m, in \u001b[0;36mEnvironment.get_from_scenario\u001b[0;34m(self, get_observations, get_rewards, get_infos, get_dones, dict_agent_names)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_observations:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents:\n\u001b[1;32m    161\u001b[0m         observation \u001b[38;5;241m=\u001b[39m TorchUtils\u001b[38;5;241m.\u001b[39mrecursive_clone(\n\u001b[0;32m--> 162\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscenario\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m         )\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m dict_agent_names:\n\u001b[1;32m    165\u001b[0m             obs\u001b[38;5;241m.\u001b[39mupdate({agent\u001b[38;5;241m.\u001b[39mname: observation})\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/vmas/scenarios/navigation_comm.py:273\u001b[0m, in \u001b[0;36mScenario.observation\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m    271\u001b[0m     combined_obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([obs, comms_obs, sensor_obs], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Combine along the last dimension\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m     combined_obs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomms_obs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# # Verify combined_obs size\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# print(f\"combined-obs-value= {combined_obs}\")\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# print(f\"combined-obs-shape= {combined_obs.shape}\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m#     print(f\"Error reshaping combined_obs: {e}\")\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m#     print(f\"combined_obs size: {combined_obs.size()}\")\u001b[39;00m\n\u001b[1;32m    297\u001b[0m combined_obs \u001b[38;5;241m=\u001b[39m combined_obs\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 4 and 5"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import multiprocessing\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor, TanhNormal\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "import os\n",
    "import PIL.Image as PILImage\n",
    "\n",
    "# Set device based on CUDA availability\n",
    "torch.manual_seed(0)\n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = torch.device(0) if torch.cuda.is_available() and not is_fork else torch.device(\"cpu\")\n",
    "vmas_device = device\n",
    "\n",
    "def setup_environment(scenario_name, n_agents, max_steps, frames_per_batch, vmas_device):\n",
    "    num_vmas_envs = frames_per_batch // max_steps\n",
    "    \n",
    "    env = VmasEnv(\n",
    "        scenario=scenario_name,\n",
    "        num_envs=num_vmas_envs,\n",
    "        continuous_actions=True,\n",
    "        max_steps=max_steps,\n",
    "        device=vmas_device,\n",
    "        n_agents=n_agents,\n",
    "    )\n",
    "    \n",
    "    env = TransformedEnv(\n",
    "        env,\n",
    "        RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")])\n",
    "    )\n",
    "    \n",
    "    check_env_specs(env)\n",
    "    \n",
    "    return env\n",
    "\n",
    "def setup_policy_network(env, device, share_parameters_policy=True):\n",
    "    policy_net = torch.nn.Sequential(\n",
    "        MultiAgentMLP(\n",
    "            n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "            n_agent_outputs=2 * env.action_spec.shape[-1],\n",
    "            n_agents=env.n_agents,\n",
    "            centralised=False,\n",
    "            share_params=share_parameters_policy,\n",
    "            device=device,\n",
    "            depth=2,\n",
    "            num_cells=256,\n",
    "            activation_class=torch.nn.Tanh,\n",
    "        ),\n",
    "        NormalParamExtractor(),\n",
    "    )\n",
    "\n",
    "    policy_module = TensorDictModule(\n",
    "        policy_net,\n",
    "        in_keys=[(\"agents\", \"observation\")],\n",
    "        out_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
    "    )\n",
    "    \n",
    "    policy = ProbabilisticActor(\n",
    "        module=policy_module,\n",
    "        spec=env.unbatched_action_spec,\n",
    "        in_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
    "        out_keys=[env.action_key],\n",
    "        distribution_class=TanhNormal,\n",
    "        distribution_kwargs={\n",
    "            \"min\": env.unbatched_action_spec[env.action_key].space.low,\n",
    "            \"max\": env.unbatched_action_spec[env.action_key].space.high,\n",
    "        },\n",
    "        return_log_prob=True,\n",
    "        log_prob_key=(\"agents\", \"sample_log_prob\"),\n",
    "    )\n",
    "    \n",
    "    return policy\n",
    "\n",
    "def setup_value_network(env, device, mappo=True, share_parameters_critic=True):\n",
    "    critic_net = MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "        n_agent_outputs=1,\n",
    "        n_agents=env.n_agents,\n",
    "        centralised=mappo,\n",
    "        share_params=share_parameters_critic,\n",
    "        device=device,\n",
    "        depth=2,\n",
    "        num_cells=256,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    )\n",
    "\n",
    "    critic = TensorDictModule(\n",
    "        module=critic_net,\n",
    "        in_keys=[(\"agents\", \"observation\")],\n",
    "        out_keys=[(\"agents\", \"state_value\")],\n",
    "    )\n",
    "    \n",
    "    return critic\n",
    "\n",
    "def setup_ppo_loss(policy, critic, env, clip_epsilon=0.2, entropy_eps=1e-4, gamma=0.9, lmbda=0.9):\n",
    "    loss_module = ClipPPOLoss(\n",
    "        actor_network=policy,\n",
    "        critic_network=critic,\n",
    "        clip_epsilon=clip_epsilon,\n",
    "        entropy_coef=entropy_eps,\n",
    "        normalize_advantage=False,\n",
    "    )\n",
    "    \n",
    "    loss_module.set_keys(\n",
    "        reward=env.reward_key,\n",
    "        action=env.action_key,\n",
    "        sample_log_prob=(\"agents\", \"sample_log_prob\"),\n",
    "        value=(\"agents\", \"state_value\"),\n",
    "        done=(\"agents\", \"done\"),\n",
    "        terminated=(\"agents\", \"terminated\"),\n",
    "    )\n",
    "    \n",
    "    loss_module.make_value_estimator(ValueEstimators.GAE, gamma=gamma, lmbda=lmbda)\n",
    "    \n",
    "    return loss_module\n",
    "\n",
    "def train_ppo(env, policy, critic, loss_module, n_iters, frames_per_batch, minibatch_size, num_epochs, max_grad_norm, lr, device):\n",
    "    collector = SyncDataCollector(\n",
    "        env,\n",
    "        policy,\n",
    "        device=vmas_device,\n",
    "        storing_device=device,\n",
    "        frames_per_batch=frames_per_batch,\n",
    "        total_frames=frames_per_batch * n_iters,\n",
    "    )\n",
    "\n",
    "    replay_buffer = ReplayBuffer(\n",
    "        storage=LazyTensorStorage(frames_per_batch, device=device),\n",
    "        sampler=SamplerWithoutReplacement(),\n",
    "        batch_size=minibatch_size,\n",
    "    )\n",
    "    \n",
    "    optim = torch.optim.Adam(loss_module.parameters(), lr)\n",
    "    \n",
    "    pbar = tqdm(total=n_iters, desc=\"episode_reward_mean = 0\")\n",
    "    episode_reward_mean_list = []\n",
    "    \n",
    "    for tensordict_data in collector:\n",
    "        tensordict_data.set(\n",
    "            (\"next\", \"agents\", \"done\"),\n",
    "            tensordict_data.get((\"next\", \"done\"))\n",
    "            .unsqueeze(-1)\n",
    "            .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "        )\n",
    "        tensordict_data.set(\n",
    "            (\"next\", \"agents\", \"terminated\"),\n",
    "            tensordict_data.get((\"next\", \"terminated\"))\n",
    "            .unsqueeze(-1)\n",
    "            .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss_module.value_estimator(\n",
    "                tensordict_data,\n",
    "                params=loss_module.critic_network_params,\n",
    "                target_params=loss_module.target_critic_network_params,\n",
    "            )\n",
    "\n",
    "        data_view = tensordict_data.reshape(-1)\n",
    "        replay_buffer.extend(data_view)\n",
    "\n",
    "        for _ in range(num_epochs):\n",
    "            for _ in range(frames_per_batch // minibatch_size):\n",
    "                subdata = replay_buffer.sample()\n",
    "                loss_vals = loss_module(subdata)\n",
    "\n",
    "                loss_value = (\n",
    "                    loss_vals[\"loss_objective\"]\n",
    "                    + loss_vals[\"loss_critic\"]\n",
    "                    + loss_vals[\"loss_entropy\"]\n",
    "                )\n",
    "\n",
    "                loss_value.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_grad_norm)\n",
    "                optim.step()\n",
    "                optim.zero_grad()\n",
    "\n",
    "        collector.update_policy_weights_()\n",
    "\n",
    "        # Logging\n",
    "        done = tensordict_data.get((\"next\", \"agents\", \"done\"))\n",
    "        episode_reward_mean = (\n",
    "            tensordict_data.get((\"next\", \"agents\", \"episode_reward\"))[done].mean().item()\n",
    "        )\n",
    "        episode_reward_mean_list.append(episode_reward_mean)\n",
    "        pbar.set_description(f\"episode_reward_mean = {episode_reward_mean}\", refresh=False)\n",
    "        pbar.update()\n",
    "    \n",
    "    plt.plot(episode_reward_mean_list)\n",
    "    plt.xlabel(\"Training iterations\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Episode reward mean\")\n",
    "    plt.show()\n",
    "\n",
    "def evaluate(env, policy, max_steps, scenario_name):\n",
    "    env.frames = []\n",
    "    \n",
    "    def rendering_callback(env, td):\n",
    "        frame = env.render(mode=\"rgb_array\")\n",
    "        env.frames.append(frame)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        env.rollout(\n",
    "            max_steps=max_steps,\n",
    "            policy=policy,\n",
    "            callback=rendering_callback,\n",
    "            auto_cast_to_device=True,\n",
    "            break_when_any_done=False,\n",
    "        )\n",
    "\n",
    "    env.frames = [PILImage.fromarray(frame) if isinstance(frame, np.ndarray) else frame for frame in env.frames]\n",
    "    \n",
    "    gif_path = f\"{scenario_name}.gif\"\n",
    "    frames = env.frames\n",
    "    frames[0].save(\n",
    "        gif_path,\n",
    "        save_all=True,\n",
    "        append_images=frames[1:],\n",
    "        duration=300,\n",
    "        loop=0,\n",
    "    )\n",
    "    display(Image(gif_path))\n",
    "\n",
    "# In the main function, pass the scenario_name when calling evaluate:\n",
    "def main():\n",
    "    # Configurations\n",
    "    scenario_name = \"navigation\"\n",
    "    n_agents = 3\n",
    "    max_steps = 100\n",
    "    frames_per_batch = 6_000\n",
    "    n_iters = 10\n",
    "    minibatch_size = 400\n",
    "    num_epochs = 1\n",
    "    lr = 0.0003\n",
    "    max_grad_norm = 1.0\n",
    "    \n",
    "    # Environment setup\n",
    "    env = setup_environment(scenario_name, n_agents, max_steps, frames_per_batch, vmas_device)\n",
    "    \n",
    "    # Policy and Value Networks\n",
    "    policy = setup_policy_network(env, device)\n",
    "    critic = setup_value_network(env, device)\n",
    "    \n",
    "    # PPO Loss Setup\n",
    "    loss_module = setup_ppo_loss(policy, critic, env)\n",
    "    \n",
    "    # Train PPO\n",
    "    train_ppo(env, policy, critic, loss_module, n_iters, frames_per_batch, minibatch_size, num_epochs, max_grad_norm, lr, device)\n",
    "    \n",
    "    # Evaluation\n",
    "    evaluate(env, policy, max_steps, scenario_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
