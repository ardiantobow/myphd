{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardie85/PHD/Research/code/.venv/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\n",
      "Step 1 of Episode 0\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 2 of Episode 0\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 3 of Episode 0\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 4 of Episode 0\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 5 of Episode 0\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 6, reward -0.003520488739013672, next_obs (0.9, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 6 of Episode 0\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  6.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.9, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward -0.003543376922607422, next_obs (0.9, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 7 of Episode 0\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9), action 8, reward -0.0031458139419555664, next_obs (-0.9, 0.9, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.9, 0.8, 0.0, 0.0, 0.0, -0.9), action 3, reward 0.0008879899978637695, next_obs (0.9, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 8 of Episode 0\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.9, 0.0, 0.0, 0.0, -0.9), action 5, reward -0.013306140899658203, next_obs (-0.9, 0.9, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.9, 0.8, 0.0, 0.0, 0.0, -0.9), action 3, reward 0.00507807731628418, next_obs (0.9, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 9 of Episode 0\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.9, 0.0, 0.0, 0.0, -0.9), action 0, reward -0.012521028518676758, next_obs (-0.9, 0.9, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.9, 0.8, 0.0, 0.0, 0.0, -0.9), action 3, reward 0.008124351501464844, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 10 of Episode 0\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.9, 0.0, 0.0, 0.0, -0.9), action 1, reward -0.002727508544921875, next_obs (-0.9, 0.9, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.006865978240966797, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "done status for agent 0: False\n",
      "done status for agent 1: False\n",
      "Success percentage of each agent at the end of episode 0:\n",
      "Agent 0: 0.0%\n",
      "Agent 1: 0.0%\n",
      "Overall success percentage for all agents up to episode 0: 0.0%\n",
      "Episode 1\n",
      "Step 1 of Episode 1\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 2 of Episode 1\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 3 of Episode 1\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 7, reward 0.003092527389526367, next_obs (0.9, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 4 of Episode 1\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  7.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9), action 2, reward -0.00663149356842041, next_obs (-0.9, 0.9, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.9, 0.8, 0.0, 0.0, 0.0, -0.9), action 3, reward 0.006606578826904297, next_obs (0.9, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 5 of Episode 1\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.9, 0.0, 0.0, 0.0, -0.9), action 2, reward -0.013282299041748047, next_obs (-0.9, 0.9, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.9, 0.8, 0.0, 0.0, 0.0, -0.9), action 5, reward 0.002772808074951172, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 6 of Episode 1\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  2.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.9, 0.0, 0.0, 0.0, -0.9), action 8, reward -0.014873862266540527, next_obs (-0.9, 0.9, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0012660026550292969, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 7 of Episode 1\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.9, 0.0, 0.0, 0.0, -0.9), action 3, reward -0.015453696250915527, next_obs (-0.9, 0.9, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0009192228317260742, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 8 of Episode 1\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.9, 0.0, 0.0, 0.0, -0.9), action 8, reward -0.015799283981323242, next_obs (-0.9, 0.9, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0006722211837768555, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 9 of Episode 1\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  8.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.9, 0.0, 0.0, 0.0, -0.9), action 3, reward -0.016083836555480957, next_obs (-0.9, 0.9, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0004945993423461914, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 10 of Episode 1\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-0.9000,  1.0000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.9, 0.0, 0.0, 0.0, -0.9), action 2, reward -0.019635438919067383, next_obs (-0.9, 1.0, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 3, reward 0.0037784576416015625, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "done status for agent 0: False\n",
      "done status for agent 1: False\n",
      "Success percentage of each agent at the end of episode 1:\n",
      "Agent 0: 0.0%\n",
      "Agent 1: 0.0%\n",
      "Overall success percentage for all agents up to episode 1: 0.0%\n",
      "Episode 2\n",
      "Step 1 of Episode 2\n",
      "Observation before passed to take an action from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-0.9, 0.8, 0.0, 0.0, 0.0, -0.9), action 4, reward 0.003092527389526367, next_obs (-1.0, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 2 of Episode 2\n",
      "Observation before passed to take an action from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -0.9000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  4.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-1.0, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0030388832092285156, next_obs (-1.0, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 3 of Episode 2\n",
      "Observation before passed to take an action from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-1.0, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0022437572479248047, next_obs (-1.0, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 4 of Episode 2\n",
      "Observation before passed to take an action from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-1.0, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0016627311706542969, next_obs (-1.0, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 5 of Episode 2\n",
      "Observation before passed to take an action from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-1.0, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0012358427047729492, next_obs (-1.0, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 6 of Episode 2\n",
      "Observation before passed to take an action from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-1.0, 0.8, 0.0, 0.0, 0.0, -0.9), action 5, reward -0.009283900260925293, next_obs (-1.0, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward 0.0, next_obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 7 of Episode 2\n",
      "Observation before passed to take an action from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-1.0, 0.8, 0.0, 0.0, 0.0, -0.9), action 3, reward -0.013218998908996582, next_obs (-1.0, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.8, 0.0, 0.0, 0.0, -0.9), action 5, reward -0.0031458139419555664, next_obs (0.8, 0.9, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 8 of Episode 2\n",
      "Observation before passed to take an action from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  5.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-1.0, 0.8, 0.0, 0.0, 0.0, -0.9), action 1, reward -0.004394650459289551, next_obs (-1.0, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.9, 0.0, 0.0, 0.0, -0.9), action 0, reward -0.0031991004943847656, next_obs (0.8, 0.9, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 9 of Episode 2\n",
      "Observation before passed to take an action from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-1.0, 0.8, 0.0, 0.0, 0.0, -0.9), action 3, reward -0.005542397499084473, next_obs (-1.0, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.9, 0.0, 0.0, 0.0, -0.9), action 1, reward 0.004241228103637695, next_obs (0.8, 0.9, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "Step 10 of Episode 2\n",
      "Observation before passed to take an action from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  0.8000,  0.9000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Observation before passed to take an action from agent 1: tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000, -1.0000,  0.8000,\n",
      "         0.0000,  0.0000,  0.0000, -0.9000,  3.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000], device='cuda:0', dtype=torch.float64)\n",
      "Observation after passed to take an action from agent: 1: tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "agent action size from somewhere in environment: 2\n",
      "Physical Observation to update from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 0: tensor([-1.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 0 - Updated Q-table for obs (-1.0, 0.8, 0.0, 0.0, 0.0, -0.9), action 0, reward -0.005229830741882324, next_obs (-1.0, 0.8, 0.0, 0.0, 0.0, -0.9)\n",
      "Physical Observation to update from agent 1: tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Physical Next observation to update from agent 1: tensor([ 0.8000,  0.9000,  0.0000,  0.0000,  0.0000, -0.9000], device='cuda:0',\n",
      "       dtype=torch.float64)\n",
      "Agent 1 - Updated Q-table for obs (0.8, 0.9, 0.0, 0.0, 0.0, -0.9), action 1, reward 0.011517524719238281, next_obs (0.8, 0.9, 0.0, 0.0, 0.0, -0.9)\n",
      "-----------------\n",
      "done status for agent 0: False\n",
      "done status for agent 1: False\n",
      "Success percentage of each agent at the end of episode 2:\n",
      "Agent 0: 0.0%\n",
      "Agent 1: 0.0%\n",
      "Overall success percentage for all agents up to episode 2: 0.0%\n",
      "Success percentage for agent 0 = 0.0%\n",
      "Success percentage for agent 1 = 0.0%\n",
      "Overall success percentage for all agents = 0.0%\n",
      "It took: 0.8997635841369629s for 30 steps across 3 episodes of 1 parallel environments on device cuda for navigation_comm scenario.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file navigation_comm.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"navigation_comm.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from vmas import make_env\n",
    "from vmas.simulator.core import Agent\n",
    "from vmas.simulator.scenario import BaseScenario\n",
    "from typing import Union\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "from IPython.display import HTML, display as ipython_display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.spaces import Discrete \n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, agent_id, alpha=0.1, gamma=0.99, epsilon=0.4, communication_weight=0.5):\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.q_table = {}\n",
    "        self.env = env\n",
    "        self.agent_id = agent_id\n",
    "        self.communication_weight = communication_weight  # Weight parameter for incorporating messages\n",
    "\n",
    "    def get_action(self, agent, env, agent_id, agent_obs):\n",
    "        print(f\"Observation after passed to take an action from agent: {agent_id}: {agent_obs}\")\n",
    "        # print(f\"self agent ID: {self.agent_id}\")\n",
    "\n",
    "        agent_obs_cpu = agent_obs[:6].cpu().numpy()  # Transfer only the required slice to CPU\n",
    "        agent_obs = tuple(np.round(agent_obs_cpu, decimals=5))  # Round the observation\n",
    "\n",
    "        if agent_obs not in self.q_table:\n",
    "            self.q_table[agent_obs] = np.zeros(self.env.action_space[self.agent_id].n)\n",
    "        \n",
    "        # self.print_q_table()\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Select a random action\n",
    "            action = np.random.randint(env.action_space[self.agent_id].n)\n",
    "            # print(f\"Agent {agent_id} - action type is random, for obs {agent_obs}= {action}\")    \n",
    "        else:\n",
    "            # Select the action with the highest Q-value\n",
    "            action = np.argmax(self.q_table[agent_obs])\n",
    "            # print(f\"Agent {agent_id} - action type is max action, for obs {agent_obs}= {action}\") \n",
    "        \n",
    "        # print(f\"action from the function: {action}\")\n",
    "        return (action,)  # Return as a tuple\n",
    "\n",
    "    def update_q_table(self,  agent, env, agent_id, obs, action, reward, next_obs):\n",
    "        obs_key = tuple(np.round(obs.cpu().numpy(), decimals=5))  # Only transfer to CPU when necessary\n",
    "        next_obs_key = tuple(np.round(next_obs.cpu().numpy(), decimals=5))\n",
    "        action = int(action.item())  # Convert tensor to Python scalar\n",
    "\n",
    "        # print (f\"reward obtained = {reward}\")\n",
    "\n",
    "        if isinstance(self.env.action_space[self.agent_id], Discrete):\n",
    "            action_space_size = self.env.action_space[self.agent_id].n\n",
    "        else:\n",
    "            raise ValueError(\"This Q-learning implementation requires a discrete action space.\")\n",
    "\n",
    "        if obs_key not in self.q_table:\n",
    "            self.q_table[obs_key] = np.zeros(action_space_size)\n",
    "\n",
    "        if next_obs_key not in self.q_table:\n",
    "            self.q_table[next_obs_key] = np.zeros(action_space_size)\n",
    "\n",
    "        best_next_action = np.argmax(self.q_table[next_obs_key])\n",
    "        td_target = reward + self.gamma * self.q_table[next_obs_key][best_next_action]\n",
    "\n",
    "        td_error = td_target - self.q_table[obs_key][action]\n",
    "        self.q_table[obs_key][action] += self.alpha * td_error\n",
    "\n",
    "        print(f\"Agent {self.agent_id} - Updated Q-table for obs {obs_key}, action {action}, reward {reward}, next_obs {next_obs_key}\")\n",
    "        \n",
    "    \n",
    "    def print_q_table(self):\n",
    "        print(f\"Q-table for Agent {self.agent_id}:\")\n",
    "        for state, actions in self.q_table.items():\n",
    "            print(f\"  State: {state}\")\n",
    "            for action, q_value in enumerate(actions):\n",
    "                print(f\"    Action: {action}, Q-value: {q_value:.5f}\")\n",
    "        print(f\"End of Q-table for Agent {self.agent_id}\\n\")\n",
    "\n",
    "class VMASEnvRunner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        render: bool,\n",
    "        num_envs: int,\n",
    "        num_episodes: int,\n",
    "        max_steps_per_episode: int,\n",
    "        device: str,\n",
    "        scenario: Union[str, BaseScenario],\n",
    "        continuous_actions: bool,\n",
    "        random_action: bool,\n",
    "        n_agents: int,\n",
    "        obs_discrete: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.render = render\n",
    "        self.num_envs = num_envs\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "        self.device = device\n",
    "        self.scenario = scenario\n",
    "        self.continuous_actions = continuous_actions\n",
    "        self.random_action = random_action\n",
    "        self.obs_discrete = obs_discrete\n",
    "        self.kwargs = kwargs\n",
    "        self.frame_list = []  \n",
    "        self.q_learning_agents = []\n",
    "        self.rewards_history = []  \n",
    "        self.action_counts = {i: {} for i in range(n_agents)}  \n",
    "        self.agent_rewards_history = {i: [] for i in range(n_agents)}\n",
    "        self.successful_episodes_individual = {i: 0 for i in range(n_agents)}  # Track successful episodes for each agent\n",
    "        self.successful_episodes_all_agents = 0  # Track episodes where all agents succeed\n",
    "\n",
    "    def discretize(self, data, bins):\n",
    "        bins = np.array(bins)\n",
    "        if np.isscalar(data):\n",
    "            data = np.array([data])\n",
    "        bin_indices = np.digitize(data, bins) - 1  # np.digitize returns indices starting from 1\n",
    "        bin_indices = np.clip(bin_indices, 0, len(bins) - 1)  # Ensure indices are within the valid range\n",
    "        bin_values = bins[bin_indices]\n",
    "        bin_values = np.round(bin_values, 2)  # Round the bin values to two decimal places\n",
    "        return bin_indices, bin_values\n",
    "\n",
    "    def discretize_tensor_slice(self, tensor_slice, bins):\n",
    "        tensor_np = tensor_slice.cpu().numpy()  # Convert to numpy for easier handling\n",
    "        indices, values = self.discretize(tensor_np, bins)\n",
    "        indices = torch.tensor(indices, device=tensor_slice.device)\n",
    "        values = torch.tensor(values, device=tensor_slice.device)\n",
    "        return indices, values\n",
    "\n",
    "    def _get_deterministic_obs(self, env, observation):\n",
    "        pos_bins = np.linspace(-1, 1, num=21)\n",
    "        vel_bins = np.linspace(0, 0, num=21)\n",
    "        lidar_bins = np.linspace(0, 1, num=11)\n",
    "\n",
    "        pos = observation[0:2]\n",
    "        vel = observation[2:4]\n",
    "        goal_pose = observation[4:6]\n",
    "        comms_data = observation[6:13]\n",
    "        sensor_data = observation[13:]\n",
    "\n",
    "        discrete_pos_indices, discrete_pos_values = self.discretize_tensor_slice(pos, pos_bins)\n",
    "        discrete_vel_indices, discrete_vel_values = self.discretize_tensor_slice(vel, vel_bins)\n",
    "        discrete_goal_pose_indices, discrete_goal_pose_values = self.discretize_tensor_slice(goal_pose, pos_bins)\n",
    "        discrete_sensor_data_indices, discrete_sensor_data_values = self.discretize_tensor_slice(sensor_data, lidar_bins)\n",
    "\n",
    "        concatenated_tensor_values = torch.cat(\n",
    "            [discrete_pos_values, discrete_vel_values, discrete_goal_pose_values, comms_data, discrete_sensor_data_values],\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        return concatenated_tensor_values\n",
    "\n",
    "    def _get_deterministic_action(self, agent: Agent, env, agent_id, agent_obs):\n",
    "        if self.continuous_actions:\n",
    "            if agent.silent:\n",
    "                action = torch.tensor([[-1, 0.5]], device=env.device)\n",
    "            else:\n",
    "                if agent_id == 0:\n",
    "                    action = torch.tensor([[-1, 0.5, 2]], device=env.device)\n",
    "                else:\n",
    "                    action = torch.tensor([[-1, 0.5, 1]], device=env.device)\n",
    "        else:\n",
    "            physical_obs = agent_obs[0:6]\n",
    "\n",
    "            if not agent.silent:\n",
    "                comm_obs = agent_obs[6:]\n",
    "            \n",
    "            physical_action = self.q_learning_agents[agent_id].get_action(agent, env, agent_id, physical_obs)\n",
    "            physical_action_tensor = torch.tensor(physical_action, device=self.device)\n",
    "\n",
    "            if agent.silent:\n",
    "                action = physical_action_tensor\n",
    "            else:\n",
    "                physical_obs_tensor = torch.tensor(physical_obs, device=self.device)\n",
    "                comm_action_tensor = torch.cat([physical_obs_tensor, physical_action_tensor], dim=0) \n",
    "\n",
    "                zero_tensor = torch.zeros(6, dtype=torch.float64, device=self.device)\n",
    "                first_row = torch.cat((physical_action_tensor, zero_tensor))\n",
    "                action = torch.stack((first_row, comm_action_tensor)).unsqueeze(0)\n",
    "        \n",
    "        # action = torch.tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000], [-0.9000,  0.8000,  0.0000,  0.0000,  0.0000, -0.9000,  8.0000]]],device='cuda:0', dtype=torch.float64)\n",
    "        # print(f\"action from function: {action}\")\n",
    "        return action\n",
    "\n",
    "    def generate_gif(self, scenario_name):\n",
    "        fps = 25\n",
    "        clip = ImageSequenceClip(self.frame_list, fps=fps)\n",
    "        clip.write_gif(f'{scenario_name}.gif', fps=fps)\n",
    "        return HTML(f'<img src=\"{scenario_name}.gif\">')\n",
    "\n",
    "    def plot_action_distribution(self):\n",
    "        num_agents = len(self.action_counts)\n",
    "\n",
    "        for agent_id, counts in self.action_counts.items():\n",
    "            unique_actions, action_counts = np.unique(list(counts.values()), return_counts=True)\n",
    "            action_dict = dict(zip(unique_actions, action_counts))\n",
    "            plt.bar(action_dict.keys(), action_dict.values(), label=f'Agent {agent_id}', alpha=0.7)\n",
    "\n",
    "        plt.xlabel('Action')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Action Distribution for Each Agent')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_rewards_history(self):\n",
    "        num_agents = len(self.agent_rewards_history)\n",
    "\n",
    "        for agent_id, rewards in self.agent_rewards_history.items():\n",
    "            plt.plot(range(1, self.num_episodes + 1), rewards, label=f'Agent {agent_id}')\n",
    "\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Total Reward per Episode for Each Agent')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def run_vmas_env(self):\n",
    "        scenario_name = self.scenario if isinstance(self.scenario, str) else self.scenario.__class__.__name__\n",
    "\n",
    "        env = make_env(\n",
    "            scenario=self.scenario,\n",
    "            num_envs=self.num_envs,\n",
    "            device=self.device,\n",
    "            continuous_actions=self.continuous_actions,\n",
    "            seed=0,\n",
    "            **self.kwargs\n",
    "        )\n",
    "\n",
    "        for agent_id, agent in enumerate(env.agents):\n",
    "            self.q_learning_agents.append(QLearningAgent(env, agent_id, communication_weight=0.5))\n",
    "\n",
    "        init_time = time.time()\n",
    "        total_steps = 0\n",
    "\n",
    "        for e in range(self.num_episodes):\n",
    "            print(f\"Episode {e}\")\n",
    "            obs_cont = env.reset()\n",
    "            done = torch.tensor([False] * self.num_envs, device=self.device)\n",
    "            step = 0\n",
    "\n",
    "            episode_rewards = {i: 0 for i in range(len(self.q_learning_agents))}\n",
    "            episode_done_counts = {i: 0 for i in range(len(env.agents))}  # Track done counts for each agent\n",
    "\n",
    "\n",
    "            while not torch.all(done).item() and step < self.max_steps_per_episode:\n",
    "                step += 1\n",
    "                total_steps += 1\n",
    "                print(f\"Step {step} of Episode {e}\")\n",
    "\n",
    "                actions = []\n",
    "\n",
    "                for i, agent in enumerate(env.agents):\n",
    "                    if self.obs_discrete:\n",
    "                        discrete_obs = self._get_deterministic_obs(env, obs_cont[i])\n",
    "\n",
    "                    if self.random_action:\n",
    "                        action = env.get_random_action(agent)\n",
    "                    else:\n",
    "                        print(f\"Observation before passed to take an action from agent {i}: {discrete_obs}\")\n",
    "                        action = self._get_deterministic_action(agent, env, i, discrete_obs)\n",
    "\n",
    "                    actions.append(action)\n",
    "\n",
    "\n",
    "                next_obs_cont, rews, dones, info = env.step(actions)\n",
    "                done = dones\n",
    "\n",
    "                for i, agent in enumerate(env.agents):\n",
    "                    if self.obs_discrete:\n",
    "                        discrete_obs = self._get_deterministic_obs(env, obs_cont[i])\n",
    "                        discrete_next_obs = self._get_deterministic_obs(env, next_obs_cont[i])\n",
    "\n",
    "                    physical_obs_for_update = discrete_obs[0:6]\n",
    "                    physical_nextobs_for_update = discrete_next_obs[0:6]\n",
    "                    physical_actions_for_update = actions[i][0, 0, 0].unsqueeze(0)\n",
    "\n",
    "                    # print(f\"update q-table for agent {i}\")\n",
    "                    # if(i==1):\n",
    "                    #     self.q_learning_agents[1].update_q_table(\n",
    "                    #         physical_obs_for_update, physical_actions_for_update, rews[i].item(), physical_nextobs_for_update\n",
    "                    #     )\n",
    "                    # print(f\"Physical observation to update from agent {i}: {physical_obs_for_update}\")\n",
    "                    print(f\"Physical Observation to update from agent {i}: {physical_obs_for_update}\")\n",
    "                    print(f\"Physical Next observation to update from agent {i}: {physical_nextobs_for_update}\")\n",
    "\n",
    "                    self.q_learning_agents[i].update_q_table(agent, env, i,\n",
    "                        physical_obs_for_update, physical_actions_for_update, rews[i].item(), physical_nextobs_for_update\n",
    "                    )\n",
    "\n",
    "                    episode_rewards[i] += rews[i].item()\n",
    "\n",
    "                    if (done[0][i]):  # Increment individual agent's done count\n",
    "                        episode_done_counts[i] += 1\n",
    "                    \n",
    "\n",
    "                obs_cont = next_obs_cont\n",
    "\n",
    "                if self.render:\n",
    "                    frame = env.render(\n",
    "                        mode=\"rgb_array\",\n",
    "                        agent_index_focus=None,\n",
    "                    )\n",
    "                    self.frame_list.append(frame)\n",
    "\n",
    "                print(\"-----------------\")\n",
    "\n",
    "            for agent_id, total_reward in episode_rewards.items():\n",
    "                self.agent_rewards_history[agent_id].append(total_reward)\n",
    "\n",
    "            # Update success based on individual agent's done status\n",
    "            for agent_id in range(len(env.agents)):\n",
    "                print(f\"done status for agent {agent_id}: {done[0][agent_id]}\")\n",
    "                # print(f\"done status for agent {agent_id} v2: {done[0, agent_id]}\")\n",
    "                if done[0,agent_id]:\n",
    "                    self.successful_episodes_individual[agent_id] += episode_done_counts[agent_id] / step  # Calculate success rate for the agent\n",
    "\n",
    "            # Update success based on all agents' done status\n",
    "            if torch.all(done).item():\n",
    "                self.successful_episodes_all_agents += 1\n",
    "            \n",
    "            # Calculate success percentages for each agent\n",
    "            success_percents = [self.successful_episodes_individual[i] / (e + 1) * 100 for i in range(len(env.agents))]\n",
    "            overall_success_percent = self.successful_episodes_all_agents / (e + 1) * 100\n",
    "\n",
    "            print(f\"Success percentage of each agent at the end of episode {e}:\")\n",
    "\n",
    "            for i, percent in enumerate(success_percents):\n",
    "                print(f\"Agent {i}: {percent}%\")\n",
    "\n",
    "            print(f\"Overall success percentage for all agents up to episode {e}: {overall_success_percent}%\")\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Print final success percentages for each agent\n",
    "        for agent_id, count in self.successful_episodes_individual.items():\n",
    "            print(f\"Success percentage for agent {agent_id} = {count / self.num_episodes * 100}%\")\n",
    "        \n",
    "        # Print overall success percentage for all agents\n",
    "        overall_success_percentage = self.successful_episodes_all_agents / self.num_episodes * 100\n",
    "        print(f\"Overall success percentage for all agents = {overall_success_percentage}%\")\n",
    "\n",
    "\n",
    "        total_time = time.time() - init_time\n",
    "        print(\n",
    "            f\"It took: {total_time}s for {total_steps} steps across {self.num_episodes} episodes of {self.num_envs} parallel environments on device {self.device} \"\n",
    "            f\"for {scenario_name} scenario.\"\n",
    "        )\n",
    "\n",
    "        # success_percentage = (self.successful_episodes_overall / self.num_episodes) * 100\n",
    "        # print(f\"Percentage of successful episodes: {success_percentage}%\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scenario_name = \"navigation_comm\"\n",
    "    use_cuda = True\n",
    "\n",
    "    env_runner = VMASEnvRunner(\n",
    "        render=True,\n",
    "        num_envs=1,\n",
    "        num_episodes=3,\n",
    "        max_steps_per_episode=10,\n",
    "        device=torch.device(\"cuda\" if use_cuda else \"cpu\"),\n",
    "        scenario=scenario_name,\n",
    "        continuous_actions=False,\n",
    "        random_action=False,\n",
    "        n_agents=2,\n",
    "        obs_discrete=True,\n",
    "        agents_with_same_goal=2,\n",
    "        collisions=True,\n",
    "        shared_rew=False,\n",
    "    )\n",
    "\n",
    "    env_runner.run_vmas_env()\n",
    "    env_runner.plot_rewards_history()\n",
    "    # for agent in env_runner.q_learning_agents:\n",
    "    #     agent.print_q_table()\n",
    "\n",
    "    ipython_display(env_runner.generate_gif(scenario_name))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
