{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: -2\n",
      "Episode: 100, Total Reward: -1\n",
      "Episode: 200, Total Reward: -2\n",
      "Episode: 300, Total Reward: 0\n",
      "Episode: 400, Total Reward: 0\n",
      "Episode: 500, Total Reward: 0\n",
      "Episode: 600, Total Reward: 0\n",
      "Episode: 700, Total Reward: 0\n",
      "Episode: 800, Total Reward: -1\n",
      "Episode: 900, Total Reward: -1\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the environment\n",
    "# Let's consider a simple 3x3 grid world\n",
    "# S : Start\n",
    "# G : Goal\n",
    "# x : Obstacle\n",
    "# 0 : Empty cell\n",
    "# The agent can move in four directions: up, down, left, right\n",
    "# The goal is to reach the 'G' cell while avoiding obstacles 'x'\n",
    "\n",
    "env = np.array([\n",
    "    ['S', '0', 'x'],\n",
    "    ['0', 'x', '0'],\n",
    "    ['0', '0', 'G']\n",
    "])\n",
    "\n",
    "# Define parameters\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = np.prod(env.shape)\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "epsilon = 0.1\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "num_states = np.prod(env.shape)\n",
    "num_actions = 4  # up, down, left, right\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "# Helper function to convert 2D coordinates to a single index\n",
    "def state_to_index(state):\n",
    "    return state[0] * env.shape[1] + state[1]\n",
    "\n",
    "# Helper function to select an action using epsilon-greedy policy\n",
    "def choose_action(state):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.choice(num_actions)\n",
    "    else:\n",
    "        return np.argmax(q_table[state_to_index(state), :])\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = (0, 0)  # Start state\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        action = choose_action(state)\n",
    "        next_state = None\n",
    "        reward = None\n",
    "\n",
    "        # Take action\n",
    "        if action == 0:  # up\n",
    "            next_state = (state[0] - 1, state[1])\n",
    "        elif action == 1:  # down\n",
    "            next_state = (state[0] + 1, state[1])\n",
    "        elif action == 2:  # left\n",
    "            next_state = (state[0], state[1] - 1)\n",
    "        elif action == 3:  # right\n",
    "            next_state = (state[0], state[1] + 1)\n",
    "\n",
    "        # Check if the next state is valid\n",
    "        if 0 <= next_state[0] < env.shape[0] and 0 <= next_state[1] < env.shape[1] and env[next_state] != 'x':\n",
    "            reward = 0\n",
    "        else:\n",
    "            next_state = state  # Agent hits wall, stay in the same state\n",
    "            reward = -1\n",
    "\n",
    "        # Update Q-value\n",
    "        q_table[state_to_index(state), action] += learning_rate * (\n",
    "            reward + discount_rate * np.max(q_table[state_to_index(next_state), :]) - q_table[state_to_index(state), action])\n",
    "\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        if env[state] == 'G':\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# After training, you can use the learned Q-table to navigate the environment\n",
    "# For example, you can choose the best action in each state using the Q-values\n",
    "# and follow that policy to navigate from the start to the goal.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vmas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
