{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state tensor: tensor([0.])\n",
      "State tensor shape: torch.Size([1])\n",
      "state tensor flattened: tensor([[0.]])\n",
      "State tensor flattened shape: torch.Size([1, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1 and 16x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 293\u001b[0m\n\u001b[1;32m    291\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Define the threshold parameter\u001b[39;00m\n\u001b[1;32m    292\u001b[0m agent \u001b[38;5;241m=\u001b[39m QCBRL(actions, input_size, epsilon, gamma, alpha, lambd, threshold)\n\u001b[0;32m--> 293\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 205\u001b[0m, in \u001b[0;36mQCBRL.train\u001b[0;34m(self, episodes, max_steps, render)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Convert state to a numpy array if it's not already\u001b[39;00m\n\u001b[1;32m    203\u001b[0m state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(state)\n\u001b[0;32m--> 205\u001b[0m action, next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    207\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[7], line 172\u001b[0m, in \u001b[0;36mQCBRL.take_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate tensor flattened: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate_tensor_flattened\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState tensor flattened shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, state_tensor_flattened\u001b[38;5;241m.\u001b[39mshape) \n\u001b[0;32m--> 172\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor_flattened\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(action_probs, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    174\u001b[0m next_state \u001b[38;5;241m=\u001b[39m state\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 23\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x))\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1 and 16x64)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import pynvml\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "def discount_returns(rewards, gamma=0.99):\n",
    "    discounted_returns = []\n",
    "    running_add = 0\n",
    "    for t, r in enumerate(reversed(rewards)):\n",
    "        # print(t)\n",
    "        running_add = running_add * gamma ** t + r\n",
    "        discounted_returns.insert(0, running_add)\n",
    "    return discounted_returns\n",
    "\n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, actions, epsilon=0.1, gamma=0.99, alpha=0.1, lambd=0.9):\n",
    "        self.actions = actions\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.lambd = lambd\n",
    "        self.policy_network = PolicyNetwork(16, actions)\n",
    "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=self.alpha)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "        # print(\"State size:\", state_tensor.size())  # Add this line to check state tensor size\n",
    "        action_probs = self.policy_network(state_tensor)\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "        return action\n",
    "\n",
    "    def update_policy(self, rewards, log_probs):\n",
    "        returns = []\n",
    "        discounted_reward = 0\n",
    "        for r in reversed(rewards):\n",
    "            discounted_reward = r + self.gamma * discounted_reward\n",
    "            returns.insert(0, discounted_reward)\n",
    "\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "\n",
    "        policy_gradient = []\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            policy_gradient.append(-log_prob * R)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_gradient = torch.stack(policy_gradient).sum()\n",
    "        policy_gradient.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "class Case:\n",
    "    added_states = set()\n",
    "\n",
    "    def __init__(self, problem, solution, trust_value=1):\n",
    "        self.problem = np.array(problem)\n",
    "        self.solution = solution\n",
    "        self.trust_value = trust_value\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(state, case_base, threshold=0.5):\n",
    "        similarities = {}\n",
    "        for case in case_base:\n",
    "            similarities[case] = Case.sim_q(state, case.problem)\n",
    "        \n",
    "        sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if sorted_similarities:\n",
    "            most_similar_case = sorted_similarities[0][0] if sorted_similarities[0][1] >= threshold else None\n",
    "        else:\n",
    "            most_similar_case = None\n",
    "        \n",
    "        return most_similar_case\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(c, case_base, temporary_case_base):\n",
    "        if c not in temporary_case_base:\n",
    "            temporary_case_base.append(c)\n",
    "        \n",
    "        similar_cases = [case for case in case_base if np.array_equal(case.problem, c.problem)]\n",
    "        for similar_case in similar_cases:\n",
    "            if similar_case not in temporary_case_base:\n",
    "                temporary_case_base.append(similar_case)\n",
    "\n",
    "    @staticmethod\n",
    "    def revise(case_base, temporary_case_base, episode_ended_successfully):\n",
    "        for case in temporary_case_base:\n",
    "            if episode_ended_successfully and case in case_base:\n",
    "                case.trust_value += 0.1\n",
    "            elif not episode_ended_successfully and case in case_base:\n",
    "                case.trust_value -= 0.1\n",
    "            case.trust_value = max(0, min(case.trust_value,1))\n",
    "\n",
    "    @staticmethod\n",
    "    def retain(case_base, temporary_case_base, episode_ended_successfully, threshold):\n",
    "        if episode_ended_successfully:\n",
    "            for case in reversed(temporary_case_base):\n",
    "                state = tuple(np.atleast_1d(case.problem))\n",
    "                if state not in Case.added_states:\n",
    "                    case_base.append(case)\n",
    "                    Case.added_states.add(state)\n",
    "            \n",
    "            filtered_case_base = []\n",
    "            for case in case_base:\n",
    "                if case.trust_value >= threshold:\n",
    "                    filtered_case_base.append(case)\n",
    "            return filtered_case_base\n",
    "        else:\n",
    "            return case_base\n",
    "\n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)\n",
    "        state2 = np.atleast_1d(state2)\n",
    "        CNDMaxDist = 6\n",
    "        v = state1.size\n",
    "        DistQ = np.sum([Case.Dmin_phi(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def Dmin_phi(X1, X2):\n",
    "        return np.max(np.abs(X1 - X2))\n",
    "\n",
    "class QCBRL:\n",
    "    def __init__(self, actions, input_size, epsilon, gamma, alpha, lambd, threshold):\n",
    "        self.problem_solver = ProblemSolver(actions, epsilon, gamma, alpha, lambd)\n",
    "        self.case_base = []\n",
    "        self.threshold = threshold\n",
    "        self.temporary_case_base = []\n",
    "        self.policy_network = PolicyNetwork(input_size, actions)\n",
    "\n",
    "    def take_action(self, state):\n",
    "        # Convert state to a numpy array if it's not already\n",
    "        state_array = np.asarray(state)\n",
    "        # Convert state to a tensor and add batch dimension if needed\n",
    "        state_tensor = torch.tensor(state_array, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        similar_solution = Case.retrieve(state_tensor, self.case_base)\n",
    "        if similar_solution is not None:\n",
    "            action = similar_solution.solution\n",
    "            next_state = state\n",
    "        else:\n",
    "            # Flatten the state tensor\n",
    "            state_tensor_flattened = state_tensor.view(1, -1)\n",
    "            # print (f\"state tensor: {state_tensor}\")\n",
    "            # print(\"State tensor shape:\", state_tensor.shape) \n",
    "            # print(f\"state tensor flattened: {state_tensor_flattened}\")\n",
    "            # print(\"State tensor flattened shape:\", state_tensor_flattened.shape) \n",
    "            action_probs = self.policy_network(state_tensor_flattened)\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "            next_state = state\n",
    "\n",
    "        # Reuse case and return the selected action and the next state\n",
    "        c = Case(state, action)\n",
    "        Case.reuse(c, self.case_base, self.temporary_case_base)\n",
    "        return action, next_state\n",
    "\n",
    "\n",
    "    def train(self, episodes, max_steps, render=False):\n",
    "        env = gym.make('FrozenLake-v1')\n",
    "        rewards = []\n",
    "        episode_rewards = []\n",
    "        memory_usage = []\n",
    "        gpu_memory_usage = []\n",
    "        successful_episodes = 0\n",
    "\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "        for episode in range(1, episodes + 1):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            self.temporary_case_base = []\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                if render:\n",
    "                    env.render()\n",
    "\n",
    "                # Convert state to a numpy array if it's not already\n",
    "                state = np.array(state)\n",
    "\n",
    "                action, next_state = self.take_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    rewards.append(total_reward)\n",
    "                    episode_rewards.append(total_reward)\n",
    "                    if reward == 1.0:\n",
    "                        episode_ended_successfully = True\n",
    "                        successful_episodes += 1\n",
    "                    else:\n",
    "                        episode_ended_successfully = False\n",
    "\n",
    "                    print(f\"Episode {episode} ended after {step + 1} steps with total reward: {total_reward}\")\n",
    "                    break\n",
    "\n",
    "            Case.revise(self.case_base, self.temporary_case_base, episode_ended_successfully)\n",
    "            self.case_base = Case.retain(self.case_base, self.temporary_case_base, episode_ended_successfully, self.threshold)\n",
    "\n",
    "            memory_usage.append(psutil.virtual_memory().percent)\n",
    "            gpu_memory_usage.append(pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1024**2)\n",
    "\n",
    "        self.save_case_base_temporary()\n",
    "        self.save_case_base()\n",
    "\n",
    "        env.close()\n",
    "        self.plot_rewards(episode_rewards)\n",
    "        self.plot_resources(memory_usage, gpu_memory_usage)\n",
    "\n",
    "        success_percentage = (successful_episodes / episodes) * 100\n",
    "        print(f\"Percentage of Successful Episodes: {success_percentage}%\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def save_case_base_temporary(self):\n",
    "        filename = \"case_base_temporary.json\"\n",
    "        case_base_data = [{\"problem\": case.problem.tolist(), \"solution\": case.solution, \"trust_value\": case.trust_value} for case in self.temporary_case_base]\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "    def save_case_base(self):\n",
    "        filename = \"case_base.json\"\n",
    "        case_base_data = [{\"problem\": case.problem.tolist(), \"solution\": case.solution, \"trust_value\": case.trust_value} for case in self.case_base]\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "            print(\"Case base saved successfully.\")\n",
    "        \n",
    "    def load_case_base(self):\n",
    "        filename = \"case_base.json\"\n",
    "        try:\n",
    "            with open(filename, 'r') as file:\n",
    "                case_base_data = json.load(file)\n",
    "                self.case_base = [Case(np.array(case[\"problem\"]), case[\"solution\"], case[\"trust_value\"]) for case in case_base_data]\n",
    "                print(\"Case base loaded successfully.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Case base file not found. Starting with an empty case base.\")\n",
    "\n",
    "    def plot_rewards(self, rewards):\n",
    "        plt.plot(rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Rewards over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show() \n",
    "\n",
    "    def plot_resources(self, memory_usage, gpu_memory_usage):\n",
    "        plt.plot(memory_usage, label='Memory (%)')\n",
    "        plt.plot(gpu_memory_usage, label='GPU Memory (MB)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Resource Usage')\n",
    "        plt.title('Resource Usage over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    actions = 4\n",
    "    input_size = 16  # The state space size in FrozenLake-v1 is 16\n",
    "    epsilon = 0.1  # Define the epsilon parameter\n",
    "    gamma = 0.9  # Define the gamma parameter\n",
    "    alpha = 0.1  # Define the alpha parameter\n",
    "    lambd = 0.5  # Define the lambda parameter\n",
    "    threshold = 0.5  # Define the threshold parameter\n",
    "    agent = QCBRL(actions, input_size, epsilon, gamma, alpha, lambd, threshold)\n",
    "    agent.train(episodes=10000, max_steps=100000)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
