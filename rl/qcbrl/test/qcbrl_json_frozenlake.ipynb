{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retrieve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 156\u001b[0m\n\u001b[1;32m    154\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m  \u001b[38;5;66;03m# Example number of actions\u001b[39;00m\n\u001b[1;32m    155\u001b[0m agent \u001b[38;5;241m=\u001b[39m QCBRL(actions)\n\u001b[0;32m--> 156\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m agent\u001b[38;5;241m.\u001b[39msave_case_base(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcase_base.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 71\u001b[0m, in \u001b[0;36mQCBRL.train\u001b[0;34m(self, episodes, max_steps)\u001b[0m\n\u001b[1;32m     67\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_steps\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# self.env.render()  # Render the environment\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# print(\"State: {}\".format(state))\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     action, next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# print(\"Action: {}\".format(action))\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[0;32mIn[2], line 111\u001b[0m, in \u001b[0;36mQCBRL.take_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    107\u001b[0m     state_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(state)  \u001b[38;5;66;03m# Ensure state is a numpy array\u001b[39;00m\n\u001b[1;32m    109\u001b[0m state_tuple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(state_array\u001b[38;5;241m.\u001b[39mtolist())  \u001b[38;5;66;03m# Convert numpy array to tuple\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m similar_case \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve\u001b[49m(state_array, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcase_base)  \u001b[38;5;66;03m# Find similar case in the case base\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m similar_case \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     action \u001b[38;5;241m=\u001b[39m similar_case\u001b[38;5;241m.\u001b[39maction\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retrieve' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, actions, epsilon=0.1, gamma=0.99, alpha=0.1, lambd=0.9):\n",
    "        self.actions = actions\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.lambd = lambd\n",
    "        self.Q = {}  # Q-values table\n",
    "        self.e = {}  # Eligibility traces table\n",
    "\n",
    "    def choose_action(self, sq):\n",
    "        sq_tuple = sq  # Convert numpy array to tuple\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            if sq_tuple in self.Q:  # Use the tuple as the key\n",
    "                return np.argmax(self.Q[sq_tuple])\n",
    "            else:\n",
    "                return np.random.choice(self.actions)\n",
    "\n",
    "\n",
    "    def update_Q(self, sq, action, reward, next_sq, next_action):\n",
    "        if sq not in self.Q:\n",
    "            self.Q[sq] = np.zeros(self.actions)\n",
    "            self.e[sq] = np.zeros(self.actions)\n",
    "\n",
    "        delta = reward + self.gamma * self.Q.get(next_sq, np.zeros(self.actions))[next_action] - self.Q[sq][action]\n",
    "        self.e[sq][action] += 1\n",
    "\n",
    "        for state in self.Q:\n",
    "            for a in range(self.actions):\n",
    "                self.Q[state][a] += self.alpha * delta * self.e[state][a]\n",
    "                self.e[state][a] *= self.gamma * self.lambd\n",
    "\n",
    "    def solve_problem(self, sq, reward, next_sq):\n",
    "        action = self.choose_action(sq)\n",
    "        if reward is not None:  # If reward is received\n",
    "            next_action = self.choose_action(next_sq)\n",
    "            self.update_Q(sq, action, reward, next_sq, next_action)\n",
    "            sq = next_sq\n",
    "            return action, sq\n",
    "        else:\n",
    "            return action, sq\n",
    "\n",
    "def sim_q(sq, c):\n",
    "    # Example of a similarity function comparing qualitative states\n",
    "    similarity = np.random.rand()  # Replace this with your own similarity calculation\n",
    "    return similarity\n",
    "\n",
    "class QCBRL:\n",
    "    def __init__(self, actions, threshold=0.5, epsilon=0.1, gamma=0.99, alpha=0.1, lambd=0.9):\n",
    "        self.problem_solver = ProblemSolver(actions, epsilon, gamma, alpha, lambd)\n",
    "        self.C_B = {}\n",
    "        self.threshold = threshold\n",
    "        self.env = gym.make('FrozenLake-v1')\n",
    "\n",
    "    def train(self, episodes, max_steps):\n",
    "        total_rewards = []  # List to store total rewards for each episode\n",
    "\n",
    "        for episode in range(1, episodes+1):\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0\n",
    "            for step in range(1, max_steps+1):\n",
    "                # self.env.render()  # Render the environment\n",
    "                # print(\"State: {}\".format(state))\n",
    "                action, next_state = self.take_action(state)\n",
    "                # print(\"Action: {}\".format(action))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                # print(\"next_state: {}\".format(next_state))\n",
    "                # print(\"Reward: {}\".format(reward))\n",
    "                total_reward += reward\n",
    "                c = (state, action, reward, next_state)\n",
    "                self.reuse(c)\n",
    "                # Determine if the episode ended successfully\n",
    "                episode_ended_successfully = done\n",
    "                if episode_ended_successfully:\n",
    "                    new_C_B = {}\n",
    "                    for key, value in self.C_B.items():\n",
    "                        if isinstance(key, tuple):\n",
    "                            new_key = key\n",
    "                        else:\n",
    "                            new_key = (key,)\n",
    "                        new_C_B[new_key] = value\n",
    "                    self.C_B = new_C_B\n",
    "                    break  # Exit loop if episode ends\n",
    "                state = next_state\n",
    "            total_rewards.append(total_reward)  # Append total reward for current episode\n",
    "            print(f\"Episode {episode} finished after {step} steps with total reward: {total_reward}\")\n",
    "\n",
    "        # Plot the graph\n",
    "        plt.plot(range(1, episodes + 1), total_rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Agent Performance Over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def take_action(self, state):\n",
    "        action, _ = self.problem_solver.solve_problem(state, None, None)\n",
    "        return action, state\n",
    "\n",
    "    def reuse(self, c):\n",
    "        hashed_c = tuple(array.tobytes() if isinstance(array, np.ndarray) else array for array in c)\n",
    "        self.C_B[hashed_c] = c\n",
    "\n",
    "    def convert_to_serializable(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, dict):\n",
    "            converted_dict = {}\n",
    "            for k, v in obj.items():\n",
    "                if isinstance(k, tuple):\n",
    "                    k = '_'.join(map(str, k))  # Convert tuple keys to strings\n",
    "                converted_dict[self.convert_to_serializable(k)] = self.convert_to_serializable(v)\n",
    "            return converted_dict\n",
    "        elif isinstance(obj, list):\n",
    "            return [self.convert_to_serializable(item) for item in obj]\n",
    "        elif isinstance(obj, tuple):\n",
    "            return tuple(self.convert_to_serializable(item) for item in obj)\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    def save_case_base(self, filename):\n",
    "        # Convert NumPy arrays to lists and tuple keys to string keys\n",
    "        converted_case_base = self.convert_to_serializable(self.C_B)\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(converted_case_base, file)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    actions = 4  # Example number of actions\n",
    "    agent = QCBRL(actions)\n",
    "    agent.train(episodes=200, max_steps=1000)\n",
    "    agent.save_case_base(\"case_base.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
