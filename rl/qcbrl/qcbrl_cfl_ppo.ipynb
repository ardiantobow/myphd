{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'environment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvironment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Env\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Set random seeds for reproducibility\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'environment'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import pynvml\n",
    "import json\n",
    "\n",
    "# Register the custom FrozenLake environment\n",
    "register(\n",
    "    id='CustomRewardFrozenLake-v1',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.78,  # Adjust the reward threshold if needed\n",
    ")\n",
    "\n",
    "# Define the custom FrozenLake environment with modified rewards\n",
    "class CustomRewardFrozenLake(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"CustomRewardFrozenLake-v1\")\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        if reward == 0 and not done:\n",
    "            reward = 0\n",
    "        elif reward == 0 and done:\n",
    "            reward = -5\n",
    "        elif reward == 1:\n",
    "            reward = 1\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "# Define Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Define Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        # print(f\"Critic Input Dim: {input_dim}\")\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define Experience Buffer\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def add(self, state, action, log_prob, value, reward, done):\n",
    "        self.buffer.append((state, action, log_prob, value, reward, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, log_probs, values, rewards, dones = zip(*batch)\n",
    "        return states, actions, log_probs, values, rewards, dones\n",
    "\n",
    "# PPO loss function\n",
    "def calculate_ppo_loss(actor, critic, states, actions, log_probs_old, values_old, advantages, epsilon=0.2, c1=0.5, c2=0.01):\n",
    "    policy = actor(states)\n",
    "    dist = torch.distributions.Categorical(policy)\n",
    "\n",
    "    log_probs = dist.log_prob(actions)\n",
    "    ratios = torch.exp(log_probs - log_probs_old)\n",
    "    surr1 = ratios * advantages\n",
    "    surr2 = torch.clamp(ratios, 1.0 - epsilon, 1.0 + epsilon) * advantages\n",
    "\n",
    "    actor_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "    values = critic(states)\n",
    "    critic_loss = (values - values_old).pow(2).mean()\n",
    "\n",
    "    entropy = dist.entropy().mean()\n",
    "\n",
    "    total_loss = actor_loss + c1 * critic_loss - c2 * entropy\n",
    "\n",
    "    return actor_loss, critic_loss\n",
    "\n",
    "# Discounted rewards function\n",
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    discounted_rewards = []\n",
    "    running_add = 0\n",
    "    for r in reversed(rewards):\n",
    "        running_add = running_add * gamma + r\n",
    "        discounted_rewards.insert(0, running_add)\n",
    "    return discounted_rewards\n",
    "\n",
    "# Function to compute advantages\n",
    "def compute_advantages(critic, states, rewards):\n",
    "    # print(f\"Critic of compute advantages: {critic}\")\n",
    "    # print(f\"states of compute advantages: {states}\")\n",
    "    # print(f\"rewards of compute advantages: {rewards}\")\n",
    "    values = critic(states).squeeze()\n",
    "    rewards_tensor = torch.tensor(rewards, dtype=torch.float32)  # Convert rewards to a PyTorch tensor\n",
    "    # print(f\"Length of rewards_tensor: {len(rewards)}\")\n",
    "    # print(f\"Length of values_tensor: {len(values)}\")\n",
    "    advantages = rewards_tensor - values\n",
    "    return advantages\n",
    "\n",
    "class Case:\n",
    "    added_states = set()  # Class attribute to store states already added to the case base\n",
    "\n",
    "    def __init__(self, problem, solution, trust_value=1):\n",
    "        self.problem = np.array(problem)  # Convert problem to numpy array\n",
    "        self.solution = solution\n",
    "        self.trust_value = trust_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)  # Ensure state1 is at least 1-dimensional\n",
    "        state2 = np.atleast_1d(state2)  # Ensure state2 is at least 1-dimensional\n",
    "        CNDMaxDist = 6  # Maximum distance between two nodes in the CND\n",
    "        v = state1.size  # Total number of objects the agent can perceive\n",
    "        DistQ = np.sum([Case.Dmin_phi(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def Dmin_phi(X1, X2):\n",
    "        return np.max(np.abs(X1 - X2))\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(state, case_base, threshold=0.2):\n",
    "        similarities = {}\n",
    "        for case in case_base:\n",
    "            similarities[case] = Case.sim_q(state, case.problem)  # Compare state with the problem part of the case\n",
    "        \n",
    "        sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if sorted_similarities:\n",
    "            most_similar_case = sorted_similarities[0][0] if sorted_similarities[0][1] >= threshold else None\n",
    "        else:\n",
    "            most_similar_case = None\n",
    "        \n",
    "        return most_similar_case\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(c, temporary_case_base):\n",
    "        temporary_case_base.append(c)\n",
    "        # Store the new case from the problem solver\n",
    "        # if c not in temporary_case_base:\n",
    "            # temporary_case_base.append(c)\n",
    "        \n",
    "        # Check if there are similar cases in case_base\n",
    "        # similar_cases = [case for case in case_base if np.array_equal(case.problem, c.problem)]\n",
    "        # for similar_case in similar_cases:\n",
    "            # temporary_case_base.append(similar_case)\n",
    "            # if similar_case not in temporary_case_base:\n",
    "            #     temporary_case_base.append(similar_case)\n",
    "\n",
    "    @staticmethod\n",
    "    def revise(case_base, temporary_case_base, successful_episodes):\n",
    "        for case in temporary_case_base:\n",
    "            if successful_episodes and case in case_base:\n",
    "                case.trust_value += 0.1  # Increment trust value if the episode ended successfully and the case is in the case base\n",
    "            elif not successful_episodes and case in case_base:\n",
    "                case.trust_value -= 0.1  # Decrement trust value if the episode ended unsuccessfully and the case is in the case base\n",
    "            case.trust_value = max(0, min(case.trust_value,1))  # Ensure trust value is within[0,1]\n",
    "\n",
    "    @staticmethod\n",
    "    def retain(case_base, temporary_case_base, successful_episodes, threshold=0):\n",
    "        if successful_episodes:\n",
    "            # Iterate through the temporary case base to find the last occurrence of each unique state\n",
    "            for case in reversed(temporary_case_base):\n",
    "                state = tuple(np.atleast_1d(case.problem))\n",
    "                # Check if the state is already in the case base or has been added previously\n",
    "                if state not in Case.added_states:\n",
    "                    # Add the case to the case base if the state is new\n",
    "                    case_base.append(case)\n",
    "                    Case.added_states.add(state)\n",
    "            \n",
    "            # Filter case_base based on trust_value\n",
    "            filtered_case_base = []\n",
    "            for case in case_base:\n",
    "                # print(f\"trust value >= Threshold?: {case.trust_value} >= {threshold}?\")\n",
    "                if case.trust_value >= threshold:\n",
    "                    # print(f\"problem | trust value: {case.problem} | {case.trust_value}\")\n",
    "                    # print(\"case saved dong\")\n",
    "                    filtered_case_base.append(case)\n",
    "                else:\n",
    "                    # print(f\"problem | trust value: {case.problem} | {case.trust_value}\")\n",
    "                    # print(\"case unsaved dong\")\n",
    "                    pass\n",
    "\n",
    "            return filtered_case_base\n",
    "        else:\n",
    "            return case_base  # Return original case_base if episode is not successful\n",
    "\n",
    "class QCBRL:\n",
    "    def __init__(self, num_actions, env):\n",
    "        self.num_actions = num_actions\n",
    "        self.env = env\n",
    "        self.problem_solver = ProblemSolver(env)\n",
    "        self.case_base = []\n",
    "        self.temporary_case_base = []\n",
    "\n",
    "    def run(self, episodes, max_steps, alpha=0.1, gamma=0.9, epsilon=0.1, render=False):\n",
    "        total_rewards = []\n",
    "        # episode_rewards = []\n",
    "        memory_usage = []\n",
    "        gpu_memory_usage = []\n",
    "        successful_episodes = False\n",
    "        num_successful_episodes = 0\n",
    "        total_steps_list = [] \n",
    "\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            # print(f\"State: {state}\")\n",
    "            self.temporary_case_base = []\n",
    "            episode_reward = 0\n",
    "            total_steps = 0\n",
    "            \n",
    "            states = []\n",
    "            actions = []\n",
    "            log_probs = []\n",
    "            values = []\n",
    "            rewards = []\n",
    "            dones = []\n",
    "\n",
    "            for _ in range(max_steps):\n",
    "                # if render:\n",
    "                #     env.render()\n",
    "                action = self.take_action(state, epsilon)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                state_one_hot = torch.zeros(1, env.observation_space.n)\n",
    "                state_one_hot[0, state] = 1\n",
    "                states.append(state_one_hot)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "\n",
    "                c = Case(state, action)\n",
    "                Case.reuse(c, self.temporary_case_base)\n",
    "\n",
    "                policy = self.problem_solver.actor(state_one_hot)\n",
    "                dist = torch.distributions.Categorical(policy)\n",
    "                log_prob = dist.log_prob(torch.tensor(action))\n",
    "                log_probs.append(log_prob)\n",
    "                value = self.problem_solver.critic(state_one_hot)\n",
    "                values.append(value)\n",
    "\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                total_steps += 1\n",
    "\n",
    "                if done:\n",
    "                    successful_episodes = reward > 0\n",
    "                    break\n",
    "                \n",
    "                \n",
    "                \n",
    "            # print(f\"States before policy: {states}\")\n",
    "            # print(f\"Rewards before policy: {rewards}\")\n",
    "            self.problem_solver.update_policy(states, actions, log_probs, values, rewards, dones)\n",
    "            \n",
    "            # episode_rewards.append(sum(rewards))\n",
    "            \n",
    "            if episode_reward > 0:  # If the agent reached the goal state\n",
    "                num_successful_episodes += 1\n",
    "                total_steps_list.append(total_steps)  # Append total steps for this episode   \n",
    "            else:\n",
    "                total_steps_list.append(0)\n",
    "\n",
    "            total_rewards.append(episode_reward)\n",
    "            print(f\"Episode {episode + 1}, Total Reward: {episode_reward}\")\n",
    "\n",
    "            Case.revise(self.case_base, self.temporary_case_base, successful_episodes)\n",
    "            self.case_base = Case.retain(self.case_base, self.temporary_case_base, successful_episodes)\n",
    "            \n",
    "            memory_usage.append(psutil.virtual_memory().percent)\n",
    "            gpu_memory_usage.append(pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1024**2)\n",
    "\n",
    "        \n",
    "        self.save_case_base_temporary()  # Save temporary case base after training\n",
    "        self.save_case_base()  # Save case base after training\n",
    "\n",
    "        success_rate = (num_successful_episodes / episodes) * 100\n",
    "        # print(f\"Successful episodes: {num_successful_episodes}\")\n",
    "\n",
    "        # env.close()\n",
    "        return total_rewards, success_rate, memory_usage, gpu_memory_usage, total_steps_list\n",
    "\n",
    "    def take_action(self, state, epsilon):\n",
    "\n",
    "        similar_solution = Case.retrieve(state, self.case_base)\n",
    "        if similar_solution is not None:\n",
    "            action = similar_solution.solution\n",
    "            # print(\"action from case base\")\n",
    "        else:\n",
    "            action = self.problem_solver.choose_action(state, epsilon)\n",
    "            # print(\"action from problem solver\")\n",
    "        \n",
    "        # action = self.problem_solver.choose_action(state, epsilon)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def save_case_base_temporary(self):\n",
    "        filename = \"case_base_temporary.json\"\n",
    "        case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else int(case.problem), \n",
    "                        \"solution\": int(case.solution), \n",
    "                        \"trust_value\": int(case.trust_value)} for case in self.temporary_case_base]\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "        print(\"Temporary case base saved successfully.\")\n",
    "\n",
    "    def save_case_base(self):\n",
    "        filename = \"case_base.json\"\n",
    "        case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else int(case.problem), \n",
    "                        \"solution\": int(case.solution), \n",
    "                        \"trust_value\": int(case.trust_value)} for case in self.case_base]\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "            print(\"Case base saved successfully.\")  # Add this line to check if the case base is being saved\n",
    "        \n",
    "    def load_case_base(self):\n",
    "        filename = \"case_base.json\"\n",
    "        try:\n",
    "            with open(filename, 'r') as file:\n",
    "                case_base_data = json.load(file)\n",
    "                self.case_base = [Case(np.array(case[\"problem\"]), case[\"solution\"], case[\"trust_value\"]) for case in case_base_data]\n",
    "                print(\"Case base loaded successfully.\")  # Add this line to check if the case base is being loaded\n",
    "        except FileNotFoundError:\n",
    "            print(\"Case base file not found. Starting with an empty case base.\")\n",
    "\n",
    "    \n",
    "    def display_success_rate(self, success_rate):\n",
    "        print(f\"Success rate: {success_rate}%\")\n",
    "\n",
    "\n",
    "    def plot_rewards(self, rewards):\n",
    "        plt.plot(rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Rewards over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_resources(self, memory_usage, gpu_memory_usage):\n",
    "        plt.plot(memory_usage, label='Memory (%)')\n",
    "        plt.plot(gpu_memory_usage, label='GPU Memory (MB)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Resource Usage')\n",
    "        plt.title('Resource Usage over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_total_steps(self, total_steps_list):\n",
    "        plt.plot(total_steps_list)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Steps')\n",
    "        plt.title('Total Steps for Successful Episodes over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show() \n",
    "\n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.actor = Actor(env.observation_space.n, env.action_space.n)\n",
    "        self.critic = Critic(env.observation_space.n)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=0.001)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=0.001)\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        state_one_hot = torch.zeros(1, self.env.observation_space.n)\n",
    "        state_one_hot[0, state] = 1\n",
    "        policy = self.actor(state_one_hot)\n",
    "        action_probs = torch.softmax(policy, dim=-1)\n",
    "        action = np.random.choice(env.action_space.n, p=action_probs.detach().numpy().flatten())\n",
    "        return action\n",
    "\n",
    "    def update_policy(self, states, actions, log_probs, values, rewards, dones):\n",
    "        # print(f\"Rewards before advantages: {rewards}\")\n",
    "        discounted_rewards = discount_rewards(rewards)\n",
    "        # print(f\"Discounted Rewards before advantages: {discounted_rewards}\")\n",
    "        # print(f\"State before advantages: {states}\")\n",
    "        advantages = compute_advantages(self.critic, torch.stack(states), discounted_rewards)\n",
    "        \n",
    "        actions_tensor = torch.tensor(actions, dtype=torch.int64)\n",
    "        log_probs_tensor = torch.cat(log_probs)\n",
    "        values_tensor = torch.cat(values)\n",
    "        advantages_tensor = torch.tensor(advantages, dtype=torch.float32)\n",
    "\n",
    "        actor_loss, critic_loss = calculate_ppo_loss(self.actor, self.critic, torch.stack(states), actions_tensor, log_probs_tensor, values_tensor, advantages_tensor)\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = CustomRewardFrozenLake()\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    agent = QCBRL(num_actions, env)\n",
    "    rewards, success_rate, memory_usage, gpu_memory_usage, total_step_list = agent.run(episodes=1000, max_steps=1000, alpha=0.1, gamma=0.9, epsilon=0.1)\n",
    "\n",
    "    agent.display_success_rate(success_rate)\n",
    "    agent.plot_rewards(rewards)\n",
    "    agent.plot_resources(memory_usage, gpu_memory_usage)\n",
    "    agent.plot_total_steps(total_step_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
