{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action from problem solver\n",
      "State: [0.] - Action: 2 - Reward: 0 - Next State: 0 - Done: False\n",
      "action from problem solver\n",
      "State: [0.] - Action: 2 - Reward: 0 - Next State: 4 - Done: False\n",
      "action from problem solver\n",
      "State: [4.] - Action: 2 - Reward: 0 - Next State: 0 - Done: False\n",
      "action from problem solver\n",
      "State: [0.] - Action: 2 - Reward: 0 - Next State: 1 - Done: False\n",
      "action from problem solver\n",
      "State: [1.] - Action: 2 - Reward: 0 - Next State: 1 - Done: False\n",
      "action from problem solver\n",
      "State: [1.] - Action: 2 - Reward: -5 - Next State: 5 - Done: True\n",
      "action from problem solver\n",
      "State: [0.] - Action: 2 - Reward: 0 - Next State: 1 - Done: False\n",
      "action from problem solver\n",
      "State: [1.] - Action: 2 - Reward: -5 - Next State: 5 - Done: True\n",
      "action from problem solver\n",
      "State: [0.] - Action: 2 - Reward: 0 - Next State: 1 - Done: False\n",
      "action from problem solver\n",
      "State: [1.] - Action: 2 - Reward: 0 - Next State: 2 - Done: False\n",
      "action from problem solver\n",
      "State: [2.] - Action: 2 - Reward: 0 - Next State: 6 - Done: False\n",
      "action from problem solver\n",
      "State: [6.] - Action: 2 - Reward: 0 - Next State: 10 - Done: False\n",
      "action from problem solver\n",
      "State: [10.] - Action: 2 - Reward: 0 - Next State: 6 - Done: False\n",
      "action from problem solver\n",
      "State: [6.] - Action: 2 - Reward: -5 - Next State: 7 - Done: True\n",
      "action from problem solver\n",
      "State: [0.] - Action: 2 - Reward: 0 - Next State: 4 - Done: False\n",
      "action from problem solver\n",
      "State: [4.] - Action: 2 - Reward: -5 - Next State: 5 - Done: True\n",
      "action from problem solver\n",
      "State: [0.] - Action: 2 - Reward: 0 - Next State: 0 - Done: False\n",
      "action from problem solver\n",
      "State: [0.] - Action: 2 - Reward: 0 - Next State: 4 - Done: False\n",
      "action from problem solver\n",
      "State: [4.] - Action: 2 - Reward: -5 - Next State: 5 - Done: True\n",
      "action from problem solver\n",
      "State: [0.] - Action: 2 - Reward: 0 - Next State: 4 - Done: False\n",
      "action from problem solver\n",
      "State: [4.] - Action: 2 - Reward: 0 - Next State: 8 - Done: False\n",
      "action from problem solver\n",
      "State: [8.] - Action: 2 - Reward: 0 - Next State: 4 - Done: False\n",
      "action from problem solver\n",
      "State: [4.] - Action: 1 - Reward: 0 - Next State: 4 - Done: False\n",
      "action from problem solver\n",
      "State: [4.] - Action: 2 - Reward: 0 - Next State: 0 - Done: False\n",
      "action from problem solver\n",
      "State: [0.] - Action: 2 - Reward: 0 - Next State: 1 - Done: False\n",
      "action from problem solver\n",
      "State: [1.] - Action: 2 - Reward: 0 - Next State: 1 - Done: False\n",
      "action from problem solver\n",
      "State: [1.] - Action: 2 - Reward: 0 - Next State: 1 - Done: False\n",
      "action from problem solver\n",
      "State: [1.] - Action: 0 - Reward: 0 - Next State: 1 - Done: False\n",
      "action from problem solver\n",
      "State: [1.] - Action: 2 - Reward: 0 - Next State: 2 - Done: False\n",
      "action from problem solver\n",
      "State: [2.] - Action: 2 - Reward: 0 - Next State: 6 - Done: False\n",
      "action from problem solver\n",
      "State: [6.] - Action: 2 - Reward: -5 - Next State: 7 - Done: True\n",
      "action from problem solver\n",
      "State: [0.] - Action: 2 - Reward: 0 - Next State: 4 - Done: False\n",
      "action from problem solver\n",
      "State: [4.] - Action: 2 - Reward: 0 - Next State: 0 - Done: False\n",
      "action from problem solver\n",
      "State: [0.] - Action: 2 - Reward: 0 - Next State: 0 - Done: False\n",
      "action from problem solver\n",
      "State: [0.] - Action: 2 - Reward: 0 - Next State: 4 - Done: False\n",
      "action from problem solver\n",
      "State: [4.] - Action: 2 - Reward: 0 - Next State: 0 - Done: False\n",
      "action from problem solver\n",
      "State: [0.] - Action: 2 - Reward: 0 - Next State: 0 - Done: False\n",
      "action from problem solver\n",
      "State: [0.] - Action: 2 - Reward: 0 - Next State: 4 - Done: False\n",
      "action from problem solver\n",
      "State: [4.] - Action: 2 - Reward: 0 - Next State: 8 - Done: False\n",
      "action from problem solver\n",
      "State: [8.] - Action: 2 - Reward: 0 - Next State: 4 - Done: False\n",
      "action from problem solver\n",
      "State: [4.] - Action: 2 - Reward: 0 - Next State: 0 - Done: False\n",
      "action from problem solver\n",
      "State: [0.] - Action: 2 - Reward: 0 - Next State: 1 - Done: False\n",
      "action from problem solver\n",
      "State: [1.] - Action: 2 - Reward: 0 - Next State: 2 - Done: False\n",
      "action from problem solver\n",
      "State: [2.] - Action: 2 - Reward: 0 - Next State: 2 - Done: False\n",
      "action from problem solver\n",
      "State: [2.] - Action: 2 - Reward: 0 - Next State: 6 - Done: False\n",
      "action from problem solver\n",
      "State: [6.] - Action: 2 - Reward: 0 - Next State: 10 - Done: False\n",
      "action from problem solver\n",
      "State: [10.] - Action: 2 - Reward: -5 - Next State: 11 - Done: True\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x32 and 1x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 391\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# print(num_actions)\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;66;03m# print(num_states)\u001b[39;00m\n\u001b[1;32m    390\u001b[0m agent \u001b[38;5;241m=\u001b[39m QCBRL(num_actions, env)\n\u001b[0;32m--> 391\u001b[0m rewards, success_rate, memory_usage, gpu_memory_usage \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m agent\u001b[38;5;241m.\u001b[39mdisplay_success_rate(success_rate)\n\u001b[1;32m    394\u001b[0m agent\u001b[38;5;241m.\u001b[39mplot_rewards(rewards)\n",
      "Cell \u001b[0;32mIn[9], line 307\u001b[0m, in \u001b[0;36mQCBRL.run\u001b[0;34m(self, episodes, max_steps, alpha, gamma, epsilon, render)\u001b[0m\n\u001b[1;32m    304\u001b[0m         num_successful_episodes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    306\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(episode_reward)\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproblem_solver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_rewards\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m success_rate \u001b[38;5;241m=\u001b[39m (num_successful_episodes \u001b[38;5;241m/\u001b[39m episodes) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rewards, success_rate, memory_usage, gpu_memory_usage\n",
      "Cell \u001b[0;32mIn[9], line 162\u001b[0m, in \u001b[0;36mProblemSolver.train\u001b[0;34m(self, states, actions, rewards)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state, action, reward \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(states, actions, rewards):\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mpush(state, action, \u001b[38;5;28;01mNone\u001b[39;00m, reward, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mupdate_target_network()\n",
      "Cell \u001b[0;32mIn[9], line 135\u001b[0m, in \u001b[0;36mDQNAgent.update_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m done_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(batch\u001b[38;5;241m.\u001b[39mdone, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    134\u001b[0m current_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net(state_batch)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, action_batch)\n\u001b[0;32m--> 135\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    136\u001b[0m target_q_values \u001b[38;5;241m=\u001b[39m reward_batch \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m next_q_values \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m~\u001b[39mdone_mask\n\u001b[1;32m    138\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(current_q_values, target_q_values\u001b[38;5;241m.\u001b[39mdetach())\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 62\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 62\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     63\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x32 and 1x128)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import pynvml\n",
    "from collections import Counter\n",
    "from collections import namedtuple\n",
    "from gym.envs.registration import register\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# Register the wrapper with a new environment ID\n",
    "register(\n",
    "    id='CustomRewardFrozenLake-v1',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '4x4', 'is_slippery': True},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=1,  # Adjust the reward threshold if needed\n",
    ")\n",
    "\n",
    "class CustomRewardFrozenLake(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"CustomRewardFrozenLake-v1\")\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        if reward == 0 and not done:\n",
    "            reward = 0\n",
    "        elif reward == 0 and done:\n",
    "            reward = -5\n",
    "        elif reward == 1:\n",
    "            reward = 1\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        # state = self.env.reset()\n",
    "        # state_array = np.array([state], dtype=np.float32)  # Convert the state to a NumPy array\n",
    "        # return state_array\n",
    "        return self.env.reset()\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "# Define the DQN Model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the Replay Buffer\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        if args[2] is None:  # Check if next_state is None\n",
    "            args = list(args)\n",
    "            args[2] = 0  # Replace None with 0 (or any other placeholder value)\n",
    "            args = tuple(args)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_size, output_size, lr=0.01, gamma=0.99, batch_size=32, capacity=10000):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if input_size is not None:  # Check if input_size is not None\n",
    "            self.policy_net = DQN(input_size, output_size).to(self.device)\n",
    "            self.target_net = DQN(input_size, output_size).to(self.device)\n",
    "        else:\n",
    "            # For scalar observation spaces, use a simpler network with one input neuron\n",
    "            self.policy_net = DQN(1, output_size).to(self.device)\n",
    "            self.target_net = DQN(1, output_size).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = ReplayBuffer(capacity)\n",
    "        self.output_size = output_size \n",
    "\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randrange(self.output_size)\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "            q_values = self.policy_net(state)\n",
    "            return q_values.max(1)[1].item()\n",
    "\n",
    "    def update_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = torch.tensor(batch.state, dtype=torch.float32).to(self.device)\n",
    "        action_batch = torch.tensor(batch.action, dtype=torch.long).unsqueeze(1).to(self.device)\n",
    "        reward_batch = torch.tensor(batch.reward, dtype=torch.float32).unsqueeze(1).to(self.device)\n",
    "        next_state_batch = torch.tensor(batch.next_state, dtype=torch.float32).to(self.device)\n",
    "        done_mask = torch.tensor(batch.done, dtype=torch.bool).to(self.device)\n",
    "\n",
    "        current_q_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        next_q_values = self.target_net(next_state_batch).max(1)[0].unsqueeze(1)\n",
    "        target_q_values = reward_batch + self.gamma * next_q_values * ~done_mask\n",
    "\n",
    "        loss = self.criterion(current_q_values, target_q_values.detach())\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "# Modify the Problem Solver to Use DQN\n",
    "class ProblemSolver:\n",
    "    def __init__(self, num_actions, env):\n",
    "        self.num_actions = num_actions\n",
    "        input_size = 1\n",
    "        self.env = env\n",
    "        # print(\"Observation Space Shape:\", env.observation_space.shape)\n",
    "        self.agent = DQNAgent(input_size, num_actions)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        return self.agent.choose_action(state, epsilon=0.1)\n",
    "\n",
    "    def train(self, states, actions, rewards):\n",
    "        for state, action, reward in zip(states, actions, rewards):\n",
    "            self.agent.memory.push(state, action, None, reward, False)\n",
    "        self.agent.update_model()\n",
    "        self.agent.update_target_network()\n",
    "\n",
    "\n",
    "class Case:\n",
    "    added_states = set()  # Class attribute to store states already added to the case base\n",
    "\n",
    "    def __init__(self, problem, solution, trust_value=1):\n",
    "        self.problem = np.array(problem)  # Convert problem to numpy array\n",
    "        self.solution = solution\n",
    "        self.trust_value = trust_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)  # Ensure state1 is at least 1-dimensional\n",
    "        state2 = np.atleast_1d(state2)  # Ensure state2 is at least 1-dimensional\n",
    "        CNDMaxDist = 6  # Maximum distance between two nodes in the CND\n",
    "        v = state1.size  # Total number of objects the agent can perceive\n",
    "        DistQ = np.sum([Case.Dmin_phi(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def Dmin_phi(X1, X2):\n",
    "        return np.max(np.abs(X1 - X2))\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(state, case_base, threshold=0.2):\n",
    "        similarities = {}\n",
    "        for case in case_base:\n",
    "            similarities[case] = Case.sim_q(state, case.problem)  # Compare state with the problem part of the case\n",
    "        \n",
    "        sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if sorted_similarities:\n",
    "            most_similar_case = sorted_similarities[0][0] if sorted_similarities[0][1] >= threshold else None\n",
    "        else:\n",
    "            most_similar_case = None\n",
    "        \n",
    "        return most_similar_case\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(c, temporary_case_base):\n",
    "        temporary_case_base.append(c)\n",
    "        # Store the new case from the problem solver\n",
    "        # if c not in temporary_case_base:\n",
    "            # temporary_case_base.append(c)\n",
    "        \n",
    "        # Check if there are similar cases in case_base\n",
    "        # similar_cases = [case for case in case_base if np.array_equal(case.problem, c.problem)]\n",
    "        # for similar_case in similar_cases:\n",
    "            # temporary_case_base.append(similar_case)\n",
    "            # if similar_case not in temporary_case_base:\n",
    "            #     temporary_case_base.append(similar_case)\n",
    "\n",
    "    @staticmethod\n",
    "    def revise(case_base, temporary_case_base, successful_episodes):\n",
    "        for case in temporary_case_base:\n",
    "            if successful_episodes and case in case_base:\n",
    "                case.trust_value += 0.1  # Increment trust value if the episode ended successfully and the case is in the case base\n",
    "            elif not successful_episodes and case in case_base:\n",
    "                case.trust_value -= 0.1  # Decrement trust value if the episode ended unsuccessfully and the case is in the case base\n",
    "            case.trust_value = max(0, min(case.trust_value,1))  # Ensure trust value is within[0,1]\n",
    "\n",
    "    @staticmethod\n",
    "    def retain(case_base, temporary_case_base, successful_episodes, threshold=0):\n",
    "        if successful_episodes:\n",
    "            # Iterate through the temporary case base to find the last occurrence of each unique state\n",
    "            for case in reversed(temporary_case_base):\n",
    "                state = tuple(np.atleast_1d(case.problem))\n",
    "                # Check if the state is already in the case base or has been added previously\n",
    "                if state not in Case.added_states:\n",
    "                    # Add the case to the case base if the state is new\n",
    "                    case_base.append(case)\n",
    "                    Case.added_states.add(state)\n",
    "            \n",
    "            # Filter case_base based on trust_value\n",
    "            filtered_case_base = []\n",
    "            for case in case_base:\n",
    "                # print(f\"trust value >= Threshold?: {case.trust_value} >= {threshold}?\")\n",
    "                if case.trust_value >= threshold:\n",
    "                    # print(f\"problem | trust value: {case.problem} | {case.trust_value}\")\n",
    "                    # print(\"case saved dong\")\n",
    "                    filtered_case_base.append(case)\n",
    "                else:\n",
    "                    # print(f\"problem | trust value: {case.problem} | {case.trust_value}\")\n",
    "                    # print(\"case unsaved dong\")\n",
    "                    pass\n",
    "\n",
    "            return filtered_case_base\n",
    "        else:\n",
    "            return case_base  # Return original case_base if episode is not successful\n",
    "\n",
    "            \n",
    "class QCBRL:\n",
    "    def __init__(self, num_actions, env):\n",
    "        self.num_actions = num_actions\n",
    "        self.env = env\n",
    "        self.problem_solver = ProblemSolver(num_actions, env)\n",
    "        self.case_base = []\n",
    "        self.temporary_case_base = []\n",
    "\n",
    "    def run(self, episodes=100, max_steps=100, alpha=0.1, gamma=0.9, epsilon=0.1, render=False):\n",
    "        rewards = []\n",
    "        memory_usage = []\n",
    "        gpu_memory_usage = []\n",
    "        successful_episodes = False\n",
    "        num_successful_episodes = 0\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            state = np.array([state], dtype=np.float32)  # Convert the state to a NumPy array\n",
    "            episode_reward = 0\n",
    "            states = []\n",
    "            actions = []\n",
    "            episode_rewards = []\n",
    "\n",
    "            # print(\"Reset State Value:\", state)\n",
    "            # print(\"Reset State Shape:\", state.shape)\n",
    "\n",
    "            for _ in range(max_steps):\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                \n",
    "                # print(f\"First State Pass:{state}\")\n",
    "                action = self.take_action(state, epsilon)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                print(f\"State: {state} - Action: {action} - Reward: {reward} - Next State: {next_state} - Done: {done}\")\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                state = np.array([next_state], dtype=np.float32)  # Convert the next state to a NumPy array\n",
    "                episode_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    successful_episodes = reward > 0\n",
    "                    break\n",
    "                \n",
    "            if successful_episodes:\n",
    "                num_successful_episodes += 1\n",
    "\n",
    "            rewards.append(episode_reward)\n",
    "            self.problem_solver.train(states, actions, episode_rewards)\n",
    "        \n",
    "        success_rate = (num_successful_episodes / episodes) * 100\n",
    "        return rewards, success_rate, memory_usage, gpu_memory_usage\n",
    "\n",
    "\n",
    "\n",
    "    def take_action(self, state, epsilon):\n",
    "        # print(f\"Second State Pass:{state}\")\n",
    "        similar_solution = Case.retrieve(state, self.case_base)\n",
    "        if similar_solution is not None:\n",
    "            action = similar_solution.solution\n",
    "            print(\"action from case base\")\n",
    "        else:\n",
    "            action = self.problem_solver.choose_action(state)\n",
    "            print(\"action from problem solver\")\n",
    "        \n",
    "        # action = self.problem_solver.choose_action(state, epsilon)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def save_case_base_temporary(self):\n",
    "        filename = \"case_base_temporary.json\"\n",
    "        case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else int(case.problem), \n",
    "                        \"solution\": int(case.solution), \n",
    "                        \"trust_value\": int(case.trust_value)} for case in self.temporary_case_base]\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "        print(\"Temporary case base saved successfully.\")\n",
    "\n",
    "    def save_case_base(self):\n",
    "        filename = \"case_base.json\"\n",
    "        case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else int(case.problem), \n",
    "                        \"solution\": int(case.solution), \n",
    "                        \"trust_value\": int(case.trust_value)} for case in self.case_base]\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "            print(\"Case base saved successfully.\")  # Add this line to check if the case base is being saved\n",
    "        \n",
    "    def load_case_base(self):\n",
    "        filename = \"case_base.json\"\n",
    "        try:\n",
    "            with open(filename, 'r') as file:\n",
    "                case_base_data = json.load(file)\n",
    "                self.case_base = [Case(np.array(case[\"problem\"]), case[\"solution\"], case[\"trust_value\"]) for case in case_base_data]\n",
    "                print(\"Case base loaded successfully.\")  # Add this line to check if the case base is being loaded\n",
    "        except FileNotFoundError:\n",
    "            print(\"Case base file not found. Starting with an empty case base.\")\n",
    "\n",
    "    \n",
    "    def display_success_rate(self, success_rate):\n",
    "        print(f\"Success rate: {success_rate}%\")\n",
    "\n",
    "\n",
    "    def plot_rewards(self, rewards):\n",
    "        plt.plot(rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Rewards over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show() \n",
    "\n",
    "    def plot_resources(self, memory_usage, gpu_memory_usage):\n",
    "        plt.plot(memory_usage, label='Memory (%)')\n",
    "        plt.plot(gpu_memory_usage, label='GPU Memory (MB)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Resource Usage')\n",
    "        plt.title('Resource Usage over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True)\n",
    "    # env = gym.make('FrozenLake-v1')\n",
    "    env = CustomRewardFrozenLake()\n",
    "    \n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # print(num_actions)\n",
    "    # print(num_states)\n",
    "    agent = QCBRL(num_actions, env)\n",
    "    rewards, success_rate, memory_usage, gpu_memory_usage = agent.run(episodes=1000, max_steps=1000, alpha=0.1, gamma=0.9, epsilon=0.1)\n",
    "\n",
    "    agent.display_success_rate(success_rate)\n",
    "    agent.plot_rewards(rewards)\n",
    "    # agent.plot_resources(memory_usage, gpu_memory_usage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
