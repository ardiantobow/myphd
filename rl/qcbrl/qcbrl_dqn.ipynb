{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x0 and 16x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 299\u001b[0m\n\u001b[1;32m    297\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n\u001b[1;32m    298\u001b[0m agent \u001b[38;5;241m=\u001b[39m QCBRL(output_dim, input_dim)\n\u001b[0;32m--> 299\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 190\u001b[0m, in \u001b[0;36mQCBRL.train\u001b[0;34m(self, episodes, max_steps, render)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[1;32m    189\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m--> 190\u001b[0m action, next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    192\u001b[0m next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(next_state)\n",
      "Cell \u001b[0;32mIn[22], line 234\u001b[0m, in \u001b[0;36mQCBRL.take_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)  \u001b[38;5;66;03m# Convert to tensor\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproblem_solver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m state\n\u001b[1;32m    237\u001b[0m c \u001b[38;5;241m=\u001b[39m Case(state, action)\n",
      "Cell \u001b[0;32mIn[22], line 50\u001b[0m, in \u001b[0;36mProblemSolver.choose_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     49\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m q_values\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 22\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x0 and 16x128)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import pynvml\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, actions, input_size, epsilon=0.1, gamma=0.99, alpha=0.1, lambd=0.9):\n",
    "        self.actions = actions\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.lambd = lambd\n",
    "\n",
    "        self.policy_net = DQN(input_size, actions)\n",
    "        self.target_net = DQN(input_size, actions)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=alpha)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).view(1, -1)\n",
    "                q_values = self.policy_net(state)\n",
    "                return q_values.argmax().item()\n",
    "\n",
    "    def update_Q(self, state, action, reward, next_state, next_action):\n",
    "        state = torch.FloatTensor(state).view(1, -1)\n",
    "        next_state = torch.FloatTensor(next_state).view(1, -1)\n",
    "        action = torch.LongTensor([[action]])\n",
    "        reward = torch.FloatTensor([reward])\n",
    "        done = torch.BoolTensor([False])\n",
    "\n",
    "        # Q-value of the action taken\n",
    "        q_value = self.policy_net(state).gather(1, action)\n",
    "\n",
    "        # Q-value of the best action in the next state according to the policy network\n",
    "        next_q_value = self.target_net(next_state).max(1)[0].detach()\n",
    "        target = reward + self.gamma * next_q_value\n",
    "\n",
    "        # Update Q-value of the action taken\n",
    "        loss = self.loss_fn(q_value, target.unsqueeze(1))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network\n",
    "        self.soft_update_target_network()\n",
    "\n",
    "    def soft_update_target_network(self):\n",
    "        for target_param, policy_param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n",
    "            target_param.data.copy_(self.lambd * target_param.data + (1.0 - self.lambd) * policy_param.data)\n",
    "\n",
    "\n",
    "class Case:\n",
    "    added_states = set()  # Class attribute to store states already added to the case base\n",
    "\n",
    "    def __init__(self, problem, solution, trust_value=1):\n",
    "        self.problem = np.array(problem)  # Convert problem to numpy array\n",
    "        self.solution = solution\n",
    "        self.trust_value = trust_value\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(state, case_base, threshold=0.5):\n",
    "        similarities = {}\n",
    "        for case in case_base:\n",
    "            similarities[case] = Case.sim_q(state, case.problem)  # Compare state with the problem part of the case\n",
    "\n",
    "        sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if sorted_similarities:\n",
    "            most_similar_case = sorted_similarities[0][0] if sorted_similarities[0][1] >= threshold else None\n",
    "        else:\n",
    "            most_similar_case = None\n",
    "\n",
    "        return most_similar_case\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(c, case_base, temporary_case_base):\n",
    "        # Store the new case from the problem solver\n",
    "        if c not in temporary_case_base:\n",
    "            temporary_case_base.append(c)\n",
    "\n",
    "        # Check if there are similar cases in case_base\n",
    "        similar_cases = [case for case in case_base if np.array_equal(case.problem, c.problem)]\n",
    "        for similar_case in similar_cases:\n",
    "            if similar_case not in temporary_case_base:\n",
    "                temporary_case_base.append(similar_case)\n",
    "\n",
    "    @staticmethod\n",
    "    def revise(case_base, temporary_case_base, episode_ended_successfully):\n",
    "        for case in temporary_case_base:\n",
    "            if episode_ended_successfully and case in case_base:\n",
    "                case.trust_value += 0.1  # Increment trust value if the episode ended successfully and the case is in the case base\n",
    "            elif not episode_ended_successfully and case in case_base:\n",
    "                case.trust_value -= 0.1  # Decrement trust value if the episode ended unsuccessfully and the case is in the case base\n",
    "            case.trust_value = max(0, min(case.trust_value, 1))  # Ensure trust value is within[0,1]\n",
    "\n",
    "    @staticmethod\n",
    "    def retain(case_base, temporary_case_base, episode_ended_successfully, threshold):\n",
    "        if episode_ended_successfully:\n",
    "            # Iterate through the temporary case base to find the last occurrence of each unique state\n",
    "            for case in reversed(temporary_case_base):\n",
    "                state = tuple(np.atleast_1d(case.problem))\n",
    "                # Check if the state is already in the case base or has been added previously\n",
    "                if state not in Case.added_states:\n",
    "                    # Add the case to the case base if the state is new\n",
    "                    case_base.append(case)\n",
    "                    Case.added_states.add(state)\n",
    "\n",
    "            # Filter case_base based on trust_value\n",
    "            filtered_case_base = []\n",
    "            for case in case_base:\n",
    "                if case.trust_value >= threshold:\n",
    "                    filtered_case_base.append(case)\n",
    "\n",
    "            return filtered_case_base\n",
    "        else:\n",
    "            return case_base  # Return original case_base if episode is not successful\n",
    "\n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)  # Ensure state1 is at least 1-dimensional\n",
    "        state2 = np.atleast_1d(state2)  # Ensure state2 is at least 1-dimensional\n",
    "        CNDMaxDist = 6  # Maximum distance between two nodes in the CND\n",
    "        v = state1.size  # Total number of objects the agent can perceive\n",
    "        DistQ = np.sum([Case.Dmin_phi(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def Dmin_phi(X1, X2):\n",
    "        return np.max(np.abs(X1 - X2))\n",
    "\n",
    "\n",
    "class QCBRL:\n",
    "    def __init__(self, actions, input_size, threshold=0.2, epsilon=0.1, gamma=0.99, alpha=0.1, lambd=0.9):\n",
    "        self.problem_solver = ProblemSolver(actions, input_size, epsilon, gamma, alpha, lambd)\n",
    "        self.case_base = []\n",
    "        self.threshold = threshold\n",
    "        self.temporary_case_base = []\n",
    "        # self.load_case_base()  # Load case base at initialization\n",
    "\n",
    "    def train(self, episodes, max_steps, render=False):\n",
    "        env = gym.make('FrozenLake-v1')\n",
    "        rewards = []\n",
    "        episode_rewards = []\n",
    "        memory_usage = []\n",
    "        gpu_memory_usage = []\n",
    "        successful_episodes = 0  # Initialize counter for successful episodes\n",
    "\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # GPU Index\n",
    "\n",
    "        for episode in range(1, episodes + 1):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            self.temporary_case_base = []\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                if render:\n",
    "                    env.render()\n",
    "                action, next_state = self.take_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = np.array(next_state)\n",
    "                total_reward += reward\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    rewards.append(total_reward)\n",
    "                    episode_rewards.append(total_reward)\n",
    "                    if reward == 1.0:\n",
    "                        episode_ended_successfully = True\n",
    "                        successful_episodes += 1  # Increment successful episode counter\n",
    "                    else:\n",
    "                        episode_ended_successfully = False\n",
    "\n",
    "                    print(f\"Episode {episode} ended after {step + 1} steps with total reward: {total_reward}\")\n",
    "                    break\n",
    "\n",
    "            Case.revise(self.case_base, self.temporary_case_base, episode_ended_successfully)\n",
    "            self.case_base = Case.retain(self.case_base, self.temporary_case_base, episode_ended_successfully,\n",
    "                                          self.threshold)\n",
    "\n",
    "            memory_usage.append(psutil.virtual_memory().percent)\n",
    "            gpu_memory_usage.append(pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1024 ** 2)\n",
    "\n",
    "        self.save_case_base_temporary()\n",
    "        self.save_case_base()\n",
    "\n",
    "        env.close()\n",
    "        self.plot_rewards(episode_rewards)\n",
    "        self.plot_resources(memory_usage, gpu_memory_usage)\n",
    "\n",
    "        # Calculate and display percentage of successful episodes\n",
    "        success_percentage = (successful_episodes / episodes) * 100\n",
    "        print(f\"Percentage of Successful Episodes: {success_percentage}%\")\n",
    "\n",
    "    def take_action(self, state):\n",
    "        similar_solution = Case.retrieve(state, self.case_base)\n",
    "        if similar_solution is not None:\n",
    "            action = similar_solution.solution\n",
    "            next_state = state  # Assuming the next state remains the same\n",
    "        else:\n",
    "            state_tensor = torch.FloatTensor(state)  # Convert to tensor\n",
    "            action = self.problem_solver.choose_action(state_tensor)\n",
    "            next_state = state\n",
    "        \n",
    "        c = Case(state, action)\n",
    "        Case.reuse(c, self.case_base, self.temporary_case_base)\n",
    "\n",
    "        return action, next_state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save_case_base_temporary(self):\n",
    "        filename = \"case_base_temporary.json\"\n",
    "        case_base_data = [{\"problem\": case.problem.tolist(), \"solution\": case.solution, \"trust_value\": case.trust_value}\n",
    "                          for case in self.temporary_case_base]\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "    def save_case_base(self):\n",
    "        filename = \"case_base.json\"\n",
    "        case_base_data = [{\"problem\": case.problem.tolist(), \"solution\": case.solution, \"trust_value\": case.trust_value}\n",
    "                          for case in self.case_base]\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "        print(\"Case base saved successfully.\")  # Add this line to check if the case base is being saved\n",
    "\n",
    "    def load_case_base(self):\n",
    "        filename = \"case_base.json\"\n",
    "        try:\n",
    "            with open(filename, 'r') as file:\n",
    "                case_base_data = json.load(file)\n",
    "                self.case_base = [Case(np.array(case[\"problem\"]), case[\"solution\"], case[\"trust_value\"]) for case in\n",
    "                                  case_base_data]\n",
    "                print(\"Case base loaded successfully.\")  # Add this line to check if the case base is being loaded\n",
    "        except FileNotFoundError:\n",
    "            print(\"Case base file not found. Starting with an empty case base.\")\n",
    "\n",
    "    def plot_rewards(self, rewards):\n",
    "        plt.plot(rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Rewards over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_resources(self, memory_usage, gpu_memory_usage):\n",
    "        plt.plot(memory_usage, label='Memory (%)')\n",
    "        plt.plot(gpu_memory_usage, label='GPU Memory (MB)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Resource Usage')\n",
    "        plt.title('Resource Usage over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('FrozenLake-v1')\n",
    "    if isinstance(env.observation_space, gym.spaces.Box):\n",
    "        input_dim = env.observation_space.shape[0]\n",
    "    else:\n",
    "        input_dim = env.observation_space.n\n",
    "    output_dim = env.action_space.n\n",
    "    agent = QCBRL(output_dim, input_dim)\n",
    "    agent.train(episodes=10000, max_steps=100000)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
