{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardie85/PHD/Research/code/.venv/lib/python3.10/site-packages/gym/envs/registration.py:595: UserWarning: \u001b[33mWARN: Overriding environment CustomRewardFrozenLake-v1\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {id}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: -5\n",
      "Episode 2, Total Reward: -5\n",
      "Episode 3, Total Reward: -5\n",
      "Episode 4, Total Reward: -5\n",
      "Episode 5, Total Reward: -5\n",
      "Episode 6, Total Reward: -5\n",
      "Episode 7, Total Reward: -5\n",
      "Episode 8, Total Reward: -5\n",
      "Episode 9, Total Reward: -5\n",
      "Episode 10, Total Reward: -5\n",
      "Episode 11, Total Reward: -5\n",
      "Episode 12, Total Reward: -5\n",
      "Episode 13, Total Reward: 1\n",
      "Episode 14, Total Reward: 1\n",
      "Episode 15, Total Reward: -5\n",
      "Episode 16, Total Reward: 1\n",
      "Episode 17, Total Reward: -5\n",
      "Episode 18, Total Reward: -5\n",
      "Episode 19, Total Reward: -5\n",
      "Episode 20, Total Reward: -5\n",
      "Episode 21, Total Reward: 1\n",
      "Episode 22, Total Reward: 1\n",
      "Episode 23, Total Reward: 1\n",
      "Episode 24, Total Reward: 1\n",
      "Episode 25, Total Reward: 1\n",
      "Episode 26, Total Reward: 1\n",
      "Episode 27, Total Reward: 1\n",
      "Episode 28, Total Reward: 1\n",
      "Episode 29, Total Reward: 1\n",
      "Episode 30, Total Reward: -5\n",
      "Episode 31, Total Reward: 1\n",
      "Episode 32, Total Reward: 1\n",
      "Episode 33, Total Reward: 1\n",
      "Episode 34, Total Reward: 1\n",
      "Episode 35, Total Reward: -5\n",
      "Episode 36, Total Reward: 1\n",
      "Episode 37, Total Reward: 1\n",
      "Episode 38, Total Reward: 1\n",
      "Episode 39, Total Reward: 1\n",
      "Episode 40, Total Reward: 1\n",
      "Episode 41, Total Reward: 1\n",
      "Episode 42, Total Reward: 1\n",
      "Episode 43, Total Reward: 1\n",
      "Episode 44, Total Reward: 1\n",
      "Episode 45, Total Reward: 1\n",
      "Episode 46, Total Reward: 1\n",
      "Episode 47, Total Reward: 1\n",
      "Episode 48, Total Reward: 1\n",
      "Episode 49, Total Reward: 1\n",
      "Episode 50, Total Reward: -5\n",
      "Episode 51, Total Reward: -5\n",
      "Episode 52, Total Reward: 1\n",
      "Episode 53, Total Reward: 1\n",
      "Episode 54, Total Reward: 1\n",
      "Episode 55, Total Reward: 1\n",
      "Episode 56, Total Reward: 1\n",
      "Episode 57, Total Reward: -5\n",
      "Episode 58, Total Reward: 1\n",
      "Episode 59, Total Reward: 1\n",
      "Episode 60, Total Reward: -5\n",
      "Episode 61, Total Reward: 1\n",
      "Episode 62, Total Reward: 1\n",
      "Episode 63, Total Reward: 1\n",
      "Episode 64, Total Reward: 1\n",
      "Episode 65, Total Reward: 1\n",
      "Episode 66, Total Reward: 1\n",
      "Episode 67, Total Reward: 1\n",
      "Episode 68, Total Reward: 1\n",
      "Episode 69, Total Reward: 1\n",
      "Episode 70, Total Reward: 1\n",
      "Episode 71, Total Reward: -5\n",
      "Episode 72, Total Reward: 1\n",
      "Episode 73, Total Reward: 1\n",
      "Episode 74, Total Reward: 1\n",
      "Episode 75, Total Reward: 1\n",
      "Episode 76, Total Reward: 1\n",
      "Episode 77, Total Reward: 1\n",
      "Episode 78, Total Reward: -5\n",
      "Episode 79, Total Reward: 1\n",
      "Episode 80, Total Reward: -5\n",
      "Episode 81, Total Reward: -5\n",
      "Episode 82, Total Reward: 1\n",
      "Episode 83, Total Reward: 1\n",
      "Episode 84, Total Reward: 1\n",
      "Episode 85, Total Reward: 1\n",
      "Episode 86, Total Reward: 1\n",
      "Episode 87, Total Reward: 1\n",
      "Episode 88, Total Reward: 1\n",
      "Episode 89, Total Reward: 1\n",
      "Episode 90, Total Reward: 1\n",
      "Episode 91, Total Reward: -5\n",
      "Episode 92, Total Reward: 1\n",
      "Episode 93, Total Reward: 1\n",
      "Episode 94, Total Reward: 1\n",
      "Episode 95, Total Reward: 1\n",
      "Episode 96, Total Reward: -5\n",
      "Episode 97, Total Reward: 1\n",
      "Episode 98, Total Reward: 1\n",
      "Episode 99, Total Reward: 1\n",
      "Episode 100, Total Reward: 1\n",
      "Success rate: 71.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import pynvml\n",
    "from collections import Counter\n",
    "from gym.envs.registration import register\n",
    "\n",
    "# Register the wrapper with a new environment ID\n",
    "register(\n",
    "    id='CustomRewardFrozenLake-v1',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=1,  # Adjust the reward threshold if needed\n",
    ")\n",
    "\n",
    "class CustomRewardFrozenLake(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"CustomRewardFrozenLake-v1\")\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        if reward == 0 and not done:\n",
    "            reward = 0\n",
    "        elif reward == 0 and done:\n",
    "            reward = -5\n",
    "        elif reward == 1:\n",
    "            reward = 1\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, num_actions, env):\n",
    "        self.num_actions = num_actions\n",
    "        self.Q_values = np.zeros((env.observation_space.n, num_actions))\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            return np.random.choice(self.num_actions)  # Random action\n",
    "        else:\n",
    "            return np.argmax(self.Q_values[state])  # Greedy action\n",
    "\n",
    "    def update_Q(self, state, action, reward, next_state, next_action, alpha, gamma):\n",
    "        td_target = reward + gamma * self.Q_values[next_state, next_action]\n",
    "        td_error = td_target - self.Q_values[state, action]\n",
    "        self.Q_values[state, action] += alpha * td_error\n",
    "\n",
    "\n",
    "class Case:\n",
    "    added_states = set()  # Class attribute to store states already added to the case base\n",
    "\n",
    "    def __init__(self, problem, solution, trust_value=1):\n",
    "        self.problem = np.array(problem)  # Convert problem to numpy array\n",
    "        self.solution = solution\n",
    "        self.trust_value = trust_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)  # Ensure state1 is at least 1-dimensional\n",
    "        state2 = np.atleast_1d(state2)  # Ensure state2 is at least 1-dimensional\n",
    "        CNDMaxDist = 6  # Maximum distance between two nodes in the CND\n",
    "        v = state1.size  # Total number of objects the agent can perceive\n",
    "        DistQ = np.sum([Case.Dmin_phi(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def Dmin_phi(X1, X2):\n",
    "        return np.max(np.abs(X1 - X2))\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(state, case_base, threshold=0.5):\n",
    "        similarities = {}\n",
    "        for case in case_base:\n",
    "            similarities[case] = Case.sim_q(state, case.problem)  # Compare state with the problem part of the case\n",
    "        \n",
    "        sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if sorted_similarities:\n",
    "            most_similar_case = sorted_similarities[0][0] if sorted_similarities[0][1] >= threshold else None\n",
    "        else:\n",
    "            most_similar_case = None\n",
    "        \n",
    "        return most_similar_case\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(c, case_base, temporary_case_base):\n",
    "        # Store the new case from the problem solver\n",
    "        if c not in temporary_case_base:\n",
    "            temporary_case_base.append(c)\n",
    "        \n",
    "        # Check if there are similar cases in case_base\n",
    "        similar_cases = [case for case in case_base if np.array_equal(case.problem, c.problem)]\n",
    "        for similar_case in similar_cases:\n",
    "            temporary_case_base.append(similar_case)\n",
    "            # if similar_case not in temporary_case_base:\n",
    "            #     temporary_case_base.append(similar_case)\n",
    "\n",
    "    @staticmethod\n",
    "    def revise(case_base, temporary_case_base, episode_ended_successfully):\n",
    "        for case in temporary_case_base:\n",
    "            if episode_ended_successfully and case in case_base:\n",
    "                case.trust_value += 0.1  # Increment trust value if the episode ended successfully and the case is in the case base\n",
    "            elif not episode_ended_successfully and case in case_base:\n",
    "                case.trust_value -= 0.1  # Decrement trust value if the episode ended unsuccessfully and the case is in the case base\n",
    "            case.trust_value = max(0, min(case.trust_value,1))  # Ensure trust value is within[0,1]\n",
    "\n",
    "    @staticmethod\n",
    "    def retain(case_base, temporary_case_base, episode_ended_successfully, threshold=0.2):\n",
    "        if episode_ended_successfully:\n",
    "            # Iterate through the temporary case base to find the last occurrence of each unique state\n",
    "            for case in reversed(temporary_case_base):\n",
    "                state = tuple(np.atleast_1d(case.problem))\n",
    "                # Check if the state is already in the case base or has been added previously\n",
    "                if state not in Case.added_states:\n",
    "                    # Add the case to the case base if the state is new\n",
    "                    case_base.append(case)\n",
    "                    Case.added_states.add(state)\n",
    "            \n",
    "            # Filter case_base based on trust_value\n",
    "            filtered_case_base = []\n",
    "            for case in case_base:\n",
    "                # print(f\"trust value >= Threshold?: {case.trust_value} >= {threshold}?\")\n",
    "                if case.trust_value >= threshold:\n",
    "                    # print(f\"problem | trust value: {case.problem} | {case.trust_value}\")\n",
    "                    # print(\"case saved dong\")\n",
    "                    filtered_case_base.append(case)\n",
    "                else:\n",
    "                    # print(f\"problem | trust value: {case.problem} | {case.trust_value}\")\n",
    "                    # print(\"case unsaved dong\")\n",
    "                    pass\n",
    "\n",
    "            return filtered_case_base\n",
    "        else:\n",
    "            return case_base  # Return original case_base if episode is not successful\n",
    "\n",
    "            \n",
    "class QCBRL:\n",
    "    def __init__(self, num_actions, env):\n",
    "        self.num_actions = num_actions\n",
    "        self.env = env\n",
    "        self.problem_solver = ProblemSolver(num_actions, env)\n",
    "        self.case_base = []\n",
    "        self.temporary_case_base = []\n",
    "\n",
    "    def run(self, episodes=100, max_steps=100, alpha=0.1, gamma=0.9, epsilon=0.1, render=False):\n",
    "        rewards = []\n",
    "        episode_rewards = []\n",
    "        memory_usage = []\n",
    "        gpu_memory_usage = []\n",
    "        successful_episodes = 0\n",
    "\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0\n",
    "            self.temporary_case_base = []\n",
    "            episode_ended_successfully = False\n",
    "\n",
    "            for _ in range(max_steps):\n",
    "                if render:\n",
    "                    env.render()\n",
    "                action = self.take_action(state, epsilon)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                c = Case(state, action)\n",
    "                Case.reuse(c, self.case_base, self.temporary_case_base)\n",
    "\n",
    "                next_action = self.take_action(next_state, epsilon)\n",
    "                self.problem_solver.update_Q(state, action, reward, next_state, next_action, alpha, gamma)\n",
    "\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    if reward > 0:  # If the agent reached the goal state\n",
    "                        successful_episodes += 1\n",
    "                    break\n",
    "\n",
    "            rewards.append(total_reward)\n",
    "            print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "            Case.revise(self.case_base, self.temporary_case_base, episode_ended_successfully)\n",
    "            self.case_base = Case.retain(self.case_base, self.temporary_case_base, episode_ended_successfully)\n",
    "            \n",
    "            memory_usage.append(psutil.virtual_memory().percent)\n",
    "            gpu_memory_usage.append(pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1024**2)\n",
    "\n",
    "        success_rate = (successful_episodes / episodes) * 100\n",
    "        print(f\"Success rate: {success_rate}%\")\n",
    "\n",
    "        return rewards\n",
    "\n",
    "    def take_action(self, state, epsilon):\n",
    "\n",
    "        similar_solution = Case.retrieve(state, self.case_base)\n",
    "        if similar_solution is not None:\n",
    "            action = similar_solution.solution\n",
    "            # print(\"action from case base\")\n",
    "        else:\n",
    "            action = self.problem_solver.choose_action(state, epsilon)\n",
    "            # print(\"action from problem solver\")\n",
    "        \n",
    "        return action\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = CustomRewardFrozenLake()\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    agent = QCBRL(num_actions, env)\n",
    "    agent.run(episodes=100, max_steps=100, alpha=0.1, gamma=0.9, epsilon=0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
