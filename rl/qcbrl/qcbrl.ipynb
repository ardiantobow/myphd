{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardie85/PHD/Research/code/.venv/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 ended after 7 steps with total reward: 0.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'QCBRL' object has no attribute 'revise'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 189\u001b[0m\n\u001b[1;32m    187\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m  \u001b[38;5;66;03m# Number of actions in FrozenLake-v1\u001b[39;00m\n\u001b[1;32m    188\u001b[0m agent \u001b[38;5;241m=\u001b[39m QCBRL(actions)\n\u001b[0;32m--> 189\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 136\u001b[0m, in \u001b[0;36mQCBRL.train\u001b[0;34m(self, episodes, max_steps, render)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ended after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m steps with total reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrevise\u001b[49m(episode_ended_successfully\u001b[38;5;241m=\u001b[39m(reward \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m), temporary_case_base\u001b[38;5;241m=\u001b[39mtemporary_case_base)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Record memory usage after each episode\u001b[39;00m\n\u001b[1;32m    139\u001b[0m memory_usage\u001b[38;5;241m.\u001b[39mappend(psutil\u001b[38;5;241m.\u001b[39mvirtual_memory()\u001b[38;5;241m.\u001b[39mpercent)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'QCBRL' object has no attribute 'revise'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import pynvml\n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, actions, epsilon=0.1, gamma=0.99, alpha=0.1, lambd=0.9):\n",
    "        self.actions = actions\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.lambd = lambd\n",
    "        self.Q = {}  # Q-values table\n",
    "        self.e = {}  # Eligibility traces table\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.isscalar(state):  # Check if state is a scalar (integer or float)\n",
    "            state_array = np.array([state])  # Convert scalar to numpy array\n",
    "        else:\n",
    "            state_array = np.asarray(state)  # Ensure state is a numpy array\n",
    "\n",
    "        state_tuple = (state_array.item(),) if state_array.ndim == 0 else tuple(state_array.tolist())\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            if state_tuple in self.Q:  # Use the tuple as the key\n",
    "                return np.argmax(self.Q[state_tuple])\n",
    "            else:\n",
    "                return np.random.choice(self.actions)\n",
    "\n",
    "\n",
    "    def update_Q(self, state, action, reward, next_state, next_action):\n",
    "        state_array = np.asarray(state)  # Ensure state is a numpy array\n",
    "        state_tuple = tuple(state_array.tolist())  # Convert numpy array to tuple\n",
    "\n",
    "        if state_tuple not in self.Q:\n",
    "            self.Q[state_tuple] = np.zeros(self.actions)\n",
    "            self.e[state_tuple] = np.zeros(self.actions)\n",
    "\n",
    "        delta = reward + self.gamma * self.Q.get(tuple(np.array(next_state).tolist()), np.zeros(self.actions))[next_action] - self.Q[state_tuple][action]\n",
    "        self.e[state_tuple][action] += 1\n",
    "\n",
    "        for s in self.Q:\n",
    "            for a in range(self.actions):\n",
    "                self.Q[s][a] += self.alpha * delta * self.e[s][a]\n",
    "                self.e[s][a] *= self.gamma * self.lambd\n",
    "\n",
    "class Case:\n",
    "    def __init__(self, problem, solution, trust_value=1):\n",
    "        self.problem = np.array(problem)  # Convert problem to numpy array\n",
    "        self.solution = solution\n",
    "        self.trust_value = trust_value\n",
    "\n",
    "\n",
    "\n",
    "# Inside the retrieve function\n",
    "def retrieve(state, case_base, threshold=0.5):\n",
    "    similarities = {}\n",
    "    for case in case_base:\n",
    "        similarities[case] = sim_q(state, case.problem)  # Compare state with the problem part of the case\n",
    "    \n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    if sorted_similarities:\n",
    "        most_similar_case = sorted_similarities[0][0] if sorted_similarities[0][1] >= threshold else None\n",
    "    else:\n",
    "        most_similar_case = None\n",
    "    \n",
    "    return most_similar_case\n",
    "\n",
    "\n",
    "def reuse(c, case_base):\n",
    "    case_base.append(c)\n",
    "\n",
    "def revise(case_base, episode_ended_successfully):\n",
    "    if episode_ended_successfully:\n",
    "        for case in case_base:\n",
    "            case.trust_value += 0.1  # Increment trust value if episode ended successfully\n",
    "    else:\n",
    "        for case in case_base:\n",
    "            case.trust_value -= 0.1  # Decrement trust value if episode did not end successfully\n",
    "            case.trust_value = max(0, case.trust_value)  # Ensure trust value doesn't go below 0\n",
    "\n",
    "def retain(case_base, episode_ended_successfully, threshold=0):\n",
    "    if episode_ended_successfully:\n",
    "        for case in case_base:\n",
    "            case_base.append(case)\n",
    "\n",
    "    case_base[:] = [case for case in case_base if case.trust_value >= threshold]\n",
    "\n",
    "\n",
    "def sim_q(state1, state2):\n",
    "    # Example implementation of qualitative similarity function\n",
    "    state1 = np.atleast_1d(state1)  # Ensure state1 is at least 1-dimensional\n",
    "    state2 = np.atleast_1d(state2)  # Ensure state2 is at least 1-dimensional\n",
    "    CNDMaxDist = 6  # Maximum distance between two nodes in the CND\n",
    "    v = state1.size  # Total number of objects the agent can perceive\n",
    "    DistQ = np.sum([Dmin_phi(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "    similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "    return similarity\n",
    "\n",
    "def Dmin_phi(X1, X2):\n",
    "    # Example implementation of minimum CND distance function\n",
    "    return np.min(np.abs(X1 - X2))\n",
    "\n",
    "class QCBRL:\n",
    "    def __init__(self, actions, threshold=0.5, epsilon=0.1, gamma=0.99, alpha=0.1, lambd=0.9):\n",
    "        self.problem_solver = ProblemSolver(actions, epsilon, gamma, alpha, lambd)\n",
    "        self.case_base = []\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def train(self, episodes, max_steps, render=True):\n",
    "        env = gym.make('FrozenLake-v1')\n",
    "        rewards = []\n",
    "        episode_rewards = []\n",
    "        memory_usage = []\n",
    "        gpu_memory_usage = []\n",
    "\n",
    "        # Initialize pynvml for GPU monitoring\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # GPU Index\n",
    "\n",
    "        for episode in range(1, episodes + 1):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            for step in range(max_steps):\n",
    "                if render:\n",
    "                    env.render()\n",
    "                action, next_state = self.take_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                # print(f\"reward {reward}\")\n",
    "                next_state = np.array(next_state)\n",
    "                total_reward += reward\n",
    "                c = Case(state, action)\n",
    "                self.reuse(c)\n",
    "                episode_ended_successfully = done and reward == 1.0\n",
    "                self.revise(episode_ended_successfully)\n",
    "                self.retain(episode_ended_successfully)\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    rewards.append(total_reward)\n",
    "                    episode_rewards.append(total_reward)\n",
    "                    print(f\"Episode {episode} ended after {step + 1} steps with total reward: {total_reward}\")\n",
    "                    break\n",
    "\n",
    "            # Record memory usage after each episode\n",
    "            memory_usage.append(psutil.virtual_memory().percent)\n",
    "            gpu_memory = pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1024**2  # Convert to MB\n",
    "            gpu_memory_usage.append(gpu_memory)\n",
    "            print(f\"Memory usage: {memory_usage[-1]}, GPU memory usage: {gpu_memory}\")\n",
    "\n",
    "        env.close()\n",
    "        print(\"Rewards:\", rewards)\n",
    "        print(\"Memory usage:\", memory_usage)\n",
    "        print(\"GPU memory usage:\", gpu_memory_usage)\n",
    "        self.plot_rewards(episode_rewards)\n",
    "        self.plot_resources(memory_usage, gpu_memory_usage)\n",
    "\n",
    "\n",
    "    def take_action(self, state):\n",
    "        if np.isscalar(state):  # Check if state is a scalar (integer or float)\n",
    "            state_array = np.array([state])  # Convert scalar to numpy array\n",
    "        else:\n",
    "            state_array = np.asarray(state)  # Ensure state is a numpy array\n",
    "\n",
    "        similar_solution = retrieve(state_array, self.case_base)  # Find similar case in the case base\n",
    "        if similar_solution is not None:\n",
    "            action = similar_solution.solution\n",
    "            next_state = state  # Since we don't have a next state in the case structure\n",
    "        else:\n",
    "            action = self.problem_solver.choose_action(state_array)\n",
    "            next_state = state_array\n",
    "\n",
    "        # Ensure next_state is always a NumPy array\n",
    "        if not isinstance(next_state, np.ndarray):\n",
    "            next_state = np.array(next_state)\n",
    "\n",
    "        return action, next_state\n",
    "\n",
    "    def reuse(self, c):\n",
    "        reuse(c, self.case_base)\n",
    "\n",
    "    def revise(self, episode_ended_successfully):\n",
    "        revise(self.case_base, episode_ended_successfully)\n",
    "\n",
    "    def retain(self, episode_ended_successfully):\n",
    "        retain(self.case_base, episode_ended_successfully, self.threshold)\n",
    "\n",
    "    def save_case_base(self, filename):\n",
    "        case_base_data = [(case.problem.tolist(), case.solution, case.trust_value) for case in self.case_base]\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "    def load_case_base(self, filename):\n",
    "        with open(filename, 'r') as file:\n",
    "            case_base_data = json.load(file)\n",
    "            self.case_base = [Case(np.array(case[0]), case[1], case[2]) for case in case_base_data]\n",
    "\n",
    "    def plot_rewards(self, rewards):\n",
    "        plt.plot(rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Rewards over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show() \n",
    "\n",
    "\n",
    "    def plot_resources(self, memory_usage, gpu_memory_usage):\n",
    "        plt.plot(memory_usage, label='Memory (%)')\n",
    "        plt.plot(gpu_memory_usage, label='GPU Memory (MB)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Resource Usage')\n",
    "        plt.title('Resource Usage over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    actions = 4  # Number of actions in FrozenLake-v1\n",
    "    agent = QCBRL(actions)\n",
    "    agent.train(episodes=100, max_steps=100)\n",
    "    agent.save_case_base(\"case_base.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
