{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_states: Discrete(16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ardie85/PHD/Research/code/.venv/lib/python3.10/site-packages/gym/envs/registration.py:595: UserWarning: \u001b[33mWARN: Overriding environment CustomRewardFrozenLake-v1\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {id}\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x0 and 1x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 319\u001b[0m\n\u001b[1;32m    316\u001b[0m num_actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n\u001b[1;32m    318\u001b[0m agent \u001b[38;5;241m=\u001b[39m QCBRL(num_states, num_actions, env)\n\u001b[0;32m--> 319\u001b[0m rewards, success_rate, memory_usage, gpu_memory_usage \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m agent\u001b[38;5;241m.\u001b[39mdisplay_success_rate(success_rate)\n\u001b[1;32m    322\u001b[0m agent\u001b[38;5;241m.\u001b[39mplot_rewards(rewards)\n",
      "Cell \u001b[0;32mIn[9], line 249\u001b[0m, in \u001b[0;36mQCBRL.run\u001b[0;34m(self, episodes, max_steps, alpha, gamma, epsilon, render)\u001b[0m\n\u001b[1;32m    246\u001b[0m         num_successful_episodes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    248\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(episode_reward)\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproblem_solver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards_episode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m success_rate \u001b[38;5;241m=\u001b[39m (num_successful_episodes \u001b[38;5;241m/\u001b[39m episodes) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rewards, success_rate, memory_usage, gpu_memory_usage\n",
      "Cell \u001b[0;32mIn[9], line 103\u001b[0m, in \u001b[0;36mProblemSolver.update_policy\u001b[0;34m(self, states, actions, rewards, next_states, dones, gamma)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m         next_state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(next_state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m         next_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_critic_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m         returns\u001b[38;5;241m.\u001b[39mappend(reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m next_value)\n\u001b[1;32m    106\u001b[0m values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(values)\n",
      "Cell \u001b[0;32mIn[9], line 66\u001b[0m, in \u001b[0;36mActorCriticNetwork.forward_critic\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_critic\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 66\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     67\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2_critic(x)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x0 and 1x128)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import pynvml\n",
    "from collections import Counter\n",
    "from gym.envs.registration import register\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "# Register the wrapper with a new environment ID\n",
    "register(\n",
    "    id='CustomRewardFrozenLake-v1',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '4x4', 'is_slippery': True},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=1,  # Adjust the reward threshold if needed\n",
    ")\n",
    "\n",
    "class CustomRewardFrozenLake(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"CustomRewardFrozenLake-v1\")\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        if reward == 0 and not done:\n",
    "            reward = 0\n",
    "        elif reward == 0 and done:\n",
    "            reward = -5\n",
    "        elif reward == 1:\n",
    "            reward = 1\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ActorCriticNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        self.fc1_actor = nn.Linear(input_size, 128)  \n",
    "        self.fc2_actor = nn.Linear(128, output_size)\n",
    "        \n",
    "        self.fc1_critic = nn.Linear(input_size, 128)  \n",
    "        self.fc2_critic = nn.Linear(128, 1)\n",
    "\n",
    "    def forward_actor(self, x):\n",
    "        x = F.relu(self.fc1_actor(x))\n",
    "        x = self.fc2_actor(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def forward_critic(self, x):\n",
    "        x = F.relu(self.fc1_critic(x))\n",
    "        x = self.fc2_critic(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, num_states, num_actions, env):\n",
    "        self.num_actions = num_actions\n",
    "        self.num_states = num_states\n",
    "        input_size = 1  # Input size for the state\n",
    "        self.actor_critic_network = ActorCriticNetwork(input_size, num_actions)\n",
    "        self.optimizer_actor = optim.Adam(self.actor_critic_network.parameters(), lr=0.01)\n",
    "        self.optimizer_critic = optim.Adam(self.actor_critic_network.parameters(), lr=0.01)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  \n",
    "        action_probs = self.actor_critic_network.forward_actor(state_tensor)\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "        return action\n",
    "\n",
    "    def update_policy(self, states, actions, rewards, next_states, dones, gamma):\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        advantages = []\n",
    "        returns = []\n",
    "\n",
    "        for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action_probs = self.actor_critic_network.forward_actor(state_tensor)\n",
    "            value = self.actor_critic_network.forward_critic(state_tensor)\n",
    "            values.append(value)\n",
    "            log_probs.append(torch.log(action_probs.squeeze(0)[action]))\n",
    "            \n",
    "            if done:\n",
    "                returns.append(reward)\n",
    "            else:\n",
    "                next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "                next_value = self.actor_critic_network.forward_critic(next_state_tensor)\n",
    "                returns.append(reward + gamma * next_value)\n",
    "\n",
    "        values = torch.cat(values)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        advantages = returns - values\n",
    "\n",
    "        actor_loss = []\n",
    "        critic_loss = []\n",
    "\n",
    "        for log_prob, advantage, value, ret in zip(log_probs, advantages, values, returns):\n",
    "            actor_loss.append(-log_prob * advantage)\n",
    "            critic_loss.append(F.smooth_l1_loss(value, ret))\n",
    "        \n",
    "        self.optimizer_actor.zero_grad()\n",
    "        actor_loss = torch.stack(actor_loss).sum()\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        self.optimizer_actor.step()\n",
    "\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        critic_loss = torch.stack(critic_loss).sum()\n",
    "        critic_loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "\n",
    "\n",
    "class Case:\n",
    "    added_states = set()  \n",
    "\n",
    "    def __init__(self, problem, solution, trust_value=1):\n",
    "        self.problem = np.array(problem)  \n",
    "        self.solution = solution\n",
    "        self.trust_value = trust_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)  \n",
    "        state2 = np.atleast_1d(state2)  \n",
    "        CNDMaxDist = 6  \n",
    "        v = state1.size  \n",
    "        DistQ = np.sum([Case.Dmin_phi(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def Dmin_phi(X1, X2):\n",
    "        return np.max(np.abs(X1 - X2))\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(state, case_base, threshold=0.2):\n",
    "        similarities = {}\n",
    "        for case in case_base:\n",
    "            similarities[case] = Case.sim_q(state, case.problem)  \n",
    "        \n",
    "        sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if sorted_similarities:\n",
    "            most_similar_case = sorted_similarities[0][0] if sorted_similarities[0][1] >= threshold else None\n",
    "        else:\n",
    "            most_similar_case = None\n",
    "        \n",
    "        return most_similar_case\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(c, temporary_case_base):\n",
    "        temporary_case_base.append(c)\n",
    "\n",
    "    @staticmethod\n",
    "    def revise(case_base, temporary_case_base, successful_episodes):\n",
    "        for case in temporary_case_base:\n",
    "            if successful_episodes and case in case_base:\n",
    "                case.trust_value += 0.1  \n",
    "            elif not successful_episodes and case in case_base:\n",
    "                case.trust_value -= 0.1  \n",
    "            case.trust_value = max(0, min(case.trust_value,1))\n",
    "\n",
    "    @staticmethod\n",
    "    def retain(case_base, temporary_case_base, successful_episodes, threshold=0):\n",
    "        if successful_episodes:\n",
    "            for case in reversed(temporary_case_base):\n",
    "                state = tuple(np.atleast_1d(case.problem))\n",
    "                if state not in Case.added_states:\n",
    "                    case_base.append(case)\n",
    "                    Case.added_states.add(state)\n",
    "            \n",
    "            filtered_case_base = []\n",
    "            for case in case_base:\n",
    "                if case.trust_value >= threshold:\n",
    "                    filtered_case_base.append(case)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            return filtered_case_base\n",
    "        else:\n",
    "            return case_base  \n",
    "\n",
    "class QCBRL:\n",
    "    def __init__(self, num_states, num_actions, env):\n",
    "        self.num_actions = num_actions\n",
    "        self.num_states = num_states\n",
    "        self.env = env\n",
    "        self.problem_solver = ProblemSolver(num_states, num_actions, env)\n",
    "        self.case_base = []\n",
    "        self.temporary_case_base = []\n",
    "\n",
    "    def run(self, episodes=100, max_steps=100, alpha=0.1, gamma=0.9, epsilon=0.1, render=False):\n",
    "        rewards = []\n",
    "        memory_usage = []\n",
    "        gpu_memory_usage = []\n",
    "        successful_episodes = False\n",
    "        num_successful_episodes = 0\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            state = np.array([state], dtype=np.float32)  \n",
    "            episode_reward = 0\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards_episode = []\n",
    "            next_states = []\n",
    "            dones = []\n",
    "\n",
    "            for _ in range(max_steps):\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                \n",
    "                action = self.take_action(state, epsilon)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards_episode.append(reward)\n",
    "                next_states.append(next_state)\n",
    "                dones.append(done)\n",
    "\n",
    "                state = np.array([next_state], dtype=np.float32)  \n",
    "                episode_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    successful_episodes = reward > 0\n",
    "                    break\n",
    "                \n",
    "            if successful_episodes:\n",
    "                num_successful_episodes += 1\n",
    "\n",
    "            rewards.append(episode_reward)\n",
    "            self.problem_solver.update_policy(states, actions, rewards_episode, next_states, dones, gamma)\n",
    "        \n",
    "        success_rate = (num_successful_episodes / episodes) * 100\n",
    "        return rewards, success_rate, memory_usage, gpu_memory_usage\n",
    "\n",
    "    def take_action(self, state, epsilon):\n",
    "        similar_solution = Case.retrieve(state, self.case_base)\n",
    "        if similar_solution is not None:\n",
    "            action = similar_solution.solution\n",
    "        else:\n",
    "            action = self.problem_solver.choose_action(state)\n",
    "        return action\n",
    "    \n",
    "    def save_case_base_temporary(self):\n",
    "        filename = \"case_base_temporary.json\"\n",
    "        case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else int(case.problem), \n",
    "                        \"solution\": int(case.solution), \n",
    "                        \"trust_value\": int(case.trust_value)} for case in self.temporary_case_base]\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "        print(\"Temporary case base saved successfully.\")\n",
    "\n",
    "    def save_case_base(self):\n",
    "        filename = \"case_base.json\"\n",
    "        case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else int(case.problem), \n",
    "                        \"solution\": int(case.solution), \n",
    "                        \"trust_value\": int(case.trust_value)} for case in self.case_base]\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "            print(\"Case base saved successfully.\")\n",
    "        \n",
    "    def load_case_base(self):\n",
    "        filename = \"case_base.json\"\n",
    "        try:\n",
    "            with open(filename, 'r') as file:\n",
    "                case_base_data = json.load(file)\n",
    "                self.case_base = [Case(np.array(case[\"problem\"]), case[\"solution\"], case[\"trust_value\"]) for case in case_base_data]\n",
    "                print(\"Case base loaded successfully.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Case base file not found. Starting with an empty case base.\")\n",
    "\n",
    "    def display_success_rate(self, success_rate):\n",
    "        print(f\"Success rate: {success_rate}%\")\n",
    "\n",
    "    def plot_rewards(self, rewards):\n",
    "        plt.plot(rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Rewards over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show() \n",
    "\n",
    "    def plot_resources(self, memory_usage, gpu_memory_usage):\n",
    "        plt.plot(memory_usage, label='Memory (%)')\n",
    "        plt.plot(gpu_memory_usage, label='GPU Memory (MB)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Resource Usage')\n",
    "        plt.title('Resource Usage over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = CustomRewardFrozenLake()\n",
    "    \n",
    "    num_states = env.observation_space\n",
    "    print(f\"num_states: {num_states}\")\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    agent = QCBRL(num_states, num_actions, env)\n",
    "    rewards, success_rate, memory_usage, gpu_memory_usage = agent.run(episodes=1000, max_steps=1000, alpha=0.1, gamma=0.9, epsilon=0.1)\n",
    "\n",
    "    agent.display_success_rate(success_rate)\n",
    "    agent.plot_rewards(rewards)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
