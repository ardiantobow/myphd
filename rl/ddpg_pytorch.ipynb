{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45921/2456497224.py:118: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  states = torch.tensor(states, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Reward: -1149.30\n",
      "Episode: 1, Reward: -1195.62\n",
      "Episode: 2, Reward: -1308.27\n",
      "Episode: 3, Reward: -1245.50\n",
      "Episode: 4, Reward: -1061.01\n",
      "Episode: 5, Reward: -1456.54\n",
      "Episode: 6, Reward: -1172.99\n",
      "Episode: 7, Reward: -1363.89\n",
      "Episode: 8, Reward: -1496.50\n",
      "Episode: 9, Reward: -1685.64\n",
      "Episode: 10, Reward: -1417.73\n",
      "Episode: 11, Reward: -1507.81\n",
      "Episode: 12, Reward: -1486.66\n",
      "Episode: 13, Reward: -1768.95\n",
      "Episode: 14, Reward: -1736.23\n",
      "Episode: 15, Reward: -1550.42\n",
      "Episode: 16, Reward: -1738.48\n",
      "Episode: 17, Reward: -1238.34\n",
      "Episode: 18, Reward: -1583.64\n",
      "Episode: 19, Reward: -1731.01\n",
      "Episode: 20, Reward: -1604.64\n",
      "Episode: 21, Reward: -1758.75\n",
      "Episode: 22, Reward: -1688.66\n",
      "Episode: 23, Reward: -1327.49\n",
      "Episode: 24, Reward: -1356.89\n",
      "Episode: 25, Reward: -1372.26\n",
      "Episode: 26, Reward: -1370.86\n",
      "Episode: 27, Reward: -1488.33\n",
      "Episode: 28, Reward: -1264.92\n",
      "Episode: 29, Reward: -1223.73\n",
      "Episode: 30, Reward: -1394.56\n",
      "Episode: 31, Reward: -1470.93\n",
      "Episode: 32, Reward: -1280.16\n",
      "Episode: 33, Reward: -1310.82\n",
      "Episode: 34, Reward: -1478.01\n",
      "Episode: 35, Reward: -1283.90\n",
      "Episode: 36, Reward: -1410.21\n",
      "Episode: 37, Reward: -1353.39\n",
      "Episode: 38, Reward: -1150.99\n",
      "Episode: 39, Reward: -1387.48\n",
      "Episode: 40, Reward: -1408.89\n",
      "Episode: 41, Reward: -1340.28\n",
      "Episode: 42, Reward: -1222.55\n",
      "Episode: 43, Reward: -998.43\n",
      "Episode: 44, Reward: -1177.02\n",
      "Episode: 45, Reward: -1012.61\n",
      "Episode: 46, Reward: -830.48\n",
      "Episode: 47, Reward: -1098.55\n",
      "Episode: 48, Reward: -959.45\n",
      "Episode: 49, Reward: -1003.23\n",
      "Episode: 50, Reward: -1127.80\n",
      "Episode: 51, Reward: -819.01\n",
      "Episode: 52, Reward: -992.13\n",
      "Episode: 53, Reward: -980.56\n",
      "Episode: 54, Reward: -996.40\n",
      "Episode: 55, Reward: -865.35\n",
      "Episode: 56, Reward: -887.43\n",
      "Episode: 57, Reward: -1187.51\n",
      "Episode: 58, Reward: -1118.66\n",
      "Episode: 59, Reward: -1063.54\n",
      "Episode: 60, Reward: -1046.30\n",
      "Episode: 61, Reward: -796.05\n",
      "Episode: 62, Reward: -634.43\n",
      "Episode: 63, Reward: -882.28\n",
      "Episode: 64, Reward: -578.66\n",
      "Episode: 65, Reward: -876.64\n",
      "Episode: 66, Reward: -576.30\n",
      "Episode: 67, Reward: -943.71\n",
      "Episode: 68, Reward: -1041.05\n",
      "Episode: 69, Reward: -916.16\n",
      "Episode: 70, Reward: -1046.21\n",
      "Episode: 71, Reward: -502.84\n",
      "Episode: 72, Reward: -803.48\n",
      "Episode: 73, Reward: -864.09\n",
      "Episode: 74, Reward: -575.68\n",
      "Episode: 75, Reward: -1043.05\n",
      "Episode: 76, Reward: -920.40\n",
      "Episode: 77, Reward: -387.20\n",
      "Episode: 78, Reward: -1012.36\n",
      "Episode: 79, Reward: -1207.36\n",
      "Episode: 80, Reward: -530.28\n",
      "Episode: 81, Reward: -915.28\n",
      "Episode: 82, Reward: -1139.76\n",
      "Episode: 83, Reward: -747.16\n",
      "Episode: 84, Reward: -1222.55\n",
      "Episode: 85, Reward: -980.32\n",
      "Episode: 86, Reward: -1051.70\n",
      "Episode: 87, Reward: -253.52\n",
      "Episode: 88, Reward: -830.37\n",
      "Episode: 89, Reward: -634.83\n",
      "Episode: 90, Reward: -712.06\n",
      "Episode: 91, Reward: -623.35\n",
      "Episode: 92, Reward: -587.78\n",
      "Episode: 93, Reward: -634.71\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 97\u001b[0m\n\u001b[1;32m     93\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# Render the environment\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# Select action with exploration (add noise)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/gym/core.py:286\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/gym/core.py:286\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PHD/Research/code/.venv/lib/python3.10/site-packages/gym/envs/classic_control/pendulum.py:211\u001b[0m, in \u001b[0;36mPendulumEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    210\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Ornstein-Uhlenbeck Noise for exploration\n",
    "class OUNoise:\n",
    "    def __init__(self, action_dim, mu=0, theta=0.15, sigma=0.2):\n",
    "        self.action_dim = action_dim\n",
    "        self.mu = mu * np.ones(action_dim)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.mu.copy()\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "# Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim + action_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the environment\n",
    "env = gym.make('Pendulum-v1')  # Pendulum environment for continuous action space\n",
    "\n",
    "# Define dimensions\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "# Initialize actor and critic networks\n",
    "actor = Actor(state_dim, action_dim)\n",
    "critic = Critic(state_dim, action_dim)\n",
    "\n",
    "# Initialize target networks\n",
    "target_actor = Actor(state_dim, action_dim)\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "target_actor.eval()\n",
    "\n",
    "target_critic = Critic(state_dim, action_dim)\n",
    "target_critic.load_state_dict(critic.state_dict())\n",
    "target_critic.eval()\n",
    "\n",
    "# Initialize optimizers\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=0.0001)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize replay buffer\n",
    "replay_buffer = []\n",
    "replay_buffer_size = 100000\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize Ornstein-Uhlenbeck noise process\n",
    "noise = OUNoise(action_dim)\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# Training loop\n",
    "for episode in range(500):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    while True:\n",
    "        # Render the environment\n",
    "        env.render()\n",
    "\n",
    "        # Select action with exploration (add noise)\n",
    "        with torch.no_grad():\n",
    "            action = actor(torch.tensor(state, dtype=torch.float32)).numpy() + noise.sample()\n",
    "\n",
    "        # Clip action to ensure it's within action bounds\n",
    "        action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "\n",
    "        # Step through environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Store transition in replay buffer\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(replay_buffer) > replay_buffer_size:\n",
    "            replay_buffer.pop(0)\n",
    "\n",
    "        # Sample random minibatch from replay buffer\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            minibatch = random.sample(replay_buffer, batch_size)\n",
    "            states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "            states = torch.tensor(states, dtype=torch.float32)\n",
    "            actions = torch.tensor(actions, dtype=torch.float32)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "            dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "            # Compute Q targets\n",
    "            with torch.no_grad():\n",
    "                next_actions = target_actor(next_states)\n",
    "                q_targets = rewards.view(-1, 1) + gamma * (1 - dones.view(-1, 1)) * target_critic(next_states, next_actions)\n",
    "\n",
    "            # Update critic\n",
    "            q_values = critic(states, actions)\n",
    "            critic_loss = nn.MSELoss()(q_values, q_targets)\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            # Update actor\n",
    "            predicted_actions = actor(states)\n",
    "            actor_loss = -critic(states, predicted_actions).mean()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            # Update target networks\n",
    "            for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "                target_param.data.copy_(param.data * 0.001 + target_param.data * (1.0 - 0.001))\n",
    "\n",
    "            for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "                target_param.data.copy_(param.data * 0.001 + target_param.data * (1.0 - 0.001))\n",
    "\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print(\"Episode: {}, Reward: {:.2f}\".format(episode, episode_reward))\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
