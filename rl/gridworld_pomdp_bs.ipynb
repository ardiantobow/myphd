{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'action' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 108\u001b[0m\n\u001b[1;32m    104\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Update history with observation and action\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     agent\u001b[38;5;241m.\u001b[39mupdate_history(observation, \u001b[43maction\u001b[49m)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# Update belief state with observation and action history\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     belief_state\u001b[38;5;241m.\u001b[39mupdate_belief_state(observation, action)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'action' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GridWorldPOMDP:\n",
    "    def __init__(self, width, height, goal, obstacles):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.goal = goal\n",
    "        self.obstacles = obstacles\n",
    "        self.agent_pos = (0, 0)\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = (0, 0)\n",
    "        return self.get_observation()\n",
    "\n",
    "    def get_observation(self):\n",
    "        return self.agent_pos  # Partial observation of agent's current position\n",
    "\n",
    "    def step(self, action):\n",
    "        if isinstance(action, int):  # Check if action is an integer\n",
    "            # Map action index to (dx, dy) tuple\n",
    "            if action == 0:\n",
    "                dx, dy = 0, 1  # Move up\n",
    "            elif action == 1:\n",
    "                dx, dy = 0, -1  # Move down\n",
    "            elif action == 2:\n",
    "                dx, dy = -1, 0  # Move left\n",
    "            elif action == 3:\n",
    "                dx, dy = 1, 0  # Move right\n",
    "        else:  # Assume action is already a tuple\n",
    "            dx, dy = action\n",
    "\n",
    "        x, y = self.agent_pos\n",
    "        new_x = min(max(x + dx, 0), self.width - 1)\n",
    "        new_y = min(max(y + dy, 0), self.height - 1)\n",
    "\n",
    "        self.agent_pos = (new_x, new_y)\n",
    "\n",
    "        reward = -1  # Small negative reward for each step\n",
    "        done = self.agent_pos == self.goal  # Episode ends when goal is reached\n",
    "        return self.get_observation(), reward, done\n",
    "\n",
    "class ParticleFilter:\n",
    "    def __init__(self, width, height, num_particles):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.num_particles = num_particles\n",
    "        self.particles = self.initialize_particles()\n",
    "\n",
    "    def initialize_particles(self):\n",
    "        particles = []\n",
    "        for _ in range(self.num_particles):\n",
    "            x = np.random.randint(0, self.width)\n",
    "            y = np.random.randint(0, self.height)\n",
    "            particles.append((x, y))\n",
    "        return particles\n",
    "\n",
    "    def update_belief_state(self, observation, action):\n",
    "        # Update belief state based on observation and action\n",
    "        # For simplicity, we'll just randomly shuffle the particles\n",
    "        np.random.shuffle(self.particles)\n",
    "\n",
    "    def estimate_state(self):\n",
    "        # Estimate current state based on particles (e.g., mean or mode)\n",
    "        # For simplicity, we'll return the mode of particles\n",
    "        counts = {}\n",
    "        for particle in self.particles:\n",
    "            counts[particle] = counts.get(particle, 0) + 1\n",
    "        estimated_state = max(counts, key=counts.get)\n",
    "        return estimated_state\n",
    "\n",
    "class POMDPAgent:\n",
    "    def __init__(self, belief_state):\n",
    "        self.belief_state = belief_state\n",
    "        self.history = []\n",
    "\n",
    "    def update_history(self, observation, action):\n",
    "        self.history.append((observation, action))\n",
    "\n",
    "    def choose_action(self):\n",
    "        # Choose action based on belief state and history\n",
    "        # For simplicity, we'll randomly select an action\n",
    "        return np.random.randint(0, 4)  # 4 possible actions (up, down, left, right)\n",
    "\n",
    "# Main loop\n",
    "if __name__ == \"__main__\":\n",
    "    # Define grid world environment\n",
    "    width = 5\n",
    "    height = 5\n",
    "    goal = (4, 4)\n",
    "    obstacles = [(1, 1), (2, 2), (3, 3)]\n",
    "    env = GridWorldPOMDP(width, height, goal, obstacles)\n",
    "\n",
    "    # Initialize belief state with particle filter\n",
    "    num_particles = 100\n",
    "    belief_state = ParticleFilter(width, height, num_particles)\n",
    "\n",
    "    # Create POMDP agent with belief state\n",
    "    agent = POMDPAgent(belief_state)\n",
    "\n",
    "    # Training loop\n",
    "    num_episodes = 100\n",
    "    for episode in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        action = None  # Initialize action variable\n",
    "\n",
    "        while not done:\n",
    "            # Update history with observation and action\n",
    "            agent.update_history(observation, action)\n",
    "\n",
    "            # Update belief state with observation and action history\n",
    "            belief_state.update_belief_state(observation, action)\n",
    "\n",
    "            # Estimate current state based on belief state\n",
    "            estimated_state = belief_state.estimate_state()\n",
    "\n",
    "            # Choose action based on belief state and history\n",
    "            action = agent.choose_action()\n",
    "\n",
    "            # Take action and observe next state, reward, and new observation\n",
    "            next_observation, reward, done = env.step(action)\n",
    "\n",
    "            # Print belief state and estimated state\n",
    "            print(\"Belief state:\", belief_state.particles)\n",
    "            print(\"Estimated state:\", estimated_state)\n",
    "\n",
    "            # Print agent movement\n",
    "            print(\"Agent movement:\", observation, \"->\", next_observation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
