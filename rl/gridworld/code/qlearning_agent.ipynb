{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: -110\n",
      "Episode: 2, Reward: -117\n",
      "Episode: 3, Reward: 79\n",
      "Episode: 4, Reward: -103\n",
      "Episode: 5, Reward: 96\n",
      "Episode: 6, Reward: -101\n",
      "Episode: 7, Reward: 84\n",
      "Episode: 8, Reward: 81\n",
      "Episode: 9, Reward: 92\n",
      "Episode: 10, Reward: -104\n",
      "Episode: 11, Reward: -102\n",
      "Episode: 12, Reward: 76\n",
      "Episode: 13, Reward: -117\n",
      "Episode: 14, Reward: 73\n",
      "Episode: 15, Reward: 78\n",
      "Episode: 16, Reward: -102\n",
      "Episode: 17, Reward: -101\n",
      "Episode: 18, Reward: 57\n",
      "Episode: 19, Reward: 78\n",
      "Episode: 20, Reward: 67\n",
      "Episode: 21, Reward: -107\n",
      "Episode: 22, Reward: -118\n",
      "Episode: 23, Reward: -119\n",
      "Episode: 24, Reward: -110\n",
      "Episode: 25, Reward: 86\n",
      "Episode: 26, Reward: -109\n",
      "Episode: 27, Reward: 55\n",
      "Episode: 28, Reward: 85\n",
      "Episode: 29, Reward: 91\n",
      "Episode: 30, Reward: -143\n",
      "Episode: 31, Reward: 15\n",
      "Episode: 32, Reward: 29\n",
      "Episode: 33, Reward: 34\n",
      "Episode: 34, Reward: -159\n",
      "Episode: 35, Reward: -48\n",
      "Episode: 36, Reward: 72\n",
      "Episode: 37, Reward: -127\n",
      "Episode: 38, Reward: -160\n"
     ]
    },
    {
     "ename": "TclError",
     "evalue": "invalid command name \".!canvas\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 98\u001b[0m\n\u001b[1;32m     96\u001b[0m env \u001b[38;5;241m=\u001b[39m Env()\n\u001b[1;32m     97\u001b[0m agent \u001b[38;5;241m=\u001b[39m QLearningAgent(env, actions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(env\u001b[38;5;241m.\u001b[39mn_actions)))\n\u001b[0;32m---> 98\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_episodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 57\u001b[0m, in \u001b[0;36mQLearningAgent.run_episodes\u001b[0;34m(self, env, num_episodes)\u001b[0m\n\u001b[1;32m     54\u001b[0m step_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     next_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_action(\u001b[38;5;28mstr\u001b[39m(next_state))\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearn(\u001b[38;5;28mstr\u001b[39m(state), action, reward, \u001b[38;5;28mstr\u001b[39m(next_state), next_action)\n",
      "File \u001b[0;32m~/PHD/Research/code/myphd/rl/gridworld/code/environment_dynamic.py:88\u001b[0m, in \u001b[0;36mEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     85\u001b[0m         base_action[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m UNIT\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# move agent\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrectangle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_action\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_action\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# move rectangle to top level of canvas\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mtag_raise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrectangle)\n",
      "File \u001b[0;32m/usr/lib/python3.10/tkinter/__init__.py:2949\u001b[0m, in \u001b[0;36mCanvas.move\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   2947\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmove\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   2948\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Move an item TAGORID given in ARGS.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2949\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmove\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTclError\u001b[0m: invalid command name \".!canvas\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from environment_dynamic_obs_random import Env\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, actions):\n",
    "        self.env = env\n",
    "        self.actions = actions\n",
    "        self.learning_rate = 0.01\n",
    "        self.discount_factor = 0.9\n",
    "        self.epsilon = 0.1\n",
    "        self.q_table = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0])\n",
    "        self.rewards_per_episode = []\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, next_action):\n",
    "        current_q = self.q_table[state][action]\n",
    "        max_next_q = max(self.q_table[next_state])\n",
    "        new_q = (current_q + self.learning_rate *\n",
    "                (reward + self.discount_factor * max_next_q - current_q))\n",
    "        self.q_table[state][action] = new_q\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            state_action = self.q_table[state]\n",
    "            action = self.arg_max(state_action)\n",
    "        return action\n",
    "\n",
    "    @staticmethod\n",
    "    def arg_max(state_action):\n",
    "        max_index_list = []\n",
    "        max_value = state_action[0]\n",
    "        for index, value in enumerate(state_action):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)\n",
    "\n",
    "    def run_episodes(self, env, num_episodes):\n",
    "        env = env\n",
    "        success_count = 0\n",
    "        success_steps = []\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            action = self.get_action(str(state))\n",
    "            total_reward = 0\n",
    "            step_count = 0\n",
    "\n",
    "            while True:\n",
    "                next_state, reward, done = env.step(action)\n",
    "                next_action = self.get_action(str(next_state))\n",
    "                self.learn(str(state), action, reward, str(next_state), next_action)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "                if done:\n",
    "                    self.rewards_per_episode.append(total_reward)\n",
    "                    if total_reward > 0:\n",
    "                        success_count += 1\n",
    "                        success_steps.append(step_count)\n",
    "                    print(f\"Episode: {episode + 1}, Reward: {total_reward}\")\n",
    "                    break\n",
    "                \n",
    "                step_count += 1\n",
    "\n",
    "        success_rate = success_count / num_episodes * 100\n",
    "        print(f\"Percentage of successful episodes: {success_rate}%\")\n",
    "        self.plot_rewards_per_episode()\n",
    "        self.plot_success_steps(success_steps)\n",
    "\n",
    "    def plot_rewards_per_episode(self):\n",
    "        plt.plot(self.rewards_per_episode)\n",
    "        plt.title('Rewards per Episode')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_success_steps(success_steps):\n",
    "        plt.plot(success_steps)\n",
    "        plt.title('Steps Required for Successful Episodes')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Steps')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    agent = QLearningAgent(env, actions=list(range(env.n_actions)))\n",
    "    agent.run_episodes(env, 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
