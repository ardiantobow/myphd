Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 0)
action agent 0 before update: 0
reward agent 0 before update: -1
next state agent 0 before update: (0, 0)
agent0 not hit !!!!!
state agent 1 before update: (4, 0)
action agent 1 before update: 2
reward agent 1 before update: -1
next state agent 1 before update: (4, 1)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 0)
action agent 0 before update: 1
reward agent 0 before update: -1
next state agent 0 before update: (0, 0)
agent0 not hit !!!!!
state agent 1 before update: (4, 1)
action agent 1 before update: 4
reward agent 1 before update: -1
next state agent 1 before update: (4, 1)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 0)
action agent 0 before update: 3
reward agent 0 before update: -1
next state agent 0 before update: (0, 0)
agent0 not hit !!!!!
state agent 1 before update: (4, 1)
action agent 1 before update: 0
reward agent 1 before update: -1
next state agent 1 before update: (4, 1)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 0)
action agent 0 before update: 4
reward agent 0 before update: -1
next state agent 0 before update: (1, 0)
agent0 not hit !!!!!
state agent 1 before update: (4, 1)
action agent 1 before update: 1
reward agent 1 before update: -1
next state agent 1 before update: (4, 0)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (1, 0)
action agent 0 before update: 2
reward agent 0 before update: -1
next state agent 0 before update: (1, 1)
agent0 not hit !!!!!
state agent 1 before update: (4, 0)
action agent 1 before update: 3
reward agent 1 before update: -1
next state agent 1 before update: (3, 0)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (1, 1)
action agent 0 before update: 0
reward agent 0 before update: -1
next state agent 0 before update: (1, 1)
agent0 not hit !!!!!
state agent 1 before update: (3, 0)
action agent 1 before update: 0
reward agent 1 before update: -1
next state agent 1 before update: (3, 0)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (1, 1)
action agent 0 before update: 3
reward agent 0 before update: -1
next state agent 0 before update: (0, 1)
agent0 not hit !!!!!
state agent 1 before update: (3, 0)
action agent 1 before update: 4
reward agent 1 before update: -1
next state agent 1 before update: (4, 0)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 1)
action agent 0 before update: 1
reward agent 0 before update: -1
next state agent 0 before update: (0, 0)
agent0 not hit !!!!!
state agent 1 before update: (4, 0)
action agent 1 before update: 4
reward agent 1 before update: -1
next state agent 1 before update: (4, 0)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 0)
action agent 0 before update: 0
reward agent 0 before update: -1
next state agent 0 before update: (0, 0)
agent0 not hit !!!!!
state agent 1 before update: (4, 0)
action agent 1 before update: 1
reward agent 1 before update: -1
next state agent 1 before update: (4, 0)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 0)
action agent 0 before update: 2
reward agent 0 before update: -1
next state agent 0 before update: (0, 1)
agent0 not hit !!!!!
state agent 1 before update: (4, 0)
action agent 1 before update: 0
reward agent 1 before update: -1
next state agent 1 before update: (4, 0)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 1)
action agent 0 before update: 3
reward agent 0 before update: -1
next state agent 0 before update: (0, 1)
agent0 not hit !!!!!
state agent 1 before update: (4, 0)
action agent 1 before update: 3
reward agent 1 before update: -1
next state agent 1 before update: (3, 0)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 1)
action agent 0 before update: 0
reward agent 0 before update: -1
next state agent 0 before update: (0, 1)
agent0 not hit !!!!!
state agent 1 before update: (3, 0)
action agent 1 before update: 2
reward agent 1 before update: -1
next state agent 1 before update: (3, 1)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 1)
action agent 0 before update: 0
reward agent 0 before update: -1
next state agent 0 before update: (0, 1)
agent0 not hit !!!!!
state agent 1 before update: (3, 1)
action agent 1 before update: 2
reward agent 1 before update: -1
next state agent 1 before update: (3, 2)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 1)
action agent 0 before update: 2
reward agent 0 before update: -1
next state agent 0 before update: (0, 2)
agent0 not hit !!!!!
state agent 1 before update: (3, 2)
action agent 1 before update: 3
reward agent 1 before update: 10
next state agent 1 before update: (2, 2)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 2)
action agent 0 before update: 2
reward agent 0 before update: -1
next state agent 0 before update: (0, 3)
agent0 not hit !!!!!
state agent 1 before update: (2, 2)
action agent 1 before update: 4
reward agent 1 before update: -1
next state agent 1 before update: (3, 2)
agent1 hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 3)
action agent 0 before update: 3
reward agent 0 before update: -1
next state agent 0 before update: (0, 3)
agent0 not hit !!!!!
state agent 1 before update: (3, 2)
action agent 1 before update: 3
reward agent 1 before update: 0
next state agent 1 before update: (2, 2)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 3)
action agent 0 before update: 0
reward agent 0 before update: -1
next state agent 0 before update: (0, 3)
agent0 not hit !!!!!
state agent 1 before update: (2, 2)
action agent 1 before update: 4
reward agent 1 before update: -1
next state agent 1 before update: (3, 2)
agent1 hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 3)
action agent 0 before update: 2
reward agent 0 before update: -1
next state agent 0 before update: (0, 4)
agent0 not hit !!!!!
state agent 1 before update: (3, 2)
action agent 1 before update: 3
reward agent 1 before update: 0
next state agent 1 before update: (2, 2)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 4)
action agent 0 before update: 2
reward agent 0 before update: -1
next state agent 0 before update: (0, 4)
agent0 not hit !!!!!
state agent 1 before update: (2, 2)
action agent 1 before update: 4
reward agent 1 before update: -1
next state agent 1 before update: (3, 2)
agent1 hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 4)
action agent 0 before update: 3
reward agent 0 before update: -1
next state agent 0 before update: (0, 4)
agent0 not hit !!!!!
state agent 1 before update: (3, 2)
action agent 1 before update: 3
reward agent 1 before update: 0
next state agent 1 before update: (2, 2)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 4)
action agent 0 before update: 0
reward agent 0 before update: -1
next state agent 0 before update: (0, 4)
agent0 not hit !!!!!
state agent 1 before update: (2, 2)
action agent 1 before update: 4
reward agent 1 before update: -1
next state agent 1 before update: (3, 2)
agent1 hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 4)
action agent 0 before update: 2
reward agent 0 before update: -1
next state agent 0 before update: (0, 4)
agent0 not hit !!!!!
state agent 1 before update: (3, 2)
action agent 1 before update: 3
reward agent 1 before update: 0
next state agent 1 before update: (2, 2)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 4)
action agent 0 before update: 3
reward agent 0 before update: -1
next state agent 0 before update: (0, 4)
agent0 not hit !!!!!
state agent 1 before update: (2, 2)
action agent 1 before update: 2
reward agent 1 before update: -1
next state agent 1 before update: (2, 3)
agent1 hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 4)
action agent 0 before update: 0
reward agent 0 before update: -1
next state agent 0 before update: (0, 4)
agent0 not hit !!!!!
state agent 1 before update: (2, 3)
action agent 1 before update: 2
reward agent 1 before update: -1
next state agent 1 before update: (2, 4)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 4)
action agent 0 before update: 1
reward agent 0 before update: -1
next state agent 0 before update: (0, 3)
agent0 not hit !!!!!
state agent 1 before update: (2, 4)
action agent 1 before update: 4
reward agent 1 before update: -1
next state agent 1 before update: (3, 4)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 3)
action agent 0 before update: 4
reward agent 0 before update: -1
next state agent 0 before update: (1, 3)
agent0 not hit !!!!!
state agent 1 before update: (3, 4)
action agent 1 before update: 2
reward agent 1 before update: -1
next state agent 1 before update: (3, 4)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (1, 3)
action agent 0 before update: 3
reward agent 0 before update: -1
next state agent 0 before update: (0, 3)
agent0 not hit !!!!!
state agent 1 before update: (3, 4)
action agent 1 before update: 4
reward agent 1 before update: -1
next state agent 1 before update: (4, 4)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 3)
action agent 0 before update: 2
reward agent 0 before update: -1
next state agent 0 before update: (0, 4)
agent0 not hit !!!!!
state agent 1 before update: (4, 4)
action agent 1 before update: 3
reward agent 1 before update: -1
next state agent 1 before update: (3, 4)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (0, 4)
action agent 0 before update: 4
reward agent 0 before update: -1
next state agent 0 before update: (1, 4)
agent0 not hit !!!!!
state agent 1 before update: (3, 4)
action agent 1 before update: 3
reward agent 1 before update: -1
next state agent 1 before update: (2, 4)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (1, 4)
action agent 0 before update: 2
reward agent 0 before update: -1
next state agent 0 before update: (1, 4)
agent0 not hit !!!!!
state agent 1 before update: (2, 4)
action agent 1 before update: 1
reward agent 1 before update: -1
next state agent 1 before update: (2, 3)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (1, 4)
action agent 0 before update: 2
reward agent 0 before update: -1
next state agent 0 before update: (1, 4)
agent0 not hit !!!!!
state agent 1 before update: (2, 3)
action agent 1 before update: 0
reward agent 1 before update: -1
next state agent 1 before update: (2, 3)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (1, 4)
action agent 0 before update: 4
reward agent 0 before update: -1
next state agent 0 before update: (2, 4)
agent0 not hit !!!!!
state agent 1 before update: (2, 3)
action agent 1 before update: 4
reward agent 1 before update: -1
next state agent 1 before update: (3, 3)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (2, 4)
action agent 0 before update: 3
reward agent 0 before update: -1
next state agent 0 before update: (1, 4)
agent0 not hit !!!!!
state agent 1 before update: (3, 3)
action agent 1 before update: 1
reward agent 1 before update: -1
next state agent 1 before update: (3, 2)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (1, 4)
action agent 0 before update: 4
reward agent 0 before update: -1
next state agent 0 before update: (2, 4)
agent0 not hit !!!!!
state agent 1 before update: (3, 2)
action agent 1 before update: 3
reward agent 1 before update: 0
next state agent 1 before update: (2, 2)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (2, 4)
action agent 0 before update: 4
reward agent 0 before update: -1
next state agent 0 before update: (3, 4)
agent0 not hit !!!!!
state agent 1 before update: (2, 2)
action agent 1 before update: 4
reward agent 1 before update: -1
next state agent 1 before update: (3, 2)
agent1 hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (3, 4)
action agent 0 before update: 2
reward agent 0 before update: -1
next state agent 0 before update: (3, 4)
agent0 not hit !!!!!
state agent 1 before update: (3, 2)
action agent 1 before update: 3
reward agent 1 before update: 0
next state agent 1 before update: (2, 2)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (3, 4)
action agent 0 before update: 4
reward agent 0 before update: -1
next state agent 0 before update: (4, 4)
agent0 not hit !!!!!
state agent 1 before update: (2, 2)
action agent 1 before update: 4
reward agent 1 before update: -1
next state agent 1 before update: (3, 2)
agent1 hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (4, 4)
action agent 0 before update: 4
reward agent 0 before update: -1
next state agent 0 before update: (4, 4)
agent0 not hit !!!!!
state agent 1 before update: (3, 2)
action agent 1 before update: 3
reward agent 1 before update: 0
next state agent 1 before update: (2, 2)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (4, 4)
action agent 0 before update: 2
reward agent 0 before update: -1
next state agent 0 before update: (4, 4)
agent0 not hit !!!!!
state agent 1 before update: (2, 2)
action agent 1 before update: 4
reward agent 1 before update: -1
next state agent 1 before update: (3, 2)
agent1 hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (4, 4)
action agent 0 before update: 0
reward agent 0 before update: -1
next state agent 0 before update: (4, 4)
agent0 not hit !!!!!
state agent 1 before update: (3, 2)
action agent 1 before update: 1
reward agent 1 before update: -1
next state agent 1 before update: (3, 1)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (4, 4)
action agent 0 before update: 1
reward agent 0 before update: -1
next state agent 0 before update: (4, 3)
agent0 not hit !!!!!
state agent 1 before update: (3, 1)
action agent 1 before update: 2
reward agent 1 before update: -1
next state agent 1 before update: (3, 2)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (4, 3)
action agent 0 before update: 2
reward agent 0 before update: -1
next state agent 0 before update: (4, 4)
agent0 not hit !!!!!
state agent 1 before update: (3, 2)
action agent 1 before update: 3
reward agent 1 before update: 0
next state agent 1 before update: (2, 2)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (4, 4)
action agent 0 before update: 3
reward agent 0 before update: -1
next state agent 0 before update: (3, 4)
agent0 not hit !!!!!
state agent 1 before update: (2, 2)
action agent 1 before update: 4
reward agent 1 before update: -1
next state agent 1 before update: (3, 2)
agent1 hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (3, 4)
action agent 0 before update: 3
reward agent 0 before update: -1
next state agent 0 before update: (2, 4)
agent0 not hit !!!!!
state agent 1 before update: (3, 2)
action agent 1 before update: 3
reward agent 1 before update: 0
next state agent 1 before update: (2, 2)
agent1 not hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (2, 4)
action agent 0 before update: 0
reward agent 0 before update: -1
next state agent 0 before update: (2, 4)
agent0 not hit !!!!!
state agent 1 before update: (2, 2)
action agent 1 before update: 0
reward agent 1 before update: 0
next state agent 1 before update: (2, 2)
agent1 hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (2, 4)
action agent 0 before update: 1
reward agent 0 before update: -1
next state agent 0 before update: (2, 3)
agent0 not hit !!!!!
state agent 1 before update: (2, 2)
action agent 1 before update: 0
reward agent 1 before update: 0
next state agent 1 before update: (2, 2)
agent1 hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (2, 3)
action agent 0 before update: 1
reward agent 0 before update: 100
next state agent 0 before update: (2, 2)
agent0 not hit !!!!!
state agent 1 before update: (2, 2)
action agent 1 before update: 0
reward agent 1 before update: 100
next state agent 1 before update: (2, 2)
agent1 hit !!!!!
Action for Agent 0 from problem solver
Action for Agent 1 from problem solver
state agent 0 before update: (2, 2)
action agent 0 before update: 0
reward agent 0 before update: 0
next state agent 0 before update: (2, 2)
agent0 hit !!!!!
state agent 1 before update: (2, 2)
action agent 1 before update: 0
reward agent 1 before update: 0
next state agent 1 before update: (2, 2)
agent1 hit !!!!!
Episode: 10, Total Steps: 48, Total Rewards: [54, 76], Status Episode: True
