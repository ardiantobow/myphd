{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, actions):\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    wins = []\n",
    "    next_states = []\n",
    "    agents_reached_target = 0\n",
    "    \n",
    "    self.update_grid_colors()\n",
    "    circle_pos = self.get_circle_grid_position()\n",
    "    \n",
    "    reward_position = 0\n",
    "    reward_bonus = 0\n",
    "    reward = 0\n",
    "    done = False\n",
    "    win = False\n",
    "\n",
    "    for idx, (agent, action) in enumerate(zip(self.agents, actions)):\n",
    "\n",
    "        if self.locked[idx]:  # If agent is locked, skip action processing\n",
    "            next_states.append([self.coords_to_state(agent['coords']), wins[idx], None])  # Use the last known win status\n",
    "            rewards.append(0)\n",
    "            dones.append(True)  # Locked agents should be marked as done\n",
    "            continue\n",
    "\n",
    "        state = agent['coords']\n",
    "        base_action = np.array([0, 0])\n",
    "        message = None\n",
    "        physical_action = action[0]\n",
    "\n",
    "        if physical_action == 0:\n",
    "            base_action[0] = base_action[0]\n",
    "            base_action[1] = base_action[1]\n",
    "        elif physical_action == 1:  # up\n",
    "            if state[1] > UNIT:\n",
    "                base_action[1] -= UNIT\n",
    "        elif physical_action == 2:  # down\n",
    "            if state[1] < (HEIGHT - 1) * UNIT:\n",
    "                base_action[1] += UNIT\n",
    "        elif physical_action == 3:  # left\n",
    "            if state[0] > UNIT:\n",
    "                base_action[0] -= UNIT\n",
    "        elif physical_action == 4:  # right\n",
    "            if state[0] < (WIDTH - 1) * UNIT:\n",
    "                base_action[0] += UNIT\n",
    "\n",
    "        initial_pos = self.coords_to_state(state)\n",
    "        initial_distance = abs(initial_pos[0] - circle_pos[0]) + abs(initial_pos[1] - circle_pos[1])\n",
    "\n",
    "        self.canvas.move(agent['image_obj'], base_action[0], base_action[1])\n",
    "        self.canvas.tag_raise(agent['image_obj'])\n",
    "        next_state = self.canvas.coords(agent['image_obj'])\n",
    "        \n",
    "        new_pos = self.coords_to_state(next_state)\n",
    "        new_distance = abs(new_pos[0] - circle_pos[0]) + abs(new_pos[1] - circle_pos[1])\n",
    "\n",
    "        reward_position = initial_distance - new_distance\n",
    "\n",
    "        if new_pos == self.coords_to_state(self.canvas.coords(self.circle)):  # Agent hits the target\n",
    "                \n",
    "            agents_reached_target += 1\n",
    "            if not self.first_agent_reached:\n",
    "                reward_bonus = 100\n",
    "                self.first_agent_reached = True\n",
    "            else:\n",
    "                reward_bonus = 0\n",
    "\n",
    "            reward_bonus = 100\n",
    "            done = False # problem for case base\n",
    "            win = True\n",
    "            self.locked[idx] = True #problem for case base\n",
    "            self.update_grid_colors((0, 0, 255))\n",
    "                \n",
    "        elif new_pos in [self.coords_to_state(self.canvas.coords(self.triangle1)), self.coords_to_state(self.canvas.coords(self.triangle2))]:  # Agent hits an obstacle\n",
    "            reward_bonus = -10\n",
    "            done = False\n",
    "            win = False\n",
    "            self.locked[idx] = True\n",
    "            self.update_grid_colors((255, 0, 0))\n",
    "        else:\n",
    "            reward_bonus = -1\n",
    "            # done = False\n",
    "            # win = False\n",
    "            \n",
    "        reward = reward_bonus\n",
    "        # reward = reward_bonus + reward_position\n",
    "\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        wins.append(win)  # Track the win status for this agent\n",
    "        \n",
    "        next_state_obs = self.coords_to_state(next_state)\n",
    "        next_state_comms = []\n",
    "        if not self.is_agent_silent:\n",
    "            for other_agent in self.agents:\n",
    "                if other_agent == agent:\n",
    "                    continue\n",
    "\n",
    "                other_agent_message = actions[other_agent['id']][1]\n",
    "                next_state_comms.append(other_agent_message)\n",
    "\n",
    "        next_state_observation = [next_state_obs, win, next_state_comms]\n",
    "\n",
    "        next_states.append(next_state_observation)\n",
    "\n",
    "        agent['coords'] = next_state\n",
    "        \n",
    "    if all(wins):\n",
    "        self.update_grid_colors((0, 255, 0))\n",
    "\n",
    "    if agents_reached_target == self.num_agents and not self.mega_bonus_given:\n",
    "        for i in range(len(rewards)):\n",
    "            rewards[i] += 1000  # Mega bonus\n",
    "        self.mega_bonus_given = True\n",
    "       \n",
    "    if all(self.locked):\n",
    "        dones = [True] * self.num_agents\n",
    "        self.locked = [False] * self.num_agents\n",
    "\n",
    "    # self.render()\n",
    "    return next_states, rewards, dones\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
