{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- starting point of Episode 0 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 82 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 83 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 84 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 85 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 86 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 87 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 88 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 89 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 90 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 91 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 92 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 93 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 94 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 95 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 96 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 97 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 98 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 99 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 100 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 101 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 102 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 103 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 104 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 105 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 106 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 107 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 108 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 109 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 110 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 111 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 112 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 113 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 114 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 115 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 116 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 117 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 118 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 119 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 120 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 121 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 122 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 123 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 124 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 125 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 126 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 127 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 128 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 129 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 130 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 131 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 132 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 133 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 134 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 135 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 136 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 137 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 138 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 139 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 140 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 141 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 142 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 143 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 144 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 145 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 146 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 147 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 148 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 149 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 150 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 151 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 152 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 153 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 154 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 155 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 156 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 157 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 158 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 159 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 160 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 161 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 162 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 163 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 164 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 165 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 166 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 167 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d30820>, <__main__.Case object at 0x71a995d31c60>, <__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d7c940>, <__main__.Case object at 0x71a995d7ca60>, <__main__.Case object at 0x71a995d7caf0>, <__main__.Case object at 0x71a995d7cc10>, <__main__.Case object at 0x71a995d7cca0>, <__main__.Case object at 0x71a995d7cd30>, <__main__.Case object at 0x71a995d7ce50>, <__main__.Case object at 0x71a995d7cf40>, <__main__.Case object at 0x71a995d7cfd0>, <__main__.Case object at 0x71a995d7d060>, <__main__.Case object at 0x71a995d7d120>, <__main__.Case object at 0x71a995d7d240>, <__main__.Case object at 0x71a995d7d2d0>, <__main__.Case object at 0x71a995d7d360>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7d5d0>, <__main__.Case object at 0x71a995d7d660>, <__main__.Case object at 0x71a995d7d720>, <__main__.Case object at 0x71a995d7d870>, <__main__.Case object at 0x71a995d7d900>, <__main__.Case object at 0x71a995d7d9f0>, <__main__.Case object at 0x71a995d7da80>, <__main__.Case object at 0x71a995d7dba0>, <__main__.Case object at 0x71a995d7dc30>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7dd80>, <__main__.Case object at 0x71a995d7ded0>, <__main__.Case object at 0x71a995d7df60>, <__main__.Case object at 0x71a995d7dff0>, <__main__.Case object at 0x71a995d7e0b0>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7e470>, <__main__.Case object at 0x71a995d7e560>, <__main__.Case object at 0x71a995d7e5f0>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7e740>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7e980>, <__main__.Case object at 0x71a995d7ea40>, <__main__.Case object at 0x71a995d7eb60>, <__main__.Case object at 0x71a995d7ebf0>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7ed40>, <__main__.Case object at 0x71a995d7ee60>, <__main__.Case object at 0x71a995d7eef0>, <__main__.Case object at 0x71a995d7ef80>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7f160>, <__main__.Case object at 0x71a995d7f1f0>, <__main__.Case object at 0x71a995d7f280>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7f460>, <__main__.Case object at 0x71a995d7e200>, <__main__.Case object at 0x71a995d7f5b0>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7f790>, <__main__.Case object at 0x71a995d7f820>, <__main__.Case object at 0x71a995d7f8b0>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7fa90>, <__main__.Case object at 0x71a995d7e890>, <__main__.Case object at 0x71a995d7fbe0>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7fe80>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7f190>, <__main__.Case object at 0x71a995d881f0>, <__main__.Case object at 0x71a995d88280>, <__main__.Case object at 0x71a995d883a0>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d88550>, <__main__.Case object at 0x71a995d885e0>, <__main__.Case object at 0x71a995d886a0>, <__main__.Case object at 0x71a995d7f7c0>, <__main__.Case object at 0x71a995d88850>, <__main__.Case object at 0x71a995d88940>, <__main__.Case object at 0x71a995d88a30>, <__main__.Case object at 0x71a995d7fac0>, <__main__.Case object at 0x71a995d88c10>, <__main__.Case object at 0x71a995d88c70>, <__main__.Case object at 0x71a995d88d90>, <__main__.Case object at 0x71a995d7fe20>, <__main__.Case object at 0x71a995d88f40>, <__main__.Case object at 0x71a995d88fd0>, <__main__.Case object at 0x71a995d89090>, <__main__.Case object at 0x71a995d891b0>, <__main__.Case object at 0x71a995d89240>, <__main__.Case object at 0x71a995d892d0>, <__main__.Case object at 0x71a995d89390>, <__main__.Case object at 0x71a995d894b0>, <__main__.Case object at 0x71a995d88190>, <__main__.Case object at 0x71a995d89600>, <__main__.Case object at 0x71a995d89720>, <__main__.Case object at 0x71a995d89810>, <__main__.Case object at 0x71a995d884f0>, <__main__.Case object at 0x71a995d899c0>, <__main__.Case object at 0x71a995d89a50>, <__main__.Case object at 0x71a995d89b70>, <__main__.Case object at 0x71a995d887f0>, <__main__.Case object at 0x71a995d89d20>, <__main__.Case object at 0x71a995d89db0>, <__main__.Case object at 0x71a995d89f00>, <__main__.Case object at 0x71a995d89f90>, <__main__.Case object at 0x71a995d8a080>, <__main__.Case object at 0x71a995d8a110>, <__main__.Case object at 0x71a995d8a260>, <__main__.Case object at 0x71a995d88ee0>, <__main__.Case object at 0x71a995d8a3b0>, <__main__.Case object at 0x71a995d8a470>, <__main__.Case object at 0x71a995d8a5c0>, <__main__.Case object at 0x71a995d8a650>, <__main__.Case object at 0x71a995d8a6e0>, <__main__.Case object at 0x71a995d8a800>, <__main__.Case object at 0x71a995d8a8f0>, <__main__.Case object at 0x71a995d8a980>, <__main__.Case object at 0x71a995d8aa10>, <__main__.Case object at 0x71a995d8aad0>, <__main__.Case object at 0x71a995d8abf0>, <__main__.Case object at 0x71a995d8ab90>, <__main__.Case object at 0x71a995d8ad10>, <__main__.Case object at 0x71a995d8add0>, <__main__.Case object at 0x71a995d8aef0>, <__main__.Case object at 0x71a995d8af80>, <__main__.Case object at 0x71a995d8b010>, <__main__.Case object at 0x71a995d8b0d0>, <__main__.Case object at 0x71a995d8b1f0>, <__main__.Case object at 0x71a995d8b280>, <__main__.Case object at 0x71a995d8b310>, <__main__.Case object at 0x71a995d8b3d0>, <__main__.Case object at 0x71a995d8b4f0>, <__main__.Case object at 0x71a995d8b580>, <__main__.Case object at 0x71a995d8b610>, <__main__.Case object at 0x71a995d8b6d0>, <__main__.Case object at 0x71a995d8b7f0>, <__main__.Case object at 0x71a995d8b880>, <__main__.Case object at 0x71a995d8b910>, <__main__.Case object at 0x71a995d8b9d0>, <__main__.Case object at 0x71a995d8baf0>, <__main__.Case object at 0x71a995d8bb80>, <__main__.Case object at 0x71a995d8bc10>, <__main__.Case object at 0x71a995d8bcd0>, <__main__.Case object at 0x71a995d8bdf0>, <__main__.Case object at 0x71a995d8be80>, <__main__.Case object at 0x71a995d8bf10>, <__main__.Case object at 0x71a995d8bfd0>, <__main__.Case object at 0x71a995d8af20>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d8c280>, <__main__.Case object at 0x71a995d8c3a0>, <__main__.Case object at 0x71a995d8b520>, <__main__.Case object at 0x71a995d8c520>, <__main__.Case object at 0x71a995d8c610>, <__main__.Case object at 0x71a995d8c700>]\n",
      "agent0 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d56020>, <__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7c730>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7c850>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7ca90>, <__main__.Case object at 0x71a995d7cb50>, <__main__.Case object at 0x71a995d7cbb0>, <__main__.Case object at 0x71a995d7c790>, <__main__.Case object at 0x71a995d7cd90>, <__main__.Case object at 0x71a995d7ce80>, <__main__.Case object at 0x71a995d7cee0>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7d0c0>, <__main__.Case object at 0x71a995d7d180>, <__main__.Case object at 0x71a995d7d1e0>, <__main__.Case object at 0x71a995d7c670>, <__main__.Case object at 0x71a995d7d3c0>, <__main__.Case object at 0x71a995d7d480>, <__main__.Case object at 0x71a995d7d4e0>, <__main__.Case object at 0x71a995d7c6a0>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7d780>, <__main__.Case object at 0x71a995d7d840>, <__main__.Case object at 0x71a995d7c700>, <__main__.Case object at 0x71a995d7da20>, <__main__.Case object at 0x71a995d7dae0>, <__main__.Case object at 0x71a995d7db40>, <__main__.Case object at 0x71a995d7c8e0>, <__main__.Case object at 0x71a995d7dd20>, <__main__.Case object at 0x71a995d7dde0>, <__main__.Case object at 0x71a995d7dea0>, <__main__.Case object at 0x71a995d7cc40>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7e110>, <__main__.Case object at 0x71a995d7e170>, <__main__.Case object at 0x71a995d7e2c0>, <__main__.Case object at 0x71a995d7e3b0>, <__main__.Case object at 0x71a995d7e4a0>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7e6e0>, <__main__.Case object at 0x71a995d7e7a0>, <__main__.Case object at 0x71a995d7e800>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d7e9e0>, <__main__.Case object at 0x71a995d7eaa0>, <__main__.Case object at 0x71a995d7eb00>, <__main__.Case object at 0x71a995d7d8a0>, <__main__.Case object at 0x71a995d7ece0>, <__main__.Case object at 0x71a995d7eda0>, <__main__.Case object at 0x71a995d7ee00>, <__main__.Case object at 0x71a995d7dbd0>, <__main__.Case object at 0x71a995d7efe0>, <__main__.Case object at 0x71a995d7f0a0>, <__main__.Case object at 0x71a995d7f100>, <__main__.Case object at 0x71a995d7df00>, <__main__.Case object at 0x71a995d7f2e0>, <__main__.Case object at 0x71a995d7f3a0>, <__main__.Case object at 0x71a995d7f400>, <__main__.Case object at 0x71a995d7f550>, <__main__.Case object at 0x71a995d7f610>, <__main__.Case object at 0x71a995d7f6d0>, <__main__.Case object at 0x71a995d7f730>, <__main__.Case object at 0x71a995d7e590>, <__main__.Case object at 0x71a995d7f910>, <__main__.Case object at 0x71a995d7f9d0>, <__main__.Case object at 0x71a995d7fa30>, <__main__.Case object at 0x71a995d7fb80>, <__main__.Case object at 0x71a995d7fc40>, <__main__.Case object at 0x71a995d7fd00>, <__main__.Case object at 0x71a995d7fdc0>, <__main__.Case object at 0x71a995d7eb90>, <__main__.Case object at 0x71a995d7ffa0>, <__main__.Case object at 0x71a995d880a0>, <__main__.Case object at 0x71a995d88100>, <__main__.Case object at 0x71a995d88040>, <__main__.Case object at 0x71a995d882e0>, <__main__.Case object at 0x71a995d883d0>, <__main__.Case object at 0x71a995d88490>, <__main__.Case object at 0x71a995d88160>, <__main__.Case object at 0x71a995d88640>, <__main__.Case object at 0x71a995d88700>, <__main__.Case object at 0x71a995d88760>, <__main__.Case object at 0x71a995d884c0>, <__main__.Case object at 0x71a995d88970>, <__main__.Case object at 0x71a995d88a60>, <__main__.Case object at 0x71a995d88b20>, <__main__.Case object at 0x71a995d887c0>, <__main__.Case object at 0x71a995d88cd0>, <__main__.Case object at 0x71a995d88dc0>, <__main__.Case object at 0x71a995d88e80>, <__main__.Case object at 0x71a995d88b50>, <__main__.Case object at 0x71a995d89030>, <__main__.Case object at 0x71a995d890f0>, <__main__.Case object at 0x71a995d89150>, <__main__.Case object at 0x71a995d88eb0>, <__main__.Case object at 0x71a995d89330>, <__main__.Case object at 0x71a995d893f0>, <__main__.Case object at 0x71a995d89450>, <__main__.Case object at 0x71a995d895a0>, <__main__.Case object at 0x71a995d89660>, <__main__.Case object at 0x71a995d89750>, <__main__.Case object at 0x71a995d897b0>, <__main__.Case object at 0x71a995d89900>, <__main__.Case object at 0x71a995d899f0>, <__main__.Case object at 0x71a995d89ab0>, <__main__.Case object at 0x71a995d89b10>, <__main__.Case object at 0x71a995d89c60>, <__main__.Case object at 0x71a995d89d50>, <__main__.Case object at 0x71a995d89e10>, <__main__.Case object at 0x71a995d89ed0>, <__main__.Case object at 0x71a995d88b80>, <__main__.Case object at 0x71a995d8a0b0>, <__main__.Case object at 0x71a995d8a170>, <__main__.Case object at 0x71a995d8a320>, <__main__.Case object at 0x71a995d8a350>, <__main__.Case object at 0x71a995d8a410>, <__main__.Case object at 0x71a995d8a4d0>, <__main__.Case object at 0x71a995d8a590>, <__main__.Case object at 0x71a995d891e0>, <__main__.Case object at 0x71a995d8a740>, <__main__.Case object at 0x71a995d8a830>, <__main__.Case object at 0x71a995d8a890>, <__main__.Case object at 0x71a995d894e0>, <__main__.Case object at 0x71a995d8aa70>, <__main__.Case object at 0x71a995d8ab30>, <__main__.Case object at 0x71a995d8acb0>, <__main__.Case object at 0x71a995d89840>, <__main__.Case object at 0x71a995d8ad70>, <__main__.Case object at 0x71a995d8ae30>, <__main__.Case object at 0x71a995d8ae90>, <__main__.Case object at 0x71a995d89ba0>, <__main__.Case object at 0x71a995d8b070>, <__main__.Case object at 0x71a995d8b130>, <__main__.Case object at 0x71a995d8b190>, <__main__.Case object at 0x71a995d89f30>, <__main__.Case object at 0x71a995d8b370>, <__main__.Case object at 0x71a995d8b430>, <__main__.Case object at 0x71a995d8b490>, <__main__.Case object at 0x71a995d8a290>, <__main__.Case object at 0x71a995d8b670>, <__main__.Case object at 0x71a995d8b730>, <__main__.Case object at 0x71a995d8b790>, <__main__.Case object at 0x71a995d8a5f0>, <__main__.Case object at 0x71a995d8b970>, <__main__.Case object at 0x71a995d8ba30>, <__main__.Case object at 0x71a995d8ba90>, <__main__.Case object at 0x71a995d8a920>, <__main__.Case object at 0x71a995d8bc70>, <__main__.Case object at 0x71a995d8bd30>, <__main__.Case object at 0x71a995d8bd90>, <__main__.Case object at 0x71a995d8ac20>, <__main__.Case object at 0x71a995d8bf70>, <__main__.Case object at 0x71a995d8c070>, <__main__.Case object at 0x71a995d8c0d0>, <__main__.Case object at 0x71a995d8c220>, <__main__.Case object at 0x71a995d8c2e0>, <__main__.Case object at 0x71a995d8c3d0>, <__main__.Case object at 0x71a995d8c430>, <__main__.Case object at 0x71a995d8c1f0>, <__main__.Case object at 0x71a995d8c640>, <__main__.Case object at 0x71a995d8c730>]\n",
      "agent1 comm temp case base: []\n",
      "Episode succeeded, case (5, 4) is empty. Temporary case base stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) is empty. Temporary case base stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) is empty. Temporary case base stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 0, 0.5)\n",
      "Episode succeeded, case (6, 6) is empty. Temporary case base stored to the case base: ((6, 6), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 0.5)\n",
      "Episode succeeded, case (5, 5) is empty. Temporary case base stored to the case base: ((5, 5), 4, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 3, 0.5)\n",
      "Episode succeeded, case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 0, 0.5)\n",
      "Episode succeeded, case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) is empty. Temporary case base stored to the case base: ((8, 6), 1, 0.5)\n",
      "Episode succeeded, case (9, 6) is empty. Temporary case base stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 4, 0.5)\n",
      "Episode succeeded, case (7, 6) is empty. Temporary case base stored to the case base: ((7, 6), 4, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 0, 0.5)\n",
      "Episode succeeded, case (8, 7) is empty. Temporary case base stored to the case base: ((8, 7), 1, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 2, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 0, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 2, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 0, 0.5)\n",
      "Episode succeeded, case (9, 5) is empty. Temporary case base stored to the case base: ((9, 5), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 4, 0.5)\n",
      "Episode succeeded, case (9, 4) is empty. Temporary case base stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 0, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 4, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 1, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 4, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 4, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 0, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 1, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 1, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 4, 0.5)\n",
      "Episode succeeded, case (9, 7) is empty. Temporary case base stored to the case base: ((9, 7), 1, 0.5)\n",
      "Episode succeeded, case (8, 7) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 7), 4, 0.5)\n",
      "Episode succeeded, case (7, 7) is empty. Temporary case base stored to the case base: ((7, 7), 4, 0.5)\n",
      "Episode succeeded, case (8, 7) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 7), 3, 0.5)\n",
      "Episode succeeded, case (7, 7) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 7), 4, 0.5)\n",
      "Episode succeeded, case (7, 8) is empty. Temporary case base stored to the case base: ((7, 8), 1, 0.5)\n",
      "Episode succeeded, case (6, 8) is empty. Temporary case base stored to the case base: ((6, 8), 4, 0.5)\n",
      "Episode succeeded, case (6, 8) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 8), 0, 0.5)\n",
      "Episode succeeded, case (6, 9) is empty. Temporary case base stored to the case base: ((6, 9), 1, 0.5)\n",
      "Episode succeeded, case (6, 9) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 9), 2, 0.5)\n",
      "Episode succeeded, case (5, 9) is empty. Temporary case base stored to the case base: ((5, 9), 4, 0.5)\n",
      "Episode succeeded, case (6, 9) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 9), 3, 0.5)\n",
      "Episode succeeded, case (7, 9) is empty. Temporary case base stored to the case base: ((7, 9), 3, 0.5)\n",
      "Episode succeeded, case (7, 8) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 8), 2, 0.5)\n",
      "Episode succeeded, case (7, 9) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 9), 1, 0.5)\n",
      "Episode succeeded, case (7, 9) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 9), 0, 0.5)\n",
      "Episode succeeded, case (8, 9) is empty. Temporary case base stored to the case base: ((8, 9), 3, 0.5)\n",
      "Episode succeeded, case (8, 9) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 9), 0, 0.5)\n",
      "Episode succeeded, case (9, 9) is empty. Temporary case base stored to the case base: ((9, 9), 3, 0.5)\n",
      "Episode succeeded, case (9, 8) is empty. Temporary case base stored to the case base: ((9, 8), 2, 0.5)\n",
      "Episode succeeded, case (9, 8) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 8), 4, 0.5)\n",
      "Episode succeeded, case (9, 8) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 8), 0, 0.5)\n",
      "Episode succeeded, case (8, 8) is empty. Temporary case base stored to the case base: ((8, 8), 4, 0.5)\n",
      "Episode succeeded, case (8, 7) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 7), 2, 0.5)\n",
      "Episode succeeded, case (8, 7) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 7), 0, 0.5)\n",
      "Episode succeeded, case (9, 7) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 7), 3, 0.5)\n",
      "Episode succeeded, case (9, 7) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 7), 4, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 2, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 4, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 0, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 4, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 2, 0.5)\n",
      "Episode succeeded, case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 4, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 4, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 1, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 6), 4, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 0, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 0, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 1, 0.5)\n",
      "Episode succeeded, case (9, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 5), 4, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 4, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 0, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 3, 0.5)\n",
      "Episode succeeded, case (9, 3) is empty. Temporary case base stored to the case base: ((9, 3), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 4), 1, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 4, 0.5)\n",
      "Episode succeeded, case (7, 4) is empty. Temporary case base stored to the case base: ((7, 4), 4, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 3, 0.5)\n",
      "Episode succeeded, case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 1, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 0, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 0, 0.5)\n",
      "Episode succeeded, case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 1, 0.5)\n",
      "Episode succeeded, case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 4, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 1, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 0, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 4, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 1, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 0, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 0, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 1, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 0, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 3, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 1, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 4, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 0, 0.5)\n",
      "Episode succeeded, case (7, 3) is empty. Temporary case base stored to the case base: ((7, 3), 4, 0.5)\n",
      "Episode succeeded, case (7, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 3), 0, 0.5)\n",
      "Episode succeeded, case (7, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 3), 0, 0.5)\n",
      "Episode succeeded, case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 3, 0.5)\n",
      "Episode succeeded, case (7, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 2), 4, 0.5)\n",
      "Episode succeeded, case (7, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 3), 1, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 3, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 3, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 4, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 0, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 3), 1, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 1, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 4, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 4, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 4, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 3, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 2), 0, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 4, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 0, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 1, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 1, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 1, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 0, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 4, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 0, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 1, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 4, 0.5)\n",
      "Episode succeeded, case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 4, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 0, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 0), 1, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 1, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 1, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 1, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 0, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 2, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 0), 0, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 4, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 0, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 6), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 6), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 6), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 7), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 7), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 7), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 8), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 8), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 9), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (5, 9), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 9), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 9), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 9), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 8), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 8), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 3), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 4), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 1), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 2), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 3), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 0), solution: 4, tv: 0.5\n",
      "Episode: 0, Total Steps: 168, Total Rewards: [-108, -67], Status Episode: False\n",
      "------------------------------------------End of episode 0 loop--------------------\n",
      "----- starting point of Episode 1 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 0) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.5, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 0) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 0.5, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 0.5, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (9, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 0.5, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (9, 3) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 0.5, 74)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 0.5, 99)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 0.5, 157)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.5, 159)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.5, 165)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.5, 166)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 0.5, 167)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 1 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 1 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 1 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 1 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 1 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 1 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 1 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 1 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 1 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 1 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 1 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 1 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 1 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 6) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 7) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (5, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 9) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 8) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7cc70>, <__main__.Case object at 0x71a995d7ceb0>, <__main__.Case object at 0x71a995d7d0f0>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7d5d0>, <__main__.Case object at 0x71a995d7d870>, <__main__.Case object at 0x71a995d7d9f0>, <__main__.Case object at 0x71a995d7dc90>, <__main__.Case object at 0x71a995d7d330>, <__main__.Case object at 0x71a995d7dfc0>, <__main__.Case object at 0x71a995d7e140>, <__main__.Case object at 0x71a995d7e470>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7e8c0>, <__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d7ebf0>, <__main__.Case object at 0x71a995d7ed10>, <__main__.Case object at 0x71a995d7eef0>, <__main__.Case object at 0x71a995d7f160>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d56020>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d7cb80>, <__main__.Case object at 0x71a995d7cdc0>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7d360>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7d720>]\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 4) is empty. Temporary case base stored to the case base: ((5, 4), 3, 0.5)\n",
      "Integrated case process. comm case (6, 4) is empty. Temporary case base stored to the case base: ((6, 4), 3, 0.5)\n",
      "Integrated case process. comm case (6, 5) is empty. Temporary case base stored to the case base: ((6, 5), 1, 0.5)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 0.5)\n",
      "Integrated case process. comm case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 3, 0.5)\n",
      "Integrated case process. comm case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 2, 0.5)\n",
      "Integrated case process. comm case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 0.5)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 2, 0.5)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 0.5)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 0.5)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d7cac0>, <__main__.Case object at 0x71a995d7cd00>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7d4b0>, <__main__.Case object at 0x71a995d7d660>, <__main__.Case object at 0x71a995d7d900>, <__main__.Case object at 0x71a995d7da80>, <__main__.Case object at 0x71a995d7dd80>, <__main__.Case object at 0x71a995d7df60>, <__main__.Case object at 0x71a995d7e080>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7dc30>, <__main__.Case object at 0x71a995d7e5f0>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7e950>, <__main__.Case object at 0x71a995d7ea10>, <__main__.Case object at 0x71a995d7e3e0>, <__main__.Case object at 0x71a995d7eec0>, <__main__.Case object at 0x71a995d7ef80>, <__main__.Case object at 0x71a995d7f040>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 0.6, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.6, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.6, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 1, tv: 0.09999999999999998, time steps: 163\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.09999999999999998, time steps: 161\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.6, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 0.6, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 1, tv: 0.09999999999999998, time steps: 156\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 0.09999999999999998, time steps: 155\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 4, tv: 0.09999999999999998, time steps: 153\n",
      "case content after REVISE for agent 1, problem: (8, 7), solution: 1, tv: 0.09999999999999998, time steps: 150\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 3, tv: 0.09999999999999998, time steps: 145\n",
      "case content after REVISE for agent 1, problem: (9, 4), solution: 2, tv: 0.09999999999999998, time steps: 143\n",
      "case content after REVISE for agent 1, problem: (9, 7), solution: 1, tv: 0.09999999999999998, time steps: 131\n",
      "case content after REVISE for agent 1, problem: (7, 7), solution: 4, tv: 0.09999999999999998, time steps: 129\n",
      "case content after REVISE for agent 1, problem: (7, 8), solution: 1, tv: 0.09999999999999998, time steps: 126\n",
      "case content after REVISE for agent 1, problem: (6, 8), solution: 4, tv: 0.09999999999999998, time steps: 125\n",
      "case content after REVISE for agent 1, problem: (6, 9), solution: 1, tv: 0.09999999999999998, time steps: 123\n",
      "case content after REVISE for agent 1, problem: (5, 9), solution: 4, tv: 0.09999999999999998, time steps: 121\n",
      "case content after REVISE for agent 1, problem: (7, 9), solution: 3, tv: 0.09999999999999998, time steps: 119\n",
      "case content after REVISE for agent 1, problem: (8, 9), solution: 3, tv: 0.09999999999999998, time steps: 115\n",
      "case content after REVISE for agent 1, problem: (9, 9), solution: 3, tv: 0.09999999999999998, time steps: 113\n",
      "case content after REVISE for agent 1, problem: (9, 8), solution: 2, tv: 0.09999999999999998, time steps: 112\n",
      "case content after REVISE for agent 1, problem: (8, 8), solution: 4, tv: 0.09999999999999998, time steps: 109\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 0.6, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 2, tv: 0.09999999999999998, time steps: 79\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 4, tv: 0.09999999999999998, time steps: 76\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 0.6, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 0.6, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 0.6, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 2, tv: 0.09999999999999998, time steps: 67\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 4, tv: 0.09999999999999998, time steps: 65\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 0.6, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 1, tv: 0.09999999999999998, time steps: 62\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 4, tv: 0.09999999999999998, time steps: 48\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 2, tv: 0.09999999999999998, time steps: 45\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.6, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 4, tv: 0.09999999999999998, time steps: 12\n",
      "Episode succeeded, case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 4, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 4, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.5\n",
      "Episode: 1, Total Steps: 25, Total Rewards: [-124, 90], Status Episode: False\n",
      "------------------------------------------End of episode 1 loop--------------------\n",
      "----- starting point of Episode 2 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.5\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 0.6, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 0.6, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 0.6, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.5\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 0.6, 74)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 0.6, 99)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (1, 0): pull reward: -0.5\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 0.6, 157)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: -0.031128874149274566\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.6, 159)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.6, 165)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.09197675146838602\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 2) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.6, 166)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: -0.48666660302878295\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 2) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 0.6, 167)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: -0.13364426160612553\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 11 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 3) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 3) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 3) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 3) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 3) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 3) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.5, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 4 to next state (1, 3): pull reward: 0.4714146175938039\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7c760>, <__main__.Case object at 0x71a995d7cfd0>, <__main__.Case object at 0x71a995d7d5d0>, <__main__.Case object at 0x71a995d7d330>, <__main__.Case object at 0x71a995d7e470>, <__main__.Case object at 0x71a995d7e8c0>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7cac0>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7d900>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7d870>, <__main__.Case object at 0x71a995d7cd30>, <__main__.Case object at 0x71a995d7e140>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7ee60>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7d660>]\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.5, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.5, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.5, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.5, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.5, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.5, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.5, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.5, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.5, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.5, time steps: 24\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.5\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7caf0>, <__main__.Case object at 0x71a995d7c940>, <__main__.Case object at 0x71a995d7db10>, <__main__.Case object at 0x71a995d7dfc0>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7ebc0>, <__main__.Case object at 0x71a995d7f160>, <__main__.Case object at 0x71a995d7cd00>, <__main__.Case object at 0x71a995d7d4b0>, <__main__.Case object at 0x71a995d7dd80>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 0.7, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.7, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.7, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.7, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 0.7, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 0.7, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 0.7, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 0.7, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 0.7, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 0.7, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.7, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.6, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.7\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.7\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.7\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.7\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.7\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "Episode: 2, Total Steps: 12, Total Rewards: [-111, 90], Status Episode: False\n",
      "------------------------------------------End of episode 2 loop--------------------\n",
      "----- starting point of Episode 3 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.7, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.3284271247461903\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 0.7, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.2639320225002102\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 0.7, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 1): pull reward: 0.43329233976779524\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 0.7, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 4 to next state (3, 1): pull reward: 0.2216368076478048\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 0.7, 74)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 1 to next state (3, 0): pull reward: -0.48041398272464053\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 0.7, 99)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 3 to next state (2, 0): pull reward: -0.1745151646909595\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 0.7, 157)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.7, 159)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0.1745151646909595\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.7, 165)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.7, 166)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 0.7, 167)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 4 to next state (4, 0): pull reward: 0.06155281280883029\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 11 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 12 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 3 to next state (3, 0): pull reward: -0.06155281280883029\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 13 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 14 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 3 to next state (2, 0): pull reward: -0.1745151646909595\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 15 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 16 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 1): pull reward: 0.43329233976779524\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 17 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 3 to next state (1, 1): pull reward: -0.3185447058276478\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 18 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 19 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.3786796564403576\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 20 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.3786796564403576\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 21 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0.3185447058276478\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 22 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 3 to next state (1, 1): pull reward: -0.3185447058276478\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 23 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.3786796564403576\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 24 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 25 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.3284271247461903\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 26 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.3284271247461903\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 27 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 28 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.3284271247461903\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 29 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 30 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 31 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.3284271247461903\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 32 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 33 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.2639320225002102\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 34 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (1, 0): pull reward: -0.2639320225002102\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 35 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.3786796564403576\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 36 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: 0.3185447058276478\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 37 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 2) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 3, 0.6, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 4 to next state (2, 2): pull reward: 0.3885620753588994\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7df60>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7ee60>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7e650>, <__main__.Case object at 0x71a995d7ce50>, <__main__.Case object at 0x71a995d7dc00>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7c910>, <__main__.Case object at 0x71a995d7f580>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7fbe0>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7fac0>, <__main__.Case object at 0x71a995d7e3e0>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7dff0>, <__main__.Case object at 0x71a995d7f3d0>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7e800>, <__main__.Case object at 0x71a995d7ccd0>, <__main__.Case object at 0x71a995d7c970>, <__main__.Case object at 0x71a995d7cbe0>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7d300>, <__main__.Case object at 0x71a995d7d390>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7d930>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7e290>, <__main__.Case object at 0x71a995d7e770>, <__main__.Case object at 0x71a995d7e830>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d7cc70>, <__main__.Case object at 0x71a995d7e0b0>, <__main__.Case object at 0x71a995d7f1f0>, <__main__.Case object at 0x71a995d7ceb0>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7ed10>, <__main__.Case object at 0x71a995d7f820>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7d960>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7f0d0>, <__main__.Case object at 0x71a995d7eef0>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7dc30>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d7f1c0>, <__main__.Case object at 0x71a995d7fdc0>, <__main__.Case object at 0x71a995d7f370>, <__main__.Case object at 0x71a995d7cbb0>, <__main__.Case object at 0x71a995d7eb60>, <__main__.Case object at 0x71a995d7cb20>, <__main__.Case object at 0x71a995d7ce20>, <__main__.Case object at 0x71a995d7d150>, <__main__.Case object at 0x71a995d7cc10>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7d810>, <__main__.Case object at 0x71a995d7db70>, <__main__.Case object at 0x71a995d7c940>, <__main__.Case object at 0x71a995d7df90>, <__main__.Case object at 0x71a995d7e1a0>, <__main__.Case object at 0x71a995d7e530>, <__main__.Case object at 0x71a995d7fa00>]\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.5, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.5, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.5, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.5, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.5, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.5, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.5, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.5, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.5, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.5, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.5, time steps: 24\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.5\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d7d0f0>, <__main__.Case object at 0x71a995d7e980>, <__main__.Case object at 0x71a995d7d9f0>, <__main__.Case object at 0x71a995d7dc90>, <__main__.Case object at 0x71a995d7ebf0>, <__main__.Case object at 0x71a995d7d5a0>, <__main__.Case object at 0x71a995d7db10>, <__main__.Case object at 0x71a995d7e7d0>, <__main__.Case object at 0x71a995d7d7b0>, <__main__.Case object at 0x71a995d7f880>, <__main__.Case object at 0x71a995d7e890>, <__main__.Case object at 0x71a995d7fc70>, <__main__.Case object at 0x71a995d7fe80>, <__main__.Case object at 0x71a995d7f7c0>, <__main__.Case object at 0x71a995d7ef80>, <__main__.Case object at 0x71a995d7ea10>, <__main__.Case object at 0x71a995d7e5c0>, <__main__.Case object at 0x71a995d7de10>, <__main__.Case object at 0x71a995d7f4c0>, <__main__.Case object at 0x71a995d7f310>, <__main__.Case object at 0x71a995d7f550>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7ca30>, <__main__.Case object at 0x71a995d7cd60>, <__main__.Case object at 0x71a995d7d000>, <__main__.Case object at 0x71a995d7d090>, <__main__.Case object at 0x71a995d7d450>, <__main__.Case object at 0x71a995d7d750>, <__main__.Case object at 0x71a995d7d9c0>, <__main__.Case object at 0x71a995d7c8e0>, <__main__.Case object at 0x71a995d7de70>, <__main__.Case object at 0x71a995d7e0e0>, <__main__.Case object at 0x71a995d7e350>, <__main__.Case object at 0x71a995d7e440>, <__main__.Case object at 0x71a995d7e9b0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 0.7999999999999999, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.7999999999999999, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.7999999999999999, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.7999999999999999, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 0.7999999999999999, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 0.7999999999999999, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 0.7999999999999999, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 0.7999999999999999, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 0.7999999999999999, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 0.7999999999999999, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.7999999999999999, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.7, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.7\n",
      "Episode: 3, Total Steps: 38, Total Rewards: [-137, 90], Status Episode: False\n",
      "------------------------------------------End of episode 3 loop--------------------\n",
      "----- starting point of Episode 4 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.7999999999999999, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.3284271247461903\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 0.7999999999999999, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.3786796564403576\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 0.7999999999999999, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0.3185447058276478\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 0.7999999999999999, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 1 to next state (2, 0): pull reward: -0.43329233976779524\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 0.7999999999999999, 74)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0.1745151646909595\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 0.7999999999999999, 99)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 0.7999999999999999, 157)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.7999999999999999, 159)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 2 to next state (3, 1): pull reward: 0.48041398272464053\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.7999999999999999, 165)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 1 to next state (3, 0): pull reward: -0.48041398272464053\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.7999999999999999, 166)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 2 to next state (3, 1): pull reward: 0.48041398272464053\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 0.7999999999999999, 167)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 4 to next state (4, 1): pull reward: 0.08113883008418976\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7e9b0>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7e8c0>, <__main__.Case object at 0x71a995d7cd00>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7ef50>, <__main__.Case object at 0x71a995d7e710>, <__main__.Case object at 0x71a995d7f370>, <__main__.Case object at 0x71a995d7cb20>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7f250>, <__main__.Case object at 0x71a995d7eb60>]\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.5, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.5, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.5, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.5, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.5, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.5, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.5, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.5, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.5, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.5, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.5, time steps: 24\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.5\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7fa00>, <__main__.Case object at 0x71a995d7d5d0>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7f790>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d7ec50>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7cbb0>, <__main__.Case object at 0x71a995d7d150>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 0.8999999999999999, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.8999999999999999, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.8999999999999999, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.8999999999999999, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 0.8999999999999999, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 0.8999999999999999, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 0.8999999999999999, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 0.8999999999999999, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 0.8999999999999999, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 0.8999999999999999, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.8999999999999999, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.29999999999999993, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.8999999999999999\n",
      "Episode: 4, Total Steps: 11, Total Rewards: [-110, 90], Status Episode: False\n",
      "------------------------------------------End of episode 4 loop--------------------\n",
      "----- starting point of Episode 5 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.8999999999999999, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 0.8999999999999999, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.3284271247461903\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 0.8999999999999999, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 0.8999999999999999, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.2639320225002102\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 0.8999999999999999, 74)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 0.8999999999999999, 99)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 1): pull reward: 0.43329233976779524\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 0.8999999999999999, 157)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 4 to next state (3, 1): pull reward: 0.2216368076478048\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.8999999999999999, 159)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 2 to next state (3, 2): pull reward: 0.46310484133429486\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.8999999999999999, 165)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 1 to next state (3, 1): pull reward: -0.46310484133429486\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.8999999999999999, 166)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 0.8999999999999999, 167)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 3 to next state (2, 1): pull reward: -0.2216368076478048\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 11 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 12 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 13 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 4 to next state (3, 1): pull reward: 0.2216368076478048\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 14 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 2 to next state (3, 2): pull reward: 0.46310484133429486\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 15 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 4 to next state (4, 2): pull reward: 0.1180339887498949\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 16 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 2) with action 0 to next state (4, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 17 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 2) with action 3 to next state (3, 2): pull reward: -0.1180339887498949\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 18 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 2 to next state (3, 3): pull reward: 0.41092720756334733\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 19 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 3) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 3) with action 2 to next state (3, 4): pull reward: 0.20710678118654757\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 20 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 4) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 4) with action 0 to next state (3, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 21 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 4) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 4) with action 1 to next state (3, 3): pull reward: -0.20710678118654757\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 22 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 3) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 3) with action 0 to next state (3, 3): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 23 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 3) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 3) with action 1 to next state (3, 2): pull reward: -0.41092720756334733\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 24 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 1 to next state (3, 1): pull reward: -0.46310484133429486\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 25 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 26 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 2 to next state (3, 2): pull reward: 0.46310484133429486\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 27 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 0 to next state (3, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 28 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 2 to next state (3, 3): pull reward: 0.41092720756334733\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 29 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 3) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 3) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 3) with action 1 to next state (3, 2): pull reward: -0.41092720756334733\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 5 in steps 30 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 3 to next state (2, 2): pull reward: -0.29617957362320024\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7cc10>, <__main__.Case object at 0x71a995d7d6f0>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7fdc0>, <__main__.Case object at 0x71a995d7fa00>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d7cbb0>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7e3e0>, <__main__.Case object at 0x71a995d7e5f0>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7faf0>, <__main__.Case object at 0x71a995d7dd50>, <__main__.Case object at 0x71a995d7ebc0>, <__main__.Case object at 0x71a995d7d330>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7c940>, <__main__.Case object at 0x71a995d7c700>, <__main__.Case object at 0x71a995d7d3c0>, <__main__.Case object at 0x71a995d7feb0>, <__main__.Case object at 0x71a995d7e800>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7cd90>, <__main__.Case object at 0x71a995d7d480>, <__main__.Case object at 0x71a995d7d930>, <__main__.Case object at 0x71a995d7dcf0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7eef0>, <__main__.Case object at 0x71a995d7cc70>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7f340>]\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.5, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.5, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.5, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.5, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.5, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.5, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.5, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.5, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.5, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.5, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.5, time steps: 24\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.5\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7c790>, <__main__.Case object at 0x71a995d7f820>, <__main__.Case object at 0x71a995d7f250>, <__main__.Case object at 0x71a995d7e0b0>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7cee0>, <__main__.Case object at 0x71a995d7ed10>, <__main__.Case object at 0x71a995d7ec50>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7d5d0>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7c910>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7ceb0>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7e530>, <__main__.Case object at 0x71a995d7e2c0>, <__main__.Case object at 0x71a995d7f5b0>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7d180>, <__main__.Case object at 0x71a995d7d210>, <__main__.Case object at 0x71a995d7df90>, <__main__.Case object at 0x71a995d7ca90>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7d1e0>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7dea0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 0.9999999999999999, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.9999999999999999, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 0.9999999999999999, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.9999999999999999, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 0.9999999999999999, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 0.9999999999999999, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 0.9999999999999999, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 0.9999999999999999, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 0.9999999999999999, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 0.9999999999999999, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 0.9999999999999999, time steps: 24\n",
      "Episode succeeded, case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 4, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 4, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 4, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 4, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (4, 4), solution: 2, tv: 0.5\n",
      "Episode: 5, Total Steps: 31, Total Rewards: [-130, 90], Status Episode: False\n",
      "------------------------------------------End of episode 5 loop--------------------\n",
      "----- starting point of Episode 6 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 0.9999999999999999, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 0.9999999999999999, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 0.9999999999999999, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.3284271247461903\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 1) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 0.9999999999999999, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0.2639320225002102\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 2) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 0.9999999999999999, 74)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 2) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 0.9999999999999999, 99)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: -0.2639320225002102\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 0.9999999999999999, 157)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 0.9999999999999999, 159)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 1) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 1, 0.9999999999999999, 165)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0.2639320225002102\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 2) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.9999999999999999, 166)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0.43329233976779524\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 2) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 0.9999999999999999, 167)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: -0.3185447058276478\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 11 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 2, 0.5, 30)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.3786796564403576\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 12 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 2, 0.5, 30)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.3786796564403576\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 13 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 2, 0.5, 30)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: 0.3185447058276478\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 14 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 2) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 2, 0.5, 30)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 2 to next state (1, 3): pull reward: 0.2216368076478048\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7cc70>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7ef50>, <__main__.Case object at 0x71a995d7dc30>, <__main__.Case object at 0x71a995d7fbe0>, <__main__.Case object at 0x71a995d7f580>, <__main__.Case object at 0x71a995d7c760>, <__main__.Case object at 0x71a995d7c700>, <__main__.Case object at 0x71a995d7feb0>, <__main__.Case object at 0x71a995d7d480>, <__main__.Case object at 0x71a995d7d150>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7eef0>, <__main__.Case object at 0x71a995d7e8c0>, <__main__.Case object at 0x71a995d7f790>, <__main__.Case object at 0x71a995d7c7c0>, <__main__.Case object at 0x71a995d7dc00>, <__main__.Case object at 0x71a995d7e650>, <__main__.Case object at 0x71a995d7c940>, <__main__.Case object at 0x71a995d7e800>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7dcf0>]\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.5, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.5, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.5, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.5, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.5, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.5, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.5, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.5, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.5, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.5, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.5, time steps: 24\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.5\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7eb60>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7cc10>, <__main__.Case object at 0x71a995d7e6b0>, <__main__.Case object at 0x71a995d7cb20>, <__main__.Case object at 0x71a995d7db40>, <__main__.Case object at 0x71a995d7ea40>, <__main__.Case object at 0x71a995d7d900>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7d3c0>, <__main__.Case object at 0x71a995d7cbe0>, <__main__.Case object at 0x71a995d7d930>, <__main__.Case object at 0x71a995d7d1b0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 2, tv: 0.6, time steps: 30\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 2, tv: 0.6\n",
      "Episode: 6, Total Steps: 15, Total Rewards: [-114, 90], Status Episode: False\n",
      "------------------------------------------End of episode 6 loop--------------------\n",
      "----- starting point of Episode 7 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.3284271247461903\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.3284271247461903\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.3284271247461903\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.2639320225002102\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0.1745151646909595\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 1, 1, 165)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 4 to next state (4, 0): pull reward: 0.06155281280883029\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 166)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 167)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 4 to next state (5, 0): pull reward: -0.06155281280883029\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 11 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (5, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (5, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (5, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (5, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (5, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (5, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (5, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (5, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (5, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (5, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (5, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (5, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 2, 0.6, 30)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 0) with action 2 to next state (5, 1): pull reward: 0.48041398272464053\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 12 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (5, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 2, 0.6, 30)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 1) with action 3 to next state (4, 1): pull reward: 0.08113883008418976\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d7f250>, <__main__.Case object at 0x71a995d7f0d0>, <__main__.Case object at 0x71a995d7fdc0>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7d780>, <__main__.Case object at 0x71a995d7eaa0>, <__main__.Case object at 0x71a995d7fa00>, <__main__.Case object at 0x71a995d7dde0>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7cc10>, <__main__.Case object at 0x71a995d7d330>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7cbb0>, <__main__.Case object at 0x71a995d7cd90>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7e3e0>]\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.5, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.5, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.5, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.5, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.5, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.5, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.5, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.5, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.5, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.5, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.5, time steps: 24\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.5\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d7c790>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7dd50>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d7f580>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7f1c0>, <__main__.Case object at 0x71a995d7cb20>, <__main__.Case object at 0x71a995d7fee0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 2, tv: 0.7, time steps: 30\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 2, tv: 0.7\n",
      "Episode: 7, Total Steps: 13, Total Rewards: [-112, 90], Status Episode: False\n",
      "------------------------------------------End of episode 7 loop--------------------\n",
      "----- starting point of Episode 8 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.3284271247461903\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.3786796564403576\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0.3185447058276478\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 1 to next state (2, 0): pull reward: -0.43329233976779524\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 1): pull reward: 0.43329233976779524\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 4 to next state (3, 1): pull reward: 0.2216368076478048\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 1, 1, 165)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 1 to next state (3, 0): pull reward: -0.48041398272464053\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 166)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 2 to next state (3, 1): pull reward: 0.48041398272464053\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 167)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 3 to next state (2, 1): pull reward: -0.2216368076478048\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 11 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 2, 0.7, 30)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 3 to next state (1, 1): pull reward: -0.3185447058276478\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 12 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 2, 0.7, 30)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: 0.3185447058276478\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 13 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 2) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 2, 0.7, 30)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 4 to next state (2, 2): pull reward: 0.3885620753588994\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7fdc0>, <__main__.Case object at 0x71a995d7eaa0>, <__main__.Case object at 0x71a995d7dde0>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7e650>, <__main__.Case object at 0x71a995d7dc30>, <__main__.Case object at 0x71a995d7d480>, <__main__.Case object at 0x71a995d7d960>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d7eef0>, <__main__.Case object at 0x71a995d7f0d0>, <__main__.Case object at 0x71a995d7d780>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7f790>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7f370>, <__main__.Case object at 0x71a995d7c7c0>]\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.5, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.5, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.5, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.5, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.5, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.5, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.5, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.5, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.5, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.5, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.5, time steps: 24\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.5\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7faf0>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7c700>, <__main__.Case object at 0x71a995d7f250>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7fa00>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7c760>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7dff0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 2, tv: 0.7999999999999999, time steps: 30\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 2, tv: 0.7999999999999999\n",
      "Episode: 8, Total Steps: 14, Total Rewards: [-113, 90], Status Episode: False\n",
      "------------------------------------------End of episode 8 loop--------------------\n",
      "----- starting point of Episode 9 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.3284271247461903\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: -0.3284271247461903\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.3284271247461903\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.2639320225002102\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (1, 0): pull reward: -0.2639320225002102\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 1, 1, 165)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0.3786796564403576\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 166)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: -0.3786796564403576\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 167)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 11 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 2, 0.7999999999999999, 30)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 12 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 2, 0.7999999999999999, 30)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.3786796564403576\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 13 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 2, 0.7999999999999999, 30)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0.3185447058276478\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 14 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 2, 0.7999999999999999, 30)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 4 to next state (3, 1): pull reward: 0.2216368076478048\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 15 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 2, 0.7999999999999999, 30)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 2 to next state (3, 2): pull reward: 0.46310484133429486\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 16 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 2, 0.7999999999999999, 30)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 4 to next state (4, 2): pull reward: 0.1180339887498949\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 17 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 2, 0.7999999999999999, 30)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 2) with action 2 to next state (4, 3): pull reward: 0.5\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 18 loop -----\n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (4, 3) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (4, 3) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (4, 3) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (4, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (4, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (4, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (4, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (4, 3) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (4, 3) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (4, 3) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (4, 3) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 2, 0.7999999999999999, 30)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 3) with action 2 to next state (4, 4): pull reward: 0.5\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7eef0>, <__main__.Case object at 0x71a995d7d780>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7fdc0>, <__main__.Case object at 0x71a995d7dde0>, <__main__.Case object at 0x71a995d7d480>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7e8c0>, <__main__.Case object at 0x71a995d7feb0>, <__main__.Case object at 0x71a995d7f1c0>, <__main__.Case object at 0x71a995d7cd00>, <__main__.Case object at 0x71a995d7e5f0>, <__main__.Case object at 0x71a995d7cbe0>, <__main__.Case object at 0x71a995d7da20>, <__main__.Case object at 0x71a995d7ca90>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d7c940>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d7d150>, <__main__.Case object at 0x71a995d7d900>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7ea40>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7eb60>, <__main__.Case object at 0x71a995d7cf10>]\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.09999999999999998, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.09999999999999998, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.09999999999999998, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.09999999999999998, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.09999999999999998, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.09999999999999998, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.09999999999999998, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.09999999999999998, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.09999999999999998, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.09999999999999998, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.09999999999999998, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.09999999999999998, time steps: 24\n",
      "Episode succeeded, case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 4, 0.5)\n",
      "Episode succeeded, case (0, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 1), 0, 0.5)\n",
      "Episode succeeded, case (0, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 1), 3, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 3, 0.5)\n",
      "Episode succeeded, case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 3, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 1, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 0, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 1), 1, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 0, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (2, 0), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d7f3d0>, <__main__.Case object at 0x71a995d7f0d0>, <__main__.Case object at 0x71a995d7c790>, <__main__.Case object at 0x71a995d7f370>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7eaa0>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7d960>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7cc70>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7f5e0>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7f2e0>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7d390>, <__main__.Case object at 0x71a995d7d180>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 2, tv: 0.8999999999999999, time steps: 30\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 2, tv: 0.8999999999999999\n",
      "Episode: 9, Total Steps: 19, Total Rewards: [82, 90], Status Episode: True\n",
      "------------------------------------------End of episode 9 loop--------------------\n",
      "----- starting point of Episode 10 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.5, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 0.5, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 0.5, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 0.5, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 0.5, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 0.5, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 0.5, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7c970>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7d150>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7d780>, <__main__.Case object at 0x71a995d7d480>, <__main__.Case object at 0x71a995d7cd90>, <__main__.Case object at 0x71a995d7dcf0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d7ea40>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7cc10>, <__main__.Case object at 0x71a995d7d510>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 0.09999999999999998, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 3, tv: 0.09999999999999998, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "Episode succeeded, case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 4, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 2, 1)\n",
      "Integrated case process. comm case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 1)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 2, 1)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.5\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d7c940>, <__main__.Case object at 0x71a995d7e200>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7eb60>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7fa00>, <__main__.Case object at 0x71a995d7ef50>, <__main__.Case object at 0x71a995d7c9a0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d7d6f0>, <__main__.Case object at 0x71a995d7e650>, <__main__.Case object at 0x71a995d7d900>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7e8f0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 2, tv: 0.4999999999999999, time steps: 30\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 0.5)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 0.5)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 0.5)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 0.5)\n",
      "Integrated case process. comm case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 4, 0.5)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 4, 0.5)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 0.5)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 2, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5\n",
      "Episode: 10, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 10 loop--------------------\n",
      "----- starting point of Episode 11 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.6, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 0.6, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 0.6, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 0.6, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 0.6, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 0.6, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 0.6, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 0.6, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 0.5, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 0.5, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 0.5, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7c970>, <__main__.Case object at 0x71a995d7d780>, <__main__.Case object at 0x71a995d7cee0>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7e230>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d56020>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7e3e0>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7e530>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d7e9b0>, <__main__.Case object at 0x71a995d7d390>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.7, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 0.7, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.7, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.6, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.6, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.6, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.6, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.6, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.6, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7d150>, <__main__.Case object at 0x71a995d7cd90>, <__main__.Case object at 0x71a995d7cc10>, <__main__.Case object at 0x71a995d7f370>, <__main__.Case object at 0x71a995d7d8d0>, <__main__.Case object at 0x71a995d7c910>, <__main__.Case object at 0x71a995d7eec0>, <__main__.Case object at 0x71a995d7e0b0>, <__main__.Case object at 0x71a995d7d3c0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7d480>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7f3d0>, <__main__.Case object at 0x71a995d7e2c0>, <__main__.Case object at 0x71a995d7ce50>, <__main__.Case object at 0x71a995d7fbb0>, <__main__.Case object at 0x71a995d7e920>, <__main__.Case object at 0x71a995d7d930>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 2, tv: 0.09999999999999987, time steps: 30\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.09999999999999998, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.09999999999999998, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.09999999999999998, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.09999999999999998, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.09999999999999998, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 4, tv: 0.09999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.09999999999999998, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.09999999999999998, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 11, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 11 loop--------------------\n",
      "----- starting point of Episode 12 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.7, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 0.7, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 0.7, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 0.7, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 0.7, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 0.7, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 0.7, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 0.7, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 0.6, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 0.6, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 0.6, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7fa00>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7fbb0>, <__main__.Case object at 0x71a995d7d780>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7f370>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7d390>, <__main__.Case object at 0x71a995d7d480>, <__main__.Case object at 0x71a995d7ce50>, <__main__.Case object at 0x71a995d7c970>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7cc10>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.7999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.7999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 0.7999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 0.7999999999999999, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.7999999999999999, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.7, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.19999999999999996, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.19999999999999996, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.19999999999999996, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.19999999999999996, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.19999999999999996, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.19999999999999996, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.19999999999999996, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.7\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d7db40>, <__main__.Case object at 0x71a995d7ceb0>, <__main__.Case object at 0x71a995d7f5e0>, <__main__.Case object at 0x71a995d7e2c0>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7c700>, <__main__.Case object at 0x71a995d7cd90>, <__main__.Case object at 0x71a995d7c910>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7e9b0>, <__main__.Case object at 0x71a995d7f3d0>, <__main__.Case object at 0x71a995d7e920>, <__main__.Case object at 0x71a995d7cee0>, <__main__.Case object at 0x71a995d7fbe0>, <__main__.Case object at 0x71a995d7d150>, <__main__.Case object at 0x71a995d7d8d0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 0.6)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 0.7)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 0.7)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 0.7)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 0.7)\n",
      "Integrated case process. comm case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 4, 0.7)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 4, 0.7)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 0.7)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.7)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7\n",
      "Episode: 12, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 12 loop--------------------\n",
      "----- starting point of Episode 13 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.7999999999999999, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 0.7999999999999999, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 0.7999999999999999, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 0.7999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 0.7999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 0.7999999999999999, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 0.7999999999999999, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 0.7999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 0.7, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 0.7, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 0.7, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d7d480>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7f5e0>, <__main__.Case object at 0x71a995d7cd90>, <__main__.Case object at 0x71a995d7d960>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d56020>, <__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7f0d0>, <__main__.Case object at 0x71a995d7c940>, <__main__.Case object at 0x71a995d7db70>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7c700>, <__main__.Case object at 0x71a995d7e290>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.8999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 0.8999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.8999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 0.8999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 0.8999999999999999, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.8999999999999999, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.7999999999999999, time steps: 10\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 2, 1)\n",
      "Integrated case process. comm case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 1)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 2, 1)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7c970>, <__main__.Case object at 0x71a995d7d390>, <__main__.Case object at 0x71a995d7e8c0>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7ceb0>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7faf0>, <__main__.Case object at 0x71a995d7ebc0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7ce50>, <__main__.Case object at 0x71a995d7fbe0>, <__main__.Case object at 0x71a995d7e530>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7db40>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7e200>, <__main__.Case object at 0x71a995d7d570>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.29999999999999993, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.29999999999999993, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.29999999999999993, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.29999999999999993, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.29999999999999993, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 4, tv: 0.29999999999999993, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.29999999999999993, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.29999999999999993, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 13, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 13 loop--------------------\n",
      "----- starting point of Episode 14 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.8999999999999999, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 0.8999999999999999, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 0.8999999999999999, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 0.8999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 0.8999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 0.8999999999999999, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 0.8999999999999999, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 0.8999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 0.7999999999999999, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 0.7999999999999999, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 0.7999999999999999, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7d480>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7e8c0>, <__main__.Case object at 0x71a995d7c850>, <__main__.Case object at 0x71a995d7fee0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7d960>, <__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d7faf0>, <__main__.Case object at 0x71a995d7e680>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 0.9999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 0.9999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.9999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 0.9999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 0.9999999999999999, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.9999999999999999, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.8999999999999999, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.6, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.6, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.6, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.6, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.6, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.6, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7d150>, <__main__.Case object at 0x71a995d7ebc0>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7cd90>, <__main__.Case object at 0x71a995d7d390>, <__main__.Case object at 0x71a995d7d870>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7c640>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7dff0>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7f5e0>, <__main__.Case object at 0x71a995d7c970>, <__main__.Case object at 0x71a995d7c790>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7d510>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.8999999999999999)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.8999999999999999\n",
      "Episode: 14, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 14 loop--------------------\n",
      "----- starting point of Episode 15 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.9999999999999999, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 0.9999999999999999, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 0.9999999999999999, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 0.9999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 0.9999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 0.9999999999999999, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 0.9999999999999999, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 0.9999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 0.8999999999999999, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 0.8999999999999999, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 0.8999999999999999, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d7ce80>, <__main__.Case object at 0x71a995d7faf0>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7fbb0>, <__main__.Case object at 0x71a995d7e650>, <__main__.Case object at 0x71a995d7ddb0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d7d480>, <__main__.Case object at 0x71a995d7c850>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d7f460>, <__main__.Case object at 0x71a995d7d900>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 0.9999999999999999, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.19999999999999996, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.19999999999999996, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.19999999999999996, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.19999999999999996, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.19999999999999996, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.19999999999999996, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.19999999999999996, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.9999999999999999\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7e3e0>, <__main__.Case object at 0x71a995d7c790>, <__main__.Case object at 0x71a995d7ce50>, <__main__.Case object at 0x71a995d7e8c0>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7feb0>, <__main__.Case object at 0x71a995d7d4b0>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7eec0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7ec50>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7fbe0>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7cc10>, <__main__.Case object at 0x71a995d7fac0>, <__main__.Case object at 0x71a995d7dea0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.3999999999999999, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.4999999999999999, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.4999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.4999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.4999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.4999999999999999, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 4, tv: 0.4999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.4999999999999999, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.4999999999999999, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.4999999999999999\n",
      "Episode: 15, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 15 loop--------------------\n",
      "----- starting point of Episode 16 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 0.9999999999999999, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 0.9999999999999999, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 0.9999999999999999, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d7eec0>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7faf0>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7e8c0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7f460>, <__main__.Case object at 0x71a995d7ec50>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7ce80>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7ce50>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 2, 1)\n",
      "Integrated case process. comm case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 1)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 2, 1)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7e0b0>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7f370>, <__main__.Case object at 0x71a995d7c790>, <__main__.Case object at 0x71a995d7feb0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d7dea0>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7fbe0>, <__main__.Case object at 0x71a995d7d390>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7e650>, <__main__.Case object at 0x71a995d7e3e0>, <__main__.Case object at 0x71a995d7ee90>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.09999999999999987, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.09999999999999987, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.09999999999999987, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.09999999999999987, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.09999999999999987, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 4, tv: 0.09999999999999987, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.09999999999999987, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.09999999999999987, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 0.9999999999999999)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.9999999999999999\n",
      "Episode: 16, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 16 loop--------------------\n",
      "----- starting point of Episode 17 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7ce80>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7cb50>, <__main__.Case object at 0x71a995d7c850>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d7f370>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7f790>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d56020>, <__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7fdc0>, <__main__.Case object at 0x71a995d7fac0>, <__main__.Case object at 0x71a995d7ebc0>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7dd50>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.6, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.6, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.6, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.6, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.6, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.6, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7feb0>, <__main__.Case object at 0x71a995d7f5b0>, <__main__.Case object at 0x71a995d7ceb0>, <__main__.Case object at 0x71a995d7d960>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7ec80>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d7fbb0>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7e3e0>, <__main__.Case object at 0x71a995d7cbb0>, <__main__.Case object at 0x71a995d7d6f0>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7e9b0>, <__main__.Case object at 0x71a995d7cee0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.5999999999999999, time steps: 10\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 1)\n",
      "Integrated case process. comm case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 4, 1)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.5999999999999999\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 17, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 17 loop--------------------\n",
      "----- starting point of Episode 18 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7c850>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7eb60>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d7e9b0>, <__main__.Case object at 0x71a995d7cb50>, <__main__.Case object at 0x71a995d7f370>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d7f580>, <__main__.Case object at 0x71a995d7c910>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.19999999999999996, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.19999999999999996, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.19999999999999996, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.19999999999999996, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.19999999999999996, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.19999999999999996, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.19999999999999996, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7cee0>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7dd50>, <__main__.Case object at 0x71a995d7ea40>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d7d180>, <__main__.Case object at 0x71a995d7faf0>, <__main__.Case object at 0x71a995d7cd90>, <__main__.Case object at 0x71a995d7de10>, <__main__.Case object at 0x71a995d7db70>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7ccd0>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7f790>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7cc70>, <__main__.Case object at 0x71a995d7f0d0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.19999999999999984, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 4, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 18, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 18 loop--------------------\n",
      "----- starting point of Episode 19 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7db70>, <__main__.Case object at 0x71a995d7ce80>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7fdc0>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7fa00>, <__main__.Case object at 0x71a995d7d2a0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d56020>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7f580>, <__main__.Case object at 0x71a995d7ccd0>, <__main__.Case object at 0x71a995d7f790>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7c790>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 2, 1)\n",
      "Integrated case process. comm case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 1)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 2, 1)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7f250>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7cc70>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7ebc0>, <__main__.Case object at 0x71a995d7f670>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d7f0d0>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7feb0>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7e1a0>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7e2c0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.19999999999999996, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.19999999999999996, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.19999999999999996, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.19999999999999996, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 4, tv: 0.19999999999999996, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.19999999999999996, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 19, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 19 loop--------------------\n",
      "----- starting point of Episode 20 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7d3c0>, <__main__.Case object at 0x71a995d7f370>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7e9b0>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7c700>, <__main__.Case object at 0x71a995d7cf70>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d7c850>, <__main__.Case object at 0x71a995d7d5d0>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7d180>, <__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d7ea40>, <__main__.Case object at 0x71a995d7dff0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.6, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.6, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.6, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.6, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.6, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.6, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7cee0>, <__main__.Case object at 0x71a995d7e0b0>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7fbe0>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7fac0>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7e530>, <__main__.Case object at 0x71a995d7c970>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7f5b0>, <__main__.Case object at 0x71a995d7d720>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7c940>, <__main__.Case object at 0x71a995d7e920>, <__main__.Case object at 0x71a995d7d930>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7f5e0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.6, time steps: 10\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 1)\n",
      "Integrated case process. comm case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 4, 1)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 20, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 20 loop--------------------\n",
      "----- starting point of Episode 21 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7feb0>, <__main__.Case object at 0x71a995d7f250>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7f370>, <__main__.Case object at 0x71a995d7c700>, <__main__.Case object at 0x71a995d7e1a0>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7e950>, <__main__.Case object at 0x71a995d7d900>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7d3c0>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7d480>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.19999999999999996, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.19999999999999996, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.19999999999999996, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.19999999999999996, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.19999999999999996, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.19999999999999996, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.19999999999999996, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7f5e0>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7dff0>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7e9b0>, <__main__.Case object at 0x71a995d7cd90>, <__main__.Case object at 0x71a995d7fdc0>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7d150>, <__main__.Case object at 0x71a995d7e050>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d7c970>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7ea40>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7ce80>, <__main__.Case object at 0x71a995d7e530>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7ec50>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 4, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 21, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 21 loop--------------------\n",
      "----- starting point of Episode 22 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7ea40>, <__main__.Case object at 0x71a995d7ce80>, <__main__.Case object at 0x71a995d7d5d0>, <__main__.Case object at 0x71a995d7fa00>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7dd50>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7d870>, <__main__.Case object at 0x71a995d7ebc0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 2, 1)\n",
      "Integrated case process. comm case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 1)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 2, 1)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d56020>, <__main__.Case object at 0x71a995d7c850>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7d180>, <__main__.Case object at 0x71a995d7e0b0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d7ec50>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7f2e0>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7e530>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d7eb60>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.19999999999999996, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.19999999999999996, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.19999999999999996, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.19999999999999996, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 4, tv: 0.19999999999999996, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.19999999999999996, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 22, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 22 loop--------------------\n",
      "----- starting point of Episode 23 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7dff0>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7cee0>, <__main__.Case object at 0x71a995d7dea0>, <__main__.Case object at 0x71a995d7cbb0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7f370>, <__main__.Case object at 0x71a995d7d4b0>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7cd90>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7cfa0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.6, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.6, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.6, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.6, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.6, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.6, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7f5e0>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7e0b0>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7d390>, <__main__.Case object at 0x71a995d7e2f0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d7fbe0>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7d330>, <__main__.Case object at 0x71a995d7c970>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7f3d0>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7fbb0>, <__main__.Case object at 0x71a995d7d6f0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.6, time steps: 10\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 1)\n",
      "Integrated case process. comm case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 4, 1)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 23, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 23 loop--------------------\n",
      "----- starting point of Episode 24 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d7f2e0>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7dea0>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7c850>, <__main__.Case object at 0x71a995d7d630>, <__main__.Case object at 0x71a995d7c910>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7fa00>, <__main__.Case object at 0x71a995d7fbb0>, <__main__.Case object at 0x71a995d7dff0>, <__main__.Case object at 0x71a995d7cee0>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d56020>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7cb50>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.19999999999999996, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.19999999999999996, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.19999999999999996, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.19999999999999996, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.19999999999999996, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.19999999999999996, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.19999999999999996, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d7d6f0>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7f250>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7d5d0>, <__main__.Case object at 0x71a995d7e1a0>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7ceb0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7e950>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7d870>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7cbb0>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d7d390>, <__main__.Case object at 0x71a995d7e800>, <__main__.Case object at 0x71a995d7ccd0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 4, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 24, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 24 loop--------------------\n",
      "----- starting point of Episode 25 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d7dff0>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7e8c0>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7dea0>, <__main__.Case object at 0x71a995d7d6f0>, <__main__.Case object at 0x71a995d7d690>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7e2c0>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7f5e0>, <__main__.Case object at 0x71a995d7f2e0>, <__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d7c910>, <__main__.Case object at 0x71a995d7f250>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 2, 1)\n",
      "Integrated case process. comm case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 1)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 2, 1)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d7fa00>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7fbb0>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7e290>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7d5d0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d7d960>, <__main__.Case object at 0x71a995d7cee0>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7cc70>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7d1b0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.19999999999999996, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.19999999999999996, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.19999999999999996, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.19999999999999996, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 4, tv: 0.19999999999999996, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.19999999999999996, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 25, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 25 loop--------------------\n",
      "----- starting point of Episode 26 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d7e3e0>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7dea0>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d7fac0>, <__main__.Case object at 0x71a995d7dcc0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7cee0>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7dff0>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7ceb0>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d7d6c0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.6, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.6, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.6, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.6, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.6, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.6, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d7d960>, <__main__.Case object at 0x71a995d7cc70>, <__main__.Case object at 0x71a995d7ce50>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7f5b0>, <__main__.Case object at 0x71a995d7d2a0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7c910>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7e8c0>, <__main__.Case object at 0x71a995d7d6f0>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7f0d0>, <__main__.Case object at 0x71a995d7c940>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.6, time steps: 10\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 1)\n",
      "Integrated case process. comm case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 4, 1)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 26, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 26 loop--------------------\n",
      "----- starting point of Episode 27 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d7e950>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7fac0>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7fa00>, <__main__.Case object at 0x71a995d7e5c0>, <__main__.Case object at 0x71a995d7d3c0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d7d630>, <__main__.Case object at 0x71a995d7f0d0>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d7e0b0>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7cb50>, <__main__.Case object at 0x71a995d7c7c0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.19999999999999996, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.19999999999999996, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.19999999999999996, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.19999999999999996, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.19999999999999996, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.19999999999999996, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.19999999999999996, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7c940>, <__main__.Case object at 0x71a995d7ef50>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7c850>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7f2e0>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7d810>, <__main__.Case object at 0x71a995d7cf70>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7dea0>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7d8d0>, <__main__.Case object at 0x71a995d7f5b0>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7e1d0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 4, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 27, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 27 loop--------------------\n",
      "----- starting point of Episode 28 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7e3e0>, <__main__.Case object at 0x71a995d7ce50>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d7d8d0>, <__main__.Case object at 0x71a995d7dff0>, <__main__.Case object at 0x71a995d7cd90>, <__main__.Case object at 0x71a995d7c760>, <__main__.Case object at 0x71a995d7d180>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d7cb50>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7f310>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d7eb60>, <__main__.Case object at 0x71a995d7cfa0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 2, 1)\n",
      "Integrated case process. comm case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 1)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 2, 1)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7e9b0>, <__main__.Case object at 0x71a995d7de10>, <__main__.Case object at 0x71a995d7dea0>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7ceb0>, <__main__.Case object at 0x71a995d7cc70>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7f5b0>, <__main__.Case object at 0x71a995d7d720>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7cee0>, <__main__.Case object at 0x71a995d7f790>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.19999999999999996, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.19999999999999996, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.19999999999999996, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.19999999999999996, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 4, tv: 0.19999999999999996, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.19999999999999996, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 28, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 28 loop--------------------\n",
      "----- starting point of Episode 29 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7d720>, <__main__.Case object at 0x71a995d7ce50>, <__main__.Case object at 0x71a995d7cd90>, <__main__.Case object at 0x71a995d7e9b0>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7d330>, <__main__.Case object at 0x71a995d7dd50>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7e3e0>, <__main__.Case object at 0x71a995d7dff0>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7ce80>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.6, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.6, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.6, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.6, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.6, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.6, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7f5b0>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7d8d0>, <__main__.Case object at 0x71a995d7d180>, <__main__.Case object at 0x71a995d7dea0>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7c9a0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7eb60>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d7c760>, <__main__.Case object at 0x71a995d7de10>, <__main__.Case object at 0x71a995d7ceb0>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7f580>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.6, time steps: 10\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 1)\n",
      "Integrated case process. comm case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 4, 1)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 29, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 29 loop--------------------\n",
      "----- starting point of Episode 30 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d7d150>, <__main__.Case object at 0x71a995d7eec0>, <__main__.Case object at 0x71a995d7ceb0>, <__main__.Case object at 0x71a995d7ce50>, <__main__.Case object at 0x71a995d7d330>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7f0d0>, <__main__.Case object at 0x71a995d7db40>, <__main__.Case object at 0x71a995d7e200>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7fa00>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7d720>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7e0b0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7c790>, <__main__.Case object at 0x71a995d7ea40>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.19999999999999996, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.19999999999999996, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.19999999999999996, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.19999999999999996, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.19999999999999996, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.19999999999999996, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.19999999999999996, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d7f580>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7ce80>, <__main__.Case object at 0x71a995d7db70>, <__main__.Case object at 0x71a995d7e9b0>, <__main__.Case object at 0x71a995d7f310>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7d4b0>, <__main__.Case object at 0x71a995d7d390>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7cee0>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7c940>, <__main__.Case object at 0x71a995d7cd90>, <__main__.Case object at 0x71a995d7dd50>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d7c970>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7e020>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 4, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 30, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 30 loop--------------------\n",
      "----- starting point of Episode 31 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7d390>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d7e3e0>, <__main__.Case object at 0x71a995d7ef50>, <__main__.Case object at 0x71a995d7f5e0>, <__main__.Case object at 0x71a995d7cf70>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d7c790>, <__main__.Case object at 0x71a995d7cee0>, <__main__.Case object at 0x71a995d7dd50>, <__main__.Case object at 0x71a995d7cf40>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7c850>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 2, 1)\n",
      "Integrated case process. comm case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 1)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 2, 1)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7e530>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7cd90>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7fac0>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7f5b0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7cbb0>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7c940>, <__main__.Case object at 0x71a995d7c970>, <__main__.Case object at 0x71a995d7fbe0>, <__main__.Case object at 0x71a995d7f0d0>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7d3c0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.19999999999999996, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.19999999999999996, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.19999999999999996, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.19999999999999996, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 4, tv: 0.19999999999999996, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.19999999999999996, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "Episode: 31, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 31 loop--------------------\n",
      "----- starting point of Episode 32 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d7eec0>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7d390>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7ef50>, <__main__.Case object at 0x71a995d7e530>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7d6f0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7cbb0>, <__main__.Case object at 0x71a995d7f0d0>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7e3e0>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7f370>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.6, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.6, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.6, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.6, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.6, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.6, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7c970>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7cd90>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7ee90>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7c940>, <__main__.Case object at 0x71a995d7fbe0>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7f5e0>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7e200>, <__main__.Case object at 0x71a995d7d930>, <__main__.Case object at 0x71a995d7f340>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.6, time steps: 10\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 1)\n",
      "Integrated case process. comm case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 4, 1)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 32, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 32 loop--------------------\n",
      "----- starting point of Episode 33 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7ec50>, <__main__.Case object at 0x71a995d7e200>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7ce50>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7c7c0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7d8d0>, <__main__.Case object at 0x71a995d7d930>, <__main__.Case object at 0x71a995d7d390>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7e0b0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7d570>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 1, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.19999999999999996, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.19999999999999996, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.19999999999999996, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.19999999999999996, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.19999999999999996, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.19999999999999996, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.19999999999999996, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (2, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 1), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7d870>, <__main__.Case object at 0x71a995d7f370>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7e530>, <__main__.Case object at 0x71a995d7cf40>, <__main__.Case object at 0x71a995d7ceb0>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7d690>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7f580>, <__main__.Case object at 0x71a995d7ef50>, <__main__.Case object at 0x71a995d7d6f0>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7feb0>, <__main__.Case object at 0x71a995d7f670>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 4, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 33, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 33 loop--------------------\n",
      "----- starting point of Episode 34 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 1, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 1, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7eec0>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7ce80>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7d6f0>, <__main__.Case object at 0x71a995d7f4c0>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7db70>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7f0d0>, <__main__.Case object at 0x71a995d7c700>, <__main__.Case object at 0x71a995d7fac0>, <__main__.Case object at 0x71a995d7ef50>, <__main__.Case object at 0x71a995d7feb0>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7d330>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7c970>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7fbb0>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7f580>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7d5d0>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7cbb0>, <__main__.Case object at 0x71a995d7d540>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.19999999999999996, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.19999999999999996, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.19999999999999996, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.19999999999999996, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 4, tv: 0.19999999999999996, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.19999999999999996, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 34, Total Steps: 11, Total Rewards: [-102, 90], Status Episode: False\n",
      "------------------------------------------End of episode 34 loop--------------------\n",
      "----- starting point of Episode 35 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.6, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 2, 0.6, 8)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (1, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 4, 0.6, 13)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 0.6, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 0.6, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 0.6, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 0.6, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 0.6, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 0.6, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 0.6, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (2, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 1), 4, 0.6, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7e8c0>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7ef50>, <__main__.Case object at 0x71a995d7ec80>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7f580>, <__main__.Case object at 0x71a995d7cbb0>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d7fac0>, <__main__.Case object at 0x71a995d7d330>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 4, tv: 0.19999999999999996, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 4, tv: 0.19999999999999996, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 0.19999999999999996, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d7d390>, <__main__.Case object at 0x71a995d7ec50>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7e860>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7ceb0>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7d5d0>, <__main__.Case object at 0x71a995d7eec0>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7d930>, <__main__.Case object at 0x71a995d7feb0>, <__main__.Case object at 0x71a995d7da50>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 4, 0.6)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 4, 0.6)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 0.6)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.6)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 1), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 1), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 35, Total Steps: 11, Total Rewards: [-102, 90], Status Episode: False\n",
      "------------------------------------------End of episode 35 loop--------------------\n",
      "----- starting point of Episode 36 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: -0.0553851381374173\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 1) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: -0.16415931915546977\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 2) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 2) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0.9733332060575659\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 2) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: -0.9733332060575659\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 2) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 2) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0.9733332060575659\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 2) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 1, 1, 165)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 2) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 166)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: 0.18395350293677204\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 1, 167)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: -0.18395350293677204\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 11 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 2) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (0, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 36 in steps 12 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 2) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 2) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (0, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: 0.18395350293677204\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 36 in steps 13 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (0, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0.06225774829854913\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 36 in steps 14 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (0, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 36 in steps 15 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (0, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 36 in steps 16 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (0, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: -0.06225774829854913\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 36 in steps 17 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (0, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using problem solver but locked, no learning\n",
      "----- starting point of Episode 36 in steps 18 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (2, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (1, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (1, 0) vs current state: (4, 4) \n",
      "similarity calculation for agent 1 - existing case: (0, 0) vs current state: (4, 4) \n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0.9911899364330736\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d7d630>, <__main__.Case object at 0x71a995d7ebc0>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7c970>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7c7c0>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7e290>, <__main__.Case object at 0x71a995d7f5b0>, <__main__.Case object at 0x71a995d7f040>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d7fac0>, <__main__.Case object at 0x71a995d7fbb0>, <__main__.Case object at 0x71a995d7eec0>, <__main__.Case object at 0x71a995d7feb0>, <__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d7ce50>, <__main__.Case object at 0x71a995d7ec80>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 4) is empty. Temporary case base stored to the case base: ((5, 4), 3, 1)\n",
      "Integrated case process. comm case (6, 4) is empty. Temporary case base stored to the case base: ((6, 4), 3, 1)\n",
      "Integrated case process. comm case (6, 5) is empty. Temporary case base stored to the case base: ((6, 5), 1, 1)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 2, 1)\n",
      "Integrated case process. comm case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 1)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d7d6f0>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7e950>, <__main__.Case object at 0x71a995d7d5d0>, <__main__.Case object at 0x71a995d7d930>, <__main__.Case object at 0x71a995d7e8c0>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7ef50>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7dff0>, <__main__.Case object at 0x71a995d7f370>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7f790>, <__main__.Case object at 0x71a995d7fbe0>, <__main__.Case object at 0x71a995d7f310>, <__main__.Case object at 0x71a995d7d8d0>, <__main__.Case object at 0x71a995d7cb50>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (2, 1), solution: 4, tv: 0.19999999999999996, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 4, tv: 0.19999999999999996, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 2, tv: 0.19999999999999996, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 3\n",
      "Episode succeeded, case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 4, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 4, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 2, tv: 0.5\n",
      "Episode: 36, Total Steps: 19, Total Rewards: [-118, 90], Status Episode: False\n",
      "------------------------------------------End of episode 36 loop--------------------\n",
      "----- starting point of Episode 37 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.955554568943759\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7d870>, <__main__.Case object at 0x71a995d7ccd0>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7e950>, <__main__.Case object at 0x71a995d7e8c0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7c910>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7e080>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7d930>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7ec50>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7d5d0>, <__main__.Case object at 0x71a995d7ffd0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 2, tv: 0.09999999999999998, time steps: 18\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 37, Total Steps: 11, Total Rewards: [-104, 90], Status Episode: False\n",
      "------------------------------------------End of episode 37 loop--------------------\n",
      "----- starting point of Episode 38 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.955554568943759\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7c910>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7d630>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7f580>, <__main__.Case object at 0x71a995d7e200>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7f0d0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7e0b0>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7f5b0>, <__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d7f5e0>, <__main__.Case object at 0x71a995d7f040>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7eb60>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7d6f0>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7c7c0>, <__main__.Case object at 0x71a995d7e920>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7d510>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 38, Total Steps: 11, Total Rewards: [-103, 90], Status Episode: False\n",
      "------------------------------------------End of episode 38 loop--------------------\n",
      "----- starting point of Episode 39 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.955554568943759\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 1 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7f4c0>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7d930>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7e2c0>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7ec50>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7d870>, <__main__.Case object at 0x71a995d7d5d0>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7e8c0>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7d540>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7e5c0>, <__main__.Case object at 0x71a995d7e080>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7c970>, <__main__.Case object at 0x71a995d7ccd0>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7d6f0>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7e1d0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 39, Total Steps: 11, Total Rewards: [-102, 90], Status Episode: False\n",
      "------------------------------------------End of episode 39 loop--------------------\n",
      "----- starting point of Episode 40 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.955554568943759\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7c910>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d7d630>, <__main__.Case object at 0x71a995d7e680>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7f5e0>, <__main__.Case object at 0x71a995d7f0d0>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7eb60>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7e950>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7e0b0>, <__main__.Case object at 0x71a995d7f580>, <__main__.Case object at 0x71a995d7e920>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7e200>, <__main__.Case object at 0x71a995d7dcc0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 40, Total Steps: 11, Total Rewards: [-102, 90], Status Episode: False\n",
      "------------------------------------------End of episode 40 loop--------------------\n",
      "----- starting point of Episode 41 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.955554568943759\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7d870>, <__main__.Case object at 0x71a995d7f4c0>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7cdc0>, <__main__.Case object at 0x71a995d7d930>, <__main__.Case object at 0x71a995d7c970>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7ec50>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7e5c0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7ebc0>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7d5d0>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7e2c0>, <__main__.Case object at 0x71a995d7d6f0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 41, Total Steps: 11, Total Rewards: [-102, 90], Status Episode: False\n",
      "------------------------------------------End of episode 41 loop--------------------\n",
      "----- starting point of Episode 42 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.955554568943759\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7eb60>, <__main__.Case object at 0x71a995d7c910>, <__main__.Case object at 0x71a995d7c7c0>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7e0b0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7e200>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7e080>, <__main__.Case object at 0x71a995d7c9a0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d7e920>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d7d270>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 42, Total Steps: 11, Total Rewards: [-103, 90], Status Episode: False\n",
      "------------------------------------------End of episode 42 loop--------------------\n",
      "----- starting point of Episode 43 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.955554568943759\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7d870>, <__main__.Case object at 0x71a995d7e5c0>, <__main__.Case object at 0x71a995d7f4c0>, <__main__.Case object at 0x71a995d7ccd0>, <__main__.Case object at 0x71a995d7cb80>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7d5d0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7e2c0>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7c970>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d7e950>, <__main__.Case object at 0x71a995d7ebc0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7d630>, <__main__.Case object at 0x71a995d7f5e0>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7cdc0>, <__main__.Case object at 0x71a995d7f940>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 43, Total Steps: 11, Total Rewards: [-102, 90], Status Episode: False\n",
      "------------------------------------------End of episode 43 loop--------------------\n",
      "----- starting point of Episode 44 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.955554568943759\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 3 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7f580>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7eb60>, <__main__.Case object at 0x71a995d7df30>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7c910>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d7e080>, <__main__.Case object at 0x71a995d7e0b0>, <__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7cf70>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7d930>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7e200>, <__main__.Case object at 0x71a995d7c7c0>, <__main__.Case object at 0x71a995d7e920>, <__main__.Case object at 0x71a995d7f5e0>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7e380>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 44, Total Steps: 11, Total Rewards: [-103, 90], Status Episode: False\n",
      "------------------------------------------End of episode 44 loop--------------------\n",
      "----- starting point of Episode 45 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.955554568943759\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7ebc0>, <__main__.Case object at 0x71a995d7d870>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7ef50>, <__main__.Case object at 0x71a995d7e5c0>, <__main__.Case object at 0x71a995d7dc60>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7f4c0>, <__main__.Case object at 0x71a995d7cdc0>, <__main__.Case object at 0x71a995d7e950>, <__main__.Case object at 0x71a995d7d5d0>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7ffd0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7e2c0>, <__main__.Case object at 0x71a995d7ccd0>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7cb80>, <__main__.Case object at 0x71a995d7e020>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 45, Total Steps: 11, Total Rewards: [-103, 90], Status Episode: False\n",
      "------------------------------------------End of episode 45 loop--------------------\n",
      "----- starting point of Episode 46 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.955554568943759\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 3 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7c910>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7e200>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d7d630>, <__main__.Case object at 0x71a995d7d510>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d7f580>, <__main__.Case object at 0x71a995d7e920>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7f5e0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 46, Total Steps: 11, Total Rewards: [-105, 90], Status Episode: False\n",
      "------------------------------------------End of episode 46 loop--------------------\n",
      "----- starting point of Episode 47 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.955554568943759\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 2 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7f4c0>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7ebc0>, <__main__.Case object at 0x71a995d7e2c0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7d870>, <__main__.Case object at 0x71a995d7cb80>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d7d930>, <__main__.Case object at 0x71a995d7e1d0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7d6f0>, <__main__.Case object at 0x71a995d7eb60>, <__main__.Case object at 0x71a995d7e080>, <__main__.Case object at 0x71a995d7cdc0>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7ef50>, <__main__.Case object at 0x71a995d7e860>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 47, Total Steps: 11, Total Rewards: [-102, 90], Status Episode: False\n",
      "------------------------------------------End of episode 47 loop--------------------\n",
      "----- starting point of Episode 48 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.955554568943759\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 4 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7c910>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7c7c0>, <__main__.Case object at 0x71a995d7e290>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7d570>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7d630>, <__main__.Case object at 0x71a995d7e200>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7dcc0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7e5c0>, <__main__.Case object at 0x71a995d7e950>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d7e920>, <__main__.Case object at 0x71a995d7e080>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7e050>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 48, Total Steps: 11, Total Rewards: [-105, 90], Status Episode: False\n",
      "------------------------------------------End of episode 48 loop--------------------\n",
      "----- starting point of Episode 49 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.955554568943759\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7d870>, <__main__.Case object at 0x71a995d7f4c0>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7ccd0>, <__main__.Case object at 0x71a995d7f5b0>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7cdc0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7ef50>, <__main__.Case object at 0x71a995d7d930>, <__main__.Case object at 0x71a995d7e2c0>, <__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7d6f0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7cb80>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7f580>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7f670>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 49, Total Steps: 11, Total Rewards: [-104, 90], Status Episode: False\n",
      "------------------------------------------End of episode 49 loop--------------------\n",
      "----- starting point of Episode 50 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.955554568943759\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0.9393905132022553\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 4 to next state (4, 0): pull reward: 0.9130288521349241\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (4, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 2 to next state (4, 1): pull reward: 0.349030329381919\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (4, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 1) with action 1 to next state (4, 0): pull reward: -0.349030329381919\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (4, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (4, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 2 to next state (4, 1): pull reward: 0.349030329381919\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (4, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 1, 1, 165)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 1) with action 0 to next state (4, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (4, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (4, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 166)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (5, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (5, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((6, 4), 3, 1, 166)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 1) with action 0 to next state (5, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7c910>, <__main__.Case object at 0x71a995d7e950>, <__main__.Case object at 0x71a995d7e8c0>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7c7f0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d7eb60>, <__main__.Case object at 0x71a995d7d270>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7ebc0>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7c7c0>, <__main__.Case object at 0x71a995d7e920>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7e290>, <__main__.Case object at 0x71a995d7e080>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 50, Total Steps: 11, Total Rewards: [-109, 90], Status Episode: False\n",
      "------------------------------------------End of episode 50 loop--------------------\n",
      "----- starting point of Episode 51 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.955554568943759\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7d870>, <__main__.Case object at 0x71a995d7d6f0>, <__main__.Case object at 0x71a995d7f4c0>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7f0d0>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7cb80>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7cdc0>, <__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d7e5c0>, <__main__.Case object at 0x71a995d7f940>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7d630>, <__main__.Case object at 0x71a995d7ef50>, <__main__.Case object at 0x71a995d7ccd0>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7f5b0>, <__main__.Case object at 0x71a995d7f580>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 51, Total Steps: 11, Total Rewards: [-102, 90], Status Episode: False\n",
      "------------------------------------------End of episode 51 loop--------------------\n",
      "----- starting point of Episode 52 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.955554568943759\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d7ec50>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7d030>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7c910>, <__main__.Case object at 0x71a995d7e290>, <__main__.Case object at 0x71a995d7eb60>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7e380>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7f5e0>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7d930>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7e950>, <__main__.Case object at 0x71a995d7e920>, <__main__.Case object at 0x71a995d7d630>, <__main__.Case object at 0x71a995d7e8c0>, <__main__.Case object at 0x71a995d7c820>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 52, Total Steps: 11, Total Rewards: [-102, 90], Status Episode: False\n",
      "------------------------------------------End of episode 52 loop--------------------\n",
      "----- starting point of Episode 53 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.955554568943759\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7d870>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7d6f0>, <__main__.Case object at 0x71a995d7ef50>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7f4c0>, <__main__.Case object at 0x71a995d7f5b0>, <__main__.Case object at 0x71a995d7e5c0>, <__main__.Case object at 0x71a995d7cb80>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7ebc0>, <__main__.Case object at 0x71a995d7e020>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7d930>, <__main__.Case object at 0x71a995d7f0d0>, <__main__.Case object at 0x71a995d7e230>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 53, Total Steps: 11, Total Rewards: [-102, 90], Status Episode: False\n",
      "------------------------------------------End of episode 53 loop--------------------\n",
      "----- starting point of Episode 54 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.955554568943759\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7c910>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7c7c0>, <__main__.Case object at 0x71a995d7c970>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7f0d0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7e8c0>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7d930>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7e290>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d7e920>, <__main__.Case object at 0x71a995d7ccd0>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7e0b0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 54, Total Steps: 11, Total Rewards: [-102, 90], Status Episode: False\n",
      "------------------------------------------End of episode 54 loop--------------------\n",
      "----- starting point of Episode 55 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0.955554568943759\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0.9393905132022553\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 2 to next state (3, 1): pull reward: 0.28614529354171925\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7f4c0>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7f5b0>, <__main__.Case object at 0x71a995d7ef50>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7ee90>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7d870>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7cdc0>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7d570>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.6, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.6, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.6, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.6, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.6, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.6, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.6, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.6, time steps: 71\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 2, 0.5)\n",
      "Episode succeeded, case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 0.6\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7e200>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7d630>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7f5e0>, <__main__.Case object at 0x71a995d7e950>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7d5d0>, <__main__.Case object at 0x71a995d7e3e0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7ebc0>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7e5c0>, <__main__.Case object at 0x71a995d7ccd0>, <__main__.Case object at 0x71a995d7d360>, <__main__.Case object at 0x71a995d7f370>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "Episode: 55, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 55 loop--------------------\n",
      "----- starting point of Episode 56 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.5, 0)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.5, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.5, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (5, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (6, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (7, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 5) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 2, 0.5, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d7e3e0>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7e080>, <__main__.Case object at 0x71a995d7c910>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7d210>, <__main__.Case object at 0x71a995d7cca0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d7dff0>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d7f6d0>, <__main__.Case object at 0x71a995d7d4b0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.19999999999999996, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.19999999999999996, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 3, tv: 0.19999999999999996, time steps: 167\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.19999999999999996, time steps: 166\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 1, tv: 0.19999999999999996, time steps: 165\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.19999999999999996, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.19999999999999996, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.19999999999999996, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.19999999999999996, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.19999999999999996, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.6, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.6, time steps: 0\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7d870>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7cb50>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7cb80>, <__main__.Case object at 0x71a995d7eb60>, <__main__.Case object at 0x71a995d7d120>, <__main__.Case object at 0x71a995d7f2e0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7e0b0>, <__main__.Case object at 0x71a995d7cdc0>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7c940>, <__main__.Case object at 0x71a995d7c7c0>, <__main__.Case object at 0x71a995d7d480>, <__main__.Case object at 0x71a995d7f550>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7d960>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 2, 0.5)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.5)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.5)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.5\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5\n",
      "Episode: 56, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 56 loop--------------------\n",
      "----- starting point of Episode 57 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.6, 0)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.6, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.6, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 2, 0.6, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7e2c0>, <__main__.Case object at 0x71a995d7ceb0>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7f5e0>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7d660>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7de10>, <__main__.Case object at 0x71a995d7e950>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7e200>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d7f5b0>, <__main__.Case object at 0x71a995d7d1b0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 0.7, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.7, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.7, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.7, time steps: 0\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 2, 1)\n",
      "Integrated case process. comm case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 1)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 2, 1)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d7f2e0>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7f280>, <__main__.Case object at 0x71a995d7d960>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7ca60>, <__main__.Case object at 0x71a995d7e920>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7d720>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d7dff0>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d7c940>, <__main__.Case object at 0x71a995d7e710>, <__main__.Case object at 0x71a995d7d360>, <__main__.Case object at 0x71a995d7da50>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d7d630>, <__main__.Case object at 0x71a995d7cb20>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.19999999999999996, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.19999999999999996, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.19999999999999996, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 2, tv: 0.09999999999999998, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.09999999999999998, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.09999999999999998, time steps: 1\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.09999999999999998, time steps: 0\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 57, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 57 loop--------------------\n",
      "----- starting point of Episode 58 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.7, 0)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.7, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.7, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 2, 0.7, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7f5b0>, <__main__.Case object at 0x71a995d7c7c0>, <__main__.Case object at 0x71a995d7d210>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7db70>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7f6d0>, <__main__.Case object at 0x71a995d7cb50>, <__main__.Case object at 0x71a995d7d5d0>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7eb60>, <__main__.Case object at 0x71a995d7c700>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 0.7999999999999999, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.7999999999999999, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.7999999999999999, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.7999999999999999, time steps: 0\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.6, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.6, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.6, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.6, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.6, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.6, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7999999999999999\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7f310>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d7e080>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7d480>, <__main__.Case object at 0x71a995d7cc70>, <__main__.Case object at 0x71a995d7ce50>, <__main__.Case object at 0x71a995d7cd00>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7d780>, <__main__.Case object at 0x71a995d7e3e0>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7f670>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7d4b0>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7e140>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 2, 0.7)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.7)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.7)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.7)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.7\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7\n",
      "Episode: 58, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 58 loop--------------------\n",
      "----- starting point of Episode 59 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.7999999999999999, 0)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.7999999999999999, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.7999999999999999, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 2, 0.7999999999999999, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 59 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d7ce80>, <__main__.Case object at 0x71a995d7ceb0>, <__main__.Case object at 0x71a995d7c700>, <__main__.Case object at 0x71a995d7cb20>, <__main__.Case object at 0x71a995d7d660>, <__main__.Case object at 0x71a995d7dde0>, <__main__.Case object at 0x71a995d7e2c0>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7c760>, <__main__.Case object at 0x71a995d7d330>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7eb60>, <__main__.Case object at 0x71a995d7dff0>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7e800>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d7f460>, <__main__.Case object at 0x71a995d7fac0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 0.8999999999999999, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.8999999999999999, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.8999999999999999, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.8999999999999999, time steps: 0\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.19999999999999996, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.19999999999999996, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.19999999999999996, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.19999999999999996, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.19999999999999996, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.19999999999999996, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.19999999999999996, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.8999999999999999\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7e710>, <__main__.Case object at 0x71a995d7f280>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7cb80>, <__main__.Case object at 0x71a995d7e920>, <__main__.Case object at 0x71a995d7c940>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7d870>, <__main__.Case object at 0x71a995d7faf0>, <__main__.Case object at 0x71a995d7feb0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7d210>, <__main__.Case object at 0x71a995d7d960>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7d900>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7d810>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 2, tv: 0.29999999999999993, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.29999999999999993, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.29999999999999993, time steps: 1\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.29999999999999993, time steps: 0\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.6\n",
      "Episode: 59, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 59 loop--------------------\n",
      "----- starting point of Episode 60 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.8999999999999999, 0)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.8999999999999999, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 2, 0.8999999999999999, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 60 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d7d810>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7e530>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7d480>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7cf40>, <__main__.Case object at 0x71a995d7cb80>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d7dff0>, <__main__.Case object at 0x71a995d7d150>, <__main__.Case object at 0x71a995d7cca0>, <__main__.Case object at 0x71a995d7e080>, <__main__.Case object at 0x71a995d7f6d0>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7d690>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 0.9999999999999999, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.9999999999999999, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.9999999999999999, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.9999999999999999, time steps: 0\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 2, 1)\n",
      "Integrated case process. comm case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 1)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 2, 1)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.9999999999999999\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7eb60>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d7feb0>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7cbb0>, <__main__.Case object at 0x71a995d7ccd0>, <__main__.Case object at 0x71a995d7ca60>, <__main__.Case object at 0x71a995d7f280>, <__main__.Case object at 0x71a995d7c940>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7d720>, <__main__.Case object at 0x71a995d7e800>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7d3c0>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7e710>, <__main__.Case object at 0x71a995d7e920>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.19999999999999996, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.19999999999999996, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.19999999999999996, time steps: 15\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.8999999999999999)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.8999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.8999999999999999\n",
      "Episode: 60, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 60 loop--------------------\n",
      "----- starting point of Episode 61 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.9999999999999999, 0)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.9999999999999999, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.9999999999999999, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 2, 0.9999999999999999, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 61 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7e710>, <__main__.Case object at 0x71a995d7e530>, <__main__.Case object at 0x71a995d7cf40>, <__main__.Case object at 0x71a995d7f460>, <__main__.Case object at 0x71a995d7dde0>, <__main__.Case object at 0x71a995d7c7c0>, <__main__.Case object at 0x71a995d7d120>, <__main__.Case object at 0x71a995d7f370>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7d360>, <__main__.Case object at 0x71a995d7ceb0>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7ec50>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 0\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.6, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.6, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.6, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.6, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.6, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.6, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7cc70>, <__main__.Case object at 0x71a995d7c790>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7f5b0>, <__main__.Case object at 0x71a995d7ce50>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7fbe0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7c700>, <__main__.Case object at 0x71a995d7d3c0>, <__main__.Case object at 0x71a995d7d810>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7cb80>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7d330>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7e5c0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 2, tv: 0.4999999999999999, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.4999999999999999, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.4999999999999999, time steps: 1\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.4999999999999999, time steps: 0\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.4999999999999999\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "Episode: 61, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 61 loop--------------------\n",
      "----- starting point of Episode 62 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 0)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 2, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 62 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7d360>, <__main__.Case object at 0x71a995d7c880>, <__main__.Case object at 0x71a995d7d810>, <__main__.Case object at 0x71a995d7e710>, <__main__.Case object at 0x71a995d7dde0>, <__main__.Case object at 0x71a995d7e920>, <__main__.Case object at 0x71a995d7cb50>, <__main__.Case object at 0x71a995d7cdc0>, <__main__.Case object at 0x71a995d7c6d0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7dcc0>, <__main__.Case object at 0x71a995d7f460>, <__main__.Case object at 0x71a995d7f370>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7de10>, <__main__.Case object at 0x71a995d7fe80>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 0\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.19999999999999996, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.19999999999999996, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.19999999999999996, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.19999999999999996, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.19999999999999996, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.19999999999999996, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.19999999999999996, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7d3c0>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7cf40>, <__main__.Case object at 0x71a995d7d120>, <__main__.Case object at 0x71a995d7c760>, <__main__.Case object at 0x71a995d7e2c0>, <__main__.Case object at 0x71a995d7fac0>, <__main__.Case object at 0x71a995d7eef0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7ceb0>, <__main__.Case object at 0x71a995d7c700>, <__main__.Case object at 0x71a995d7ebc0>, <__main__.Case object at 0x71a995d7e530>, <__main__.Case object at 0x71a995d7c7c0>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7f7c0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 2, tv: 0.09999999999999987, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.09999999999999987, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.09999999999999987, time steps: 1\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.09999999999999987, time steps: 0\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.6\n",
      "Episode: 62, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 62 loop--------------------\n",
      "----- starting point of Episode 63 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 0)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 2, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 63 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d7f7c0>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7d390>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7ef80>, <__main__.Case object at 0x71a995d7d3c0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d7c940>, <__main__.Case object at 0x71a995d7de10>, <__main__.Case object at 0x71a995d7d480>, <__main__.Case object at 0x71a995d7db70>, <__main__.Case object at 0x71a995d7d3f0>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7cf10>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 0\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 2, 1)\n",
      "Integrated case process. comm case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 1)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 2, 1)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7feb0>, <__main__.Case object at 0x71a995d7f280>, <__main__.Case object at 0x71a995d7eef0>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7f5b0>, <__main__.Case object at 0x71a995d7cd00>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7cf40>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d7dab0>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7ea10>, <__main__.Case object at 0x71a995d7d660>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7ca60>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7e680>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.19999999999999996, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.19999999999999996, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.19999999999999996, time steps: 15\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 2, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 63, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 63 loop--------------------\n",
      "----- starting point of Episode 64 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 0)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 2, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 64 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7d660>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7ef80>, <__main__.Case object at 0x71a995d7ccd0>, <__main__.Case object at 0x71a995d7e710>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7d6f0>, <__main__.Case object at 0x71a995d7fdf0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d7ea10>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7f250>, <__main__.Case object at 0x71a995d7e3e0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 0\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.6, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.6, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.6, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.6, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.6, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.6, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7ca60>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7f0d0>, <__main__.Case object at 0x71a995d7f1c0>, <__main__.Case object at 0x71a995d7c7c0>, <__main__.Case object at 0x71a995d7ec50>, <__main__.Case object at 0x71a995d7c790>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d7f490>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7d360>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7f7c0>, <__main__.Case object at 0x71a995d7d570>, <__main__.Case object at 0x71a995d7d3c0>, <__main__.Case object at 0x71a995d7ceb0>, <__main__.Case object at 0x71a995d7cdc0>, <__main__.Case object at 0x71a995d7d900>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7ef50>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 2, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 0\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "Episode: 64, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 64 loop--------------------\n",
      "----- starting point of Episode 65 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 0)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 2, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 65 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d7ea10>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7e380>, <__main__.Case object at 0x71a995d7f7c0>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7e710>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7fbe0>, <__main__.Case object at 0x71a995d7e200>, <__main__.Case object at 0x71a995d7e1a0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7d780>, <__main__.Case object at 0x71a995d7d900>, <__main__.Case object at 0x71a995d7d660>, <__main__.Case object at 0x71a995d7ccd0>, <__main__.Case object at 0x71a995d7fdf0>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d7d210>, <__main__.Case object at 0x71a995d7fe20>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 0\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.19999999999999996, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.19999999999999996, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.19999999999999996, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.19999999999999996, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.19999999999999996, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.19999999999999996, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.19999999999999996, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7eec0>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7ef80>, <__main__.Case object at 0x71a995d7d6f0>, <__main__.Case object at 0x71a995d7cb50>, <__main__.Case object at 0x71a995d7dde0>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7e9b0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7d360>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7c7c0>, <__main__.Case object at 0x71a995d7da50>, <__main__.Case object at 0x71a995d7db40>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 2, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 0\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.6\n",
      "Episode: 65, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 65 loop--------------------\n",
      "----- starting point of Episode 66 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 0)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 2, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 66 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7db40>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7f250>, <__main__.Case object at 0x71a995d7e0b0>, <__main__.Case object at 0x71a995d7d5d0>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7e1d0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d7cf40>, <__main__.Case object at 0x71a995d7d210>, <__main__.Case object at 0x71a995d7ff70>, <__main__.Case object at 0x71a995d7ce80>, <__main__.Case object at 0x71a995d7c970>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7cd00>, <__main__.Case object at 0x71a995d7eec0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 0\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 3, 1)\n",
      "Integrated case process. comm case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 2, 1)\n",
      "Integrated case process. comm case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 1)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 2, 1)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 1)\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d7eef0>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7e9b0>, <__main__.Case object at 0x71a995d7f550>, <__main__.Case object at 0x71a995d7ec50>, <__main__.Case object at 0x71a995d7c6d0>, <__main__.Case object at 0x71a995d7cca0>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7ef80>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d7e530>, <__main__.Case object at 0x71a995d7ddb0>, <__main__.Case object at 0x71a995d7fa00>, <__main__.Case object at 0x71a995d7d810>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7dd50>, <__main__.Case object at 0x71a995d7e050>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.19999999999999996, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.19999999999999996, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.19999999999999996, time steps: 15\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 2, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 66, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 66 loop--------------------\n",
      "----- starting point of Episode 67 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 0)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 2, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 67 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7d810>, <__main__.Case object at 0x71a995d7dd50>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7f280>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7d720>, <__main__.Case object at 0x71a995d7fee0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7fa00>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7f250>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7d900>, <__main__.Case object at 0x71a995d7ea10>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7f3d0>, <__main__.Case object at 0x71a995d7f040>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 0\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.6, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.6, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.6, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.6, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.6, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.6, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.6, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.6, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 0.6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d7cd00>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7c9a0>, <__main__.Case object at 0x71a995d7dc00>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7e3e0>, <__main__.Case object at 0x71a995d7f1c0>, <__main__.Case object at 0x71a995d7cd90>, <__main__.Case object at 0x71a995d7cfa0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7db40>, <__main__.Case object at 0x71a995d7c7c0>, <__main__.Case object at 0x71a995d7e1d0>, <__main__.Case object at 0x71a995d7e230>, <__main__.Case object at 0x71a995d7e200>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7dff0>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 2, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 0\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 3, 1)\n",
      "Integrated case process. comm case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 2, 1)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 1)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 1)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "Episode: 67, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 67 loop--------------------\n",
      "----- starting point of Episode 68 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 0)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (3, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (3, 0) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 2, 1, 3)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (3, 1) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 1), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (3, 2) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (3, 2) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 4), 2, 1, 99)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 2), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 2) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 2) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 5), 3, 1, 157)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 2), 2, 1, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 3) \n",
      "Physical Action for Agent 0 from case base: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 3), 2, 1, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 68 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (4, 4) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (4, 4) \n",
      "Physical Action for Agent 0 from case base: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((7, 5), 3, 1, 159)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 3, 1, 10)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d7fa00>, <__main__.Case object at 0x71a995d7d900>, <__main__.Case object at 0x71a995d7d120>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7dd50>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7e800>, <__main__.Case object at 0x71a995d7d4b0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7ed10>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7d810>, <__main__.Case object at 0x71a995d7f280>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d77460>, <__main__.Case object at 0x71a995d7c700>, <__main__.Case object at 0x71a995d7e290>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 0\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 3, tv: 0.19999999999999996, time steps: 159\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.19999999999999996, time steps: 157\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 2, tv: 0.19999999999999996, time steps: 99\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.19999999999999996, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 0.19999999999999996, time steps: 71\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 0.19999999999999996, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 0.19999999999999996, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 0.19999999999999996, time steps: 24\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 3, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 3), 2, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (3, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((3, 0), 2, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d7f250>, <__main__.Case object at 0x71a995d7fe80>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7d720>, <__main__.Case object at 0x71a995d7fbe0>, <__main__.Case object at 0x71a995d7e710>, <__main__.Case object at 0x71a995d7d420>, <__main__.Case object at 0x71a995d7fbb0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7ea10>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7db40>, <__main__.Case object at 0x71a995d7df30>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7fca0>, <__main__.Case object at 0x71a995d7fdc0>, <__main__.Case object at 0x71a995d7d510>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 2, tv: 0.19999999999999996, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 0\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.6, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.6\n",
      "Episode: 68, Total Steps: 11, Total Rewards: [93, 90], Status Episode: True\n",
      "------------------------------------------End of episode 68 loop--------------------\n",
      "----- starting point of Episode 69 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 0)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 69 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 69 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 69 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 69 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 69 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 69 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 69 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 69 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 69 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 69 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7cd00>, <__main__.Case object at 0x71a995d7f3d0>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7f6d0>, <__main__.Case object at 0x71a995d7d150>, <__main__.Case object at 0x71a995d7e140>, <__main__.Case object at 0x71a995d7d2a0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54ee0>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7ef80>, <__main__.Case object at 0x71a995d7c700>, <__main__.Case object at 0x71a995d7e0b0>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d7c6d0>, <__main__.Case object at 0x71a995d7fe80>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.6, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.6, time steps: 0\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7e9b0>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7fbb0>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7e3e0>, <__main__.Case object at 0x71a995d7e1a0>, <__main__.Case object at 0x71a995d7d480>, <__main__.Case object at 0x71a995d7f250>, <__main__.Case object at 0x71a995d7e620>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7f5e0>, <__main__.Case object at 0x71a995d7f7c0>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7cca0>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7e020>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 3, tv: 0.19999999999999996, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 2, tv: 0.19999999999999996, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 2, tv: 0.19999999999999996, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.19999999999999996, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.19999999999999996, time steps: 15\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1\n",
      "Episode: 69, Total Steps: 11, Total Rewards: [-101, 90], Status Episode: False\n",
      "------------------------------------------End of episode 69 loop--------------------\n",
      "----- starting point of Episode 70 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (0, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.6, 0)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 70 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (1, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.6, 1)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 70 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.6, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 70 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.6, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 70 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.6, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 70 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.6, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 70 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.6, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 70 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.6, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 70 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.6, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 70 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.6, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 70 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (2, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from case base: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.6, 2)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7f7c0>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7e800>, <__main__.Case object at 0x71a995d7eec0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7c6d0>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7f3d0>, <__main__.Case object at 0x71a995d7e140>, <__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d7dd50>, <__main__.Case object at 0x71a995d7d660>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 2\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 0\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d7d810>, <__main__.Case object at 0x71a995d7d900>, <__main__.Case object at 0x71a995d7f5e0>, <__main__.Case object at 0x71a995d7e290>, <__main__.Case object at 0x71a995d7cd00>, <__main__.Case object at 0x71a995d7d150>, <__main__.Case object at 0x71a995d7c7f0>, <__main__.Case object at 0x71a995d7dc00>, <__main__.Case object at 0x71a995d7f310>, <__main__.Case object at 0x71a995d7d180>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d7dc30>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7f6d0>, <__main__.Case object at 0x71a995d7faf0>, <__main__.Case object at 0x71a995d7e3e0>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7fe20>]\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 1\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 0\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6\n",
      "Episode: 70, Total Steps: 11, Total Rewards: [-101, 90], Status Episode: False\n",
      "------------------------------------------End of episode 70 loop--------------------\n",
      "----- starting point of Episode 71 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 1.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 71 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 71 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 71 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 71 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 71 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 71 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 71 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 71 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 71 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 71 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7cca0>, <__main__.Case object at 0x71a995d7dd50>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7faf0>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7d510>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7dc30>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7f7c0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7ed10>, <__main__.Case object at 0x71a995d7e140>, <__main__.Case object at 0x71a995d7d660>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7f6d0>, <__main__.Case object at 0x71a995d7d360>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7c820>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.19999999999999996, time steps: 2\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.19999999999999996, time steps: 1\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.19999999999999996, time steps: 0\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 71, Total Steps: 11, Total Rewards: [-101, 90], Status Episode: False\n",
      "------------------------------------------End of episode 71 loop--------------------\n",
      "----- starting point of Episode 72 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 72 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 1.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 72 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 72 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 72 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 72 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 72 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 72 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 72 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 72 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 72 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7e9b0>, <__main__.Case object at 0x71a995d7c6d0>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7f490>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7fe20>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7c700>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d7f280>, <__main__.Case object at 0x71a995d7dc30>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7da50>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7cb20>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7d1b0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 72, Total Steps: 11, Total Rewards: [-102, 90], Status Episode: False\n",
      "------------------------------------------End of episode 72 loop--------------------\n",
      "----- starting point of Episode 73 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9911899364330736\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 73 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7e9b0>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7dc60>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d7e1a0>, <__main__.Case object at 0x71a995d7ed10>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7c6d0>, <__main__.Case object at 0x71a995d7e680>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7f3d0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 73, Total Steps: 11, Total Rewards: [-101, 90], Status Episode: False\n",
      "------------------------------------------End of episode 73 loop--------------------\n",
      "----- starting point of Episode 74 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9911899364330736\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 74 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7e1a0>, <__main__.Case object at 0x71a995d7e9b0>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7da50>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7d660>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7f280>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7e800>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7e860>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 74, Total Steps: 11, Total Rewards: [-101, 90], Status Episode: False\n",
      "------------------------------------------End of episode 74 loop--------------------\n",
      "----- starting point of Episode 75 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9911899364330736\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 75 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7ef80>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7e1a0>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7da50>, <__main__.Case object at 0x71a995d7d120>, <__main__.Case object at 0x71a995d7c6d0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7dc30>, <__main__.Case object at 0x71a995d7ec80>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7f250>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7e9b0>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7d360>, <__main__.Case object at 0x71a995d7e4d0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 75, Total Steps: 11, Total Rewards: [-102, 90], Status Episode: False\n",
      "------------------------------------------End of episode 75 loop--------------------\n",
      "----- starting point of Episode 76 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9911899364330736\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 76 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7dc30>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7da50>, <__main__.Case object at 0x71a995d7c6d0>, <__main__.Case object at 0x71a995d7dd50>, <__main__.Case object at 0x71a995d7e8f0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7e140>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7d270>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7ef80>, <__main__.Case object at 0x71a995d7e1a0>, <__main__.Case object at 0x71a995d7d120>, <__main__.Case object at 0x71a995d7d660>, <__main__.Case object at 0x71a995d7cb20>, <__main__.Case object at 0x71a995d7d6c0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 76, Total Steps: 11, Total Rewards: [-102, 90], Status Episode: False\n",
      "------------------------------------------End of episode 76 loop--------------------\n",
      "----- starting point of Episode 77 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9911899364330736\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 77 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7dc30>, <__main__.Case object at 0x71a995d7c6d0>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7e020>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7da50>, <__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d7e800>, <__main__.Case object at 0x71a995d7ce80>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d7d360>, <__main__.Case object at 0x71a995d7f7c0>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7d510>, <__main__.Case object at 0x71a995d7dd50>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7cfa0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 77, Total Steps: 11, Total Rewards: [-101, 90], Status Episode: False\n",
      "------------------------------------------End of episode 77 loop--------------------\n",
      "----- starting point of Episode 78 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9911899364330736\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 78 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7f3d0>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7e800>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7ef80>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7f250>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7c6d0>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7fee0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7cb20>, <__main__.Case object at 0x71a995d7ed10>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7dc30>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7f7c0>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7d660>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 78, Total Steps: 11, Total Rewards: [-101, 90], Status Episode: False\n",
      "------------------------------------------End of episode 78 loop--------------------\n",
      "----- starting point of Episode 79 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9911899364330736\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 79 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7e800>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7ef80>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7f760>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7fa00>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7f280>, <__main__.Case object at 0x71a995d7f3d0>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7ed10>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7cf10>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 79, Total Steps: 11, Total Rewards: [-101, 90], Status Episode: False\n",
      "------------------------------------------End of episode 79 loop--------------------\n",
      "----- starting point of Episode 80 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9911899364330736\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 80 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7ef80>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7da50>, <__main__.Case object at 0x71a995d7d1b0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7fe20>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d7d360>, <__main__.Case object at 0x71a995d7e500>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7e800>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7f280>, <__main__.Case object at 0x71a995d7d120>, <__main__.Case object at 0x71a995d7f7c0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 80, Total Steps: 11, Total Rewards: [-101, 90], Status Episode: False\n",
      "------------------------------------------End of episode 80 loop--------------------\n",
      "----- starting point of Episode 81 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9911899364330736\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 81 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7d360>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7c6d0>, <__main__.Case object at 0x71a995d7f3d0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7ef80>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7cb20>, <__main__.Case object at 0x71a995d7ebc0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7da50>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7dd50>, <__main__.Case object at 0x71a995d7ed10>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 81, Total Steps: 11, Total Rewards: [-102, 90], Status Episode: False\n",
      "------------------------------------------End of episode 81 loop--------------------\n",
      "----- starting point of Episode 82 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9911899364330736\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 82 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7cb20>, <__main__.Case object at 0x71a995d7d360>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7f3d0>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7e860>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7e140>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7cee0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d7d120>, <__main__.Case object at 0x71a995d7ce80>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7c6d0>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7e2f0>, <__main__.Case object at 0x71a995d7f280>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 82, Total Steps: 11, Total Rewards: [-101, 90], Status Episode: False\n",
      "------------------------------------------End of episode 82 loop--------------------\n",
      "----- starting point of Episode 83 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.06225774829854913\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 1.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 1.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 83 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7d660>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7cb20>, <__main__.Case object at 0x71a995d7f3d0>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7e4d0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d6b8e0>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7fca0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 1)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7dd50>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7d360>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7ec80>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 83, Total Steps: 11, Total Rewards: [-104, 90], Status Episode: False\n",
      "------------------------------------------End of episode 83 loop--------------------\n",
      "----- starting point of Episode 84 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 84 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7fa00>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7f3d0>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7e140>, <__main__.Case object at 0x71a995d7cca0>, <__main__.Case object at 0x71a995d7f970>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7f250>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7cb20>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7d270>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7f280>, <__main__.Case object at 0x71a995d7d360>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7e1a0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 84, Total Steps: 11, Total Rewards: [-102, 90], Status Episode: False\n",
      "------------------------------------------End of episode 84 loop--------------------\n",
      "----- starting point of Episode 85 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 85 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7d180>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7e140>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7faf0>, <__main__.Case object at 0x71a995d7e8f0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7e1a0>, <__main__.Case object at 0x71a995d7fa00>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7d120>, <__main__.Case object at 0x71a995d7fee0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7f3d0>, <__main__.Case object at 0x71a995d7cca0>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7c6d0>, <__main__.Case object at 0x71a995d7f940>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 85, Total Steps: 11, Total Rewards: [-101, 90], Status Episode: False\n",
      "------------------------------------------End of episode 85 loop--------------------\n",
      "----- starting point of Episode 86 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 86 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7d120>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7d660>, <__main__.Case object at 0x71a995d7f490>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7d180>, <__main__.Case object at 0x71a995d7e140>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7d2a0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7f7c0>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7faf0>, <__main__.Case object at 0x71a995d7ef80>, <__main__.Case object at 0x71a995d7dd50>, <__main__.Case object at 0x71a995d7d360>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 86, Total Steps: 11, Total Rewards: [-101, 90], Status Episode: False\n",
      "------------------------------------------End of episode 86 loop--------------------\n",
      "----- starting point of Episode 87 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 87 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7ebc0>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7d120>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7da50>, <__main__.Case object at 0x71a995d7f760>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7d360>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7eec0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7d660>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7cf70>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 87, Total Steps: 11, Total Rewards: [-101, 90], Status Episode: False\n",
      "------------------------------------------End of episode 87 loop--------------------\n",
      "----- starting point of Episode 88 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0.18395350293677204\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 1) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0.9911899364330736\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 1) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 1) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: -0.20904207741504255\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 88 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7eec0>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7d270>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7f3d0>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7ebc0>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7f1c0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7c6d0>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7d120>, <__main__.Case object at 0x71a995d7da50>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7f280>, <__main__.Case object at 0x71a995d7ef80>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 88, Total Steps: 11, Total Rewards: [-104, 90], Status Episode: False\n",
      "------------------------------------------End of episode 88 loop--------------------\n",
      "----- starting point of Episode 89 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 89 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7f1c0>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7e140>, <__main__.Case object at 0x71a995d7fee0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7d540>, <__main__.Case object at 0x71a995d7ef80>, <__main__.Case object at 0x71a995d7f040>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7f7c0>, <__main__.Case object at 0x71a995d7dc30>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d7dd50>, <__main__.Case object at 0x71a995d7f250>, <__main__.Case object at 0x71a995d7eec0>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7cca0>, <__main__.Case object at 0x71a995d7e4d0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 89, Total Steps: 11, Total Rewards: [-104, 90], Status Episode: False\n",
      "------------------------------------------End of episode 89 loop--------------------\n",
      "----- starting point of Episode 90 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 90 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7dc30>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7f7c0>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7d2a0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7d1b0>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7e050>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5db70>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7f1c0>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7e140>, <__main__.Case object at 0x71a995d7f250>, <__main__.Case object at 0x71a995d7faf0>, <__main__.Case object at 0x71a995d7ec80>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 90, Total Steps: 11, Total Rewards: [-101, 90], Status Episode: False\n",
      "------------------------------------------End of episode 90 loop--------------------\n",
      "----- starting point of Episode 91 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 91 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7f7c0>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7eec0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7d120>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7e620>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d7c6d0>, <__main__.Case object at 0x71a995d7cdc0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7f280>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7dc30>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7d660>, <__main__.Case object at 0x71a995d7f970>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 91, Total Steps: 11, Total Rewards: [-101, 90], Status Episode: False\n",
      "------------------------------------------End of episode 91 loop--------------------\n",
      "----- starting point of Episode 92 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: -0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 2, 1, 71)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 92 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 3), 2, 1, 74)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7cdc0>, <__main__.Case object at 0x71a995d7f940>, <__main__.Case object at 0x71a995d7c6d0>, <__main__.Case object at 0x71a995d7d6c0>, <__main__.Case object at 0x71a995d7d2a0>, <__main__.Case object at 0x71a995d7eec0>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7f1c0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7fee0>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d7dd50>, <__main__.Case object at 0x71a995d7f0d0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d7cca0>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7f7c0>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7da50>, <__main__.Case object at 0x71a995d7f250>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 92, Total Steps: 11, Total Rewards: [-104, 90], Status Episode: False\n",
      "------------------------------------------End of episode 92 loop--------------------\n",
      "----- starting point of Episode 93 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 93 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7d660>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d7e800>, <__main__.Case object at 0x71a995d7f7c0>, <__main__.Case object at 0x71a995d7da50>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7e140>, <__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7f760>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7d360>, <__main__.Case object at 0x71a995d7d120>, <__main__.Case object at 0x71a995d7ce80>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7ef80>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7f340>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 93, Total Steps: 11, Total Rewards: [-102, 90], Status Episode: False\n",
      "------------------------------------------End of episode 93 loop--------------------\n",
      "----- starting point of Episode 94 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 94 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7e800>, <__main__.Case object at 0x71a995d7da50>, <__main__.Case object at 0x71a995d7c6d0>, <__main__.Case object at 0x71a995d7e1a0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7cf70>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d7f280>, <__main__.Case object at 0x71a995d7cf10>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7d660>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7f7c0>, <__main__.Case object at 0x71a995d7d360>, <__main__.Case object at 0x71a995d7eec0>, <__main__.Case object at 0x71a995d7dc30>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 94, Total Steps: 11, Total Rewards: [-102, 90], Status Episode: False\n",
      "------------------------------------------End of episode 94 loop--------------------\n",
      "----- starting point of Episode 95 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 95 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7ce80>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7da50>, <__main__.Case object at 0x71a995d7e1a0>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7d120>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7f2e0>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7e800>, <__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7d1b0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7c6d0>, <__main__.Case object at 0x71a995d7e500>, <__main__.Case object at 0x71a995d7f1c0>, <__main__.Case object at 0x71a995d7ef80>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 95, Total Steps: 11, Total Rewards: [-101, 90], Status Episode: False\n",
      "------------------------------------------End of episode 95 loop--------------------\n",
      "----- starting point of Episode 96 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 96 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d49390>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d7eec0>, <__main__.Case object at 0x71a995d7f2e0>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7ce80>, <__main__.Case object at 0x71a995d7e1a0>, <__main__.Case object at 0x71a995d7d120>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7d660>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7cca0>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7da50>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d7cfa0>, <__main__.Case object at 0x71a995d7d180>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7dd50>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7e4d0>, <__main__.Case object at 0x71a995d7e050>, <__main__.Case object at 0x71a995d7fdc0>, <__main__.Case object at 0x71a995d7d360>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 96, Total Steps: 11, Total Rewards: [-101, 90], Status Episode: False\n",
      "------------------------------------------End of episode 96 loop--------------------\n",
      "----- starting point of Episode 97 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 97 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d495a0>, <__main__.Case object at 0x71a995d6baf0>, <__main__.Case object at 0x71a995d7f1c0>, <__main__.Case object at 0x71a995d7cca0>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7d120>, <__main__.Case object at 0x71a995d7d660>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7f490>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7f0d0>, <__main__.Case object at 0x71a995d7f2e0>, <__main__.Case object at 0x71a995d7e1a0>, <__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d7d690>, <__main__.Case object at 0x71a995d7d510>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d7c640>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7eec0>, <__main__.Case object at 0x71a995d7ce80>, <__main__.Case object at 0x71a995d7e860>, <__main__.Case object at 0x71a995d7dd50>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7e500>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 97, Total Steps: 11, Total Rewards: [-101, 90], Status Episode: False\n",
      "------------------------------------------End of episode 97 loop--------------------\n",
      "----- starting point of Episode 98 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 98 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d74ca0>, <__main__.Case object at 0x71a995d49570>, <__main__.Case object at 0x71a995d681f0>, <__main__.Case object at 0x71a995d7fdc0>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7d660>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7cdc0>, <__main__.Case object at 0x71a995d7e8f0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5c820>, <__main__.Case object at 0x71a995d54d00>, <__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d68190>, <__main__.Case object at 0x71a995d7f2e0>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7cca0>, <__main__.Case object at 0x71a995d7d120>, <__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d7d270>, <__main__.Case object at 0x71a995d7dde0>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d7ffd0>, <__main__.Case object at 0x71a995d7f0d0>, <__main__.Case object at 0x71a995d7f1c0>, <__main__.Case object at 0x71a995d7ee90>, <__main__.Case object at 0x71a995d7fe50>, <__main__.Case object at 0x71a995d7c820>, <__main__.Case object at 0x71a995d7f7c0>, <__main__.Case object at 0x71a995d7e050>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 98, Total Steps: 11, Total Rewards: [-101, 90], Status Episode: False\n",
      "------------------------------------------End of episode 98 loop--------------------\n",
      "----- starting point of Episode 99 in steps 0 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (0, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (0, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 0) \n",
      "similarity calculation for agent 1 - existing case: (9, 0) vs current state: (9, 0) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 2, 1, 24)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0.9661013619548031\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 1 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (9, 1) \n",
      "similarity calculation for agent 1 - existing case: (9, 1) vs current state: (9, 1) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 1), 3, 1, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 2 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (1, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (1, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 1) \n",
      "similarity calculation for agent 1 - existing case: (8, 1) vs current state: (8, 1) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 3 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 2) \n",
      "similarity calculation for agent 1 - existing case: (8, 2) vs current state: (8, 2) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 4 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 3) \n",
      "similarity calculation for agent 1 - existing case: (8, 3) vs current state: (8, 3) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 5 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 4) \n",
      "similarity calculation for agent 1 - existing case: (8, 4) vs current state: (8, 4) \n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 6 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (8, 5) \n",
      "similarity calculation for agent 1 - existing case: (8, 5) vs current state: (8, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 7 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (7, 5) \n",
      "similarity calculation for agent 1 - existing case: (7, 5) vs current state: (7, 5) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 8 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 5) \n",
      "similarity calculation for agent 1 - existing case: (6, 5) vs current state: (6, 5) \n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 9 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (6, 4) \n",
      "similarity calculation for agent 1 - existing case: (6, 4) vs current state: (6, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: using problem solver but locked, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 99 in steps 10 loop -----\n",
      "similarity calculation for agent 0 - existing case: (4, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 2) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (4, 4) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (3, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (9, 0) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 1) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 3) vs current state: (2, 0) \n",
      "similarity calculation for agent 0 - existing case: (8, 2) vs current state: (2, 0) \n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "similarity calculation for agent 1 - existing case: (5, 4) vs current state: (5, 4) \n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((8, 1), 2, 1, 70)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 0): pull reward: 0.0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x71a995d77280>, <__main__.Case object at 0x71a995d4a8c0>, <__main__.Case object at 0x71a995d681c0>, <__main__.Case object at 0x71a995d7e020>, <__main__.Case object at 0x71a995d7f760>, <__main__.Case object at 0x71a995d7ce80>, <__main__.Case object at 0x71a995d7dc60>, <__main__.Case object at 0x71a995d7f490>, <__main__.Case object at 0x71a995d7e8f0>, <__main__.Case object at 0x71a995d7f250>, <__main__.Case object at 0x71a995d7eec0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x71a995d5c850>, <__main__.Case object at 0x71a995d5ff70>, <__main__.Case object at 0x71a995d4a6e0>, <__main__.Case object at 0x71a995d680d0>, <__main__.Case object at 0x71a995d7cca0>, <__main__.Case object at 0x71a995d7cf10>, <__main__.Case object at 0x71a995d7f340>, <__main__.Case object at 0x71a995d7d660>, <__main__.Case object at 0x71a995d74e80>, <__main__.Case object at 0x71a995d7f970>, <__main__.Case object at 0x71a995d7ed10>]\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 2, tv: 1, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 3, tv: 1, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 0, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 1\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (4, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x71a995d480d0>, <__main__.Case object at 0x71a995d6bb20>, <__main__.Case object at 0x71a995d694e0>, <__main__.Case object at 0x71a995d7f2e0>, <__main__.Case object at 0x71a995d7ec80>, <__main__.Case object at 0x71a995d7fdc0>, <__main__.Case object at 0x71a995d7d030>, <__main__.Case object at 0x71a995d7cdc0>, <__main__.Case object at 0x71a995d7f0d0>, <__main__.Case object at 0x71a995d7c6d0>, <__main__.Case object at 0x71a995d7dd50>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 1, time steps: 167\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 1, time steps: 166\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 1, tv: 1, time steps: 165\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 1, time steps: 159\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 1, time steps: 157\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 1, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 1, time steps: 74\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 1, time steps: 71\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 1, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 1, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 2, tv: 1, time steps: 24\n",
      "Episode succeeded, case (5, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 1, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((9, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (6, 5), solution: 1, tv: 1\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 1\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 1\n",
      "cases content after RETAIN, problem: (9, 0), solution: 2, tv: 1\n",
      "Episode: 99, Total Steps: 11, Total Rewards: [-102, 90], Status Episode: False\n",
      "------------------------------------------End of episode 99 loop--------------------\n",
      "Success rate: 39.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import json\n",
    "import psutil\n",
    "import pynvml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from environment_ma_reward_distance_dynamic_notrandom import Env\n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, num_actions, env, alpha, gamma, epsilon):\n",
    "        self.env = env\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = alpha\n",
    "        self.discount_factor = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_tables = [defaultdict(lambda: [0.0] * num_actions) for _ in range(env.num_agents)]\n",
    "\n",
    "    @staticmethod\n",
    "    def arg_max(state_action):\n",
    "        max_index_list = []\n",
    "        max_value = state_action[0]\n",
    "        for index, value in enumerate(state_action):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)\n",
    "\n",
    "    def choose_action(self, agent_idx, state):\n",
    "        state = tuple(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            state_action = self.q_tables[agent_idx][state]\n",
    "            action = self.arg_max(state_action)\n",
    "        return action\n",
    "\n",
    "    def learn(self, agent_idx, state, action, reward, next_state, case_base=None):\n",
    "        state = tuple(state)\n",
    "        next_state = tuple(next_state)\n",
    "        current_q = self.q_tables[agent_idx][state][action]\n",
    "        max_next_q = max(self.q_tables[agent_idx][next_state])\n",
    "\n",
    "        # Calculate the Q-update with weighted distance-based pull reward if case_base exists\n",
    "        if case_base:\n",
    "            print(f\"Calculating weighted distance-based pull reward for agent {agent_idx}\")\n",
    "            pull_reward = 0  # Initialize pull reward\n",
    "            \n",
    "            # Iterate through each case in the agent's case base\n",
    "            for case in case_base:\n",
    "                # Ensure case.problem is iterable (list of states)\n",
    "                problems = case.problem if isinstance(case.problem, list) else [case.problem]\n",
    "                \n",
    "                # Weight for this case, e.g., based on trust value\n",
    "                weight = case.trust_value\n",
    "                \n",
    "                # Calculate the pull reward based on the weighted difference in distances\n",
    "                for p in problems:\n",
    "                    distance_current = np.linalg.norm(np.array(state) - np.array(p))\n",
    "                    distance_next = np.linalg.norm(np.array(next_state) - np.array(p))\n",
    "                    \n",
    "                    # Difference in distances weighted by trust value\n",
    "                    # pull_reward += weight * (distance_current - distance_next)\n",
    "\n",
    "                    distance_diff = distance_current - distance_next\n",
    "                    pull_reward = weight * distance_diff\n",
    "                    # pull_reward += np.log1p(distance_diff)\n",
    "            \n",
    "            print(f\"Calculating pull reward agent {agent_idx}: from state {state} with action {action} to next state {next_state}: pull reward: {pull_reward}\")\n",
    "            new_q = current_q + self.learning_rate * (reward + pull_reward + self.discount_factor * max_next_q - current_q)\n",
    "        else:\n",
    "            # Standard Q-learning update without pull reward\n",
    "            print(f\"No communication. Standard Q-learning update for agent {agent_idx}\")\n",
    "            new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)\n",
    "\n",
    "        self.q_tables[agent_idx][state][action] = new_q\n",
    "\n",
    "\n",
    "class Case:\n",
    "\n",
    "    def __init__(self, problem, solution, trust_value=0.5, total_time_steps=0):\n",
    "        self.problem = ast.literal_eval(problem) if isinstance(problem, str) else problem\n",
    "        self.solution = solution\n",
    "        self.trust_value = trust_value\n",
    "        self.total_time_steps = total_time_steps  # New attribute for total time steps\n",
    "\n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)\n",
    "        state2 = np.atleast_1d(state2)\n",
    "        CNDMaxDist = 6\n",
    "        v = state1.size\n",
    "        DistQ = np.sum([Case.dist_q(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def dist_q(X1, X2):\n",
    "        return np.min(np.abs(X1 - X2))\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(agent_idx, state, case_base, threshold=0.1):\n",
    "        state = ast.literal_eval(state) if isinstance(state, str) else state\n",
    "        for case in case_base:\n",
    "            print(f\"similarity calculation for agent {agent_idx} - existing case: {case.problem} vs current state: {state} \")\n",
    "            if state == case.problem: \n",
    "                return case\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(agent_idx, c, own_temp_case_base, comm_temp_case_base, source='own'):\n",
    "        \"\"\"Reuse step for adding cases to temporary case bases.\"\"\"\n",
    "        if source == 'own':\n",
    "            own_temp_case_base.append(c)\n",
    "        elif source == 'comm':\n",
    "            comm_temp_case_base.append(c)\n",
    "\n",
    "    @staticmethod\n",
    "    def revise(agent_idx, case_base, temporary_case_base, successful_episodes):\n",
    "        for case in case_base:\n",
    "            if any((case.problem, case.solution) == (temp_case.problem, temp_case.solution) for temp_case in temporary_case_base):\n",
    "                if successful_episodes:\n",
    "                    case.trust_value += 0.1\n",
    "                else:\n",
    "                    case.trust_value -= 0.4\n",
    "            else:\n",
    "                if successful_episodes:\n",
    "                    case.trust_value -= 0.4\n",
    "            \n",
    "            case.trust_value = max(0, min(case.trust_value, 1))\n",
    "            print(f\"case content after REVISE for agent {agent_idx}, problem: {case.problem}, solution: {case.solution}, tv: {case.trust_value}, time steps: {case.total_time_steps}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def retain(agent_idx, case_base, own_temp_case_base, comm_temp_case_base, successful_episodes, threshold=0.49):\n",
    "        if successful_episodes:\n",
    "            for temp_case in reversed(own_temp_case_base):\n",
    "                state = tuple(np.atleast_1d(temp_case.problem))\n",
    "                if not any(tuple(np.atleast_1d(case.problem)) == state for case in case_base):\n",
    "                    case_base.append(temp_case)\n",
    "                    print(f\"Episode succeeded, case {temp_case.problem} is empty. Temporary case base stored to the case base: {temp_case.problem, temp_case.solution, temp_case.trust_value}\")         \n",
    "                else:\n",
    "                    print(f\"Episode succeeded, case {temp_case.problem} for agent {agent_idx} is not empty. Temporary case base that not stored to the case base: {temp_case.problem, temp_case.solution, temp_case.trust_value}\")    \n",
    "        else:\n",
    "            print(f\"Episode not succeeded, temporary case base from own experience is not stored to the case base\")\n",
    "        \n",
    "        case_base_dict = {tuple(np.atleast_1d(case.problem)): case for case in case_base}\n",
    "\n",
    "        for temp_comm_case in reversed(comm_temp_case_base):\n",
    "            state_comm = tuple(np.atleast_1d(temp_comm_case.problem))\n",
    "            existing_case = case_base_dict.get(state_comm)\n",
    "            if existing_case is None:\n",
    "                case_base.append(temp_comm_case)\n",
    "                case_base_dict[state_comm] = temp_comm_case\n",
    "                print(f\"Integrated case process. comm case {temp_comm_case.problem} is empty. Temporary case base stored to the case base: {temp_comm_case.problem, temp_comm_case.solution, temp_comm_case.trust_value}\")         \n",
    "\n",
    "        case_base[:] = [case for case in case_base if case.trust_value >= threshold]\n",
    "\n",
    "        for case in case_base:\n",
    "            print(f\"cases content after RETAIN, problem: {case.problem}, solution: {case.solution}, tv: {case.trust_value}\")\n",
    "\n",
    "        return case_base\n",
    "\n",
    "\n",
    "class QCBRL:\n",
    "    def __init__(self, num_actions, env, episodes, max_steps, alpha, gamma, epsilon, epsilon_decay, epsilon_min, render):\n",
    "        self.num_actions = num_actions\n",
    "        self.env = env\n",
    "        self.episodes = episodes\n",
    "        self.max_steps = max_steps\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.render = render\n",
    "        self.epsilon_decay = epsilon_decay  \n",
    "        self.epsilon_min = epsilon_min  \n",
    "\n",
    "        self.problem_solvers = [ProblemSolver(num_actions, self.env, alpha, gamma, epsilon) for _ in range(self.env.num_agents)]\n",
    "        self.case_bases = [[] for _ in range(self.env.num_agents)]  # Individual case bases for each agent\n",
    "        self.own_temp_case_bases = [[] for _ in range(self.env.num_agents)]  # Temporary case bases for own experiences\n",
    "        self.comm_temp_case_bases = [[] for _ in range(self.env.num_agents)]  # Temporary case bases for communication experiences\n",
    "        self.successful_episodes = [0] * self.env.num_agents\n",
    "        self.rewards_per_episode = [[] for _ in range(self.env.num_agents)]  \n",
    "        self.total_successful_episodes = 0 \n",
    "        self.action_type = [0] * self.env.num_agents\n",
    "\n",
    "    def run(self):\n",
    "        rewards = []\n",
    "        memory_usage = []\n",
    "        gpu_memory_usage = []\n",
    "        num_successful_episodes = 0\n",
    "        total_steps_list = []\n",
    "        success_steps = []\n",
    "\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "        for episode in range(self.episodes):\n",
    "            states = self.env.reset()\n",
    "            episode_reward = [0] * self.env.num_agents\n",
    "            total_steps = 0 \n",
    "            self.own_temp_case_bases = [[] for _ in range(self.env.num_agents)]\n",
    "            self.comm_temp_case_bases = [[] for _ in range(self.env.num_agents)]\n",
    "            success_count = [0] * self.env.num_agents\n",
    "            dones = [False] * self.env.num_agents\n",
    "            win_states = [False] * self.env.num_agents\n",
    "            successful_episodes = False\n",
    "\n",
    "            while not(all(dones)):\n",
    "                print(f\"----- starting point of Episode {episode} in steps {total_steps} loop -----\")\n",
    "                \n",
    "                actions = []\n",
    "                for agent_idx in range(self.env.num_agents):\n",
    "                    state = states[agent_idx]\n",
    "                    action = self.take_action(agent_idx, state)\n",
    "                    actions.append(action)\n",
    "\n",
    "                next_states, rewards, dones = self.env.step(actions)\n",
    "\n",
    "                win_states = []\n",
    "                for agent_idx in range(self.env.num_agents):\n",
    "                    state = states[agent_idx]\n",
    "                    action = actions[agent_idx]\n",
    "                    reward = rewards[agent_idx]\n",
    "                    next_state = next_states[agent_idx]\n",
    "\n",
    "                    physical_state = tuple(state[0])\n",
    "                    win_state = state[1]\n",
    "                    comm_state = state[2]  # Communication state containing messages from other agents\n",
    "\n",
    "                    physical_next_state = tuple(next_state[0])\n",
    "                    win_next_state = next_state[1]\n",
    "                    comm_next_state = tuple(next_state[2]) if next_state[2] != 0 else next_state[2]\n",
    "\n",
    "                    physical_action = action[0]\n",
    "                    comm_action = action[1]\n",
    "\n",
    "                    print(f\"comm next state for agent {agent_idx}: {comm_next_state}\")\n",
    "                    \n",
    "                    if (comm_next_state == 0):\n",
    "                        pass\n",
    "                    else:\n",
    "                        comm_case = Case(problem=comm_next_state[0], solution=comm_next_state[1], trust_value=comm_next_state[2], total_time_steps=comm_next_state[3])\n",
    "                        Case.reuse(agent_idx, comm_case, self.own_temp_case_bases[agent_idx], self.comm_temp_case_bases[agent_idx], source='comm')\n",
    "\n",
    "\n",
    "                    c = Case(physical_state, physical_action, total_time_steps=total_steps)\n",
    "                    Case.reuse(agent_idx, c, self.own_temp_case_bases[agent_idx], self.comm_temp_case_bases[agent_idx], source='own')\n",
    "\n",
    "                    if self.action_type[agent_idx] == 0:\n",
    "                        if not env.locked[agent_idx]:\n",
    "                            print(f\"action type of agent: {agent_idx}: problem solver, agent learned\")\n",
    "                            self.problem_solvers[agent_idx].learn(agent_idx, physical_state, physical_action, reward, physical_next_state, self.case_bases[agent_idx])\n",
    "                        else:\n",
    "                            print(f\"action type of agent: {agent_idx}: using problem solver but locked, no learning\")\n",
    "                    else:\n",
    "                        print(f\"action type of agent: {agent_idx}: using solution from case base, no learning\")\n",
    "                    \n",
    "                    if (win_next_state): \n",
    "                        success_count[agent_idx] += 1\n",
    "\n",
    "                    episode_reward[agent_idx] += reward\n",
    "                    win_states.append(win_next_state)  \n",
    "\n",
    "                states = next_states\n",
    "                total_steps += 1\n",
    "\n",
    "                self.env.render()\n",
    "                \n",
    "            if self.env.win_flag:\n",
    "                self.total_successful_episodes += 1\n",
    "                success_steps.append(total_steps)\n",
    "                successful_episodes = True\n",
    "\n",
    "            for agent_idx in range(self.env.num_agents):\n",
    "                print(f\"win status of agent {agent_idx}  before update the case base: {win_states[agent_idx]}\")\n",
    "                self.rewards_per_episode[agent_idx].append(episode_reward[agent_idx])\n",
    "\n",
    "                print(f\"agent{agent_idx} own temp case base: {self.own_temp_case_bases[agent_idx]}\")\n",
    "                print(f\"agent{agent_idx} comm temp case base: {self.comm_temp_case_bases[agent_idx]}\")\n",
    "                \n",
    "                Case.revise(agent_idx, self.case_bases[agent_idx], self.own_temp_case_bases[agent_idx], win_states[agent_idx])\n",
    "                self.case_bases[agent_idx] = Case.retain(agent_idx, self.case_bases[agent_idx], self.own_temp_case_bases[agent_idx], self.comm_temp_case_bases[agent_idx], win_states[agent_idx])\n",
    "               \n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "            \n",
    "            memory_usage.append(psutil.virtual_memory().percent)\n",
    "            gpu_memory_usage.append(pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1024**2)\n",
    "\n",
    "            print(f\"Episode: {episode}, Total Steps: {total_steps}, Total Rewards: {episode_reward}, Status Episode: {successful_episodes}\")\n",
    "            print(f\"------------------------------------------End of episode {episode} loop--------------------\")\n",
    "\n",
    "        success_rate = self.total_successful_episodes / self.episodes * 100\n",
    "\n",
    "        return self.rewards_per_episode, success_rate, memory_usage, gpu_memory_usage, success_steps\n",
    "\n",
    "    def take_action(self, agent_idx, state):\n",
    "        physical_state = tuple(state[0])\n",
    "        win_state = state[1]\n",
    "        comm_state = state[2]\n",
    "\n",
    "        similar_solution = Case.retrieve(agent_idx, physical_state, self.case_bases[agent_idx])\n",
    "        if similar_solution is not None:\n",
    "            physical_action = similar_solution.solution\n",
    "            comm_action = (similar_solution.problem, similar_solution.solution, similar_solution.trust_value, similar_solution.total_time_steps)\n",
    "            self.action_type[agent_idx] = 1\n",
    "            print(f\"Physical Action for Agent {agent_idx} from case base: {physical_action}\")\n",
    "        else:\n",
    "            physical_action = self.problem_solvers[agent_idx].choose_action(agent_idx, physical_state)\n",
    "            comm_action = 0  # No communication action if using problem solver action\n",
    "            self.action_type[agent_idx] = 0\n",
    "            print(f\"Physical Action for Agent {agent_idx} from problem solver: {physical_action}\")\n",
    "\n",
    "        return (physical_action, comm_action)\n",
    "\n",
    "    def case_exists_in_case_base(self, case, case_base):\n",
    "        \"\"\"Check if a case exists in the given case base.\"\"\"\n",
    "        return any(existing_case.problem == case.problem and existing_case.solution == case.solution for existing_case in case_base)\n",
    "        \n",
    "    def save_case_base_temporary(self):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            filename = f\"cases/case_base_temporary_agent_{agent_idx}.json\"\n",
    "            case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem, \n",
    "                            \"solution\": int(case.solution), \n",
    "                            \"trust_value\": float(case.trust_value),\n",
    "                            \"total_time_steps\": int(case.total_time_steps)} for case in self.own_temp_case_bases[agent_idx] + self.comm_temp_case_bases[agent_idx]]\n",
    "            with open(filename, 'w') as file:\n",
    "                json.dump(case_base_data, file)\n",
    "            print(f\"Temporary case base for Agent {agent_idx} saved successfully.\")\n",
    "\n",
    "    def save_case_base(self):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            filename = f\"cases/case_base_agent_{agent_idx}.json\"\n",
    "            case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem, \n",
    "                            \"solution\": int(case.solution), \n",
    "                            \"trust_value\": float(case.trust_value),\n",
    "                            \"total_time_steps\": int(case.total_time_steps)} for case in self.case_bases[agent_idx]]\n",
    "            with open(filename, 'w') as file:\n",
    "                json.dump(case_base_data, file)\n",
    "            print(f\"Case base for Agent {agent_idx} saved successfully.\")\n",
    "        \n",
    "    def load_case_base(self):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            filename = f\"cases/case_base_agent_{agent_idx}.json\"\n",
    "            try:\n",
    "                with open(filename, 'r') as file:\n",
    "                    case_base_data = json.load(file)\n",
    "                    self.case_bases[agent_idx] = [Case(np.array(case[\"problem\"]), case[\"solution\"], case[\"trust_value\"], case[\"total_time_steps\"]) for case in case_base_data]\n",
    "                    print(f\"Case base for Agent {agent_idx} loaded successfully.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Case base file for Agent {agent_idx} not found. Starting with an empty case base.\")\n",
    "\n",
    "    def display_success_rate(self, success_rate):\n",
    "        print(f\"Success rate: {success_rate}%\")\n",
    "\n",
    "    def plot_rewards(self, rewards):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            plt.plot([reward for reward in rewards[agent_idx]], label=f'Agent {agent_idx}')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Rewards over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_total_steps(self, total_steps_list):\n",
    "        plt.plot(total_steps_list)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Steps')\n",
    "        plt.title('Total Steps for Successful Episodes over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_resources(self, memory_usage, gpu_memory_usage):\n",
    "        plt.plot(memory_usage, label='Memory (%)')\n",
    "        plt.plot(gpu_memory_usage, label='GPU Memory (MB)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Resource Usage')\n",
    "        plt.title('Resource Usage over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_agents = 2\n",
    "    num_obstacles = 5\n",
    "    obstacles_random_steps = 35\n",
    "    is_agent_silent = False\n",
    "    episodes = 100\n",
    "    max_steps = 1000\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    epsilon = 0.3\n",
    "    epsilon_decay = 0.995  \n",
    "    epsilon_min = 0.01  \n",
    "    render = True\n",
    "\n",
    "    env = Env(num_agents=num_agents, num_obstacles=num_obstacles, obstacles_random_steps=obstacles_random_steps, is_agent_silent=is_agent_silent)\n",
    "    \n",
    "    num_actions = len(env.action_space)\n",
    "    \n",
    "    agent = QCBRL(num_actions, env, episodes, max_steps, alpha, gamma, epsilon, epsilon_decay, epsilon_min, render)\n",
    "    rewards, success_rate, memory_usage, gpu_memory_usage, total_step_list = agent.run()\n",
    "\n",
    "    agent.display_success_rate(success_rate)\n",
    "    agent.plot_rewards(rewards)\n",
    "    agent.plot_total_steps(total_step_list)\n",
    "    agent.plot_resources(memory_usage, gpu_memory_usage)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
