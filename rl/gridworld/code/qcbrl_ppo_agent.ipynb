{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state to be measured: [0, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [1, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [1, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [1, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 1]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 1]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [1, 1]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 1]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 1]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [1, 1]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [1, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [1, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [1, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [1, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [1, 1]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 1]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 1]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [1, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [2, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [2, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [3, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [4, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [3, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [3, 1]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [3, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [4, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "Episode 1, Total Reward: -129\n",
      "state to be measured: [0, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [1, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [1, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [1, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 0]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [0, 1]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [1, 1]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "state to be measured: [2, 1]\n",
      "Case base to be measured: []\n",
      "action from problem solver\n",
      "Episode 2, Total Reward: 84\n",
      "state to be measured: [0, 0]\n",
      "Case base to be measured: [<__main__.Case object at 0x7a5a9e3c8cd0>, <__main__.Case object at 0x7a5a9e3c8b50>, <__main__.Case object at 0x7a5a9e3c8940>, <__main__.Case object at 0x7a5a9e3c8760>, <__main__.Case object at 0x7a5a9e3ab790>]\n",
      "action from case base\n",
      "state to be measured: [0, 1]\n",
      "Case base to be measured: [<__main__.Case object at 0x7a5a9e3c8cd0>, <__main__.Case object at 0x7a5a9e3c8b50>, <__main__.Case object at 0x7a5a9e3c8940>, <__main__.Case object at 0x7a5a9e3c8760>, <__main__.Case object at 0x7a5a9e3ab790>]\n",
      "action from case base\n",
      "state to be measured: [1, 1]\n",
      "Case base to be measured: [<__main__.Case object at 0x7a5a9e3c8cd0>, <__main__.Case object at 0x7a5a9e3c8b50>, <__main__.Case object at 0x7a5a9e3c8940>, <__main__.Case object at 0x7a5a9e3c8760>, <__main__.Case object at 0x7a5a9e3ab790>]\n",
      "action from case base\n",
      "state to be measured: [2, 1]\n",
      "Case base to be measured: [<__main__.Case object at 0x7a5a9e3c8cd0>, <__main__.Case object at 0x7a5a9e3c8b50>, <__main__.Case object at 0x7a5a9e3c8940>, <__main__.Case object at 0x7a5a9e3c8760>, <__main__.Case object at 0x7a5a9e3ab790>]\n",
      "action from case base\n",
      "Episode 3, Total Reward: 97\n",
      "state to be measured: [0, 0]\n",
      "Case base to be measured: [<__main__.Case object at 0x7a5a9e3c8cd0>, <__main__.Case object at 0x7a5a9e3c8b50>, <__main__.Case object at 0x7a5a9e3c8940>, <__main__.Case object at 0x7a5a9e3c8760>, <__main__.Case object at 0x7a5a9e3ab790>]\n",
      "action from case base\n",
      "state to be measured: [0, 1]\n",
      "Case base to be measured: [<__main__.Case object at 0x7a5a9e3c8cd0>, <__main__.Case object at 0x7a5a9e3c8b50>, <__main__.Case object at 0x7a5a9e3c8940>, <__main__.Case object at 0x7a5a9e3c8760>, <__main__.Case object at 0x7a5a9e3ab790>]\n",
      "action from case base\n",
      "state to be measured: [1, 1]\n",
      "Case base to be measured: [<__main__.Case object at 0x7a5a9e3c8cd0>, <__main__.Case object at 0x7a5a9e3c8b50>, <__main__.Case object at 0x7a5a9e3c8940>, <__main__.Case object at 0x7a5a9e3c8760>, <__main__.Case object at 0x7a5a9e3ab790>]\n",
      "action from case base\n",
      "state to be measured: [2, 1]\n",
      "Case base to be measured: [<__main__.Case object at 0x7a5a9e3c8cd0>, <__main__.Case object at 0x7a5a9e3c8b50>, <__main__.Case object at 0x7a5a9e3c8940>, <__main__.Case object at 0x7a5a9e3c8760>, <__main__.Case object at 0x7a5a9e3ab790>]\n",
      "action from case base\n",
      "Episode 4, Total Reward: 97\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 365\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# print(num_actions)\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# print(num_states)\u001b[39;00m\n\u001b[1;32m    364\u001b[0m agent \u001b[38;5;241m=\u001b[39m QCBRL(state_dim, action_dim, env)\n\u001b[0;32m--> 365\u001b[0m rewards, success_rate, memory_usage, gpu_memory_usage, total_step_list \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m agent\u001b[38;5;241m.\u001b[39mdisplay_success_rate(success_rate)\n\u001b[1;32m    368\u001b[0m agent\u001b[38;5;241m.\u001b[39mplot_rewards(rewards)\n",
      "Cell \u001b[0;32mIn[1], line 205\u001b[0m, in \u001b[0;36mQCBRL.run\u001b[0;34m(self, episodes, max_steps, alpha, gamma, epsilon, render)\u001b[0m\n\u001b[1;32m    202\u001b[0m handle \u001b[38;5;241m=\u001b[39m pynvml\u001b[38;5;241m.\u001b[39mnvmlDeviceGetHandleByIndex(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[0;32m--> 205\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     states, actions, rewards, dones, next_states \u001b[38;5;241m=\u001b[39m [], [], [], [], []\n\u001b[1;32m    207\u001b[0m     total_rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/PHD/Research/code/myphd/rl/gridworld/code/environment_dynamic_4obs.py:63\u001b[0m, in \u001b[0;36mEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mcoords(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrectangle)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mmove(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrectangle, UNIT \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m x, UNIT \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import pynvml\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "# from gym.envs.registration import register\n",
    "from environment_static import Env\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Actor-critic network architecture\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.actor = nn.Linear(64, action_dim)\n",
    "        self.critic = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "    \n",
    "class ProblemSolver:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.policy = ActorCritic(num_states, num_actions)\n",
    "        self.learning_rate = 0.0005\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=self.learning_rate)\n",
    "        self.gamma = 0.99\n",
    "        self.eps_clip = 0.2\n",
    "        self.update_timestep = 20\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        logits, _ = self.policy(state)\n",
    "        action_probs = torch.softmax(logits, dim=-1)\n",
    "        action = torch.multinomial(action_probs, 1)\n",
    "        return action.item()\n",
    "\n",
    "    def calculate_advantages(self, rewards, dones, values, next_value):\n",
    "        advantages = []\n",
    "        discounted_sum = 0\n",
    "        for i in range(len(rewards) - 1, -1, -1):\n",
    "            discounted_sum = rewards[i] + self.gamma * discounted_sum * (1 - dones[i])\n",
    "            advantage = discounted_sum - values[i].item()\n",
    "            advantages.insert(0, advantage)\n",
    "        return advantages\n",
    "\n",
    "    def update_policy(self, states, actions, advantages, returns):\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        advantages = torch.FloatTensor(advantages).unsqueeze(1)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "\n",
    "        unique_actions, unique_indices = torch.unique(actions, return_inverse=True)\n",
    "\n",
    "        logits, values = self.policy(states)\n",
    "        values = values.squeeze()\n",
    "\n",
    "        action_probs = torch.softmax(logits, dim=-1)\n",
    "        action_masks = torch.zeros_like(action_probs).scatter_(1, unique_actions.unsqueeze(1), 1)\n",
    "        old_action_probs = torch.sum(action_probs * action_masks[unique_indices.unsqueeze(1)], dim=1)\n",
    "        ratios = torch.exp(torch.log(old_action_probs + 1e-10) - torch.log(action_probs + 1e-10))\n",
    "\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "        actor_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        critic_loss = nn.MSELoss()(returns, values)\n",
    "\n",
    "        loss = actor_loss + 0.5 * critic_loss\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "class Case:\n",
    "    added_states = set()  # Class attribute to store states already added to the case base\n",
    "\n",
    "    def __init__(self, problem, solution, trust_value=1):\n",
    "        self.problem = problem if isinstance(problem, list) else ast.literal_eval(problem)  # Convert problem to numpy array\n",
    "        self.solution = solution\n",
    "        self.trust_value = trust_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)  # Ensure state1 is at least 1-dimensional\n",
    "        state2 = np.atleast_1d(state2)  # Ensure state2 is at least 1-dimensional\n",
    "        CNDMaxDist = 6  # Maximum distance between two nodes in the CND\n",
    "        v = state1.size  # Total number of objects the agent can perceive\n",
    "        DistQ = np.sum([Case.Dmin_phi(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def Dmin_phi(X1, X2):\n",
    "        return np.max(np.abs(X1 - X2))\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(state, case_base, threshold=0.2):\n",
    "        state = ast.literal_eval(state)\n",
    "        print(f\"state to be measured: {state}\")\n",
    "        print(f\"Case base to be measured: {case_base}\")\n",
    "        similarities = {}\n",
    "        for case in case_base:\n",
    "            # Convert strings to numerical values if necessary\n",
    "            problem_numeric = np.array(case.problem, dtype=float)\n",
    "            state_numeric = np.array(state, dtype=float)\n",
    "            similarities[case] = Case.sim_q(state_numeric, problem_numeric)  # Compare state with the problem part of the case\n",
    "        \n",
    "        sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if sorted_similarities:\n",
    "            most_similar_case = sorted_similarities[0][0] if sorted_similarities[0][1] >= threshold else None\n",
    "        else:\n",
    "            most_similar_case = None\n",
    "        \n",
    "        return most_similar_case\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(c, temporary_case_base):\n",
    "        temporary_case_base.append(c)\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def revise(case_base, temporary_case_base, successful_episodes):\n",
    "        for case in temporary_case_base:\n",
    "            if successful_episodes and case in case_base:\n",
    "                case.trust_value += 0.1  # Increment trust value if the episode ended successfully and the case is in the case base\n",
    "            elif not successful_episodes and case in case_base:\n",
    "                case.trust_value -= 0.1  # Decrement trust value if the episode ended unsuccessfully and the case is in the case base\n",
    "            case.trust_value = max(0, min(case.trust_value,1))  # Ensure trust value is within[0,1]\n",
    "\n",
    "    @staticmethod\n",
    "    def retain(case_base, temporary_case_base, successful_episodes, threshold=0):\n",
    "        if successful_episodes:\n",
    "            # Iterate through the temporary case base to find the last occurrence of each unique state\n",
    "            for case in reversed(temporary_case_base):\n",
    "                state = tuple(np.atleast_1d(case.problem))\n",
    "                # Check if the state is already in the case base or has been added previously\n",
    "                if state not in Case.added_states:\n",
    "                    # Add the case to the case base if the state is new\n",
    "                    case_base.append(case)\n",
    "                    Case.added_states.add(state)\n",
    "            \n",
    "            # Filter case_base based on trust_value\n",
    "            filtered_case_base = []\n",
    "            for case in case_base:\n",
    "                # print(f\"trust value >= Threshold?: {case.trust_value} >= {threshold}?\")\n",
    "                if case.trust_value >= threshold:\n",
    "                    # print(f\"problem | trust value: {case.problem} | {case.trust_value}\")\n",
    "                    # print(\"case saved dong\")\n",
    "                    filtered_case_base.append(case)\n",
    "                else:\n",
    "                    # print(f\"problem | trust value: {case.problem} | {case.trust_value}\")\n",
    "                    # print(\"case unsaved dong\")\n",
    "                    pass\n",
    "\n",
    "            return filtered_case_base\n",
    "        else:\n",
    "            return case_base  # Return original case_base if episode is not successful\n",
    "\n",
    "            \n",
    "class QCBRL:\n",
    "    def __init__(self, num_states, num_actions, env):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.env = env\n",
    "        self.problem_solver = ProblemSolver(num_states, num_actions)\n",
    "        self.case_base = []\n",
    "        self.temporary_case_base = []\n",
    "\n",
    "    def run(self, episodes, max_steps, alpha=0.1, gamma=0.9, epsilon=0.1, render=False):\n",
    "        # rewards = []\n",
    "        # episode_rewards = []\n",
    "        memory_usage = []\n",
    "        gpu_memory_usage = []\n",
    "        successful_episodes = False\n",
    "        num_successful_episodes = 0\n",
    "        total_steps_list = []\n",
    "\n",
    "        total_timesteps = 0\n",
    "        episode_rewards = []\n",
    "        success_count = 0\n",
    "        success_steps = []\n",
    "\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            states, actions, rewards, dones, next_states = [], [], [], [], []\n",
    "            total_rewards = 0\n",
    "            episode_reward = 0\n",
    "            total_steps = 0 \n",
    "            self.temporary_case_base = []\n",
    "            \n",
    "            for _ in range(max_steps):\n",
    "                \n",
    "                episode_steps = 0\n",
    "                # if render:\n",
    "                #     env.render()\n",
    "                action = self.take_action(state, epsilon)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "                next_states.append(next_state)\n",
    "\n",
    "                c = Case(str(state), action)\n",
    "                Case.reuse(c, self.temporary_case_base)\n",
    "                \n",
    "                state = next_state\n",
    "                total_steps += 1\n",
    "                episode_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    successful_episodes = reward > 0\n",
    "                    total_rewards += episode_reward\n",
    "                    if episode_reward >= 0:  # Considered successful if the total reward is 200 or more\n",
    "                        success_count += 1\n",
    "                        success_steps.append(episode_steps)\n",
    "                    else:\n",
    "                        success_steps.append(0)\n",
    "                    break\n",
    "                \n",
    "                \n",
    "            if episode_reward > 0:  # If the agent reached the goal state\n",
    "                num_successful_episodes += 1\n",
    "                total_steps_list.append(total_steps)  # Append total steps for this episode   \n",
    "            else:\n",
    "                total_steps_list.append(0)\n",
    "            \n",
    "            if total_timesteps % self.problem_solver.update_timestep == 0:\n",
    "                _, next_value = self.problem_solver.policy(torch.FloatTensor(next_states))\n",
    "                returns = []\n",
    "                discounted_sum = 0\n",
    "                for i in range(len(rewards) - 1, -1, -1):\n",
    "                    discounted_sum = rewards[i] + self.problem_solver.gamma * discounted_sum * (1 - dones[i])\n",
    "                    returns.insert(0, discounted_sum)\n",
    "                \n",
    "                # Convert states to tensor before passing to policy network\n",
    "                states_tensor = torch.FloatTensor(states)\n",
    "                \n",
    "                advantages = self.problem_solver.calculate_advantages(rewards, dones, torch.cat((self.problem_solver.policy(states_tensor)[1], next_value.detach()), 0), next_value)\n",
    "                self.problem_solver.update_policy(states, actions, advantages, returns)\n",
    "\n",
    "\n",
    "            episode_rewards.append(episode_reward)\n",
    "            print(f\"Episode {episode + 1}, Total Reward: {episode_reward}\")\n",
    "\n",
    "            Case.revise(self.case_base, self.temporary_case_base, successful_episodes)\n",
    "            self.case_base = Case.retain(self.case_base, self.temporary_case_base, successful_episodes)\n",
    "            \n",
    "            memory_usage.append(psutil.virtual_memory().percent)\n",
    "            gpu_memory_usage.append(pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1024**2)\n",
    "\n",
    "        \n",
    "        self.save_case_base_temporary()  # Save temporary case base after training\n",
    "        self.save_case_base()  # Save case base after training\n",
    "\n",
    "        success_rate = (num_successful_episodes / episodes) * 100\n",
    "        # print(f\"Successful episodes: {num_successful_episodes}%\")\n",
    "\n",
    "        # env.close()\n",
    "        return episode_rewards, success_rate, memory_usage, gpu_memory_usage, total_steps_list\n",
    "\n",
    "    def take_action(self, state, epsilon):\n",
    "        # print(f\"state before action: {state}\")\n",
    "        state_str = str(state)\n",
    "        similar_solution = Case.retrieve(state_str, self.case_base)\n",
    "        if similar_solution is not None:\n",
    "            action = similar_solution.solution\n",
    "            print(\"action from case base\")\n",
    "        else:\n",
    "            action = self.problem_solver.choose_action(state)\n",
    "            print(\"action from problem solver\")\n",
    "        \n",
    "        # action = self.problem_solver.choose_action(state)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def save_case_base_temporary(self):\n",
    "        filename = \"case_base_temporary.json\"\n",
    "        case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem, \n",
    "                        \"solution\": int(case.solution), \n",
    "                        \"trust_value\": int(case.trust_value)} for case in self.temporary_case_base]\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "        print(\"Temporary case base saved successfully.\")\n",
    "\n",
    "    def save_case_base(self):\n",
    "        filename = \"case_base.json\"\n",
    "        case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem, \n",
    "                        \"solution\": int(case.solution), \n",
    "                        \"trust_value\": int(case.trust_value)} for case in self.case_base]\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "            print(\"Case base saved successfully.\")  # Add this line to check if the case base is being saved\n",
    "        \n",
    "    def load_case_base(self):\n",
    "        filename = \"case_base.json\"\n",
    "        try:\n",
    "            with open(filename, 'r') as file:\n",
    "                case_base_data = json.load(file)\n",
    "                self.case_base = [Case(np.array(case[\"problem\"]), case[\"solution\"], case[\"trust_value\"]) for case in case_base_data]\n",
    "                print(\"Case base loaded successfully.\")  # Add this line to check if the case base is being loaded\n",
    "        except FileNotFoundError:\n",
    "            print(\"Case base file not found. Starting with an empty case base.\")\n",
    "\n",
    "    \n",
    "    def display_success_rate(self, success_rate):\n",
    "        print(f\"Success rate: {success_rate}%\")\n",
    "\n",
    "\n",
    "    def plot_rewards(self, rewards):\n",
    "        plt.plot(rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Rewards over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show() \n",
    "\n",
    "    def plot_resources(self, memory_usage, gpu_memory_usage):\n",
    "        plt.plot(memory_usage, label='Memory (%)')\n",
    "        plt.plot(gpu_memory_usage, label='GPU Memory (MB)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Resource Usage')\n",
    "        plt.title('Resource Usage over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_total_steps(self, total_steps_list):\n",
    "        plt.plot(total_steps_list)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Steps')\n",
    "        plt.title('Total Steps for Successful Episodes over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show() \n",
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    state_dim = len(env.reset())\n",
    "    action_dim = len(env.action_space)\n",
    "    # print(num_actions)\n",
    "    # print(num_states)\n",
    "    agent = QCBRL(state_dim, action_dim, env)\n",
    "    rewards, success_rate, memory_usage, gpu_memory_usage, total_step_list = agent.run(episodes=100, max_steps=1000, alpha=0.1, gamma=0.9, epsilon=0.1)\n",
    "\n",
    "    agent.display_success_rate(success_rate)\n",
    "    agent.plot_rewards(rewards)\n",
    "    agent.plot_total_steps(total_step_list)\n",
    "    agent.plot_resources(memory_usage, gpu_memory_usage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
