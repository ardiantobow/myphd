{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import pynvml\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "# from gym.envs.registration import register\n",
    "from environment import Env\n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, num_actions, env):\n",
    "        self.env = env\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = 0.01\n",
    "        self.discount_factor = 0.9\n",
    "        self.epsilon = 0.1\n",
    "        self.q_table = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0])\n",
    "        self.rewards_per_episode = []\n",
    "\n",
    "    @staticmethod\n",
    "    def arg_max(state_action):\n",
    "        max_index_list = []\n",
    "        max_value = state_action[0]\n",
    "        for index, value in enumerate(state_action):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        # print(f\"state inner choose_action: {state}\")\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            # action = np.random.choice(self.num_actions)\n",
    "            state_action = self.q_table[state]\n",
    "            action = self.arg_max(state_action)\n",
    "        return action\n",
    "\n",
    "    def update_Q(self, state, action, reward, next_state, next_action):\n",
    "        current_q = self.q_table[state][action]\n",
    "        max_next_q = max(self.q_table[next_state])\n",
    "        new_q = (current_q + self.learning_rate *\n",
    "                (reward + self.discount_factor * max_next_q - current_q))\n",
    "        self.q_table[state][action] = new_q\n",
    "\n",
    "\n",
    "\n",
    "class Case:\n",
    "    added_states = set()  # Class attribute to store states already added to the case base\n",
    "\n",
    "    def __init__(self, problem, solution, trust_value=1):\n",
    "        self.problem = problem if isinstance(problem, list) else ast.literal_eval(problem)  # Convert problem to numpy array\n",
    "        self.solution = solution\n",
    "        self.trust_value = trust_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)  # Ensure state1 is at least 1-dimensional\n",
    "        state2 = np.atleast_1d(state2)  # Ensure state2 is at least 1-dimensional\n",
    "        CNDMaxDist = 6  # Maximum distance between two nodes in the CND\n",
    "        v = state1.size  # Total number of objects the agent can perceive\n",
    "        DistQ = np.sum([Case.Dmin_phi(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def Dmin_phi(X1, X2):\n",
    "        return np.max(np.abs(X1 - X2))\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(state, case_base, threshold=0.2):\n",
    "        state = ast.literal_eval(state)\n",
    "        # print(f\"state to be measured: {state}\")\n",
    "        similarities = {}\n",
    "        for case in case_base:\n",
    "            # Convert strings to numerical values if necessary\n",
    "            problem_numeric = np.array(case.problem, dtype=float)\n",
    "            state_numeric = np.array(state, dtype=float)\n",
    "            similarities[case] = Case.sim_q(state_numeric, problem_numeric)  # Compare state with the problem part of the case\n",
    "        \n",
    "        sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if sorted_similarities:\n",
    "            most_similar_case = sorted_similarities[0][0] if sorted_similarities[0][1] >= threshold else None\n",
    "        else:\n",
    "            most_similar_case = None\n",
    "        \n",
    "        return most_similar_case\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(c, temporary_case_base):\n",
    "        temporary_case_base.append(c)\n",
    "        # Store the new case from the problem solver\n",
    "        # if c not in temporary_case_base:\n",
    "            # temporary_case_base.append(c)\n",
    "        \n",
    "        # Check if there are similar cases in case_base\n",
    "        # similar_cases = [case for case in case_base if np.array_equal(case.problem, c.problem)]\n",
    "        # for similar_case in similar_cases:\n",
    "            # temporary_case_base.append(similar_case)\n",
    "            # if similar_case not in temporary_case_base:\n",
    "            #     temporary_case_base.append(similar_case)\n",
    "\n",
    "    @staticmethod\n",
    "    def revise(case_base, temporary_case_base, successful_episodes):\n",
    "        for case in temporary_case_base:\n",
    "            if successful_episodes and case in case_base:\n",
    "                case.trust_value += 0.1  # Increment trust value if the episode ended successfully and the case is in the case base\n",
    "            elif not successful_episodes and case in case_base:\n",
    "                case.trust_value -= 0.1  # Decrement trust value if the episode ended unsuccessfully and the case is in the case base\n",
    "            case.trust_value = max(0, min(case.trust_value,1))  # Ensure trust value is within[0,1]\n",
    "\n",
    "    @staticmethod\n",
    "    def retain(case_base, temporary_case_base, successful_episodes, threshold=0):\n",
    "        if successful_episodes:\n",
    "            # Iterate through the temporary case base to find the last occurrence of each unique state\n",
    "            for case in reversed(temporary_case_base):\n",
    "                state = tuple(np.atleast_1d(case.problem))\n",
    "                # Check if the state is already in the case base or has been added previously\n",
    "                if state not in Case.added_states:\n",
    "                    # Add the case to the case base if the state is new\n",
    "                    case_base.append(case)\n",
    "                    Case.added_states.add(state)\n",
    "            \n",
    "            # Filter case_base based on trust_value\n",
    "            filtered_case_base = []\n",
    "            for case in case_base:\n",
    "                # print(f\"trust value >= Threshold?: {case.trust_value} >= {threshold}?\")\n",
    "                if case.trust_value >= threshold:\n",
    "                    # print(f\"problem | trust value: {case.problem} | {case.trust_value}\")\n",
    "                    # print(\"case saved dong\")\n",
    "                    filtered_case_base.append(case)\n",
    "                else:\n",
    "                    # print(f\"problem | trust value: {case.problem} | {case.trust_value}\")\n",
    "                    # print(\"case unsaved dong\")\n",
    "                    pass\n",
    "\n",
    "            return filtered_case_base\n",
    "        else:\n",
    "            return case_base  # Return original case_base if episode is not successful\n",
    "\n",
    "            \n",
    "class QCBRL:\n",
    "    def __init__(self, num_states, num_actions, env):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.env = env\n",
    "        self.problem_solver = ProblemSolver(num_actions, env)\n",
    "        self.case_base = []\n",
    "        self.temporary_case_base = []\n",
    "\n",
    "    def run(self, episodes, max_steps, alpha=0.1, gamma=0.9, epsilon=0.1, render=False):\n",
    "        rewards = []\n",
    "        # episode_rewards = []\n",
    "        memory_usage = []\n",
    "        gpu_memory_usage = []\n",
    "        successful_episodes = False\n",
    "        num_successful_episodes = 0\n",
    "        total_steps_list = []\n",
    "\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            # print(f\"state: {state}\")\n",
    "            episode_reward = 0\n",
    "            total_steps = 0 \n",
    "            self.temporary_case_base = []\n",
    "            \n",
    "            \n",
    "\n",
    "            for _ in range(max_steps):\n",
    "                if render:\n",
    "                    env.render()\n",
    "                action = self.take_action(str(state), epsilon)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "\n",
    "                # print(f\"State: {state} - Action: {action} - Reward: {reward} - Done: {done}\")\n",
    "\n",
    "                c = Case(str(state), action)\n",
    "                Case.reuse(c, self.temporary_case_base)\n",
    "\n",
    "                next_action = self.take_action(str(next_state), epsilon)\n",
    "                self.problem_solver.update_Q(str(state), action, reward, str(next_state), next_action)\n",
    "\n",
    "                state = next_state\n",
    "                total_steps += 1\n",
    "                episode_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    successful_episodes = reward > 0\n",
    "                    break\n",
    "                \n",
    "                \n",
    "            if episode_reward > 0:  # If the agent reached the goal state\n",
    "                num_successful_episodes += 1\n",
    "                total_steps_list.append(total_steps)  # Append total steps for this episode   \n",
    "            else:\n",
    "                total_steps_list.append(0)\n",
    "\n",
    "            rewards.append(episode_reward)\n",
    "            print(f\"Episode {episode + 1}, Total Reward: {episode_reward}\")\n",
    "\n",
    "            Case.revise(self.case_base, self.temporary_case_base, successful_episodes)\n",
    "            self.case_base = Case.retain(self.case_base, self.temporary_case_base, successful_episodes)\n",
    "            \n",
    "            memory_usage.append(psutil.virtual_memory().percent)\n",
    "            gpu_memory_usage.append(pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1024**2)\n",
    "\n",
    "        \n",
    "        self.save_case_base_temporary()  # Save temporary case base after training\n",
    "        self.save_case_base()  # Save case base after training\n",
    "\n",
    "        success_rate = (num_successful_episodes / episodes) * 100\n",
    "        # print(f\"Successful episodes: {num_successful_episodes}%\")\n",
    "\n",
    "        # env.close()\n",
    "        return rewards, success_rate, memory_usage, gpu_memory_usage, total_steps_list\n",
    "\n",
    "    def take_action(self, state, epsilon):\n",
    "        # print(f\"state before action: {state}\")\n",
    "\n",
    "        similar_solution = Case.retrieve(state, self.case_base)\n",
    "        if similar_solution is not None:\n",
    "            action = similar_solution.solution\n",
    "            print(\"action from case base\")\n",
    "        else:\n",
    "            action = self.problem_solver.choose_action(state)\n",
    "            print(\"action from problem solver\")\n",
    "        \n",
    "        # action = self.problem_solver.choose_action(state)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def save_case_base_temporary(self):\n",
    "        filename = \"case_base_temporary.json\"\n",
    "        case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem, \n",
    "                        \"solution\": int(case.solution), \n",
    "                        \"trust_value\": int(case.trust_value)} for case in self.temporary_case_base]\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "        print(\"Temporary case base saved successfully.\")\n",
    "\n",
    "    def save_case_base(self):\n",
    "        filename = \"case_base.json\"\n",
    "        case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem, \n",
    "                        \"solution\": int(case.solution), \n",
    "                        \"trust_value\": int(case.trust_value)} for case in self.case_base]\n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "            print(\"Case base saved successfully.\")  # Add this line to check if the case base is being saved\n",
    "        \n",
    "    def load_case_base(self):\n",
    "        filename = \"case_base.json\"\n",
    "        try:\n",
    "            with open(filename, 'r') as file:\n",
    "                case_base_data = json.load(file)\n",
    "                self.case_base = [Case(np.array(case[\"problem\"]), case[\"solution\"], case[\"trust_value\"]) for case in case_base_data]\n",
    "                print(\"Case base loaded successfully.\")  # Add this line to check if the case base is being loaded\n",
    "        except FileNotFoundError:\n",
    "            print(\"Case base file not found. Starting with an empty case base.\")\n",
    "\n",
    "    \n",
    "    def display_success_rate(self, success_rate):\n",
    "        print(f\"Success rate: {success_rate}%\")\n",
    "\n",
    "\n",
    "    def plot_rewards(self, rewards):\n",
    "        plt.plot(rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Rewards over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show() \n",
    "\n",
    "    def plot_resources(self, memory_usage, gpu_memory_usage):\n",
    "        plt.plot(memory_usage, label='Memory (%)')\n",
    "        plt.plot(gpu_memory_usage, label='GPU Memory (MB)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Resource Usage')\n",
    "        plt.title('Resource Usage over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_total_steps(self, total_steps_list):\n",
    "        plt.plot(total_steps_list)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Steps')\n",
    "        plt.title('Total Steps for Successful Episodes over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show() \n",
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    \n",
    "    num_states = len(env.reset())\n",
    "    num_actions = list(range(env.n_actions))\n",
    "\n",
    "    # print(num_actions)\n",
    "    # print(num_states)\n",
    "    agent = QCBRL(num_states, num_actions, env)\n",
    "    rewards, success_rate, memory_usage, gpu_memory_usage, total_step_list = agent.run(episodes=20, max_steps=1000, alpha=0.1, gamma=0.9, epsilon=0.1)\n",
    "\n",
    "    agent.display_success_rate(success_rate)\n",
    "    agent.plot_rewards(rewards)\n",
    "    agent.plot_total_steps(total_step_list)\n",
    "    agent.plot_resources(memory_usage, gpu_memory_usage)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
