{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Case.retrieve() missing 1 required positional argument: 'case_base'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 462\u001b[0m\n\u001b[1;32m    459\u001b[0m max_steps_per_episode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    460\u001b[0m agent \u001b[38;5;241m=\u001b[39m QCBRL(num_agents, is_agent_silent, num_actions, max_steps_per_episode)\n\u001b[0;32m--> 462\u001b[0m rewards, success_rate, memory_usage, gpu_memory_usage, total_step_list \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m agent\u001b[38;5;241m.\u001b[39mdisplay_success_rate(success_rate)\n\u001b[1;32m    466\u001b[0m agent\u001b[38;5;241m.\u001b[39mplot_rewards(rewards)\n",
      "Cell \u001b[0;32mIn[1], line 229\u001b[0m, in \u001b[0;36mQCBRL.run\u001b[0;34m(self, episodes, max_steps, alpha, gamma, epsilon, render)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents):\n\u001b[1;32m    228\u001b[0m     state \u001b[38;5;241m=\u001b[39m states[agent_idx]\n\u001b[0;32m--> 229\u001b[0m     physical_action, comm_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcase_base\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     combination_actions\u001b[38;5;241m.\u001b[39mappend((physical_action, comm_action))\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m# print(f\"state agents {agent_idx}: {state}\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 312\u001b[0m, in \u001b[0;36mQCBRL.take_action\u001b[0;34m(self, agent_idx, state, case_base)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtake_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, agent_idx, state, case_base):\n\u001b[1;32m    309\u001b[0m \n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# print(f\"agent: {agent}\")\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m     case \u001b[38;5;241m=\u001b[39m \u001b[43mCase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcase_base\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m case:\n\u001b[1;32m    315\u001b[0m         physical_action \u001b[38;5;241m=\u001b[39m case\u001b[38;5;241m.\u001b[39msolution\n",
      "\u001b[0;31mTypeError\u001b[0m: Case.retrieve() missing 1 required positional argument: 'case_base'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import pynvml\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "from environment_ma import Env\n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, env, agent_id, alpha=0.1, gamma=0.99, epsilon=0.2, communication_weight=0.5):\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.q_table = {}\n",
    "        self.env = env\n",
    "        self.agent_id = agent_id\n",
    "        self.communication_weight = communication_weight  # Weight parameter for incorporating messages\n",
    "\n",
    "    def get_action(self, agent, env, agent_id, agent_obs):\n",
    "        agent_obs_cpu = agent_obs[:6].cpu().numpy()  # Transfer only the required slice to CPU\n",
    "        agent_obs = tuple(np.round(agent_obs_cpu, decimals=5))  # Round the observation\n",
    "\n",
    "        if agent_obs not in self.q_table:\n",
    "            self.q_table[agent_obs] = np.zeros(self.env.action_space[self.agent_id].n)\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Select a random action\n",
    "            action = np.random.randint(env.action_space[self.agent_id].n)\n",
    "        else:\n",
    "            # Select the action with the highest Q-value\n",
    "            action = np.argmax(self.q_table[agent_obs])\n",
    "        \n",
    "        return (action,)  # Return as a tuple\n",
    "\n",
    "    def update_q_table(self,  agent, env, agent_id, obs, action, reward, next_obs):\n",
    "        obs_key = tuple(np.round(obs.cpu().numpy(), decimals=5))  # Only transfer to CPU when necessary\n",
    "        next_obs_key = tuple(np.round(next_obs.cpu().numpy(), decimals=5))\n",
    "        action = int(action.item())  # Convert tensor to Python scalar\n",
    "\n",
    "        # print (f\"reward obtained = {reward}\")\n",
    "\n",
    "       \n",
    "        action_space_size = self.env.action_space[self.agent_id].n\n",
    "        \n",
    "        if obs_key not in self.q_table:\n",
    "            self.q_table[obs_key] = np.zeros(action_space_size)\n",
    "\n",
    "        if next_obs_key not in self.q_table:\n",
    "            self.q_table[next_obs_key] = np.zeros(action_space_size)\n",
    "\n",
    "        best_next_action = np.argmax(self.q_table[next_obs_key])\n",
    "        td_target = reward + self.gamma * self.q_table[next_obs_key][best_next_action]\n",
    "\n",
    "        td_error = td_target - self.q_table[obs_key][action]\n",
    "        self.q_table[obs_key][action] += self.alpha * td_error\n",
    "\n",
    "        print(f\"Agent {self.agent_id} - Updated Q-table for obs {obs_key}, action {action}, reward {reward}, next_obs {next_obs_key}\")\n",
    "\n",
    "    \n",
    "    def print_q_table(self):\n",
    "        print(f\"Q-table for Agent {self.agent_id}:\")\n",
    "        for state, actions in self.q_table.items():\n",
    "            print(f\"  State: {state}\")\n",
    "            for action, q_value in enumerate(actions):\n",
    "                print(f\"    Action: {action}, Q-value: {q_value:.5f}\")\n",
    "        print(f\"End of Q-table for Agent {self.agent_id}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Case:\n",
    "    added_states = set()  # Class attribute to store states already added to the case base\n",
    "\n",
    "    def __init__(self, problem, solution, trust_value=1):\n",
    "        self.problem = problem if isinstance(problem, list) else ast.literal_eval(problem)  # Convert problem to numpy array\n",
    "        self.solution = solution\n",
    "        self.trust_value = trust_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)\n",
    "        state2 = np.atleast_1d(state2)\n",
    "        CNDMaxDist = 6  # Maximum distance between two nodes in the-0.9, 0.7, 0.0, 0.0, 0.0, -0.9) CND based on EOPRA reference\n",
    "        v = state1.size  # Total number of objects the agent can perceive\n",
    "        DistQ = np.sum([Case.dist_q(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def dist_q(X1, X2):\n",
    "        return np.min(np.abs(X1 - X2))\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(agent, env, state, case_base, threshold=0.1):\n",
    "\n",
    "        # Slice the physical observations\n",
    "        physical_obs = state[:6]\n",
    "\n",
    "        if not agent.silent:\n",
    "            comm_obs = state[6:]\n",
    "            \n",
    "        # print(f\"physical_obs = {physical_obs}\")\n",
    "\n",
    "        # Ensure the state is in a list format to avoid issues with ast.literal_eval\n",
    "        state_list = state.tolist() if isinstance(state, np.ndarray) else state\n",
    "        state_str = json.dumps(state_list)  # Convert list to a JSON string for ast.literal_eval\n",
    "\n",
    "        # Use ast.literal_eval safely to convert the string back to a list\n",
    "        state = ast.literal_eval(state_str)\n",
    "\n",
    "        similarities = {}\n",
    "        for case in case_base:\n",
    "            problem_numeric = np.array(case.problem, dtype=float)\n",
    "            state_numeric = np.array(state, dtype=float)\n",
    "            \n",
    "            # print(f\"state received = {state_numeric}\")\n",
    "            # print(f\"case received = {problem_numeric}\")\n",
    "            # print(\"---------\")\n",
    "           \n",
    "            similarities[case] = Case.sim_q(state_numeric, problem_numeric)  # Compare state with the problem part of the case\n",
    "\n",
    "        sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if sorted_similarities:\n",
    "            most_similar_case = sorted_similarities[0][0] if sorted_similarities[0][1] >= threshold else None\n",
    "        else:\n",
    "            most_similar_case = None\n",
    "\n",
    "        return most_similar_case\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(c, temporary_case_base):\n",
    "        temporary_case_base.append(c)\n",
    "\n",
    "    @staticmethod\n",
    "    def revise(case_base, temporary_case_base, successfull_task):\n",
    "        for case in temporary_case_base:\n",
    "            if successfull_task and case in case_base:\n",
    "                case.trust_value += 0.1  # Increment trust value if the episode ended successfully and the case is in the case base\n",
    "            elif not successfull_task and case in case_base:\n",
    "                case.trust_value -= 0.1  # Decrement trust value if the episode ended unsuccessfully and the case is in the case base\n",
    "            case.trust_value = max(0, min(case.trust_value, 1))  # Ensure trust value is within [0, 1]\n",
    "\n",
    "    @staticmethod\n",
    "    def retain(case_base, temporary_case_base, successfull_task, threshold=0.7):\n",
    "        if successfull_task:\n",
    "            # Iterate through the temporary case base to find the last occurrence of each unique state\n",
    "            for case in reversed(temporary_case_base):\n",
    "                state = tuple(np.atleast_1d(case.problem))\n",
    "                # Check if the state is already in the case base or has been added previously\n",
    "                if state not in Case.added_states:\n",
    "                    # Add the case to the case base if the state is new\n",
    "                    case_base.append(case)\n",
    "                    Case.added_states.add(state)\n",
    "                else:\n",
    "                    # Find the index of the existing case in the case base\n",
    "                    existing_index = next((i for i, c in enumerate(case_base) if tuple(np.atleast_1d(c.problem)) == state), None)\n",
    "                    if existing_index is not None:\n",
    "                        # Get the existing case from the case base\n",
    "                        existing_case = case_base[existing_index]\n",
    "                        # Update the trust value of the existing case with the new value from the revise step\n",
    "                        existing_case.trust_value = case.trust_value\n",
    "\n",
    "        # Filter case_base based on trust_value\n",
    "        case_base = [case for case in case_base if case.trust_value >= threshold]\n",
    "        return case_base\n",
    "\n",
    "\n",
    "\n",
    "class QCBRL:\n",
    "    def __init__(self, num_agents, is_agent_silent, num_actions, max_steps_per_episode):\n",
    "        self.actions = num_actions\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "        self.num_agents = num_agents\n",
    "        self.is_agent_silent = is_agent_silent\n",
    "        self.case_base = {i: [] for i in range(self.num_agents)}  # Separate case base for each agent\n",
    "        self.temporary_case_base = {i: [] for i in range(self.num_agents)}  # Separate temporary case base for each agent\n",
    "\n",
    "        \n",
    "        self.learning_rate = 0.01\n",
    "        self.discount_factor = 0.9\n",
    "        self.epsilon = 0.1\n",
    "        self.q_tables = [defaultdict(lambda: [0.0] * len(self.actions)) for _ in range(self.num_agents)]\n",
    "        self.rewards_per_episode = [[] for _ in range(self.num_agents)]  # Initialize rewards list for each agent\n",
    "        self.successful_episodes = [0] * self.num_agents  # Initialize successful episode count for each agent\n",
    "        self.total_successful_episodes = 0  # Initialize total successful episode count for all agents\n",
    "        self.problem_solver_agents = []\n",
    "\n",
    "    def run(self, episodes, max_steps, alpha=0.1, gamma=0.9, epsilon=0.1, render=False):\n",
    "\n",
    "        env = Env(num_agents=num_agents, is_agent_silent=is_agent_silent)\n",
    "        \n",
    "        rewards = []\n",
    "        # episode_rewards = []\n",
    "        memory_usage = []\n",
    "        gpu_memory_usage = []\n",
    "        successful_episodes = False\n",
    "        num_successful_episodes = 0\n",
    "        total_steps_list = []\n",
    "\n",
    "        success_steps = []\n",
    "\n",
    "        for agent_id, agent in enumerate(env.agents):\n",
    "            self.problem_solver_agents.append(ProblemSolver(env, agent_id, communication_weight=0.5))\n",
    "\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            states = env.reset()\n",
    "            self.temporary_case_base = []\n",
    "\n",
    "            total_reward = [0] * self.num_agents  # Initialize total reward for each agent\n",
    "            step_count = 0\n",
    "            dones = [False] * self.num_agents\n",
    "            win_states = []\n",
    "            success_episode = False\n",
    "            success_count = [0] * self.num_agents  # Track successful episodes for each agent\n",
    "            \n",
    "            self.temporary_case_base = {i: [] for i in range(len(env.agents))}\n",
    "            \n",
    "            while not (all(dones) and all(win_states)) and self.max_steps_per_episode:\n",
    "                combination_actions = []\n",
    "\n",
    "                for agent_idx in range(self.num_agents):\n",
    "                    state = states[agent_idx]\n",
    "                    physical_action, comm_action = self.take_action(agent_idx, state, self.case_base[agent_idx])\n",
    "                    combination_actions.append((physical_action, comm_action))\n",
    "                    # print(f\"state agents {agent_idx}: {state}\")\n",
    "\n",
    "                    new_case = Case(state, physical_action)\n",
    "                    Case.reuse(new_case, self.temporary_case_base[agent_idx])\n",
    "\n",
    "                next_states, rewards, dones = self.env.step(combination_actions)\n",
    "                \n",
    "\n",
    "                win_states = []\n",
    "                for agent_idx in range(self.num_agents):\n",
    "                    state = states[agent_idx]\n",
    "                    physical_action = combination_actions[agent_idx][0]\n",
    "                    comm_action = combination_actions[agent_idx][1]\n",
    "                    reward = rewards[agent_idx]\n",
    "                    next_state = next_states[agent_idx]\n",
    "                    win_state = next_state[1]  \n",
    "\n",
    "                    self.learn(agent_idx, state, physical_action, reward, next_state, comm_action)\n",
    "                    self.problem_solver_agents[agent_idx].learn(agent_idx, state, physical_action, reward, next_state)\n",
    "\n",
    "                    total_reward[agent_idx] += reward\n",
    "                    # states[agent_idx] = next_state\n",
    "\n",
    "                    # Check if agent reached target and mark episode as successful\n",
    "                    if (win_state):  # Use circle's coordinates for target\n",
    "                        success_count[agent_idx] += 1\n",
    "                        print(f\"agent{agent_idx} hit !!!!!\")\n",
    "                    else:\n",
    "                        print(f\"agent{agent_idx} not hit !!!!!\")\n",
    "\n",
    "                    # print(f\"Reward for agent {agent_idx}: {reward}\")\n",
    "                    # print(f\"next state for agent {agent_idx}: {next_state}\")\n",
    "                    # print(f\"target: {env.get_circle_grid_position()}\")\n",
    "                    \n",
    "                    \n",
    "                    win_states.append(win_state)\n",
    "                \n",
    "                states = next_states  \n",
    "\n",
    "                \n",
    "                # print(f\"State: {state} - Action: {action} - Reward: {reward} - Done: {done}\")\n",
    "\n",
    "\n",
    "            if all(dones) and all(win_states):\n",
    "                self.total_successful_episodes += 1\n",
    "                success_steps.append(step_count)\n",
    "                success_episode = True\n",
    "\n",
    "            for agent_idx in range(self.env.num_agents):\n",
    "                self.rewards_per_episode[agent_idx].append(total_reward[agent_idx])\n",
    "                print(f\"Agent {agent_idx} Hit Count: {success_count[agent_idx]}\")\n",
    "                print(f\"success hit rate for agent {agent_idx} at episode {episode}: {success_count[agent_idx]/step_count*100}%\")\n",
    "\n",
    "                Case.revise(self.case_base[agent_idx], self.temporary_case_base[agent_idx], success_episode)\n",
    "                self.case_base[agent_idx] = Case.retain(\n",
    "                    self.case_base[agent_idx], self.temporary_case_base[agent_idx], success_episode\n",
    "                )\n",
    "\n",
    "                self.save_case_base_temporary_eps(agent_idx, episode)  # Save temporary case base after each episode\n",
    "                self.save_case_base_eps(agent_idx, episode)  # Save case base after each episode\n",
    "\n",
    "            print(f\"Episode: {episode + 1}, Total Steps: {step_count}, Total Rewards: {total_reward}, Status Episode: {success_episode}\")\n",
    "            print(\"--------------------\")\n",
    "\n",
    "\n",
    "        \n",
    "        for agent_id, agent in enumerate(env.agents):\n",
    "            self.save_case_base_temporary(agent_id)  # Save temporary case base after training\n",
    "            self.save_case_base(agent_id)  # Save case base after training\n",
    "\n",
    "\n",
    "        success_rate = (num_successful_episodes / episodes) * 100\n",
    "        # print(f\"Successful episodes: {num_successful_episodes}%\")\n",
    "\n",
    "        # env.close()\n",
    "        return rewards, success_rate, memory_usage, gpu_memory_usage, total_steps_list\n",
    "\n",
    "    def take_action(self, agent_idx, state, case_base):\n",
    "\n",
    "        # print(f\"agent: {agent}\")\n",
    "\n",
    "        case = Case.retrieve(agent_idx, state, case_base)\n",
    "                    \n",
    "        if case:\n",
    "            physical_action = case.solution\n",
    "            print(f\"action type of agent {agent_idx}: case base\")\n",
    "        else:\n",
    "            physical_action = self.get_action(agent_idx, state)\n",
    "            print(f\"action type of agent {agent_idx}: problem solver\")\n",
    "        \n",
    "        if self.env.is_agent_silent:\n",
    "            communication_action = None\n",
    "        else:\n",
    "            communication_action = f\"Message send from agent {agent_idx}\"\n",
    "        \n",
    "        \n",
    "        return (physical_action, communication_action)\n",
    "\n",
    "    \n",
    "    def save_case_base_temporary(self, agent_id,):\n",
    "        filename = f\"case_base_temporary_{agent_id}.json\"\n",
    "        case_base_data = []\n",
    "        for case in self.temporary_case_base[agent_id]:\n",
    "            problem = case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem\n",
    "            solution = int(case.solution)\n",
    "            trust_value = float(case.trust_value)\n",
    "            \n",
    "            case_base_data.append({\n",
    "                \"problem\": problem,\n",
    "                \"solution\": solution,\n",
    "                \"trust_value\": trust_value\n",
    "            })\n",
    "        \n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "        print(\"Temporary case base saved successfully.\")\n",
    "\n",
    "    \n",
    "    \n",
    "    def save_case_base(self, agent_id):\n",
    "        filename = f\"case_base_{agent_id}.json\"\n",
    "        case_base_data = []\n",
    "        for case in self.case_base[agent_id]:\n",
    "            problem = case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem\n",
    "            solution = int(case.solution)\n",
    "            trust_value = float(case.trust_value)\n",
    "            \n",
    "            case_base_data.append({\n",
    "                \"problem\": problem,\n",
    "                \"solution\": solution,\n",
    "                \"trust_value\": trust_value\n",
    "            })\n",
    "        \n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "        print(\"Case base saved successfully.\")\n",
    "\n",
    "\n",
    "    def save_case_base_temporary_eps(self, agent_id, eps):\n",
    "        filename = f\"case_base_temporary_{agent_id}_{eps}.json\"\n",
    "        case_base_data = []\n",
    "        for case in self.temporary_case_base[agent_id]:\n",
    "            problem = case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem\n",
    "            solution = int(case.solution)\n",
    "            trust_value = float(case.trust_value)\n",
    "            \n",
    "            case_base_data.append({\n",
    "                \"problem\": problem,\n",
    "                \"solution\": solution,\n",
    "                \"trust_value\": trust_value\n",
    "            })\n",
    "        \n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "        print(\"Temporary case base saved successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "    def save_case_base_eps(self, agent_id, eps):\n",
    "        filename = f\"case_base_{agent_id}_{eps}.json\"\n",
    "        case_base_data = []\n",
    "        for case in self.case_base[agent_id]:\n",
    "            problem = case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem\n",
    "            solution = int(case.solution)\n",
    "            trust_value = float(case.trust_value)\n",
    "            \n",
    "            case_base_data.append({\n",
    "                \"problem\": problem,\n",
    "                \"solution\": solution,\n",
    "                \"trust_value\": trust_value\n",
    "            })\n",
    "        \n",
    "        with open(filename, 'w') as file:\n",
    "            json.dump(case_base_data, file)\n",
    "\n",
    "        print(\"Case base saved successfully.\")\n",
    "        \n",
    "    \n",
    "    def load_case_base(self, agent_id):\n",
    "        filename = f\"case_base_{agent_id}.json\"\n",
    "        try:\n",
    "            with open(filename, 'r') as file:\n",
    "                case_base_data = json.load(file)\n",
    "            self.case_base[agent_id] = [Case(problem=np.array(case[\"problem\"]) if isinstance(case[\"problem\"], list) else case[\"problem\"],\n",
    "                                            solution=case[\"solution\"],\n",
    "                                            trust_value=case[\"trust_value\"]) for case in case_base_data]\n",
    "        except FileNotFoundError:\n",
    "            self.case_base[agent_id] = []\n",
    "\n",
    "    \n",
    "    def display_success_rate(self, success_rate):\n",
    "        print(f\"Success rate: {success_rate}%\")\n",
    "\n",
    "\n",
    "    def plot_rewards(self, rewards):\n",
    "        plt.plot(rewards)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.title('Rewards over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show() \n",
    "\n",
    "    def plot_resources(self, memory_usage, gpu_memory_usage):\n",
    "        plt.plot(memory_usage, label='Memory (%)')\n",
    "        plt.plot(gpu_memory_usage, label='GPU Memory (MB)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Resource Usage')\n",
    "        plt.title('Resource Usage over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_total_steps(self, total_steps_list):\n",
    "        plt.plot(total_steps_list)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Steps')\n",
    "        plt.title('Total Steps for Successful Episodes over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show() \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_agents = 1\n",
    "    is_agent_silent = False  # Set to True or False to enable/disable communication\n",
    "    num_actions = list(range(5)) + [(i, 'send') for i in range(5)]\n",
    "    max_steps_per_episode = 100\n",
    "    agent = QCBRL(num_agents, is_agent_silent, num_actions, max_steps_per_episode)\n",
    "\n",
    "    rewards, success_rate, memory_usage, gpu_memory_usage, total_step_list = agent.run(episodes=50, max_steps=300, alpha=0.1, gamma=0.9, epsilon=0.1)\n",
    "\n",
    "\n",
    "    agent.display_success_rate(success_rate)\n",
    "    agent.plot_rewards(rewards)\n",
    "    agent.plot_total_steps(total_step_list)\n",
    "    agent.plot_resources(memory_usage, gpu_memory_usage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
