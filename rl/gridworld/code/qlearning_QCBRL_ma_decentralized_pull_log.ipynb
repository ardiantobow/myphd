{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- starting point of Episode 0 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f3c1d141ff0>, <__main__.Case object at 0x7f3c1d143430>, <__main__.Case object at 0x7f3c1d172d70>, <__main__.Case object at 0x7f3c1d17e4d0>, <__main__.Case object at 0x7f3c1d1959c0>, <__main__.Case object at 0x7f3c1d195990>, <__main__.Case object at 0x7f3c1d19d0f0>, <__main__.Case object at 0x7f3c1d1a9f60>, <__main__.Case object at 0x7f3c1d1958a0>, <__main__.Case object at 0x7f3c1d1a8c10>, <__main__.Case object at 0x7f3c1d1aa230>, <__main__.Case object at 0x7f3c1d1aa2f0>, <__main__.Case object at 0x7f3c1d1aa440>, <__main__.Case object at 0x7f3c1d1aa530>, <__main__.Case object at 0x7f3c1d1aa5c0>, <__main__.Case object at 0x7f3c1d1aa6e0>, <__main__.Case object at 0x7f3c1d1aa830>, <__main__.Case object at 0x7f3c1d1aa8f0>, <__main__.Case object at 0x7f3c1d1aa9e0>, <__main__.Case object at 0x7f3c1d1aaad0>, <__main__.Case object at 0x7f3c1d1aabc0>, <__main__.Case object at 0x7f3c1d1a9e10>, <__main__.Case object at 0x7f3c1d1aad70>, <__main__.Case object at 0x7f3c1d1aae00>, <__main__.Case object at 0x7f3c1d1aaf20>, <__main__.Case object at 0x7f3c1d1a9e40>, <__main__.Case object at 0x7f3c1d1ab0d0>, <__main__.Case object at 0x7f3c1d1ab160>, <__main__.Case object at 0x7f3c1d1ab280>, <__main__.Case object at 0x7f3c1d1ab310>, <__main__.Case object at 0x7f3c1d1ab3a0>, <__main__.Case object at 0x7f3c1d1ab460>, <__main__.Case object at 0x7f3c1d1ab580>, <__main__.Case object at 0x7f3c1d1ab610>, <__main__.Case object at 0x7f3c1d1ab6a0>, <__main__.Case object at 0x7f3c1d1ab760>, <__main__.Case object at 0x7f3c1d1ab880>, <__main__.Case object at 0x7f3c1d1ab910>, <__main__.Case object at 0x7f3c1d1ab9a0>, <__main__.Case object at 0x7f3c1d1aba60>, <__main__.Case object at 0x7f3c1d1abb80>, <__main__.Case object at 0x7f3c1d1abc10>, <__main__.Case object at 0x7f3c1d1abca0>, <__main__.Case object at 0x7f3c1d1abdc0>, <__main__.Case object at 0x7f3c1d1abeb0>, <__main__.Case object at 0x7f3c1d1aabf0>, <__main__.Case object at 0x7f3c1d1aaf50>, <__main__.Case object at 0x7f3c1d1ac100>, <__main__.Case object at 0x7f3c1d1ab2b0>, <__main__.Case object at 0x7f3c1d1ac2b0>, <__main__.Case object at 0x7f3c1d1ac340>]\n",
      "agent0 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f3c1d170580>, <__main__.Case object at 0x7f3c1d173eb0>, <__main__.Case object at 0x7f3c1d189e10>, <__main__.Case object at 0x7f3c1d18b160>, <__main__.Case object at 0x7f3c1d196ad0>, <__main__.Case object at 0x7f3c1d1a8a30>, <__main__.Case object at 0x7f3c1d1a9d80>, <__main__.Case object at 0x7f3c1d1a9f90>, <__main__.Case object at 0x7f3c1d1aa050>, <__main__.Case object at 0x7f3c1d1aa170>, <__main__.Case object at 0x7f3c1d1aa2c0>, <__main__.Case object at 0x7f3c1d1aa350>, <__main__.Case object at 0x7f3c1d1aa4d0>, <__main__.Case object at 0x7f3c1d1aa560>, <__main__.Case object at 0x7f3c1d1aa620>, <__main__.Case object at 0x7f3c1d1aa770>, <__main__.Case object at 0x7f3c1d1aa8c0>, <__main__.Case object at 0x7f3c1d1aa920>, <__main__.Case object at 0x7f3c1d1aaa10>, <__main__.Case object at 0x7f3c1d1aab00>, <__main__.Case object at 0x7f3c1d1aab60>, <__main__.Case object at 0x7f3c1d1aacb0>, <__main__.Case object at 0x7f3c1d1aada0>, <__main__.Case object at 0x7f3c1d1aae60>, <__main__.Case object at 0x7f3c1d1aaec0>, <__main__.Case object at 0x7f3c1d1ab010>, <__main__.Case object at 0x7f3c1d1ab100>, <__main__.Case object at 0x7f3c1d1ab1c0>, <__main__.Case object at 0x7f3c1d1ab340>, <__main__.Case object at 0x7f3c1d1a9ea0>, <__main__.Case object at 0x7f3c1d1ab400>, <__main__.Case object at 0x7f3c1d1ab4c0>, <__main__.Case object at 0x7f3c1d1ab520>, <__main__.Case object at 0x7f3c1d1aa0b0>, <__main__.Case object at 0x7f3c1d1ab700>, <__main__.Case object at 0x7f3c1d1ab7c0>, <__main__.Case object at 0x7f3c1d1ab820>, <__main__.Case object at 0x7f3c1d1aa470>, <__main__.Case object at 0x7f3c1d1aba00>, <__main__.Case object at 0x7f3c1d1abac0>, <__main__.Case object at 0x7f3c1d1abb20>, <__main__.Case object at 0x7f3c1d1aa860>, <__main__.Case object at 0x7f3c1d1abd00>, <__main__.Case object at 0x7f3c1d1abdf0>, <__main__.Case object at 0x7f3c1d1abe50>, <__main__.Case object at 0x7f3c1d1abfa0>, <__main__.Case object at 0x7f3c1d1ac0a0>, <__main__.Case object at 0x7f3c1d1ac160>, <__main__.Case object at 0x7f3c1d1ac1c0>, <__main__.Case object at 0x7f3c1d1ac040>, <__main__.Case object at 0x7f3c1d1ac3a0>]\n",
      "agent1 comm temp case base: []\n",
      "Episode succeeded, case (5, 4) is empty. Temporary case base stored to the case base: ((5, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 4) is empty. Temporary case base stored to the case base: ((6, 4), 3, 0.5)\n",
      "Episode succeeded, case (6, 3) is empty. Temporary case base stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 3, 0.5)\n",
      "Episode succeeded, case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 3, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 3, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 4, 0.5)\n",
      "Episode succeeded, case (6, 1) is empty. Temporary case base stored to the case base: ((6, 1), 4, 0.5)\n",
      "Episode succeeded, case (5, 1) is empty. Temporary case base stored to the case base: ((5, 1), 4, 0.5)\n",
      "Episode succeeded, case (5, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) is empty. Temporary case base stored to the case base: ((5, 2), 1, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) is empty. Temporary case base stored to the case base: ((5, 3), 2, 0.5)\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 2, 0.5)\n",
      "Episode succeeded, case (5, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.5, time steps: 50\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 49\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 48\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 46\n",
      "cases content after RETAIN, problem: (7, 2), solution: 3, tv: 0.5, time steps: 45\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.5, time steps: 44\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.5, time steps: 42\n",
      "cases content after RETAIN, problem: (9, 1), solution: 3, tv: 0.5, time steps: 40\n",
      "cases content after RETAIN, problem: (7, 1), solution: 4, tv: 0.5, time steps: 38\n",
      "cases content after RETAIN, problem: (6, 1), solution: 4, tv: 0.5, time steps: 37\n",
      "cases content after RETAIN, problem: (5, 1), solution: 4, tv: 0.5, time steps: 36\n",
      "cases content after RETAIN, problem: (5, 2), solution: 1, tv: 0.5, time steps: 34\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 31\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (5, 3), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 6\n",
      "Episode: 0, Total Steps: 51, Total Rewards: [-116, 50], Status Episode: False\n",
      "------------------------------------------End of episode 0 loop--------------------\n",
      "----- starting point of Episode 1 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.5, 6)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.5, 29)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.5, 31)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 4, 0.5, 37)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 4, 0.5, 38)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 1) with action 1 to next state (8, 0): pull reward: 0.1\n",
      "----- starting point of Episode 1 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.5, 27)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.5, 29)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 0) with action 1 to next state (6, 0): pull reward: 0.0\n",
      "----- starting point of Episode 1 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 0) with action 3 to next state (5, 0): pull reward: -0.1\n",
      "----- starting point of Episode 1 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 2, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 1), 4, 0.5, 36)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 4, 0.5, 37)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 1), 4, 0.5, 38)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 1), 2, 0.5, 42)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 2), 3, 0.5, 44)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 2), 3, 0.5, 45)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.5, 46)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.5, 48)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 3, 0.5, 49)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((5, 4), 3, 0.5, 50)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 1 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 1 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 1 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 1 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 1 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 1 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 1 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 1 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 1 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0.0\n",
      "----- starting point of Episode 1 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0.0\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f3c1d173eb0>, <__main__.Case object at 0x7f3c1d196cb0>, <__main__.Case object at 0x7f3c1d19d0f0>, <__main__.Case object at 0x7f3c1d1aa0e0>, <__main__.Case object at 0x7f3c1d1aa290>, <__main__.Case object at 0x7f3c1d1aa650>, <__main__.Case object at 0x7f3c1d1aa890>, <__main__.Case object at 0x7f3c1d1aabc0>, <__main__.Case object at 0x7f3c1d1aad70>, <__main__.Case object at 0x7f3c1d1ab8b0>, <__main__.Case object at 0x7f3c1d1ab160>, <__main__.Case object at 0x7f3c1d1ab3a0>, <__main__.Case object at 0x7f3c1d1ab580>, <__main__.Case object at 0x7f3c1d1ab880>, <__main__.Case object at 0x7f3c1d1abaf0>, <__main__.Case object at 0x7f3c1d1abd30>, <__main__.Case object at 0x7f3c1d1abf10>, <__main__.Case object at 0x7f3c1d1aa050>, <__main__.Case object at 0x7f3c1d1aa350>, <__main__.Case object at 0x7f3c1d1aa770>, <__main__.Case object at 0x7f3c1d1aaa10>, <__main__.Case object at 0x7f3c1d1aada0>, <__main__.Case object at 0x7f3c1d1aaec0>, <__main__.Case object at 0x7f3c1d1ab100>, <__main__.Case object at 0x7f3c1d1ab640>, <__main__.Case object at 0x7f3c1d1ab7c0>, <__main__.Case object at 0x7f3c1d1aa860>, <__main__.Case object at 0x7f3c1d1aa440>, <__main__.Case object at 0x7f3c1d172d70>, <__main__.Case object at 0x7f3c1d141c30>, <__main__.Case object at 0x7f3c1d1ac310>, <__main__.Case object at 0x7f3c1d1ac1f0>, <__main__.Case object at 0x7f3c1d1ab670>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f3c1d170580>, <__main__.Case object at 0x7f3c1d170760>, <__main__.Case object at 0x7f3c1d196ad0>, <__main__.Case object at 0x7f3c1d1944c0>, <__main__.Case object at 0x7f3c1d1aa380>, <__main__.Case object at 0x7f3c1d1a9f60>, <__main__.Case object at 0x7f3c1d1aaad0>, <__main__.Case object at 0x7f3c1d1aae00>, <__main__.Case object at 0x7f3c1d1ab310>, <__main__.Case object at 0x7f3c1d1ab610>, <__main__.Case object at 0x7f3c1d1abbb0>, <__main__.Case object at 0x7f3c1d1aba30>, <__main__.Case object at 0x7f3c1d1abc70>, <__main__.Case object at 0x7f3c1d1aaf50>, <__main__.Case object at 0x7f3c1d1a9f90>, <__main__.Case object at 0x7f3c1d1aa2c0>, <__main__.Case object at 0x7f3c1d1aa620>, <__main__.Case object at 0x7f3c1d172d40>, <__main__.Case object at 0x7f3c1d1abfd0>]\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (5, 4) is empty. Temporary case base stored to the case base: ((5, 4), 3, 0.5)\n",
      "Integrated case process. comm case (6, 4) is empty. Temporary case base stored to the case base: ((6, 4), 3, 0.5)\n",
      "Integrated case process. comm case (6, 3) is empty. Temporary case base stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 3, 0.5)\n",
      "Integrated case process. comm case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 3, 0.5)\n",
      "Integrated case process. comm case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 0.5)\n",
      "Integrated case process. comm case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 4, 0.5)\n",
      "Integrated case process. comm case (6, 1) is empty. Temporary case base stored to the case base: ((6, 1), 4, 0.5)\n",
      "Integrated case process. comm case (5, 1) is empty. Temporary case base stored to the case base: ((5, 1), 4, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 2, 0.5)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 0.5)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 0.5)\n",
      "Integrated case process. comm case (7, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 1), 4, 0.5)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 4, 0.5)\n",
      "Integrated case process. comm case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 2, 0.5)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.5, time steps: 50\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 49\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 48\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 46\n",
      "cases content after RETAIN, problem: (7, 2), solution: 3, tv: 0.5, time steps: 45\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.5, time steps: 44\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.5, time steps: 42\n",
      "cases content after RETAIN, problem: (7, 1), solution: 4, tv: 0.5, time steps: 38\n",
      "cases content after RETAIN, problem: (6, 1), solution: 4, tv: 0.5, time steps: 37\n",
      "cases content after RETAIN, problem: (5, 1), solution: 4, tv: 0.5, time steps: 36\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 31\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f3c1d17e4d0>, <__main__.Case object at 0x7f3c1d1958a0>, <__main__.Case object at 0x7f3c1d1ab5b0>, <__main__.Case object at 0x7f3c1d1aa1a0>, <__main__.Case object at 0x7f3c1d1aa530>, <__main__.Case object at 0x7f3c1d1aa740>, <__main__.Case object at 0x7f3c1d1aaa40>, <__main__.Case object at 0x7f3c1d1a9e10>, <__main__.Case object at 0x7f3c1d1aaf80>, <__main__.Case object at 0x7f3c1d1ab0d0>, <__main__.Case object at 0x7f3c1d1ab280>, <__main__.Case object at 0x7f3c1d1ab460>, <__main__.Case object at 0x7f3c1d1ab730>, <__main__.Case object at 0x7f3c1d1ab9a0>, <__main__.Case object at 0x7f3c1d1abbe0>, <__main__.Case object at 0x7f3c1d1abe20>, <__main__.Case object at 0x7f3c1d1a8c40>, <__main__.Case object at 0x7f3c1d1aa170>, <__main__.Case object at 0x7f3c1d1aa4d0>, <__main__.Case object at 0x7f3c1d1aa8c0>, <__main__.Case object at 0x7f3c1d1aacb0>, <__main__.Case object at 0x7f3c1d1aae60>, <__main__.Case object at 0x7f3c1d1ab010>, <__main__.Case object at 0x7f3c1d1ab400>, <__main__.Case object at 0x7f3c1d1ab490>, <__main__.Case object at 0x7f3c1d1abb50>, <__main__.Case object at 0x7f3c1d1abdf0>, <__main__.Case object at 0x7f3c1d189ff0>, <__main__.Case object at 0x7f3c1d1ac0d0>, <__main__.Case object at 0x7f3c1d1ac280>, <__main__.Case object at 0x7f3c1d1ac3d0>, <__main__.Case object at 0x7f3c1d1ace50>, <__main__.Case object at 0x7f3c1d1accd0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 3, tv: 0.6, time steps: 50\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 3, tv: 0.6, time steps: 49\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.6, time steps: 48\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.6, time steps: 46\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 3, tv: 0.6, time steps: 45\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 3, tv: 0.6, time steps: 44\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 0.6, time steps: 42\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 3, tv: 0.4, time steps: 40\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 4, tv: 0.6, time steps: 38\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 4, tv: 0.6, time steps: 37\n",
      "case content after REVISE for agent 1, problem: (5, 1), solution: 4, tv: 0.6, time steps: 36\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 1, tv: 0.4, time steps: 34\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.6, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.6, time steps: 29\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.6, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 2, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.6, time steps: 6\n",
      "Episode succeeded, case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 0, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 4), 3, 0.5, 33)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 3, 0.5, 33)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 3), 2, 0.5, 33)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 2), 2, 0.5, 33)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 2), 3, 0.5, 33)\n",
      "Episode succeeded, updated case base with fewer steps: ((8, 2), 3, 0.5, 33)\n",
      "Episode succeeded, updated case base with fewer steps: ((8, 1), 2, 0.5, 33)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 1), 4, 0.5, 33)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 1), 4, 0.5, 33)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 1), 4, 0.5, 33)\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (5, 4), solution: 3, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (7, 2), solution: 3, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (7, 1), solution: 4, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (6, 1), solution: 4, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (5, 1), solution: 4, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.6, time steps: 31\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6, time steps: 29\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6, time steps: 27\n",
      "cases content after RETAIN, problem: (5, 0), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6, time steps: 6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 0, tv: 0.5, time steps: 32\n",
      "Episode: 1, Total Steps: 33, Total Rewards: [-132, 79], Status Episode: False\n",
      "------------------------------------------End of episode 1 loop--------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 483\u001b[0m\n\u001b[1;32m    480\u001b[0m num_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[1;32m    482\u001b[0m agent \u001b[38;5;241m=\u001b[39m QCBRL(num_actions, env, episodes, max_steps, alpha, gamma, epsilon, epsilon_decay, epsilon_min, render)\n\u001b[0;32m--> 483\u001b[0m rewards, success_rate, memory_usage, gpu_memory_usage, total_step_list \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m agent\u001b[38;5;241m.\u001b[39mdisplay_success_rate(success_rate)\n\u001b[1;32m    486\u001b[0m agent\u001b[38;5;241m.\u001b[39mplot_rewards(rewards)\n",
      "Cell \u001b[0;32mIn[1], line 216\u001b[0m, in \u001b[0;36mQCBRL.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m handle \u001b[38;5;241m=\u001b[39m pynvml\u001b[38;5;241m.\u001b[39mnvmlDeviceGetHandleByIndex(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[0;32m--> 216\u001b[0m     states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     episode_reward \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_agents\n\u001b[1;32m    218\u001b[0m     total_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n",
      "File \u001b[0;32m~/PHD/Research/code/myphd/rl/gridworld/code/environment_ma_reward_distance_dynamic_notrandom.py:105\u001b[0m, in \u001b[0;36mEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 105\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Reinitialize agents' positions\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     agent_positions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    109\u001b[0m         [UNIT \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, UNIT \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m],  \u001b[38;5;66;03m# Top-left for Agent 1\u001b[39;00m\n\u001b[1;32m    110\u001b[0m         [(WIDTH \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m*\u001b[39m UNIT, UNIT \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# Top-right for Agent 2\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import json\n",
    "import psutil\n",
    "import pynvml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from environment_ma_reward_distance_dynamic_notrandom import Env\n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, num_actions, env, alpha, gamma, epsilon):\n",
    "        self.env = env\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = alpha\n",
    "        self.discount_factor = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.q_tables = [defaultdict(lambda: [0.0] * num_actions) for _ in range(env.num_agents)]\n",
    "\n",
    "    @staticmethod\n",
    "    def arg_max(state_action):\n",
    "        max_index_list = []\n",
    "        max_value = state_action[0]\n",
    "        for index, value in enumerate(state_action):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)\n",
    "\n",
    "    def choose_action(self, agent_idx, state):\n",
    "        state = tuple(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            state_action = self.q_tables[agent_idx][state]\n",
    "            action = self.arg_max(state_action)\n",
    "        return action\n",
    "\n",
    "    def learn(self, agent_idx, state, action, reward, next_state, case_base=None):\n",
    "        state = tuple(state)\n",
    "        next_state = tuple(next_state)\n",
    "        current_q = self.q_tables[agent_idx][state][action]\n",
    "        max_next_q = max(self.q_tables[agent_idx][next_state])\n",
    "\n",
    "        # Calculate the Q-update with weighted distance-based pull reward if case_base exists\n",
    "        if case_base:\n",
    "            print(f\"Calculating weighted distance-based pull reward for agent {agent_idx}\")\n",
    "            pull_reward = 0  # Initialize pull reward\n",
    "            \n",
    "            for case in case_base:\n",
    "                problems = case.problem if isinstance(case.problem, list) else [case.problem]\n",
    "                \n",
    "                weight = case.trust_value\n",
    "                \n",
    "                # Calculate the pull reward based on the weighted difference in distances\n",
    "                for p in problems:\n",
    "                    distance_current = np.linalg.norm(np.array(state) - np.array(p))\n",
    "                    distance_next = np.linalg.norm(np.array(next_state) - np.array(p))\n",
    "\n",
    "                    distance_diff = distance_current - distance_next\n",
    "                    distance_diff = weight * distance_diff\n",
    "                    pull_reward += np.log1p(abs(distance_diff)) * np.sign(distance_diff)\n",
    "                    pull_reward = np.clip(pull_reward, -1 * self.epsilon, self.epsilon)\n",
    "                    # pull_reward = np.clip(pull_reward, -0.1, 0.1) \n",
    "            \n",
    "            print(f\"Calculating pull reward agent {agent_idx}: from state {state} with action {action} to next state {next_state}: pull reward: {pull_reward}\")\n",
    "            new_q = current_q + self.learning_rate * (reward + pull_reward + self.discount_factor * max_next_q - current_q)\n",
    "        else:\n",
    "            # Standard Q-learning update without pull reward\n",
    "            print(f\"No communication. Standard Q-learning update for agent {agent_idx}\")\n",
    "            new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)\n",
    "\n",
    "        self.q_tables[agent_idx][state][action] = new_q\n",
    "        \n",
    "\n",
    "\n",
    "class Case:\n",
    "\n",
    "    def __init__(self, problem, solution, trust_value=0.5, total_time_steps=0):\n",
    "        self.problem = ast.literal_eval(problem) if isinstance(problem, str) else problem\n",
    "        self.solution = solution\n",
    "        self.trust_value = trust_value\n",
    "        self.total_time_steps = total_time_steps  # New attribute for total time steps\n",
    "\n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)\n",
    "        state2 = np.atleast_1d(state2)\n",
    "        CNDMaxDist = 6\n",
    "        v = state1.size\n",
    "        DistQ = np.sum([Case.dist_q(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def dist_q(X1, X2):\n",
    "        return np.min(np.abs(X1 - X2))\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(state, case_base, threshold=0.1):\n",
    "        state = ast.literal_eval(state) if isinstance(state, str) else state\n",
    "        for case in case_base:\n",
    "            if state == case.problem: \n",
    "                return case\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(agent_idx, c, own_temp_case_base, comm_temp_case_base, source='own'):\n",
    "        \"\"\"Reuse step for adding cases to temporary case bases.\"\"\"\n",
    "        if source == 'own':\n",
    "            own_temp_case_base.append(c)\n",
    "        elif source == 'comm':\n",
    "            comm_temp_case_base.append(c)\n",
    "\n",
    "    @staticmethod\n",
    "    def revise(agent_idx, case_base, temporary_case_base, successful_episodes, total_steps):\n",
    "        for case in case_base:\n",
    "            if any((case.problem, case.solution) == (temp_case.problem, temp_case.solution) for temp_case in temporary_case_base):\n",
    "                if successful_episodes:\n",
    "                    case.trust_value += 0.1\n",
    "                else:\n",
    "                    case.trust_value -= 0.4\n",
    "            else:\n",
    "                if successful_episodes:\n",
    "                    case.trust_value -= 0.1\n",
    "            \n",
    "            case.trust_value = max(0, min(case.trust_value, 1))\n",
    "            print(f\"case content after REVISE for agent {agent_idx}, problem: {case.problem}, solution: {case.solution}, tv: {case.trust_value}, time steps: {case.total_time_steps}\")\n",
    "\n",
    "    @staticmethod\n",
    "    \n",
    "    @staticmethod\n",
    "    def retain(agent_idx, case_base, own_temp_case_base, comm_temp_case_base, successful_episodes, total_steps, threshold=0.49):\n",
    "        if successful_episodes:\n",
    "            for temp_case in reversed(own_temp_case_base):\n",
    "                state = tuple(np.atleast_1d(temp_case.problem))\n",
    "\n",
    "                existing_case = next((case for case in case_base if tuple(np.atleast_1d(case.problem)) == state), None)\n",
    "                \n",
    "                if existing_case is None:\n",
    "                    case_base.append(temp_case)\n",
    "                    print(f\"Episode succeeded, case {temp_case.problem} is empty. Temporary case base stored to the case base: {temp_case.problem, temp_case.solution, temp_case.trust_value}\")\n",
    "                else:\n",
    "                    if total_steps < existing_case.total_time_steps:\n",
    "                        # Update the case in the case base if the new case has fewer total steps\n",
    "                        existing_case.solution = temp_case.solution\n",
    "                        existing_case.trust_value = max(0, temp_case.trust_value)\n",
    "                        existing_case.total_time_steps = total_steps\n",
    "                        print(f\"Episode succeeded, updated case base with fewer steps: {temp_case.problem, temp_case.solution, temp_case.trust_value, total_steps}\")\n",
    "                    else:\n",
    "                        print(f\"Episode succeeded, case {temp_case.problem} for agent {agent_idx} is not updated as it has more or equal steps.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Episode not succeeded, temporary case base from own experience is not stored to the case base\")\n",
    "\n",
    "        case_base_dict = {tuple(np.atleast_1d(case.problem)): case for case in case_base}\n",
    "\n",
    "        for temp_comm_case in reversed(comm_temp_case_base):\n",
    "            state_comm = tuple(np.atleast_1d(temp_comm_case.problem))\n",
    "            existing_case = case_base_dict.get(state_comm)\n",
    "\n",
    "            if existing_case is None:\n",
    "                case_base.append(temp_comm_case)\n",
    "                case_base_dict[state_comm] = temp_comm_case\n",
    "                print(f\"Integrated case process. comm case {temp_comm_case.problem} is empty. Temporary case base stored to the case base: {temp_comm_case.problem, temp_comm_case.solution, temp_comm_case.trust_value}\")\n",
    "            else:\n",
    "                print(f\"Integrated case process. comm case {temp_comm_case.problem} for agent {agent_idx} is not empty. Temporary case base that not stored to the case base: {temp_comm_case.problem, temp_comm_case.solution, temp_comm_case.trust_value}\")\n",
    "\n",
    "        # Remove cases with trust values below the threshold\n",
    "        case_base[:] = [case for case in case_base if case.trust_value >= threshold]\n",
    "\n",
    "        for case in case_base:\n",
    "            print(f\"cases content after RETAIN, problem: {case.problem}, solution: {case.solution}, tv: {case.trust_value}, time steps: {case.total_time_steps}\")\n",
    "\n",
    "        return case_base\n",
    "\n",
    "    \n",
    "\n",
    "class QCBRL:\n",
    "    def __init__(self, num_actions, env, episodes, max_steps, alpha, gamma, epsilon, epsilon_decay, epsilon_min, render):\n",
    "        self.num_actions = num_actions\n",
    "        self.env = env\n",
    "        self.episodes = episodes\n",
    "        self.max_steps = max_steps\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.render = render\n",
    "        self.epsilon_decay = epsilon_decay  \n",
    "        self.epsilon_min = epsilon_min  \n",
    "\n",
    "        self.problem_solvers = [ProblemSolver(num_actions, self.env, alpha, gamma, epsilon) for _ in range(self.env.num_agents)]\n",
    "        self.case_bases = [[] for _ in range(self.env.num_agents)]  # Individual case bases for each agent\n",
    "        self.own_temp_case_bases = [[] for _ in range(self.env.num_agents)]  # Temporary case bases for own experiences\n",
    "        self.comm_temp_case_bases = [[] for _ in range(self.env.num_agents)]  # Temporary case bases for communication experiences\n",
    "        self.successful_episodes = [0] * self.env.num_agents\n",
    "        self.rewards_per_episode = [[] for _ in range(self.env.num_agents)]  \n",
    "        self.total_successful_episodes = 0 \n",
    "        self.action_type = [0] * self.env.num_agents\n",
    "\n",
    "    def run(self):\n",
    "        rewards = []\n",
    "        memory_usage = []\n",
    "        gpu_memory_usage = []\n",
    "        num_successful_episodes = 0\n",
    "        total_steps_list = []\n",
    "        success_steps = []\n",
    "\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            states = self.env.reset()\n",
    "            episode_reward = [0] * self.env.num_agents\n",
    "            total_steps = 0 \n",
    "            self.own_temp_case_bases = [[] for _ in range(self.env.num_agents)]\n",
    "            self.comm_temp_case_bases = [[] for _ in range(self.env.num_agents)]\n",
    "            success_count = [0] * self.env.num_agents\n",
    "            dones = [False] * self.env.num_agents\n",
    "            win_states = [False] * self.env.num_agents\n",
    "            successful_episodes = False\n",
    "\n",
    "            while not(all(dones)):\n",
    "                print(f\"----- starting point of Episode {episode} in steps {total_steps} loop -----\")\n",
    "                \n",
    "                actions = []\n",
    "                for agent_idx in range(self.env.num_agents):\n",
    "                    state = states[agent_idx]\n",
    "                    # print(f\"state before take action: {state}\")\n",
    "                    action = self.take_action(agent_idx, state)\n",
    "                    actions.append(action)\n",
    "\n",
    "                # print(f\"actions pass to the environment\")\n",
    "                next_states, rewards, dones = self.env.step(actions)\n",
    "\n",
    "                win_states = []\n",
    "                for agent_idx in range(self.env.num_agents):\n",
    "                    state = states[agent_idx]\n",
    "                    action = actions[agent_idx]\n",
    "                    reward = rewards[agent_idx]\n",
    "                    next_state = next_states[agent_idx]\n",
    "\n",
    "                    physical_state = tuple(state[0])\n",
    "                    win_state = state[1]\n",
    "                    comm_state = state[2]  # Communication state containing messages from other agents\n",
    "\n",
    "                    physical_next_state = tuple(next_state[0])\n",
    "                    win_next_state = next_state[1]\n",
    "                    comm_next_state = tuple(next_state[2]) if next_state[2] != 0 else next_state[2]\n",
    "\n",
    "                    physical_action = action[0]\n",
    "                    comm_action = action[1]\n",
    "\n",
    "                    # Process messages received from other agents\n",
    "                    print(f\"comm next state for agent {agent_idx}: {comm_next_state}\")\n",
    "                    # print(f\"comm next state content: {comm_next_state[0]}\")\n",
    "                    \n",
    "                    # if all(element is None for element in comm_next_state):\n",
    "                    # if (comm_next_state == [None]) or (comm_next_state is None):\n",
    "                    if (comm_next_state == 0):\n",
    "                        pass\n",
    "                    else:\n",
    "                        comm_case = Case(problem=comm_next_state[0], solution=comm_next_state[1], trust_value=comm_next_state[2], total_time_steps=comm_next_state[3])\n",
    "                        Case.reuse(agent_idx, comm_case, self.own_temp_case_bases[agent_idx], self.comm_temp_case_bases[agent_idx], source='comm')\n",
    "\n",
    "                    # print(f\"state agent {agent_idx} before update: {physical_state}\")\n",
    "                    # print(f\"win state agent {agent_idx} before update: {win_next_state}\")\n",
    "                    # print(f\"action agent {agent_idx} before update: {physical_action}\")\n",
    "                    # print(f\"reward agent {agent_idx} before update: {reward}\")\n",
    "                    # print(f\"next state agent {agent_idx} before update: {physical_next_state}\")\n",
    "\n",
    "                    c = Case(physical_state, physical_action, total_time_steps=total_steps)\n",
    "                    Case.reuse(agent_idx, c, self.own_temp_case_bases[agent_idx], self.comm_temp_case_bases[agent_idx], source='own')\n",
    "\n",
    "                    # if self.action_type[agent_idx] == 0:\n",
    "                    #     if not env.locked[agent_idx]:\n",
    "                    #         print(f\"action type of agent: {agent_idx}: problem solver, agent learned\")\n",
    "                    #         self.problem_solvers[agent_idx].learn(agent_idx, physical_state, physical_action, reward, physical_next_state, self.case_bases[agent_idx])\n",
    "                    #     else:\n",
    "                    #         print(f\"action type of agent: {agent_idx}: using problem solver but locked, no learning\")\n",
    "                    \n",
    "                    if self.action_type[agent_idx] == 0:\n",
    "                        print(f\"action type of agent: {agent_idx}: problem solver, agent learned\")\n",
    "                        self.problem_solvers[agent_idx].learn(agent_idx, physical_state, physical_action, reward, physical_next_state, self.case_bases[agent_idx])\n",
    "                    else:\n",
    "                        print(f\"action type of agent: {agent_idx}: using solution from case base, no learning\")\n",
    "\n",
    "                    if (win_next_state): \n",
    "                        success_count[agent_idx] += 1\n",
    "                        # print(f\"agent{agent_idx} hit !!!!!\")\n",
    "                    # else:\n",
    "                    #     print(f\"agent{agent_idx} not hit !!!!!\")\n",
    "\n",
    "                    episode_reward[agent_idx] += reward\n",
    "                    win_states.append(win_next_state)  \n",
    "\n",
    "                states = next_states\n",
    "                total_steps += 1\n",
    "\n",
    "                self.env.render()\n",
    "                \n",
    "            if self.env.win_flag:\n",
    "                self.total_successful_episodes += 1\n",
    "                success_steps.append(total_steps)\n",
    "                successful_episodes = True\n",
    "                \n",
    "\n",
    "            \n",
    "            for agent_idx in range(self.env.num_agents):\n",
    "                print(f\"win status of agent {agent_idx}  before update the case base: {win_states[agent_idx]}\")\n",
    "                self.rewards_per_episode[agent_idx].append(episode_reward[agent_idx])\n",
    "\n",
    "                print(f\"agent{agent_idx} own temp case base: {self.own_temp_case_bases[agent_idx]}\")\n",
    "                print(f\"agent{agent_idx} comm temp case base: {self.comm_temp_case_bases[agent_idx]}\")\n",
    "                \n",
    "                \n",
    "                Case.revise(agent_idx, self.case_bases[agent_idx], self.own_temp_case_bases[agent_idx], win_states[agent_idx], total_steps = total_steps)\n",
    "                self.case_bases[agent_idx] = Case.retain(agent_idx, self.case_bases[agent_idx], self.own_temp_case_bases[agent_idx], self.comm_temp_case_bases[agent_idx], win_states[agent_idx], total_steps = total_steps)\n",
    "               \n",
    "                \n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "            \n",
    "            memory_usage.append(psutil.virtual_memory().percent)\n",
    "            gpu_memory_usage.append(pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1024**2)\n",
    "\n",
    "            print(f\"Episode: {episode}, Total Steps: {total_steps}, Total Rewards: {episode_reward}, Status Episode: {successful_episodes}\")\n",
    "            print(f\"------------------------------------------End of episode {episode} loop--------------------\")\n",
    "\n",
    "        # self.save_case_base_temporary()  # Save temporary case base after training\n",
    "        # self.save_case_base()  # Save case base after training\n",
    "\n",
    "        success_rate = self.total_successful_episodes / episodes * 100\n",
    "\n",
    "        return self.rewards_per_episode, success_rate, memory_usage, gpu_memory_usage, success_steps\n",
    "\n",
    "    def take_action(self, agent_idx, state):\n",
    "        # print(f\"state detected in take action function: {state}\")\n",
    "        physical_state = tuple(state[0])\n",
    "        win_state = state[1]\n",
    "        comm_state = state[2]\n",
    "        # similar_solution = None\n",
    "\n",
    "        # if np.random.rand() < 0.01:\n",
    "        if np.random.rand() < self.epsilon:\n",
    "        # if np.random.rand() < 0:\n",
    "            # physical_action = np.random.choice(self.num_actions)\n",
    "            # comm_action = 0\n",
    "\n",
    "            physical_action = self.problem_solvers[agent_idx].choose_action(agent_idx, physical_state)\n",
    "            comm_action = 0  # No communication action if using problem solver action\n",
    "            self.action_type[agent_idx] = 0\n",
    "            print(f\"Physical Action for Agent {agent_idx} from problem solver: {physical_action}\")\n",
    "\n",
    "        else:\n",
    "            similar_solution = Case.retrieve(physical_state, self.case_bases[agent_idx])\n",
    "            if similar_solution is not None:\n",
    "                physical_action = similar_solution.solution\n",
    "                comm_action = (similar_solution.problem, similar_solution.solution, similar_solution.trust_value, similar_solution.total_time_steps)\n",
    "                self.action_type[agent_idx] = 1\n",
    "                # print(f\"Problem detected as a similiar soulution in case base: {similar_solution.problem}\")\n",
    "                print(f\"Physical Action for Agent {agent_idx} from case base: {physical_action}\")\n",
    "                # print(f\"Communication Action for Agent {agent_idx} from case base: {comm_action}\")\n",
    "                # print(f\"Trust value detected as a similiar solution in case base: {similar_solution.trust_value}\")\n",
    "            else:\n",
    "                physical_action = self.problem_solvers[agent_idx].choose_action(agent_idx, physical_state)\n",
    "                comm_action = 0  # No communication action if using problem solver action\n",
    "                self.action_type[agent_idx] = 0\n",
    "                print(f\"Physical Action for Agent {agent_idx} from problem solver: {physical_action}\")\n",
    "\n",
    "        # print(f\"physical action returned from the take action: {physical_action}\")\n",
    "        # print(f\"comm action returned from the take action: {comm_action}\")\n",
    "\n",
    "        return (physical_action, comm_action)\n",
    "\n",
    "    def case_exists_in_case_base(self, case, case_base):\n",
    "        \"\"\"Check if a case exists in the given case base.\"\"\"\n",
    "        return any(existing_case.problem == case.problem and existing_case.solution == case.solution for existing_case in case_base)\n",
    "        \n",
    "    \n",
    "    def save_case_base_temporary(self):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            filename = f\"cases/case_base_temporary_agent_{agent_idx}.json\"\n",
    "            case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem, \n",
    "                            \"solution\": int(case.solution), \n",
    "                            \"trust_value\": int(case.trust_value),\n",
    "                            \"total_time_steps\": int(case.total_time_steps)} for case in self.own_temp_case_bases[agent_idx] + self.comm_temp_case_bases[agent_idx]]\n",
    "            with open(filename, 'w') as file:\n",
    "                json.dump(case_base_data, file)\n",
    "            print(f\"Temporary case base for Agent {agent_idx} saved successfully.\")\n",
    "\n",
    "    def save_case_base(self):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            filename = f\"cases/case_base_agent_{agent_idx}.json\"\n",
    "            case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem, \n",
    "                            \"solution\": int(case.solution), \n",
    "                            \"trust_value\": int(case.trust_value),\n",
    "                            \"total_time_steps\": int(case.total_time_steps)} for case in self.case_bases[agent_idx]]\n",
    "            with open(filename, 'w') as file:\n",
    "                json.dump(case_base_data, file)\n",
    "            print(f\"Case base for Agent {agent_idx} saved successfully.\")\n",
    "        \n",
    "    def load_case_base(self):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            filename = f\"cases/case_base_agent_{agent_idx}.json\"\n",
    "            try:\n",
    "                with open(filename, 'r') as file:\n",
    "                    case_base_data = json.load(file)\n",
    "                    self.case_bases[agent_idx] = [Case(np.array(case[\"problem\"]), case[\"solution\"], case[\"trust_value\"], case[\"total_time_steps\"]) for case in case_base_data]\n",
    "                    print(f\"Case base for Agent {agent_idx} loaded successfully.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Case base file for Agent {agent_idx} not found. Starting with an empty case base.\")\n",
    "\n",
    "    def display_success_rate(self, success_rate):\n",
    "        print(f\"Success rate: {success_rate}%\")\n",
    "\n",
    "\n",
    "    # def plot_rewards(self, rewards):\n",
    "    #     for agent_idx in range(self.env.num_agents):\n",
    "    #         plt.plot([reward for reward in rewards[agent_idx]], label=f'Agent {agent_idx}')\n",
    "    #     plt.xlabel('Episode')\n",
    "    #     plt.ylabel('Total Reward')\n",
    "    #     plt.title('Rewards over Episodes')\n",
    "    #     plt.legend()\n",
    "    #     plt.grid(True)\n",
    "    #     plt.show()\n",
    "    \n",
    "    def plot_rewards(self, rewards, window=1):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            # Calculate the moving average of rewards over the specified window size\n",
    "            moving_avg_rewards = [np.mean(rewards[agent_idx][i:i + window]) for i in range(0, len(rewards[agent_idx]), window)]\n",
    "            \n",
    "            plt.plot(moving_avg_rewards, label=f'Agent {agent_idx}')\n",
    "        \n",
    "        plt.xlabel(f'Episode (Averaged over every {window} episodes)')\n",
    "        plt.ylabel('Average Total Reward')\n",
    "        plt.title('Average Rewards over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def plot_total_steps(self, total_steps_list):\n",
    "        plt.plot(total_steps_list)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Steps')\n",
    "        plt.title('Total Steps for Successful Episodes over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_resources(self, memory_usage, gpu_memory_usage):\n",
    "        plt.plot(memory_usage, label='Memory (%)')\n",
    "        plt.plot(gpu_memory_usage, label='GPU Memory (MB)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Resource Usage')\n",
    "        plt.title('Resource Usage over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_agents = 2\n",
    "    num_obstacles = 5\n",
    "    obstacles_random_steps = 20\n",
    "    is_agent_silent = False\n",
    "    episodes=59\n",
    "    max_steps=1000\n",
    "    alpha=0.1\n",
    "    gamma=0.9\n",
    "    epsilon=0.1\n",
    "    epsilon_decay = 0.995  \n",
    "    epsilon_min = 0.01  \n",
    "    render = True\n",
    "\n",
    "    env = Env(num_agents=num_agents, num_obstacles=num_obstacles, obstacles_random_steps = obstacles_random_steps, is_agent_silent=is_agent_silent)\n",
    "    \n",
    "    num_actions = len(env.action_space)\n",
    "    \n",
    "    agent = QCBRL(num_actions, env, episodes, max_steps, alpha, gamma, epsilon, epsilon_decay, epsilon_min, render)\n",
    "    rewards, success_rate, memory_usage, gpu_memory_usage, total_step_list = agent.run()\n",
    "\n",
    "    agent.display_success_rate(success_rate)\n",
    "    agent.plot_rewards(rewards)\n",
    "    agent.plot_total_steps(total_step_list)\n",
    "    agent.plot_resources(memory_usage, gpu_memory_usage)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
