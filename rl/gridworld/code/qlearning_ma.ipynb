{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions received by class: 5\n",
      "state agents 0: [[0, 0], False, None]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -10\n",
      "next state for agent 0: [[1, 2], False, []]\n",
      "Episode: 1, Total Steps: 4, Total Rewards: [-13], Status Episode: False\n",
      "--------------------\n",
      "state agents 0: [[0, 0], False, None]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -10\n",
      "next state for agent 0: [[1, 2], False, []]\n",
      "Episode: 2, Total Steps: 13, Total Rewards: [-22], Status Episode: False\n",
      "--------------------\n",
      "state agents 0: [[0, 0], False, None]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -10\n",
      "next state for agent 0: [[2, 1], False, []]\n",
      "Episode: 3, Total Steps: 3, Total Rewards: [-12], Status Episode: False\n",
      "--------------------\n",
      "state agents 0: [[0, 0], False, None]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -10\n",
      "next state for agent 0: [[2, 1], False, []]\n",
      "Episode: 4, Total Steps: 17, Total Rewards: [-26], Status Episode: False\n",
      "--------------------\n",
      "state agents 0: [[0, 0], False, None]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 4], False, []]\n",
      "state agents 0: [[0, 4], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 4], False, []]\n",
      "state agents 0: [[1, 4], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 4], False, []]\n",
      "state agents 0: [[2, 4], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 4], False, []]\n",
      "state agents 0: [[2, 4], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 4], False, []]\n",
      "state agents 0: [[2, 4], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 3], False, []]\n",
      "state agents 0: [[2, 3], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 4], False, []]\n",
      "state agents 0: [[2, 4], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 4], False, []]\n",
      "state agents 0: [[1, 4], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 4], False, []]\n",
      "state agents 0: [[0, 4], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 4], False, []]\n",
      "state agents 0: [[0, 4], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 3], False, []]\n",
      "state agents 0: [[1, 3], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 3], False, []]\n",
      "state agents 0: [[1, 3], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 4], False, []]\n",
      "state agents 0: [[1, 4], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 4], False, []]\n",
      "state agents 0: [[1, 4], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 4], False, []]\n",
      "state agents 0: [[1, 4], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 3], False, []]\n",
      "state agents 0: [[1, 3], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 3], False, []]\n",
      "state agents 0: [[2, 3], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 3], False, []]\n",
      "state agents 0: [[1, 3], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 3], False, []]\n",
      "state agents 0: [[1, 3], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -10\n",
      "next state for agent 0: [[1, 2], False, []]\n",
      "Episode: 5, Total Steps: 48, Total Rewards: [-57], Status Episode: False\n",
      "--------------------\n",
      "state agents 0: [[0, 0], False, None]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 0], False, []]\n",
      "state agents 0: [[3, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 0], False, []]\n",
      "state agents 0: [[3, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 0], False, []]\n",
      "state agents 0: [[3, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 1], False, []]\n",
      "state agents 0: [[3, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 0], False, []]\n",
      "state agents 0: [[3, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 0], False, []]\n",
      "state agents 0: [[3, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 0], False, []]\n",
      "state agents 0: [[4, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 0], False, []]\n",
      "state agents 0: [[3, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 0], False, []]\n",
      "state agents 0: [[3, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 1], False, []]\n",
      "state agents 0: [[3, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 1], False, []]\n",
      "state agents 0: [[3, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 1], False, []]\n",
      "state agents 0: [[3, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 2], False, []]\n",
      "state agents 0: [[3, 2], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 2], False, []]\n",
      "state agents 0: [[4, 2], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 1], False, []]\n",
      "state agents 0: [[4, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 1], False, []]\n",
      "state agents 0: [[3, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -10\n",
      "next state for agent 0: [[2, 1], False, []]\n",
      "Episode: 6, Total Steps: 59, Total Rewards: [-68], Status Episode: False\n",
      "--------------------\n",
      "state agents 0: [[0, 0], False, None]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 4], False, []]\n",
      "state agents 0: [[0, 4], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 3], False, []]\n",
      "state agents 0: [[1, 3], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 3], False, []]\n",
      "state agents 0: [[2, 3], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 hit !!!!!\n",
      "Reward for agent 0: 1100\n",
      "next state for agent 0: [[2, 2], True, []]\n",
      "Episode: 7, Total Steps: 42, Total Rewards: [1059], Status Episode: True\n",
      "--------------------\n",
      "state agents 0: [[0, 0], False, None]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 0], False, []]\n",
      "state agents 0: [[3, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 0], False, []]\n",
      "state agents 0: [[4, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 0], False, []]\n",
      "state agents 0: [[4, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 0], False, []]\n",
      "state agents 0: [[4, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 1], False, []]\n",
      "state agents 0: [[4, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 0], False, []]\n",
      "state agents 0: [[4, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 0], False, []]\n",
      "state agents 0: [[4, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 0], False, []]\n",
      "state agents 0: [[4, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 1], False, []]\n",
      "state agents 0: [[4, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 1], False, []]\n",
      "state agents 0: [[4, 1], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 1], False, []]\n",
      "state agents 0: [[4, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 2], False, []]\n",
      "state agents 0: [[4, 2], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 2], False, []]\n",
      "state agents 0: [[4, 2], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 2], False, []]\n",
      "state agents 0: [[4, 2], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 3], False, []]\n",
      "state agents 0: [[4, 3], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 3], False, []]\n",
      "state agents 0: [[4, 3], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 2], False, []]\n",
      "state agents 0: [[4, 2], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 2], False, []]\n",
      "state agents 0: [[3, 2], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 hit !!!!!\n",
      "Reward for agent 0: 1100\n",
      "next state for agent 0: [[2, 2], True, []]\n",
      "Episode: 8, Total Steps: 23, Total Rewards: [1078], Status Episode: True\n",
      "--------------------\n",
      "state agents 0: [[0, 0], False, None]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -10\n",
      "next state for agent 0: [[2, 1], False, []]\n",
      "Episode: 9, Total Steps: 8, Total Rewards: [-17], Status Episode: False\n",
      "--------------------\n",
      "state agents 0: [[0, 0], False, None]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 4], False, []]\n",
      "state agents 0: [[0, 4], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 4], False, []]\n",
      "state agents 0: [[0, 4], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 4], False, []]\n",
      "state agents 0: [[0, 4], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 4], False, []]\n",
      "state agents 0: [[1, 4], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 4], False, []]\n",
      "state agents 0: [[1, 4], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 4], False, []]\n",
      "state agents 0: [[1, 4], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 3], False, []]\n",
      "state agents 0: [[1, 3], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 4], False, []]\n",
      "state agents 0: [[1, 4], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 4], False, []]\n",
      "state agents 0: [[2, 4], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 4], False, []]\n",
      "state agents 0: [[3, 4], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 4], False, []]\n",
      "state agents 0: [[2, 4], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 4], False, []]\n",
      "state agents 0: [[2, 4], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 4], False, []]\n",
      "state agents 0: [[3, 4], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 4], False, []]\n",
      "state agents 0: [[3, 4], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 4], False, []]\n",
      "state agents 0: [[3, 4], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 4], False, []]\n",
      "state agents 0: [[4, 4], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 4], False, []]\n",
      "state agents 0: [[4, 4], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 4], False, []]\n",
      "state agents 0: [[4, 4], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 4], False, []]\n",
      "state agents 0: [[3, 4], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 3], False, []]\n",
      "state agents 0: [[3, 3], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 3], False, []]\n",
      "state agents 0: [[2, 3], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 hit !!!!!\n",
      "Reward for agent 0: 1100\n",
      "next state for agent 0: [[2, 2], True, []]\n",
      "Episode: 10, Total Steps: 49, Total Rewards: [1052], Status Episode: True\n",
      "--------------------\n",
      "state agents 0: [[0, 0], False, None]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 0], False, []]\n",
      "state agents 0: [[3, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 0], False, []]\n",
      "state agents 0: [[3, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 0], False, []]\n",
      "state agents 0: [[3, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 1], False, []]\n",
      "state agents 0: [[3, 1], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 1], False, []]\n",
      "state agents 0: [[4, 1], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 1], False, []]\n",
      "state agents 0: [[4, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 2], False, []]\n",
      "state agents 0: [[4, 2], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 1], False, []]\n",
      "state agents 0: [[4, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 1], False, []]\n",
      "state agents 0: [[4, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 0], False, []]\n",
      "state agents 0: [[4, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 0], False, []]\n",
      "state agents 0: [[4, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 0], False, []]\n",
      "state agents 0: [[4, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 0], False, []]\n",
      "state agents 0: [[3, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 0], False, []]\n",
      "state agents 0: [[4, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[4, 1], False, []]\n",
      "state agents 0: [[4, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 1], False, []]\n",
      "state agents 0: [[3, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 2], False, []]\n",
      "state agents 0: [[3, 2], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 hit !!!!!\n",
      "Reward for agent 0: 1100\n",
      "next state for agent 0: [[2, 2], True, []]\n",
      "Episode: 11, Total Steps: 32, Total Rewards: [1069], Status Episode: True\n",
      "--------------------\n",
      "state agents 0: [[0, 0], False, None]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 0], False, []]\n",
      "state agents 0: [[3, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 0], False, []]\n",
      "state agents 0: [[3, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 0], False, []]\n",
      "state agents 0: [[3, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 0], False, []]\n",
      "state agents 0: [[3, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 1], False, []]\n",
      "state agents 0: [[3, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 2], False, []]\n",
      "state agents 0: [[3, 2], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 hit !!!!!\n",
      "Reward for agent 0: 1100\n",
      "next state for agent 0: [[2, 2], True, []]\n",
      "Episode: 12, Total Steps: 34, Total Rewards: [1067], Status Episode: True\n",
      "--------------------\n",
      "state agents 0: [[0, 0], False, None]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 4], False, []]\n",
      "state agents 0: [[0, 4], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 4], False, []]\n",
      "state agents 0: [[0, 4], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 4], False, []]\n",
      "state agents 0: [[0, 4], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 4], False, []]\n",
      "state agents 0: [[0, 4], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 4], False, []]\n",
      "state agents 0: [[0, 4], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 4], False, []]\n",
      "state agents 0: [[0, 4], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 4], False, []]\n",
      "state agents 0: [[0, 4], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 4], False, []]\n",
      "state agents 0: [[1, 4], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 4], False, []]\n",
      "state agents 0: [[0, 4], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 4], False, []]\n",
      "state agents 0: [[0, 4], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 3], False, []]\n",
      "state agents 0: [[1, 3], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 4], False, []]\n",
      "state agents 0: [[0, 4], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 4], False, []]\n",
      "state agents 0: [[1, 4], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 4], False, []]\n",
      "state agents 0: [[2, 4], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 3], False, []]\n",
      "state agents 0: [[2, 3], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 hit !!!!!\n",
      "Reward for agent 0: 1100\n",
      "next state for agent 0: [[2, 2], True, []]\n",
      "Episode: 13, Total Steps: 39, Total Rewards: [1062], Status Episode: True\n",
      "--------------------\n",
      "state agents 0: [[0, 0], False, None]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 3], False, []]\n",
      "state agents 0: [[1, 3], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 3], False, []]\n",
      "state agents 0: [[1, 3], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 3], False, []]\n",
      "state agents 0: [[2, 3], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 hit !!!!!\n",
      "Reward for agent 0: 1100\n",
      "next state for agent 0: [[2, 2], True, []]\n",
      "Episode: 14, Total Steps: 14, Total Rewards: [1087], Status Episode: True\n",
      "--------------------\n",
      "state agents 0: [[0, 0], False, None]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 0], False, []]\n",
      "state agents 0: [[2, 0], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 0], False, []]\n",
      "state agents 0: [[3, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 1], False, []]\n",
      "state agents 0: [[3, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 0], False, []]\n",
      "state agents 0: [[3, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 1], False, []]\n",
      "state agents 0: [[3, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 2], False, []]\n",
      "state agents 0: [[3, 2], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[3, 3], False, []]\n",
      "state agents 0: [[3, 3], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 3], False, []]\n",
      "state agents 0: [[2, 3], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 hit !!!!!\n",
      "Reward for agent 0: 1100\n",
      "next state for agent 0: [[2, 2], True, []]\n",
      "Episode: 15, Total Steps: 20, Total Rewards: [1081], Status Episode: True\n",
      "--------------------\n",
      "state agents 0: [[0, 0], False, None]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 0], False, []]\n",
      "state agents 0: [[1, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 0], False, []]\n",
      "state agents 0: [[0, 0], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 1], False, []]\n",
      "state agents 0: [[1, 1], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 1], False, []]\n",
      "state agents 0: [[0, 1], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  3\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 2], False, []]\n",
      "state agents 0: [[0, 2], False, []]\n",
      "Physical action taken 0:  2\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[0, 3], False, []]\n",
      "state agents 0: [[0, 3], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 3], False, []]\n",
      "state agents 0: [[1, 3], False, []]\n",
      "Physical action taken 0:  0\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[1, 3], False, []]\n",
      "state agents 0: [[1, 3], False, []]\n",
      "Physical action taken 0:  4\n",
      "agent0 not hit !!!!!\n",
      "Reward for agent 0: -1\n",
      "next state for agent 0: [[2, 3], False, []]\n",
      "state agents 0: [[2, 3], False, []]\n",
      "Physical action taken 0:  1\n",
      "agent0 hit !!!!!\n",
      "Reward for agent 0: 1100\n",
      "next state for agent 0: [[2, 2], True, []]\n",
      "Episode: 16, Total Steps: 36, Total Rewards: [1065], Status Episode: True\n",
      "--------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 252\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# print(f\"number of actions: {n_actions}\")\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# n_actions = list(range(5)) + [(i, 'send') for i in range(5)]\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# print(f\"number of actions: {n_actions}\")\u001b[39;00m\n\u001b[1;32m    250\u001b[0m agent \u001b[38;5;241m=\u001b[39m QLearningAgent(env, n_actions)\n\u001b[0;32m--> 252\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_episodes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_agents):\n\u001b[1;32m    255\u001b[0m     agent\u001b[38;5;241m.\u001b[39mprint_q_table(agent_id\u001b[38;5;241m=\u001b[39magent_id)\n",
      "Cell \u001b[0;32mIn[1], line 115\u001b[0m, in \u001b[0;36mQLearningAgent.run_episodes\u001b[0;34m(self, num_episodes)\u001b[0m\n\u001b[1;32m    112\u001b[0m success_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m--> 115\u001b[0m     states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# print(f\"states after reset {states}\")\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     total_reward \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_agents  \n",
      "File \u001b[0;32m~/PHD/Research/code/myphd/rl/gridworld/code/environment_ma_reward_distance.py:73\u001b[0m, in \u001b[0;36mEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m---> 73\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# Reinitialize agents' positions\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     agent_positions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     77\u001b[0m         [UNIT \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, UNIT \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m],  \u001b[38;5;66;03m# Top-left for Agent 1\u001b[39;00m\n\u001b[1;32m     78\u001b[0m         [(WIDTH \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m*\u001b[39m UNIT, UNIT \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# Top-right for Agent 2\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from environment_ma_reward_distance import Env\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, actions):\n",
    "        print(f\"Actions received by class: {actions}\")\n",
    "        self.env = env\n",
    "        self.actions = actions\n",
    "        self.learning_rate = 0.1  \n",
    "        self.discount_factor = 0.9\n",
    "        self.epsilon = 0.1  \n",
    "        self.epsilon_decay = 0.995  \n",
    "        self.epsilon_min = 0.01  \n",
    "        self.q_tables = [defaultdict(lambda: [0.0] * actions) for _ in range(env.num_agents)]\n",
    "        self.rewards_per_episode = [[] for _ in range(env.num_agents)] \n",
    "        self.successful_episodes = [0] * env.num_agents  \n",
    "        self.total_successful_episodes = 0  # \n",
    "\n",
    "    def learn(self, agent_idx, state, physical_action, reward, next_state, comm_action):\n",
    "        # print(f\"state agents type received by Learn function{agent_idx}: {state}\")\n",
    "        # print(f\"state agents type received by Learn function{agent_idx}: {type(state)}\")\n",
    "        \n",
    "        physical_state = tuple(state[0])\n",
    "        winning_state = state[1]\n",
    "        comm_state = state[2]\n",
    "\n",
    "        physical_next_state = tuple(next_state[0])\n",
    "        winning_next_state = next_state[1]\n",
    "        comm_next_state = next_state[2]\n",
    "\n",
    "        # print(f\"state agents {agent_idx} in Learning process: {state}\")\n",
    "        # print(f\"next state agents {agent_idx} in Learning process: {next_state}\")\n",
    "\n",
    "        # print(f\"physical state agents type received by Learn function{agent_idx}: {physical_state}\")\n",
    "        # print(f\"physical state agents type received by Learn function{agent_idx}: {type(physical_state)}\")\n",
    "\n",
    "        # print(f\"physical Next state agents type received by Learn function{agent_idx}: {physical_next_state}\")\n",
    "        # print(f\"physical Next state agents type received by Learn function{agent_idx}: {type(physical_next_state)}\")\n",
    "\n",
    "        current_q = self.q_tables[agent_idx][physical_state][physical_action]\n",
    "        # print(f\"current q on {agent_idx} - {physical_state} - {physical_action}: {current_q}\")\n",
    "        max_next_q = max(self.q_tables[agent_idx][physical_next_state])\n",
    "        # print(f\"next q on {agent_idx} - {physical_next_state} - {self.q_tables[agent_idx][physical_next_state]}\")\n",
    "        new_q = (current_q + self.learning_rate *\n",
    "                (reward + self.discount_factor * max_next_q - current_q))\n",
    "        self.q_tables[agent_idx][physical_state][physical_action] = new_q\n",
    "\n",
    "        # Update q-table with communication action if it's not None\n",
    "        # if comm_action is not None:\n",
    "        #     print(f\"Agent {agent_idx} sends message: {comm_action}\")\n",
    "\n",
    "    def print_q_table(self, agent_id):\n",
    "        print(f\"Q-table for Agent {agent_id}:\")\n",
    "        for state, actions in self.q_tables[agent_id].items():\n",
    "            print(f\"  State: {state}\")\n",
    "            for action, q_value in enumerate(actions):\n",
    "                print(f\"    Action: {action}, Q-value: {q_value:.5f}\")\n",
    "        print(f\"End of Q-table for Agent {agent_id}\\n\")\n",
    "\n",
    "    def get_action(self, agent_idx, state):\n",
    "        # print(f\"state agents received by get action function{agent_idx}: {state}\")\n",
    "        # print(f\"state agents type received by get action function{agent_idx}: {type(state)}\")\n",
    "        physical_state = tuple(state[0])\n",
    "        winning_state = state[1]\n",
    "        comm_state = state[2]\n",
    "        # print(f\"physical state agents received by get action function{agent_idx}: {physical_state}\")\n",
    "        # print(f\"physical state agents type received by get action function{agent_idx}: {type(physical_state)}\")\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice(self.actions)\n",
    "            # print(f\"action taken from random function by agent {agent_idx} in state {physical_state}: {action}\")\n",
    "            # print(f\"self action value: {self.actions}\")\n",
    "            # print(f\"len action probability: {self.actions}\")\n",
    "            # print(f\"action taken random: {action}\")\n",
    "        else:\n",
    "            state_action = self.q_tables[agent_idx][physical_state]\n",
    "            # print(f\"state action of agent {agent_idx} in state {physical_state}: {state_action}\")\n",
    "            action = self.arg_max(state_action)\n",
    "            # print(f\"action taken from q-table by agent {agent_idx} in state {physical_state}: {action}\")\n",
    "        return action\n",
    "\n",
    "    def choose_action(self, agent_idx, state):\n",
    "        physical_action = self.get_action(agent_idx, state)\n",
    "        \n",
    "        # print(f\"physical action choosen from get action function: {physical_action}\")\n",
    "\n",
    "        if self.env.is_agent_silent:\n",
    "            communication_action = None\n",
    "        else:\n",
    "            communication_action = f\"Message send from agent {agent_idx}\"\n",
    "            \n",
    "        return (physical_action, communication_action)\n",
    "\n",
    "    @staticmethod\n",
    "    def arg_max(state_action):\n",
    "        max_index_list = []\n",
    "        max_value = state_action[0]\n",
    "        for index, value in enumerate(state_action):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)\n",
    "\n",
    "    def run_episodes(self, num_episodes):\n",
    "        \n",
    "        success_steps = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            states = self.env.reset()\n",
    "            # print(f\"states after reset {states}\")\n",
    "            total_reward = [0] * self.env.num_agents  \n",
    "            step_count = 0\n",
    "            dones = [False] * self.env.num_agents\n",
    "            win_states = [False] * self.env.num_agents\n",
    "            \n",
    "            success_episode = False\n",
    "            success_count = [0] * self.env.num_agents  \n",
    "\n",
    "            # while not(any(dones) or all(win_states)) and step_count <100:\n",
    "            while not(any(dones) or all(win_states)):\n",
    "                combination_actions = []\n",
    "                \n",
    "                for agent_idx in range(self.env.num_agents):\n",
    "                    state = states[agent_idx]\n",
    "                    print(f\"state agents {agent_idx}: {state}\")\n",
    "                    physical_action, comm_action = self.choose_action(agent_idx, state)\n",
    "                    combination_actions.append((physical_action, comm_action))\n",
    "\n",
    "                    print(f\"Physical action taken {agent_idx}:  {physical_action}\")\n",
    "                # print(f\"Combination action taken = {combination_actions}\")\n",
    "                # print(f\"rewards = {rewards}\")\n",
    "                # print(f\"dones = {dones}\")\n",
    "\n",
    "                next_states, rewards, dones = self.env.step(combination_actions)\n",
    "                \n",
    "                # print(f\"next states = {next_states}\")\n",
    "                # print(f\"rewards = {rewards}\")\n",
    "                # print(f\"dones = {dones}\")\n",
    "                \n",
    "                # print(f\"Win States before loop: {win_states}\")\n",
    "                win_states = []\n",
    "                for agent_idx in range(self.env.num_agents):\n",
    "                    \n",
    "                    physical_action = combination_actions[agent_idx][0]\n",
    "                    comm_action = combination_actions[agent_idx][1]\n",
    "\n",
    "                    state = states[agent_idx]\n",
    "                    reward = rewards[agent_idx]\n",
    "                    next_state = next_states[agent_idx]\n",
    "                    done = dones[agent_idx]\n",
    "\n",
    "                    win_state = next_state[1]\n",
    "\n",
    "                    # print(f\"Agent {agent_idx} progress result in step {step_count}: \")\n",
    "                    # print(f\"done {agent_idx}= {done}\")  \n",
    "                    # print(f\"next state {agent_idx} before update= {next_state}\")\n",
    "                    # print(f\"win state {agent_idx} before update= {win_state}\")\n",
    "                    # print(f\"reward {agent_idx}= {reward}\")\n",
    "                    \n",
    "\n",
    "                    self.learn(agent_idx, state, physical_action, reward, next_state, comm_action)\n",
    "\n",
    "                    # Check if agent reached target and mark episode as successful\n",
    "                    if (win_state): \n",
    "                        success_count[agent_idx] += 1\n",
    "                        print(f\"agent{agent_idx} hit !!!!!\")\n",
    "                        \n",
    "                    else:\n",
    "                        print(f\"agent{agent_idx} not hit !!!!!\")\n",
    "\n",
    "                    print(f\"Reward for agent {agent_idx}: {reward}\")\n",
    "                    print(f\"next state for agent {agent_idx}: {next_state}\")\n",
    "                    # print(f\"target: {env.get_circle_grid_position()}\")\n",
    "                    \n",
    "                    total_reward[agent_idx] += reward\n",
    "                    \n",
    "                    win_states.append(win_state)  \n",
    "\n",
    "                # print(f\"Win States for check: {win_states}\")\n",
    "                step_count += 1\n",
    "                states = next_states\n",
    "\n",
    "                self.env.render()\n",
    "                \n",
    "            if all(win_states):\n",
    "                self.total_successful_episodes += 1\n",
    "                success_steps.append(step_count)\n",
    "                success_episode = True\n",
    "\n",
    "            for agent_idx in range(self.env.num_agents):\n",
    "                self.rewards_per_episode[agent_idx].append(total_reward[agent_idx])\n",
    "                # print(f\"Agent {agent_idx} Hit Count: {success_count[agent_idx]}\")\n",
    "                # print(f\"success hit rate for agent {agent_idx} at episode {episode}: {success_count[agent_idx]/step_count*100}%\")\n",
    "            \n",
    "            # print(f\"win states: {win_states}\")\n",
    "            # print(f\"is all win states? {all(win_states)}\")\n",
    "            \n",
    "\n",
    "            print(f\"Episode: {episode + 1}, Total Steps: {step_count}, Total Rewards: {total_reward}, Status Episode: {success_episode}\")\n",
    "            print(\"--------------------\")\n",
    "\n",
    "          \n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "        # Calculate success rate for each agent\n",
    "        # for agent_idx in range(self.env.num_agents):\n",
    "        #     agent_hit_counts = success_count[agent_idx]\n",
    "        #     print(f\"Agent {agent_idx + 1} Hit Count: {agent_hit_counts}\")\n",
    "\n",
    "     \n",
    "        overall_success_rate = self.total_successful_episodes / num_episodes * 100\n",
    "        print(f\"Overall Success Rate: {overall_success_rate}%\")\n",
    "\n",
    "        self.plot_rewards_per_episode()\n",
    "        self.plot_success_steps(success_steps)\n",
    "\n",
    "    def plot_rewards_per_episode(self):\n",
    "        num_agents = self.env.num_agents\n",
    "        for agent_idx in range(num_agents):\n",
    "            plt.plot(self.rewards_per_episode[agent_idx], label=f'Agent {agent_idx}')\n",
    "\n",
    "        plt.title('Rewards per Episode')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_success_steps(success_steps):\n",
    "        plt.plot(success_steps)\n",
    "        plt.title('Steps Required for Successful Episodes')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Steps')\n",
    "        plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_agents = 1\n",
    "    is_agent_silent = False \n",
    "    env = Env(num_agents=num_agents, is_agent_silent=is_agent_silent)\n",
    "    n_actions = env.n_actions\n",
    "    # print(f\"number of actions: {n_actions}\")\n",
    "    # n_actions = list(range(5)) + [(i, 'send') for i in range(5)]\n",
    "    # print(f\"number of actions: {n_actions}\")\n",
    "    agent = QLearningAgent(env, n_actions)\n",
    "\n",
    "    agent.run_episodes(100)\n",
    "\n",
    "    for agent_id in range(num_agents):\n",
    "        agent.print_q_table(agent_id=agent_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
