{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- starting point of Episode 0 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 82 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 83 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 84 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 85 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 86 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 87 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 88 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 89 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 90 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 91 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 92 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 93 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 94 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 95 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 96 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 97 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 98 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 99 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 100 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 101 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 102 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 103 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 104 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 105 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 106 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 107 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 108 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 109 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 110 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 111 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 112 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 113 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 114 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 115 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 116 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 117 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 118 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 119 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 120 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 121 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 122 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 123 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 124 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 125 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 126 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 127 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 128 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 129 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 130 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 131 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 132 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 133 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 134 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 135 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 136 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 137 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "----- starting point of Episode 0 in steps 138 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 1\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f70e50>, <__main__.Case object at 0x7f34c3f71060>, <__main__.Case object at 0x7f34c3f889d0>, <__main__.Case object at 0x7f34c3f90370>, <__main__.Case object at 0x7f34c3f9f6d0>, <__main__.Case object at 0x7f34c3f9f6a0>, <__main__.Case object at 0x7f34c3fab010>, <__main__.Case object at 0x7f34c3f93cd0>, <__main__.Case object at 0x7f34c3f9f820>, <__main__.Case object at 0x7f34c3fb3ee0>, <__main__.Case object at 0x7f34c3fbc190>, <__main__.Case object at 0x7f34c3fbc280>, <__main__.Case object at 0x7f34c3fb3b80>, <__main__.Case object at 0x7f34c3fbc3a0>, <__main__.Case object at 0x7f34c3fbc040>, <__main__.Case object at 0x7f34c3fbc670>, <__main__.Case object at 0x7f34c3fb3bb0>, <__main__.Case object at 0x7f34c3fbc8b0>, <__main__.Case object at 0x7f34c3fbc340>, <__main__.Case object at 0x7f34c3fbca60>, <__main__.Case object at 0x7f34c3fb3be0>, <__main__.Case object at 0x7f34c3fbc790>, <__main__.Case object at 0x7f34c3fbc640>, <__main__.Case object at 0x7f34c3fbc520>, <__main__.Case object at 0x7f34c3fb3c40>, <__main__.Case object at 0x7f34c3fbce80>, <__main__.Case object at 0x7f34c3f88a30>, <__main__.Case object at 0x7f34c3fbcac0>, <__main__.Case object at 0x7f34c3fb3e80>, <__main__.Case object at 0x7f34c3fb3d60>, <__main__.Case object at 0x7f34c3f903a0>, <__main__.Case object at 0x7f34c3fbd300>, <__main__.Case object at 0x7f34c3fbd480>, <__main__.Case object at 0x7f34c3fbd4e0>, <__main__.Case object at 0x7f34c3fbd600>, <__main__.Case object at 0x7f34c3fbd6c0>, <__main__.Case object at 0x7f34c3fbd8a0>, <__main__.Case object at 0x7f34c3fbd900>, <__main__.Case object at 0x7f34c3fbc3d0>, <__main__.Case object at 0x7f34c3fbd0c0>, <__main__.Case object at 0x7f34c3fbd9c0>, <__main__.Case object at 0x7f34c3fbdc00>, <__main__.Case object at 0x7f34c3fbc7f0>, <__main__.Case object at 0x7f34c3fbda80>, <__main__.Case object at 0x7f34c3fbdb40>, <__main__.Case object at 0x7f34c3fbdf00>, <__main__.Case object at 0x7f34c3fbdd80>, <__main__.Case object at 0x7f34c3fbe080>, <__main__.Case object at 0x7f34c3fbde40>, <__main__.Case object at 0x7f34c3fbe200>, <__main__.Case object at 0x7f34c3fbceb0>, <__main__.Case object at 0x7f34c3fbcdc0>, <__main__.Case object at 0x7f34c3fbe140>, <__main__.Case object at 0x7f34c3fbe500>, <__main__.Case object at 0x7f34c3fbc2b0>, <__main__.Case object at 0x7f34c3fbe680>, <__main__.Case object at 0x7f34c3fbe860>, <__main__.Case object at 0x7f34c3fbe740>, <__main__.Case object at 0x7f34c3fbe830>, <__main__.Case object at 0x7f34c3fbd510>, <__main__.Case object at 0x7f34c3fbebc0>, <__main__.Case object at 0x7f34c3fbecb0>, <__main__.Case object at 0x7f34c3fbeb90>, <__main__.Case object at 0x7f34c3fbee60>, <__main__.Case object at 0x7f34c3fbefe0>, <__main__.Case object at 0x7f34c3fbef80>, <__main__.Case object at 0x7f34c3fbf160>, <__main__.Case object at 0x7f34c3fbf2b0>, <__main__.Case object at 0x7f34c3fbf3a0>, <__main__.Case object at 0x7f34c3fbcb80>, <__main__.Case object at 0x7f34c3fbf2e0>, <__main__.Case object at 0x7f34c3fbf190>, <__main__.Case object at 0x7f34c3fbf700>, <__main__.Case object at 0x7f34c3fbf6d0>, <__main__.Case object at 0x7f34c3fbf8b0>, <__main__.Case object at 0x7f34c3fbf880>, <__main__.Case object at 0x7f34c3fbfac0>, <__main__.Case object at 0x7f34c3fbfbb0>, <__main__.Case object at 0x7f34c3fbfca0>, <__main__.Case object at 0x7f34c3fbfdf0>, <__main__.Case object at 0x7f34c3fbfee0>, <__main__.Case object at 0x7f34c3fbe890>, <__main__.Case object at 0x7f34c3fbffd0>, <__main__.Case object at 0x7f34c3fbebf0>, <__main__.Case object at 0x7f34c3fbf010>, <__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c3fc8460>, <__main__.Case object at 0x7f34c3fc8100>, <__main__.Case object at 0x7f34c3fbfe20>, <__main__.Case object at 0x7f34c3fb3f10>, <__main__.Case object at 0x7f34c3fc8820>, <__main__.Case object at 0x7f34c3fc8970>, <__main__.Case object at 0x7f34c3fbf730>, <__main__.Case object at 0x7f34c3fc8bb0>, <__main__.Case object at 0x7f34c3fc8730>, <__main__.Case object at 0x7f34c3fc8d60>, <__main__.Case object at 0x7f34c3fbfaf0>, <__main__.Case object at 0x7f34c3fc8a90>, <__main__.Case object at 0x7f34c3fc8e80>, <__main__.Case object at 0x7f34c3fc9000>, <__main__.Case object at 0x7f34c3fbff10>, <__main__.Case object at 0x7f34c3fc8700>, <__main__.Case object at 0x7f34c3fc8dc0>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fbf3d0>, <__main__.Case object at 0x7f34c3fc9480>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fc9300>, <__main__.Case object at 0x7f34c3fc97e0>, <__main__.Case object at 0x7f34c3fc9720>, <__main__.Case object at 0x7f34c3fc8940>, <__main__.Case object at 0x7f34c3fc8310>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c3fc98a0>, <__main__.Case object at 0x7f34c3fc9660>, <__main__.Case object at 0x7f34c3fc9600>, <__main__.Case object at 0x7f34c3fc9e40>, <__main__.Case object at 0x7f34c3fc9f30>, <__main__.Case object at 0x7f34c3fc8af0>, <__main__.Case object at 0x7f34c3fbfa90>, <__main__.Case object at 0x7f34c3fca1a0>, <__main__.Case object at 0x7f34c3fc8eb0>, <__main__.Case object at 0x7f34c3fca0e0>, <__main__.Case object at 0x7f34c3fc9e10>, <__main__.Case object at 0x7f34c3fca2c0>, <__main__.Case object at 0x7f34c3fbe530>, <__main__.Case object at 0x7f34c3fca6b0>, <__main__.Case object at 0x7f34c3fbf520>, <__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c3fc91b0>, <__main__.Case object at 0x7f34c3fca020>, <__main__.Case object at 0x7f34c3fc8850>, <__main__.Case object at 0x7f34c3fcabc0>, <__main__.Case object at 0x7f34c3fcab90>, <__main__.Case object at 0x7f34c3fca290>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fc9810>, <__main__.Case object at 0x7f34c3fcb130>]\n",
      "agent0 comm temp case base: []\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f72170>, <__main__.Case object at 0x7f34c3f88970>, <__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3f93ac0>, <__main__.Case object at 0x7f34c3f89b40>, <__main__.Case object at 0x7f34c3fb2740>, <__main__.Case object at 0x7f34c3fb3cd0>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3fb3fa0>, <__main__.Case object at 0x7f34c3fb3df0>, <__main__.Case object at 0x7f34c3fb3c70>, <__main__.Case object at 0x7f34c3fbc1c0>, <__main__.Case object at 0x7f34c3fb2920>, <__main__.Case object at 0x7f34c3fbc160>, <__main__.Case object at 0x7f34c3fbc580>, <__main__.Case object at 0x7f34c3fbc5b0>, <__main__.Case object at 0x7f34c3fbc430>, <__main__.Case object at 0x7f34c3fa8820>, <__main__.Case object at 0x7f34c3fbc940>, <__main__.Case object at 0x7f34c3fbca00>, <__main__.Case object at 0x7f34c3fbcb20>, <__main__.Case object at 0x7f34c3fbcc10>, <__main__.Case object at 0x7f34c3fbcca0>, <__main__.Case object at 0x7f34c3fbcd60>, <__main__.Case object at 0x7f34c3fbce20>, <__main__.Case object at 0x7f34c3f9ce80>, <__main__.Case object at 0x7f34c3fbc700>, <__main__.Case object at 0x7f34c3fbcfa0>, <__main__.Case object at 0x7f34c3fbd120>, <__main__.Case object at 0x7f34c3fbd1e0>, <__main__.Case object at 0x7f34c3fbd390>, <__main__.Case object at 0x7f34c3fbd360>, <__main__.Case object at 0x7f34c3fbd570>, <__main__.Case object at 0x7f34c3fbd3c0>, <__main__.Case object at 0x7f34c3fbd750>, <__main__.Case object at 0x7f34c3fbd840>, <__main__.Case object at 0x7f34c3fbd870>, <__main__.Case object at 0x7f34c3fbd7e0>, <__main__.Case object at 0x7f34c3fbda20>, <__main__.Case object at 0x7f34c3fbdae0>, <__main__.Case object at 0x7f34c3fbdba0>, <__main__.Case object at 0x7f34c3fbdc90>, <__main__.Case object at 0x7f34c3fbdd20>, <__main__.Case object at 0x7f34c3fbdde0>, <__main__.Case object at 0x7f34c3fbdea0>, <__main__.Case object at 0x7f34c3fbdf90>, <__main__.Case object at 0x7f34c3fbe020>, <__main__.Case object at 0x7f34c3fbe0e0>, <__main__.Case object at 0x7f34c3fbe1a0>, <__main__.Case object at 0x7f34c3fbe290>, <__main__.Case object at 0x7f34c3fbe320>, <__main__.Case object at 0x7f34c3fbe3e0>, <__main__.Case object at 0x7f34c3fbe4a0>, <__main__.Case object at 0x7f34c3fbe590>, <__main__.Case object at 0x7f34c3fbe620>, <__main__.Case object at 0x7f34c3fbe6e0>, <__main__.Case object at 0x7f34c3fbe7a0>, <__main__.Case object at 0x7f34c3fbe8f0>, <__main__.Case object at 0x7f34c3fbe980>, <__main__.Case object at 0x7f34c3fbea40>, <__main__.Case object at 0x7f34c3fbeb00>, <__main__.Case object at 0x7f34c3fbec50>, <__main__.Case object at 0x7f34c3fbed40>, <__main__.Case object at 0x7f34c3fbee00>, <__main__.Case object at 0x7f34c3fbef20>, <__main__.Case object at 0x7f34c3fbf070>, <__main__.Case object at 0x7f34c3fbf100>, <__main__.Case object at 0x7f34c3fbf220>, <__main__.Case object at 0x7f34c3fbf340>, <__main__.Case object at 0x7f34c3fbf280>, <__main__.Case object at 0x7f34c3fbf4c0>, <__main__.Case object at 0x7f34c3fbf580>, <__main__.Case object at 0x7f34c3fbf640>, <__main__.Case object at 0x7f34c3fbf790>, <__main__.Case object at 0x7f34c3fbf820>, <__main__.Case object at 0x7f34c3fbf940>, <__main__.Case object at 0x7f34c3fbfa00>, <__main__.Case object at 0x7f34c3fbfb50>, <__main__.Case object at 0x7f34c3fbfc40>, <__main__.Case object at 0x7f34c3fbfd60>, <__main__.Case object at 0x7f34c3fbfe80>, <__main__.Case object at 0x7f34c3fbfdc0>, <__main__.Case object at 0x7f34c3fc80a0>, <__main__.Case object at 0x7f34c3fc8160>, <__main__.Case object at 0x7f34c3fc8280>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fc86a0>, <__main__.Case object at 0x7f34c3fc87c0>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fc8a00>, <__main__.Case object at 0x7f34c3fc8b50>, <__main__.Case object at 0x7f34c3fc8c40>, <__main__.Case object at 0x7f34c3fc8d00>, <__main__.Case object at 0x7f34c3fc8e20>, <__main__.Case object at 0x7f34c3fc8f10>, <__main__.Case object at 0x7f34c3fc8fa0>, <__main__.Case object at 0x7f34c3fc9060>, <__main__.Case object at 0x7f34c3fc9120>, <__main__.Case object at 0x7f34c3fc9210>, <__main__.Case object at 0x7f34c3fc92a0>, <__main__.Case object at 0x7f34c3fc9360>, <__main__.Case object at 0x7f34c3fc9420>, <__main__.Case object at 0x7f34c3fc94e0>, <__main__.Case object at 0x7f34c3fc95a0>, <__main__.Case object at 0x7f34c3fc96c0>, <__main__.Case object at 0x7f34c3fc9780>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fc9900>, <__main__.Case object at 0x7f34c3fc99c0>, <__main__.Case object at 0x7f34c3fc9a80>, <__main__.Case object at 0x7f34c3fc9b70>, <__main__.Case object at 0x7f34c3fc9c00>, <__main__.Case object at 0x7f34c3fc9cc0>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fc9ed0>, <__main__.Case object at 0x7f34c3fc9fc0>, <__main__.Case object at 0x7f34c3fca080>, <__main__.Case object at 0x7f34c3fca140>, <__main__.Case object at 0x7f34c3fca230>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fca3e0>, <__main__.Case object at 0x7f34c3fca4a0>, <__main__.Case object at 0x7f34c3fca440>, <__main__.Case object at 0x7f34c3fca620>, <__main__.Case object at 0x7f34c3fca740>, <__main__.Case object at 0x7f34c3fca800>, <__main__.Case object at 0x7f34c3fca8f0>, <__main__.Case object at 0x7f34c3fca980>, <__main__.Case object at 0x7f34c3fcaa40>, <__main__.Case object at 0x7f34c3fcab00>, <__main__.Case object at 0x7f34c3fcac50>, <__main__.Case object at 0x7f34c3fcace0>, <__main__.Case object at 0x7f34c3fcada0>, <__main__.Case object at 0x7f34c3fcaec0>, <__main__.Case object at 0x7f34c3fcb010>, <__main__.Case object at 0x7f34c3fcb0a0>]\n",
      "agent1 comm temp case base: []\n",
      "Episode succeeded, case (4, 5) is empty. Temporary case base stored to the case base: ((4, 5), 1, 0.5)\n",
      "Episode succeeded, case (4, 6) is empty. Temporary case base stored to the case base: ((4, 6), 1, 0.5)\n",
      "Episode succeeded, case (3, 6) is empty. Temporary case base stored to the case base: ((3, 6), 4, 0.5)\n",
      "Episode succeeded, case (3, 7) is empty. Temporary case base stored to the case base: ((3, 7), 1, 0.5)\n",
      "Episode succeeded, case (4, 7) is empty. Temporary case base stored to the case base: ((4, 7), 3, 0.5)\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) is empty. Temporary case base stored to the case base: ((5, 6), 3, 0.5)\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) is empty. Temporary case base stored to the case base: ((6, 6), 3, 0.5)\n",
      "Episode succeeded, case (6, 5) is empty. Temporary case base stored to the case base: ((6, 5), 2, 0.5)\n",
      "Episode succeeded, case (6, 4) is empty. Temporary case base stored to the case base: ((6, 4), 2, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 7) is empty. Temporary case base stored to the case base: ((6, 7), 1, 0.5)\n",
      "Episode succeeded, case (6, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 7) is empty. Temporary case base stored to the case base: ((5, 7), 4, 0.5)\n",
      "Episode succeeded, case (4, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 7) is empty. Temporary case base stored to the case base: ((7, 7), 3, 0.5)\n",
      "Episode succeeded, case (8, 7) is empty. Temporary case base stored to the case base: ((8, 7), 3, 0.5)\n",
      "Episode succeeded, case (9, 7) is empty. Temporary case base stored to the case base: ((9, 7), 3, 0.5)\n",
      "Episode succeeded, case (9, 8) is empty. Temporary case base stored to the case base: ((9, 8), 1, 0.5)\n",
      "Episode succeeded, case (9, 8) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 8) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 6) is empty. Temporary case base stored to the case base: ((7, 6), 2, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 6) is empty. Temporary case base stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (8, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 8) is empty. Temporary case base stored to the case base: ((8, 8), 1, 0.5)\n",
      "Episode succeeded, case (8, 9) is empty. Temporary case base stored to the case base: ((8, 9), 1, 0.5)\n",
      "Episode succeeded, case (8, 9) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 9) is empty. Temporary case base stored to the case base: ((9, 9), 3, 0.5)\n",
      "Episode succeeded, case (8, 9) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 9) is empty. Temporary case base stored to the case base: ((7, 9), 4, 0.5)\n",
      "Episode succeeded, case (8, 9) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 9) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 9) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 8) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 8) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 6) is empty. Temporary case base stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) is empty. Temporary case base stored to the case base: ((9, 5), 2, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 7) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) is empty. Temporary case base stored to the case base: ((5, 5), 2, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) is empty. Temporary case base stored to the case base: ((5, 4), 2, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) is empty. Temporary case base stored to the case base: ((7, 4), 3, 0.5)\n",
      "Episode succeeded, case (7, 3) is empty. Temporary case base stored to the case base: ((7, 3), 2, 0.5)\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) is empty. Temporary case base stored to the case base: ((6, 3), 2, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 2, 0.5)\n",
      "Episode succeeded, case (6, 1) is empty. Temporary case base stored to the case base: ((6, 1), 2, 0.5)\n",
      "Episode succeeded, case (5, 1) is empty. Temporary case base stored to the case base: ((5, 1), 4, 0.5)\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 2, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.5)\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 1, 0.5)\n",
      "Episode succeeded, case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 4, 0.5)\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 1, 0.5)\n",
      "Episode succeeded, case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 1, 0.5)\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 3, 0.5)\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 1, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.5, time steps: 138\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.5, time steps: 137\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.5, time steps: 136\n",
      "cases content after RETAIN, problem: (3, 7), solution: 1, tv: 0.5, time steps: 135\n",
      "cases content after RETAIN, problem: (4, 7), solution: 3, tv: 0.5, time steps: 134\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.5, time steps: 131\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.5, time steps: 129\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.5, time steps: 128\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.5, time steps: 127\n",
      "cases content after RETAIN, problem: (6, 7), solution: 1, tv: 0.5, time steps: 124\n",
      "cases content after RETAIN, problem: (5, 7), solution: 4, tv: 0.5, time steps: 122\n",
      "cases content after RETAIN, problem: (7, 7), solution: 3, tv: 0.5, time steps: 115\n",
      "cases content after RETAIN, problem: (8, 7), solution: 3, tv: 0.5, time steps: 114\n",
      "cases content after RETAIN, problem: (9, 7), solution: 3, tv: 0.5, time steps: 113\n",
      "cases content after RETAIN, problem: (9, 8), solution: 1, tv: 0.5, time steps: 112\n",
      "cases content after RETAIN, problem: (7, 6), solution: 2, tv: 0.5, time steps: 105\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 0.5, time steps: 103\n",
      "cases content after RETAIN, problem: (8, 8), solution: 1, tv: 0.5, time steps: 100\n",
      "cases content after RETAIN, problem: (8, 9), solution: 1, tv: 0.5, time steps: 99\n",
      "cases content after RETAIN, problem: (9, 9), solution: 3, tv: 0.5, time steps: 97\n",
      "cases content after RETAIN, problem: (7, 9), solution: 4, tv: 0.5, time steps: 95\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.5, time steps: 87\n",
      "cases content after RETAIN, problem: (9, 5), solution: 2, tv: 0.5, time steps: 86\n",
      "cases content after RETAIN, problem: (5, 5), solution: 2, tv: 0.5, time steps: 75\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 73\n",
      "cases content after RETAIN, problem: (7, 4), solution: 3, tv: 0.5, time steps: 70\n",
      "cases content after RETAIN, problem: (7, 3), solution: 2, tv: 0.5, time steps: 69\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 65\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 63\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 62\n",
      "cases content after RETAIN, problem: (5, 1), solution: 4, tv: 0.5, time steps: 61\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 59\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 56\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5, time steps: 53\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 50\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 45\n",
      "cases content after RETAIN, problem: (9, 1), solution: 1, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (8, 1), solution: 4, tv: 0.5, time steps: 28\n",
      "cases content after RETAIN, problem: (7, 1), solution: 1, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 2), solution: 1, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (8, 3), solution: 3, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (9, 2), solution: 1, tv: 0.5, time steps: 6\n",
      "Episode: 0, Total Steps: 139, Total Rewards: [-135, -38], Status Episode: False\n",
      "------------------------------------------End of episode 0 loop--------------------\n",
      "----- starting point of Episode 1 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.5, 45)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.5, 50)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.5, 53)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.5, 59)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.5, 62)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.5, 63)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.5, 65)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.5, 65)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.5, 65)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.5, 65)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.5, 65)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 6) with action 4 to next state (6, 6): pull reward: 0\n",
      "----- starting point of Episode 1 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.5, 65)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.5, 65)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.5, 65)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 1 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.5, 65)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "No communication. Standard Q-learning update for agent 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f903a0>, <__main__.Case object at 0x7f34c3f93ac0>, <__main__.Case object at 0x7f34c3f89b40>, <__main__.Case object at 0x7f34c3fb3bb0>, <__main__.Case object at 0x7f34c3fb2740>, <__main__.Case object at 0x7f34c3fa8a00>, <__main__.Case object at 0x7f34c3f9f820>, <__main__.Case object at 0x7f34c3fbc190>, <__main__.Case object at 0x7f34c3fbc280>, <__main__.Case object at 0x7f34c3fbc910>, <__main__.Case object at 0x7f34c3fbc790>, <__main__.Case object at 0x7f34c3fbce80>, <__main__.Case object at 0x7f34c3fbd030>, <__main__.Case object at 0x7f34c3fbd630>, <__main__.Case object at 0x7f34c3fbd960>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f71060>, <__main__.Case object at 0x7f34c3f70e50>, <__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3f889d0>, <__main__.Case object at 0x7f34c3fb3e50>, <__main__.Case object at 0x7f34c3fb3c70>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3f9f6d0>, <__main__.Case object at 0x7f34c3fbc040>, <__main__.Case object at 0x7f34c3fbc6d0>, <__main__.Case object at 0x7f34c3fbc9d0>, <__main__.Case object at 0x7f34c3fbc640>, <__main__.Case object at 0x7f34c3fbd330>, <__main__.Case object at 0x7f34c3fbd480>, <__main__.Case object at 0x7f34c3fbd6f0>]\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (6, 3) is empty. Temporary case base stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (6, 1) is empty. Temporary case base stored to the case base: ((6, 1), 2, 0.5)\n",
      "Integrated case process. comm case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 2, 0.5)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 0.5)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 0.5)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 65\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 63\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 62\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 59\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5, time steps: 53\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 50\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 45\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f93cd0>, <__main__.Case object at 0x7f34c3f90340>, <__main__.Case object at 0x7f34c3fb3ca0>, <__main__.Case object at 0x7f34c3fb3f10>, <__main__.Case object at 0x7f34c3fb3f40>, <__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3fbd060>, <__main__.Case object at 0x7f34c3fbd2a0>, <__main__.Case object at 0x7f34c3fbc400>, <__main__.Case object at 0x7f34c3fbc820>, <__main__.Case object at 0x7f34c3fbcaf0>, <__main__.Case object at 0x7f34c3fbc520>, <__main__.Case object at 0x7f34c3fbd0f0>, <__main__.Case object at 0x7f34c3fbd540>, <__main__.Case object at 0x7f34c3fbd810>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 0.6, time steps: 138\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 0.6, time steps: 137\n",
      "case content after REVISE for agent 1, problem: (3, 6), solution: 4, tv: 0.4, time steps: 136\n",
      "case content after REVISE for agent 1, problem: (3, 7), solution: 1, tv: 0.4, time steps: 135\n",
      "case content after REVISE for agent 1, problem: (4, 7), solution: 3, tv: 0.4, time steps: 134\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 0.6, time steps: 131\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 0.6, time steps: 129\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 0.6, time steps: 128\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.6, time steps: 127\n",
      "case content after REVISE for agent 1, problem: (6, 7), solution: 1, tv: 0.4, time steps: 124\n",
      "case content after REVISE for agent 1, problem: (5, 7), solution: 4, tv: 0.4, time steps: 122\n",
      "case content after REVISE for agent 1, problem: (7, 7), solution: 3, tv: 0.4, time steps: 115\n",
      "case content after REVISE for agent 1, problem: (8, 7), solution: 3, tv: 0.4, time steps: 114\n",
      "case content after REVISE for agent 1, problem: (9, 7), solution: 3, tv: 0.4, time steps: 113\n",
      "case content after REVISE for agent 1, problem: (9, 8), solution: 1, tv: 0.4, time steps: 112\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 2, tv: 0.4, time steps: 105\n",
      "case content after REVISE for agent 1, problem: (8, 6), solution: 3, tv: 0.4, time steps: 103\n",
      "case content after REVISE for agent 1, problem: (8, 8), solution: 1, tv: 0.4, time steps: 100\n",
      "case content after REVISE for agent 1, problem: (8, 9), solution: 1, tv: 0.4, time steps: 99\n",
      "case content after REVISE for agent 1, problem: (9, 9), solution: 3, tv: 0.4, time steps: 97\n",
      "case content after REVISE for agent 1, problem: (7, 9), solution: 4, tv: 0.4, time steps: 95\n",
      "case content after REVISE for agent 1, problem: (9, 6), solution: 3, tv: 0.4, time steps: 87\n",
      "case content after REVISE for agent 1, problem: (9, 5), solution: 2, tv: 0.4, time steps: 86\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 2, tv: 0.4, time steps: 75\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 2, tv: 0.4, time steps: 73\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 3, tv: 0.4, time steps: 70\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 2, tv: 0.4, time steps: 69\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.6, time steps: 65\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.6, time steps: 63\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.6, time steps: 62\n",
      "case content after REVISE for agent 1, problem: (5, 1), solution: 4, tv: 0.4, time steps: 61\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.6, time steps: 59\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.4, time steps: 56\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.6, time steps: 53\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.6, time steps: 50\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.6, time steps: 45\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 1, tv: 0.4, time steps: 29\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 4, tv: 0.4, time steps: 28\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 1, tv: 0.4, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 1, tv: 0.4, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 3, tv: 0.4, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 1, tv: 0.4, time steps: 6\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 5), 1, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 6), 1, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 6), 3, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 6), 3, 0.5, 15)\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 5), 2, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 2, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 3), 2, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 2), 2, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 1), 2, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 0), 2, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 0), 3, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((8, 0), 3, 0.5, 15)\n",
      "Episode succeeded, updated case base with fewer steps: ((9, 0), 3, 0.5, 15)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 15\n",
      "Episode: 1, Total Steps: 15, Total Rewards: [-106, 86], Status Episode: False\n",
      "------------------------------------------End of episode 1 loop--------------------\n",
      "----- starting point of Episode 2 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.5, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.5, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.5, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 0) with action 1 to next state (6, 0): pull reward: 0\n",
      "----- starting point of Episode 2 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.5, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.5, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.5, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.5, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 0.5, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 0.5, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 0.5, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 0.5, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 6) with action 1 to next state (4, 5): pull reward: 0\n",
      "----- starting point of Episode 2 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 0.5, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 2 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 2 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 2 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 2 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 2 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 2 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 4 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 2 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 2 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 4 to next state (4, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f903a0>, <__main__.Case object at 0x7f34c3f88970>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3f88a30>, <__main__.Case object at 0x7f34c3fb3ca0>, <__main__.Case object at 0x7f34c3fbd0c0>, <__main__.Case object at 0x7f34c3fbc9d0>, <__main__.Case object at 0x7f34c3fbd480>, <__main__.Case object at 0x7f34c3fbd5a0>, <__main__.Case object at 0x7f34c3fbcac0>, <__main__.Case object at 0x7f34c3fbc3d0>, <__main__.Case object at 0x7f34c3fbc8b0>, <__main__.Case object at 0x7f34c3fbd1b0>, <__main__.Case object at 0x7f34c3fbc2b0>, <__main__.Case object at 0x7f34c3fbe5f0>, <__main__.Case object at 0x7f34c3fbe560>, <__main__.Case object at 0x7f34c3fbe260>, <__main__.Case object at 0x7f34c3fbdd80>, <__main__.Case object at 0x7f34c3fbc550>, <__main__.Case object at 0x7f34c3fbda80>, <__main__.Case object at 0x7f34c3f90370>, <__main__.Case object at 0x7f34c3fbf940>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3fa8a00>, <__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3f9f6d0>, <__main__.Case object at 0x7f34c3f72170>, <__main__.Case object at 0x7f34c3fb2920>, <__main__.Case object at 0x7f34c3fbc040>, <__main__.Case object at 0x7f34c3fbc640>, <__main__.Case object at 0x7f34c3fb3fa0>, <__main__.Case object at 0x7f34c3fbc790>, <__main__.Case object at 0x7f34c3fbc490>, <__main__.Case object at 0x7f34c3fbc0d0>, <__main__.Case object at 0x7f34c3fbd8a0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.5, time steps: 65\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.5, time steps: 62\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.5, time steps: 59\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.5, time steps: 53\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 50\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5, time steps: 45\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 5) is empty. Temporary case base stored to the case base: ((4, 5), 1, 0.5)\n",
      "Integrated case process. comm case (5, 6) is empty. Temporary case base stored to the case base: ((5, 6), 3, 0.5)\n",
      "Integrated case process. comm case (6, 6) is empty. Temporary case base stored to the case base: ((6, 6), 3, 0.5)\n",
      "Integrated case process. comm case (6, 5) is empty. Temporary case base stored to the case base: ((6, 5), 2, 0.5)\n",
      "Integrated case process. comm case (6, 4) is empty. Temporary case base stored to the case base: ((6, 4), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 65\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 63\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 62\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 59\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5, time steps: 53\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 50\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 45\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.5, time steps: 15\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f93cd0>, <__main__.Case object at 0x7f34c3f90340>, <__main__.Case object at 0x7f34c3f889a0>, <__main__.Case object at 0x7f34c3fb2740>, <__main__.Case object at 0x7f34c3fb3f10>, <__main__.Case object at 0x7f34c3fa8820>, <__main__.Case object at 0x7f34c3fbc6d0>, <__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3fbc310>, <__main__.Case object at 0x7f34c3fbcf10>, <__main__.Case object at 0x7f34c3fbc850>, <__main__.Case object at 0x7f34c3fbc3a0>, <__main__.Case object at 0x7f34c3fbcdf0>, <__main__.Case object at 0x7f34c3fbcbe0>, <__main__.Case object at 0x7f34c3fbe500>, <__main__.Case object at 0x7f34c3fbe3b0>, <__main__.Case object at 0x7f34c3fb3df0>, <__main__.Case object at 0x7f34c3fbdf60>, <__main__.Case object at 0x7f34c3fb3d60>, <__main__.Case object at 0x7f34c3fb3f70>, <__main__.Case object at 0x7f34c3fbd9c0>, <__main__.Case object at 0x7f34c3fbed70>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.6, time steps: 15\n",
      "Episode succeeded, case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 1, 0.5)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5, time steps: 21\n",
      "Episode: 2, Total Steps: 22, Total Rewards: [-121, 87], Status Episode: False\n",
      "------------------------------------------End of episode 2 loop--------------------\n",
      "----- starting point of Episode 3 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 3) with action 3 to next state (5, 3): pull reward: 0\n",
      "----- starting point of Episode 3 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 3) with action 4 to next state (6, 3): pull reward: 0\n",
      "----- starting point of Episode 3 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 5) with action 2 to next state (6, 6): pull reward: 0\n",
      "----- starting point of Episode 3 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 6) with action 3 to next state (5, 6): pull reward: 0\n",
      "----- starting point of Episode 3 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 0.6, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 21)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 21)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 3 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 21)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 3 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 21)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 2 to next state (2, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f90340>, <__main__.Case object at 0x7f34c3f889a0>, <__main__.Case object at 0x7f34c3f9f6a0>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3fb3ee0>, <__main__.Case object at 0x7f34c3fbd6c0>, <__main__.Case object at 0x7f34c3fb2980>, <__main__.Case object at 0x7f34c3fbd480>, <__main__.Case object at 0x7f34c3fbefb0>, <__main__.Case object at 0x7f34c3fbd4e0>, <__main__.Case object at 0x7f34c3fbcdc0>, <__main__.Case object at 0x7f34c3fbcee0>, <__main__.Case object at 0x7f34c3fbdc60>, <__main__.Case object at 0x7f34c3fbd450>, <__main__.Case object at 0x7f34c3fbc850>, <__main__.Case object at 0x7f34c3fbcbe0>, <__main__.Case object at 0x7f34c3fbfa30>, <__main__.Case object at 0x7f34c3fbe080>, <__main__.Case object at 0x7f34c3fbf7f0>, <__main__.Case object at 0x7f34c3fbf2e0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f72170>, <__main__.Case object at 0x7f34c3f93d00>, <__main__.Case object at 0x7f34c3f88a30>, <__main__.Case object at 0x7f34c3f9f6d0>, <__main__.Case object at 0x7f34c3fb3f10>, <__main__.Case object at 0x7f34c3fb3bb0>, <__main__.Case object at 0x7f34c3fbd630>, <__main__.Case object at 0x7f34c3fbc640>, <__main__.Case object at 0x7f34c3fbf850>, <__main__.Case object at 0x7f34c3fbd0f0>, <__main__.Case object at 0x7f34c3fbcf10>, <__main__.Case object at 0x7f34c3fbcdf0>, <__main__.Case object at 0x7f34c3fbdff0>, <__main__.Case object at 0x7f34c3fbe7a0>, <__main__.Case object at 0x7f34c3fbf610>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.5, time steps: 65\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.5, time steps: 62\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.5, time steps: 59\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.5, time steps: 53\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 50\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5, time steps: 45\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.5, time steps: 15\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 0.6)\n",
      "Integrated case process. comm case (4, 6) is empty. Temporary case base stored to the case base: ((4, 6), 1, 0.6)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 0.6)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 0.6)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.6)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.6)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.6)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.6)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.6)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.6)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.6)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 65\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 63\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 62\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 59\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5, time steps: 53\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 50\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 45\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.6, time steps: 15\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f88970>, <__main__.Case object at 0x7f34c3f93cd0>, <__main__.Case object at 0x7f34c3fa8820>, <__main__.Case object at 0x7f34c3fb3cd0>, <__main__.Case object at 0x7f34c3fb3b80>, <__main__.Case object at 0x7f34c3fb3f70>, <__main__.Case object at 0x7f34c3fbc040>, <__main__.Case object at 0x7f34c3fbe9e0>, <__main__.Case object at 0x7f34c3fbc190>, <__main__.Case object at 0x7f34c3fbc8b0>, <__main__.Case object at 0x7f34c3fbe680>, <__main__.Case object at 0x7f34c3fbde40>, <__main__.Case object at 0x7f34c3fbdf00>, <__main__.Case object at 0x7f34c3fbd810>, <__main__.Case object at 0x7f34c3fbc2e0>, <__main__.Case object at 0x7f34c3fbc3a0>, <__main__.Case object at 0x7f34c3fbe500>, <__main__.Case object at 0x7f34c3fbc7f0>, <__main__.Case object at 0x7f34c3fbf5b0>, <__main__.Case object at 0x7f34c3fbdab0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.6, time steps: 21\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 4), 1, 0.5, 20)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) is empty. Temporary case base stored to the case base: ((5, 3), 4, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5, time steps: 20\n",
      "cases content after RETAIN, problem: (5, 3), solution: 4, tv: 0.5, time steps: 7\n",
      "Episode: 3, Total Steps: 20, Total Rewards: [-119, 86], Status Episode: False\n",
      "------------------------------------------End of episode 3 loop--------------------\n",
      "----- starting point of Episode 4 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 0.7, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 4 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 4 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 4 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 4 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 4 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 4 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 4 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 4 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 4 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 4 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 82 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 83 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 84 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 85 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 86 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 87 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 88 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 89 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 90 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 91 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 92 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 93 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 4 in steps 94 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 4 in steps 95 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 96 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 97 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 98 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 99 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 100 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 101 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 102 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 103 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 104 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 105 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 106 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 107 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 4 in steps 108 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 4 in steps 109 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 110 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 111 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 112 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 113 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 114 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 4 in steps 115 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 116 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 117 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 118 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 119 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 120 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 121 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 122 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 123 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 124 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 125 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 126 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 127 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 128 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 129 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 130 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 131 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 132 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 133 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 134 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 135 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 136 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 137 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 138 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 139 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 140 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 141 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 142 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 143 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 144 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 145 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 146 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 4 in steps 147 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 148 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 149 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 150 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 151 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 152 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 153 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 154 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 155 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 156 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 157 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 158 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 159 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 160 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 161 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 162 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 163 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 164 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 165 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 166 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 167 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 168 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 169 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 170 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 171 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 172 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 173 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 174 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 175 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 176 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 177 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 4 in steps 178 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 179 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 4 in steps 180 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 181 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 182 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 183 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 184 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 185 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 186 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 187 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 188 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 189 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 190 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 191 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 192 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 193 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 4 in steps 194 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 195 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 196 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 197 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 198 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 199 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 200 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 201 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 202 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 203 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 4 in steps 204 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 2 to next state (1, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f88a30>, <__main__.Case object at 0x7f34c3fa8a00>, <__main__.Case object at 0x7f34c3fb3d90>, <__main__.Case object at 0x7f34c3fb2920>, <__main__.Case object at 0x7f34c3f93ac0>, <__main__.Case object at 0x7f34c3fbdab0>, <__main__.Case object at 0x7f34c3fbc2b0>, <__main__.Case object at 0x7f34c3fbe470>, <__main__.Case object at 0x7f34c3fbf700>, <__main__.Case object at 0x7f34c3fbd4e0>, <__main__.Case object at 0x7f34c3fbdc60>, <__main__.Case object at 0x7f34c3fbcbe0>, <__main__.Case object at 0x7f34c3fbe080>, <__main__.Case object at 0x7f34c3fbc040>, <__main__.Case object at 0x7f34c3fbe680>, <__main__.Case object at 0x7f34c3fbd810>, <__main__.Case object at 0x7f34c3fbc3a0>, <__main__.Case object at 0x7f34c3fbdf60>, <__main__.Case object at 0x7f34c3fbfe50>, <__main__.Case object at 0x7f34c3fbfb20>, <__main__.Case object at 0x7f34c3fbceb0>, <__main__.Case object at 0x7f34c3fbccd0>, <__main__.Case object at 0x7f34c3fbe6b0>, <__main__.Case object at 0x7f34c3fbd510>, <__main__.Case object at 0x7f34c3fbebc0>, <__main__.Case object at 0x7f34c3fbf040>, <__main__.Case object at 0x7f34c3fbf1f0>, <__main__.Case object at 0x7f34c3fbf010>, <__main__.Case object at 0x7f34c3fbc250>, <__main__.Case object at 0x7f34c3fbc370>, <__main__.Case object at 0x7f34c3fbc5b0>, <__main__.Case object at 0x7f34c3fbc970>, <__main__.Case object at 0x7f34c3fbfee0>, <__main__.Case object at 0x7f34c3fbd0c0>, <__main__.Case object at 0x7f34c3fbd360>, <__main__.Case object at 0x7f34c3fbd750>, <__main__.Case object at 0x7f34c3fbd7e0>, <__main__.Case object at 0x7f34c3fbf910>, <__main__.Case object at 0x7f34c3fbdd20>, <__main__.Case object at 0x7f34c3fbe020>, <__main__.Case object at 0x7f34c3fbe4a0>, <__main__.Case object at 0x7f34c3fbd9c0>, <__main__.Case object at 0x7f34c3fbe8f0>, <__main__.Case object at 0x7f34c3fbef20>, <__main__.Case object at 0x7f34c3fbf220>, <__main__.Case object at 0x7f34c3fbf820>, <__main__.Case object at 0x7f34c3fbfc40>, <__main__.Case object at 0x7f34c3fbfdc0>, <__main__.Case object at 0x7f34c3fcb100>, <__main__.Case object at 0x7f34c3f93d00>, <__main__.Case object at 0x7f34c3fc9120>, <__main__.Case object at 0x7f34c3fc9a80>, <__main__.Case object at 0x7f34c3fc9c00>, <__main__.Case object at 0x7f34c3fcace0>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fc8250>, <__main__.Case object at 0x7f34c3fbda50>, <__main__.Case object at 0x7f34c3fc9cc0>, <__main__.Case object at 0x7f34c3fc8c10>, <__main__.Case object at 0x7f34c3fc8df0>, <__main__.Case object at 0x7f34c3fc8820>, <__main__.Case object at 0x7f34c3fc9330>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fc9720>, <__main__.Case object at 0x7f34c3fc8310>, <__main__.Case object at 0x7f34c3fc9600>, <__main__.Case object at 0x7f34c3fc8af0>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fca470>, <__main__.Case object at 0x7f34c3fca8c0>, <__main__.Case object at 0x7f34c3fcaa10>, <__main__.Case object at 0x7f34c3fcacb0>, <__main__.Case object at 0x7f34c3fcae90>, <__main__.Case object at 0x7f34c3fc80a0>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fc86a0>, <__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fc8e20>, <__main__.Case object at 0x7f34c3fc92a0>, <__main__.Case object at 0x7f34c3fc96c0>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fc9ed0>, <__main__.Case object at 0x7f34c3fca140>, <__main__.Case object at 0x7f34c3fca440>, <__main__.Case object at 0x7f34c3fca980>, <__main__.Case object at 0x7f34c3fcbc70>, <__main__.Case object at 0x7f34c3fcbb50>, <__main__.Case object at 0x7f34c3fcba30>, <__main__.Case object at 0x7f34c3fcb910>, <__main__.Case object at 0x7f34c3fcb7f0>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fcb5b0>, <__main__.Case object at 0x7f34c3fcb3d0>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fcbd30>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fcbf70>, <__main__.Case object at 0x7f34c3fd80d0>, <__main__.Case object at 0x7f34c3fd81f0>, <__main__.Case object at 0x7f34c3fd82b0>, <__main__.Case object at 0x7f34c3fd8430>, <__main__.Case object at 0x7f34c3fd8550>, <__main__.Case object at 0x7f34c3fd8670>, <__main__.Case object at 0x7f34c3fd8730>, <__main__.Case object at 0x7f34c3fd88b0>, <__main__.Case object at 0x7f34c3fd89d0>, <__main__.Case object at 0x7f34c3fd8a90>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fd8c70>, <__main__.Case object at 0x7f34c3fd8d90>, <__main__.Case object at 0x7f34c3fd8eb0>, <__main__.Case object at 0x7f34c3fd8f70>, <__main__.Case object at 0x7f34c3fd90f0>, <__main__.Case object at 0x7f34c3fd91b0>, <__main__.Case object at 0x7f34c3fd92d0>, <__main__.Case object at 0x7f34c3fd9390>, <__main__.Case object at 0x7f34c3fd9510>, <__main__.Case object at 0x7f34c3fd9630>, <__main__.Case object at 0x7f34c3fd9750>, <__main__.Case object at 0x7f34c3fd9810>, <__main__.Case object at 0x7f34c3fd9990>, <__main__.Case object at 0x7f34c3fd9ab0>, <__main__.Case object at 0x7f34c3fd9bd0>, <__main__.Case object at 0x7f34c3fd9c90>, <__main__.Case object at 0x7f34c3fd9e10>, <__main__.Case object at 0x7f34c3fd9f30>, <__main__.Case object at 0x7f34c3fda050>, <__main__.Case object at 0x7f34c3fda110>, <__main__.Case object at 0x7f34c3fda290>, <__main__.Case object at 0x7f34c3fda3b0>, <__main__.Case object at 0x7f34c3fda4d0>, <__main__.Case object at 0x7f34c3fda590>, <__main__.Case object at 0x7f34c3fda710>, <__main__.Case object at 0x7f34c3fda830>, <__main__.Case object at 0x7f34c3fda950>, <__main__.Case object at 0x7f34c3fdaa10>, <__main__.Case object at 0x7f34c3fdab90>, <__main__.Case object at 0x7f34c3fdacb0>, <__main__.Case object at 0x7f34c3fdadd0>, <__main__.Case object at 0x7f34c3fdae90>, <__main__.Case object at 0x7f34c3fdb010>, <__main__.Case object at 0x7f34c3fdb130>, <__main__.Case object at 0x7f34c3fdb250>, <__main__.Case object at 0x7f34c3fdb310>, <__main__.Case object at 0x7f34c3fdb490>, <__main__.Case object at 0x7f34c3fdb550>, <__main__.Case object at 0x7f34c3fdb670>, <__main__.Case object at 0x7f34c3fdb730>, <__main__.Case object at 0x7f34c3fdb8b0>, <__main__.Case object at 0x7f34c3fdb9d0>, <__main__.Case object at 0x7f34c3fdbaf0>, <__main__.Case object at 0x7f34c3fdbcd0>, <__main__.Case object at 0x7f34c3fdbd30>, <__main__.Case object at 0x7f34c3fdbe50>, <__main__.Case object at 0x7f34c3fdbf70>, <__main__.Case object at 0x7f34c3fe4070>, <__main__.Case object at 0x7f34c3fe41f0>, <__main__.Case object at 0x7f34c3fe4310>, <__main__.Case object at 0x7f34c3fe4430>, <__main__.Case object at 0x7f34c3fe44f0>, <__main__.Case object at 0x7f34c3fe4670>, <__main__.Case object at 0x7f34c3fe4790>, <__main__.Case object at 0x7f34c3fe48b0>, <__main__.Case object at 0x7f34c3fe4970>, <__main__.Case object at 0x7f34c3fe4af0>, <__main__.Case object at 0x7f34c3fe4c10>, <__main__.Case object at 0x7f34c3fe4d30>, <__main__.Case object at 0x7f34c3fe4df0>, <__main__.Case object at 0x7f34c3fe4f70>, <__main__.Case object at 0x7f34c3fe5090>, <__main__.Case object at 0x7f34c3fe51b0>, <__main__.Case object at 0x7f34c3fe5270>, <__main__.Case object at 0x7f34c3fe53f0>, <__main__.Case object at 0x7f34c3fe5510>, <__main__.Case object at 0x7f34c3fe5630>, <__main__.Case object at 0x7f34c3fe5810>, <__main__.Case object at 0x7f34c3fe52d0>, <__main__.Case object at 0x7f34c3fe5930>, <__main__.Case object at 0x7f34c3fe59f0>, <__main__.Case object at 0x7f34c3fe5bd0>, <__main__.Case object at 0x7f34c3fe5c30>, <__main__.Case object at 0x7f34c3fe5d50>, <__main__.Case object at 0x7f34c3fe5e70>, <__main__.Case object at 0x7f34c3fe6050>, <__main__.Case object at 0x7f34c3fe60b0>, <__main__.Case object at 0x7f34c3fe61d0>, <__main__.Case object at 0x7f34c3fe62f0>, <__main__.Case object at 0x7f34c3fe63b0>, <__main__.Case object at 0x7f34c3fe6530>, <__main__.Case object at 0x7f34c3fe6650>, <__main__.Case object at 0x7f34c3fe6770>, <__main__.Case object at 0x7f34c3fe6830>, <__main__.Case object at 0x7f34c3fe4e80>, <__main__.Case object at 0x7f34c3fe6a70>, <__main__.Case object at 0x7f34c3fe6b90>, <__main__.Case object at 0x7f34c3fe6d70>, <__main__.Case object at 0x7f34c3fe6dd0>, <__main__.Case object at 0x7f34c3fe6ef0>, <__main__.Case object at 0x7f34c3fe7010>, <__main__.Case object at 0x7f34c3fe70d0>, <__main__.Case object at 0x7f34c3fe7250>, <__main__.Case object at 0x7f34c3fe7370>, <__main__.Case object at 0x7f34c3fe7490>, <__main__.Case object at 0x7f34c3fe7550>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3f9f6a0>, <__main__.Case object at 0x7f34c3f88970>, <__main__.Case object at 0x7f34c3fb3d30>, <__main__.Case object at 0x7f34c3f90370>, <__main__.Case object at 0x7f34c3f90340>, <__main__.Case object at 0x7f34c3fbd2a0>, <__main__.Case object at 0x7f34c3fbc400>, <__main__.Case object at 0x7f34c3fbcc70>, <__main__.Case object at 0x7f34c3fbc3d0>, <__main__.Case object at 0x7f34c3fbcee0>, <__main__.Case object at 0x7f34c3fbc850>, <__main__.Case object at 0x7f34c3fbf7f0>, <__main__.Case object at 0x7f34c3fbf940>, <__main__.Case object at 0x7f34c3fbc8b0>, <__main__.Case object at 0x7f34c3fbdf00>, <__main__.Case object at 0x7f34c3fb3df0>, <__main__.Case object at 0x7f34c3fbf8b0>, <__main__.Case object at 0x7f34c3fbfbb0>, <__main__.Case object at 0x7f34c3f889a0>, <__main__.Case object at 0x7f34c3fbe500>, <__main__.Case object at 0x7f34c3fbd9f0>, <__main__.Case object at 0x7f34c3fbe830>, <__main__.Case object at 0x7f34c3f93cd0>, <__main__.Case object at 0x7f34c3fbd2d0>, <__main__.Case object at 0x7f34c3fbf160>, <__main__.Case object at 0x7f34c3fbe890>, <__main__.Case object at 0x7f34c3fbfa90>, <__main__.Case object at 0x7f34c3fbecb0>, <__main__.Case object at 0x7f34c3fbc580>, <__main__.Case object at 0x7f34c3fbce50>, <__main__.Case object at 0x7f34c3fbcc40>, <__main__.Case object at 0x7f34c3fbd3c0>, <__main__.Case object at 0x7f34c3fbd390>, <__main__.Case object at 0x7f34c3fbdc90>, <__main__.Case object at 0x7f34c3fbdea0>, <__main__.Case object at 0x7f34c3fbe290>, <__main__.Case object at 0x7f34c3fbdb10>, <__main__.Case object at 0x7f34c3fbeb00>, <__main__.Case object at 0x7f34c3fbf340>, <__main__.Case object at 0x7f34c3fbec50>, <__main__.Case object at 0x7f34c3fbfb50>, <__main__.Case object at 0x7f34c3fbfe80>, <__main__.Case object at 0x7f34c3fbed10>, <__main__.Case object at 0x7f34c3fbe6e0>, <__main__.Case object at 0x7f34c3fc94e0>, <__main__.Case object at 0x7f34c3fbce20>, <__main__.Case object at 0x7f34c3fc8d00>, <__main__.Case object at 0x7f34c3fcaec0>, <__main__.Case object at 0x7f34c3fc8070>, <__main__.Case object at 0x7f34c3fc9060>, <__main__.Case object at 0x7f34c3fc8b20>, <__main__.Case object at 0x7f34c3fbe350>, <__main__.Case object at 0x7f34c3fc8100>, <__main__.Case object at 0x7f34c3fc9480>, <__main__.Case object at 0x7f34c3fc97e0>, <__main__.Case object at 0x7f34c3fbf430>, <__main__.Case object at 0x7f34c3fc9000>, <__main__.Case object at 0x7f34c3fc9f30>, <__main__.Case object at 0x7f34c3fca200>, <__main__.Case object at 0x7f34c3fca560>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c3fc8e80>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fc8280>, <__main__.Case object at 0x7f34c3fc8a00>, <__main__.Case object at 0x7f34c3fc9db0>, <__main__.Case object at 0x7f34c3fc9210>, <__main__.Case object at 0x7f34c3fc95a0>, <__main__.Case object at 0x7f34c3fc9900>, <__main__.Case object at 0x7f34c3fc85b0>, <__main__.Case object at 0x7f34c3fca080>, <__main__.Case object at 0x7f34c3fca3e0>, <__main__.Case object at 0x7f34c3fcab00>, <__main__.Case object at 0x7f34c3fc90f0>, <__main__.Case object at 0x7f34c3fcbc10>, <__main__.Case object at 0x7f34c3fcbac0>, <__main__.Case object at 0x7f34c3fcb940>, <__main__.Case object at 0x7f34c3fc9b40>, <__main__.Case object at 0x7f34c3fcb730>, <__main__.Case object at 0x7f34c3fcb610>, <__main__.Case object at 0x7f34c3fcb490>, <__main__.Case object at 0x7f34c3fcb280>, <__main__.Case object at 0x7f34c3fcbe50>, <__main__.Case object at 0x7f34c3fc9810>, <__main__.Case object at 0x7f34c3fc8b80>, <__main__.Case object at 0x7f34c3fd8190>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3fd8070>, <__main__.Case object at 0x7f34c3fd84f0>, <__main__.Case object at 0x7f34c3fd8610>, <__main__.Case object at 0x7f34c3fcac80>, <__main__.Case object at 0x7f34c3fd8310>, <__main__.Case object at 0x7f34c3fd8970>, <__main__.Case object at 0x7f34c3fbe530>, <__main__.Case object at 0x7f34c3fd8d30>, <__main__.Case object at 0x7f34c3fd8e50>, <__main__.Case object at 0x7f34c3fcb460>, <__main__.Case object at 0x7f34c3fd8b50>, <__main__.Case object at 0x7f34c3fd8c10>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fd8fd0>, <__main__.Case object at 0x7f34c3fd95d0>, <__main__.Case object at 0x7f34c3fd96f0>, <__main__.Case object at 0x7f34c3fd9870>, <__main__.Case object at 0x7f34c3fd93f0>, <__main__.Case object at 0x7f34c3fd9a50>, <__main__.Case object at 0x7f34c3fd9b70>, <__main__.Case object at 0x7f34c3fd9cf0>, <__main__.Case object at 0x7f34c3fd8340>, <__main__.Case object at 0x7f34c3fd9ed0>, <__main__.Case object at 0x7f34c3fd9ff0>, <__main__.Case object at 0x7f34c3fda170>, <__main__.Case object at 0x7f34c3fd87c0>, <__main__.Case object at 0x7f34c3fda350>, <__main__.Case object at 0x7f34c3fda470>, <__main__.Case object at 0x7f34c3fda5f0>, <__main__.Case object at 0x7f34c3fd8bb0>, <__main__.Case object at 0x7f34c3fda7d0>, <__main__.Case object at 0x7f34c3fda8f0>, <__main__.Case object at 0x7f34c3fdaa70>, <__main__.Case object at 0x7f34c3fd9000>, <__main__.Case object at 0x7f34c3fdac50>, <__main__.Case object at 0x7f34c3fdad70>, <__main__.Case object at 0x7f34c3fdaef0>, <__main__.Case object at 0x7f34c3fd9420>, <__main__.Case object at 0x7f34c3fdb0d0>, <__main__.Case object at 0x7f34c3fdb1f0>, <__main__.Case object at 0x7f34c3fdb370>, <__main__.Case object at 0x7f34c3fd98a0>, <__main__.Case object at 0x7f34c3fd9270>, <__main__.Case object at 0x7f34c3fdb790>, <__main__.Case object at 0x7f34c3fd9d20>, <__main__.Case object at 0x7f34c3fdb970>, <__main__.Case object at 0x7f34c3fdba90>, <__main__.Case object at 0x7f34c3fdbc10>, <__main__.Case object at 0x7f34c3fda1a0>, <__main__.Case object at 0x7f34c3fdbdf0>, <__main__.Case object at 0x7f34c3fdbf10>, <__main__.Case object at 0x7f34c3fda620>, <__main__.Case object at 0x7f34c3fcb8e0>, <__main__.Case object at 0x7f34c3fe42b0>, <__main__.Case object at 0x7f34c3fe43d0>, <__main__.Case object at 0x7f34c3fdaf20>, <__main__.Case object at 0x7f34c3fe4190>, <__main__.Case object at 0x7f34c3fe4730>, <__main__.Case object at 0x7f34c3fe4850>, <__main__.Case object at 0x7f34c3fdb3a0>, <__main__.Case object at 0x7f34c3fe4550>, <__main__.Case object at 0x7f34c3fe4bb0>, <__main__.Case object at 0x7f34c3fe4cd0>, <__main__.Case object at 0x7f34c3fdb7c0>, <__main__.Case object at 0x7f34c3fe49d0>, <__main__.Case object at 0x7f34c3fe5030>, <__main__.Case object at 0x7f34c3fe5150>, <__main__.Case object at 0x7f34c3fdbc40>, <__main__.Case object at 0x7f34c3fe4e50>, <__main__.Case object at 0x7f34c3fe54b0>, <__main__.Case object at 0x7f34c3fe55d0>, <__main__.Case object at 0x7f34c3fe5750>, <__main__.Case object at 0x7f34c3fdb610>, <__main__.Case object at 0x7f34c3fe5b10>, <__main__.Case object at 0x7f34c3fe40d0>, <__main__.Case object at 0x7f34c3fe5cf0>, <__main__.Case object at 0x7f34c3fe5e10>, <__main__.Case object at 0x7f34c3fe5f90>, <__main__.Case object at 0x7f34c3fe4580>, <__main__.Case object at 0x7f34c3fe6170>, <__main__.Case object at 0x7f34c3fe6290>, <__main__.Case object at 0x7f34c3fe6410>, <__main__.Case object at 0x7f34c3fe4a00>, <__main__.Case object at 0x7f34c3fe65f0>, <__main__.Case object at 0x7f34c3fe6710>, <__main__.Case object at 0x7f34c3fe6890>, <__main__.Case object at 0x7f34c3fe5ab0>, <__main__.Case object at 0x7f34c3fe6b30>, <__main__.Case object at 0x7f34c3fe6cb0>, <__main__.Case object at 0x7f34c3fe5300>, <__main__.Case object at 0x7f34c3fe6e90>, <__main__.Case object at 0x7f34c3fe6fb0>, <__main__.Case object at 0x7f34c3fe7130>, <__main__.Case object at 0x7f34c3fe5780>, <__main__.Case object at 0x7f34c3fe7310>, <__main__.Case object at 0x7f34c3fe7430>, <__main__.Case object at 0x7f34c3fe75b0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.5, time steps: 65\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.5, time steps: 62\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.5, time steps: 59\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.5, time steps: 53\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 50\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5, time steps: 45\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.5, time steps: 21\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.6, time steps: 15\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 0.7)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 0.7)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 0.7)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.7)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 0.7)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 0.7)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.7)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.7)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.7)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.7)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.7)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.7)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.7)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 65\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 63\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 62\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 59\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5, time steps: 53\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 50\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 45\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.6, time steps: 15\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f9f640>, <__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3fb3e80>, <__main__.Case object at 0x7f34c3fb3ca0>, <__main__.Case object at 0x7f34c3fb2740>, <__main__.Case object at 0x7f34c3fb3f70>, <__main__.Case object at 0x7f34c3fbc1f0>, <__main__.Case object at 0x7f34c3fbf4f0>, <__main__.Case object at 0x7f34c3fbde70>, <__main__.Case object at 0x7f34c3fbd5a0>, <__main__.Case object at 0x7f34c3fbcdc0>, <__main__.Case object at 0x7f34c3fbd450>, <__main__.Case object at 0x7f34c3fbd330>, <__main__.Case object at 0x7f34c3fbf490>, <__main__.Case object at 0x7f34c3fbc190>, <__main__.Case object at 0x7f34c3fbde40>, <__main__.Case object at 0x7f34c3fbc2e0>, <__main__.Case object at 0x7f34c3fbf9d0>, <__main__.Case object at 0x7f34c3fbff40>, <__main__.Case object at 0x7f34c3fbfdf0>, <__main__.Case object at 0x7f34c3fbf880>, <__main__.Case object at 0x7f34c3fbd120>, <__main__.Case object at 0x7f34c3fbca00>, <__main__.Case object at 0x7f34c3fbfa30>, <__main__.Case object at 0x7f34c3fbea10>, <__main__.Case object at 0x7f34c3fbeb90>, <__main__.Case object at 0x7f34c3fbf0d0>, <__main__.Case object at 0x7f34c3fbf400>, <__main__.Case object at 0x7f34c3fbf730>, <__main__.Case object at 0x7f34c3fbff10>, <__main__.Case object at 0x7f34c3fbc160>, <__main__.Case object at 0x7f34c3fbc430>, <__main__.Case object at 0x7f34c3fbca30>, <__main__.Case object at 0x7f34c3fbcfd0>, <__main__.Case object at 0x7f34c3fbcfa0>, <__main__.Case object at 0x7f34c3fbd570>, <__main__.Case object at 0x7f34c3fbd840>, <__main__.Case object at 0x7f34c3fbd870>, <__main__.Case object at 0x7f34c3fbdba0>, <__main__.Case object at 0x7f34c3fbdde0>, <__main__.Case object at 0x7f34c3fbe0e0>, <__main__.Case object at 0x7f34c3fbe1a0>, <__main__.Case object at 0x7f34c3fbe620>, <__main__.Case object at 0x7f34c3fbc310>, <__main__.Case object at 0x7f34c3fbe980>, <__main__.Case object at 0x7f34c3fbf670>, <__main__.Case object at 0x7f34c3fbfa00>, <__main__.Case object at 0x7f34c3fbfd60>, <__main__.Case object at 0x7f34c3fca9e0>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fc8f10>, <__main__.Case object at 0x7f34c3fc9360>, <__main__.Case object at 0x7f34c3fc9b70>, <__main__.Case object at 0x7f34c3fca4a0>, <__main__.Case object at 0x7f34c3fcada0>, <__main__.Case object at 0x7f34c3fbf100>, <__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c3fc8460>, <__main__.Case object at 0x7f34c3fc8970>, <__main__.Case object at 0x7f34c3fc8cd0>, <__main__.Case object at 0x7f34c3fc8a90>, <__main__.Case object at 0x7f34c3fc8700>, <__main__.Case object at 0x7f34c3fc93f0>, <__main__.Case object at 0x7f34c3fc9300>, <__main__.Case object at 0x7f34c3fc8940>, <__main__.Case object at 0x7f34c3fc9660>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fca110>, <__main__.Case object at 0x7f34c3fca3b0>, <__main__.Case object at 0x7f34c3fca710>, <__main__.Case object at 0x7f34c3fca950>, <__main__.Case object at 0x7f34c3fcaad0>, <__main__.Case object at 0x7f34c3fcad70>, <__main__.Case object at 0x7f34c3fcb130>, <__main__.Case object at 0x7f34c3fc8160>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fc87c0>, <__main__.Case object at 0x7f34c3fc9e40>, <__main__.Case object at 0x7f34c3fc8fa0>, <__main__.Case object at 0x7f34c3fc9420>, <__main__.Case object at 0x7f34c3fc9780>, <__main__.Case object at 0x7f34c3fc9ab0>, <__main__.Case object at 0x7f34c3fc9fc0>, <__main__.Case object at 0x7f34c3fca230>, <__main__.Case object at 0x7f34c3fca620>, <__main__.Case object at 0x7f34c3fcad10>, <__main__.Case object at 0x7f34c3fc8c70>, <__main__.Case object at 0x7f34c3fcbb20>, <__main__.Case object at 0x7f34c3fcba00>, <__main__.Case object at 0x7f34c3fcb880>, <__main__.Case object at 0x7f34c3fcb790>, <__main__.Case object at 0x7f34c3fcb640>, <__main__.Case object at 0x7f34c3fcb550>, <__main__.Case object at 0x7f34c3fcbc40>, <__main__.Case object at 0x7f34c3fcb370>, <__main__.Case object at 0x7f34c3fc9e70>, <__main__.Case object at 0x7f34c3fcbd90>, <__main__.Case object at 0x7f34c3fcbee0>, <__main__.Case object at 0x7f34c3fcbfd0>, <__main__.Case object at 0x7f34c3fd8130>, <__main__.Case object at 0x7f34c3fd8250>, <__main__.Case object at 0x7f34c3fd83d0>, <__main__.Case object at 0x7f34c3fcb4c0>, <__main__.Case object at 0x7f34c3fd85b0>, <__main__.Case object at 0x7f34c3fd86d0>, <__main__.Case object at 0x7f34c3fd8820>, <__main__.Case object at 0x7f34c3fd8910>, <__main__.Case object at 0x7f34c3fd8a30>, <__main__.Case object at 0x7f34c3fd8490>, <__main__.Case object at 0x7f34c3fd8790>, <__main__.Case object at 0x7f34c3fd8cd0>, <__main__.Case object at 0x7f34c3fd8df0>, <__main__.Case object at 0x7f34c3fd8f10>, <__main__.Case object at 0x7f34c3fd9090>, <__main__.Case object at 0x7f34c3fd9150>, <__main__.Case object at 0x7f34c3fd9210>, <__main__.Case object at 0x7f34c3fd9330>, <__main__.Case object at 0x7f34c3fd8af0>, <__main__.Case object at 0x7f34c3fd9570>, <__main__.Case object at 0x7f34c3fd9690>, <__main__.Case object at 0x7f34c3fd97b0>, <__main__.Case object at 0x7f34c3fd9900>, <__main__.Case object at 0x7f34c3fd99f0>, <__main__.Case object at 0x7f34c3fd9b10>, <__main__.Case object at 0x7f34c3fd9c30>, <__main__.Case object at 0x7f34c3fd9d80>, <__main__.Case object at 0x7f34c3fd9e70>, <__main__.Case object at 0x7f34c3fd9f90>, <__main__.Case object at 0x7f34c3fbe860>, <__main__.Case object at 0x7f34c3fda200>, <__main__.Case object at 0x7f34c3fda2f0>, <__main__.Case object at 0x7f34c3fda410>, <__main__.Case object at 0x7f34c3fda530>, <__main__.Case object at 0x7f34c3fbd660>, <__main__.Case object at 0x7f34c3fda680>, <__main__.Case object at 0x7f34c3fda890>, <__main__.Case object at 0x7f34c3fda9b0>, <__main__.Case object at 0x7f34c3fdab00>, <__main__.Case object at 0x7f34c3fda0b0>, <__main__.Case object at 0x7f34c3fdad10>, <__main__.Case object at 0x7f34c3fdae30>, <__main__.Case object at 0x7f34c3fd9480>, <__main__.Case object at 0x7f34c3fdb070>, <__main__.Case object at 0x7f34c3fbdb40>, <__main__.Case object at 0x7f34c3fdb2b0>, <__main__.Case object at 0x7f34c3fdb400>, <__main__.Case object at 0x7f34c3fdb190>, <__main__.Case object at 0x7f34c3fdb5b0>, <__main__.Case object at 0x7f34c3fdb6d0>, <__main__.Case object at 0x7f34c3fdb820>, <__main__.Case object at 0x7f34c3fdb910>, <__main__.Case object at 0x7f34c3fdba30>, <__main__.Case object at 0x7f34c3fdbb50>, <__main__.Case object at 0x7f34c3fdbbb0>, <__main__.Case object at 0x7f34c3fdaf80>, <__main__.Case object at 0x7f34c3fdbeb0>, <__main__.Case object at 0x7f34c3fdbfd0>, <__main__.Case object at 0x7f34c3fe4130>, <__main__.Case object at 0x7f34c3fdabf0>, <__main__.Case object at 0x7f34c3fe4370>, <__main__.Case object at 0x7f34c3fe4490>, <__main__.Case object at 0x7f34c3fe4610>, <__main__.Case object at 0x7f34c3fe46d0>, <__main__.Case object at 0x7f34c3fe47f0>, <__main__.Case object at 0x7f34c3fe4910>, <__main__.Case object at 0x7f34c3fe4a90>, <__main__.Case object at 0x7f34c3fe4b50>, <__main__.Case object at 0x7f34c3fe4c70>, <__main__.Case object at 0x7f34c3fe4250>, <__main__.Case object at 0x7f34c3fe4ee0>, <__main__.Case object at 0x7f34c3fdbd90>, <__main__.Case object at 0x7f34c3fe50f0>, <__main__.Case object at 0x7f34c3fe5210>, <__main__.Case object at 0x7f34c3fe5330>, <__main__.Case object at 0x7f34c3fe5450>, <__main__.Case object at 0x7f34c3fe5570>, <__main__.Case object at 0x7f34c3fe5690>, <__main__.Case object at 0x7f34c3fe56f0>, <__main__.Case object at 0x7f34c3fe5870>, <__main__.Case object at 0x7f34c3fe4fd0>, <__main__.Case object at 0x7f34c3fe5a50>, <__main__.Case object at 0x7f34c3fe58d0>, <__main__.Case object at 0x7f34c3fe5c90>, <__main__.Case object at 0x7f34c3fe4d90>, <__main__.Case object at 0x7f34c3fe5ed0>, <__main__.Case object at 0x7f34c3fe5f30>, <__main__.Case object at 0x7f34c3fe6110>, <__main__.Case object at 0x7f34c3fe6230>, <__main__.Case object at 0x7f34c3fe6350>, <__main__.Case object at 0x7f34c3fe64a0>, <__main__.Case object at 0x7f34c3fe6590>, <__main__.Case object at 0x7f34c3fe66b0>, <__main__.Case object at 0x7f34c3fda770>, <__main__.Case object at 0x7f34c3fe6920>, <__main__.Case object at 0x7f34c3fe69b0>, <__main__.Case object at 0x7f34c3fe6ad0>, <__main__.Case object at 0x7f34c3faafe0>, <__main__.Case object at 0x7f34c3fdb4f0>, <__main__.Case object at 0x7f34c3fe6e30>, <__main__.Case object at 0x7f34c3fe6f50>, <__main__.Case object at 0x7f34c3fe7070>, <__main__.Case object at 0x7f34c3fe71c0>, <__main__.Case object at 0x7f34c3fe72b0>, <__main__.Case object at 0x7f34c3fe6c50>, <__main__.Case object at 0x7f34c3fcb1c0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.6, time steps: 20\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 4, tv: 0.4, time steps: 7\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.6, time steps: 20\n",
      "Episode: 4, Total Steps: 205, Total Rewards: [-304, 88], Status Episode: False\n",
      "------------------------------------------End of episode 4 loop--------------------\n",
      "----- starting point of Episode 5 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 0.7999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 6) with action 1 to next state (4, 5): pull reward: 0\n",
      "----- starting point of Episode 5 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 5) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 5 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 4 to next state (4, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 4 to next state (5, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 0) with action 2 to next state (5, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 1) with action 1 to next state (5, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 0) with action 4 to next state (6, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 5 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 5) with action 3 to next state (5, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 5) with action 0 to next state (5, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 5 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 5) with action 4 to next state (6, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 5 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f889a0>, <__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3f903a0>, <__main__.Case object at 0x7f34c3fb3f10>, <__main__.Case object at 0x7f34c3fb3b80>, <__main__.Case object at 0x7f34c3fc91e0>, <__main__.Case object at 0x7f34c3fcaec0>, <__main__.Case object at 0x7f34c3fc8730>, <__main__.Case object at 0x7f34c3fc9480>, <__main__.Case object at 0x7f34c3fca200>, <__main__.Case object at 0x7f34c3fca290>, <__main__.Case object at 0x7f34c3fc8190>, <__main__.Case object at 0x7f34c3fc9390>, <__main__.Case object at 0x7f34c3fc85b0>, <__main__.Case object at 0x7f34c3fcac50>, <__main__.Case object at 0x7f34c3fcb9a0>, <__main__.Case object at 0x7f34c3fcb730>, <__main__.Case object at 0x7f34c3fcbeb0>, <__main__.Case object at 0x7f34c3fcb8e0>, <__main__.Case object at 0x7f34c3fc9c30>, <__main__.Case object at 0x7f34c3fca530>, <__main__.Case object at 0x7f34c3fc8df0>, <__main__.Case object at 0x7f34c3fc9720>, <__main__.Case object at 0x7f34c3fc8af0>, <__main__.Case object at 0x7f34c3fca470>, <__main__.Case object at 0x7f34c3fcae90>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fc9090>, <__main__.Case object at 0x7f34c3fca140>, <__main__.Case object at 0x7f34c3fc8e20>, <__main__.Case object at 0x7f34c3fcba30>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fcbd30>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fc8e50>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f72170>, <__main__.Case object at 0x7f34c3f89b40>, <__main__.Case object at 0x7f34c3f9f640>, <__main__.Case object at 0x7f34c3f93ac0>, <__main__.Case object at 0x7f34c3fb2980>, <__main__.Case object at 0x7f34c3fb3e80>, <__main__.Case object at 0x7f34c3fc8d00>, <__main__.Case object at 0x7f34c3fc88b0>, <__main__.Case object at 0x7f34c3fc98d0>, <__main__.Case object at 0x7f34c3fc9f30>, <__main__.Case object at 0x7f34c3fc91b0>, <__main__.Case object at 0x7f34c3fc99c0>, <__main__.Case object at 0x7f34c3fca650>, <__main__.Case object at 0x7f34c3fcbaf0>, <__main__.Case object at 0x7f34c3fcb580>, <__main__.Case object at 0x7f34c3fcbd60>, <__main__.Case object at 0x7f34c3fcb460>, <__main__.Case object at 0x7f34c3fc9450>, <__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3fc89d0>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fc9600>, <__main__.Case object at 0x7f34c3fb3ca0>, <__main__.Case object at 0x7f34c3fc83d0>, <__main__.Case object at 0x7f34c3fc9900>, <__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fcbb50>, <__main__.Case object at 0x7f34c3fcb7f0>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fc98a0>, <__main__.Case object at 0x7f34c3fcafb0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 65\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 63\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.6, time steps: 62\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.6, time steps: 59\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.4, time steps: 53\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.4, time steps: 50\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.4, time steps: 45\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.4, time steps: 21\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.7, time steps: 15\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) is empty. Temporary case base stored to the case base: ((5, 5), 4, 0.5)\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 3), 2, 0.5, 35)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 2), 2, 0.5, 35)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 1), 2, 0.5, 35)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 0), 2, 0.5, 35)\n",
      "Episode succeeded, case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 0.5)\n",
      "Episode succeeded, case (5, 1) is empty. Temporary case base stored to the case base: ((5, 1), 1, 0.5)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.5)\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.5)\n",
      "Episode succeeded, case (2, 1) is empty. Temporary case base stored to the case base: ((2, 1), 1, 0.5)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.7999999999999999)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 35\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 35\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 35\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 35\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (5, 1), solution: 1, tv: 0.5, time steps: 20\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (2, 1), solution: 1, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5, time steps: 6\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f9f6d0>, <__main__.Case object at 0x7f34c3f88a30>, <__main__.Case object at 0x7f34c3f90340>, <__main__.Case object at 0x7f34c3fb3d30>, <__main__.Case object at 0x7f34c3fb3ee0>, <__main__.Case object at 0x7f34c3fb3f70>, <__main__.Case object at 0x7f34c3fc9060>, <__main__.Case object at 0x7f34c3fc8070>, <__main__.Case object at 0x7f34c3fc8100>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c3fca560>, <__main__.Case object at 0x7f34c3fcb070>, <__main__.Case object at 0x7f34c3fc8430>, <__main__.Case object at 0x7f34c3fca3e0>, <__main__.Case object at 0x7f34c3fc8f40>, <__main__.Case object at 0x7f34c3fcb7c0>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3fc9120>, <__main__.Case object at 0x7f34c3fcaef0>, <__main__.Case object at 0x7f34c3fc8fd0>, <__main__.Case object at 0x7f34c3fc94b0>, <__main__.Case object at 0x7f34c3fc8310>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fcabc0>, <__main__.Case object at 0x7f34c3fc80a0>, <__main__.Case object at 0x7f34c3fc86a0>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fc9870>, <__main__.Case object at 0x7f34c3fcb910>, <__main__.Case object at 0x7f34c3fcb5b0>, <__main__.Case object at 0x7f34c3fcb3d0>, <__main__.Case object at 0x7f34c3fcbf70>, <__main__.Case object at 0x7f34c3fc9150>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.7, time steps: 20\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.7, time steps: 20\n",
      "Episode: 5, Total Steps: 35, Total Rewards: [66, 88], Status Episode: True\n",
      "------------------------------------------End of episode 5 loop--------------------\n",
      "----- starting point of Episode 6 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.8999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.5, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.8999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.5, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.8999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.5, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.8999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.5, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.8999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.5, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 0.8999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.5, 21)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 0.8999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 0.5, 35)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 0.8999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 0.5, 35)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 0.8999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 2) with action 0 to next state (6, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 0.8999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.5, 35)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 0.8999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 35)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 35)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 35)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 6) with action 4 to next state (7, 6): pull reward: 0\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 35)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (7, 6) with action 4 to next state (8, 6): pull reward: 0\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 35)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (8, 6) with action 3 to next state (7, 6): pull reward: 0\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 35)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (7, 6) with action 1 to next state (7, 5): pull reward: 0\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 35)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 6 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (7, 5) with action 0 to next state (7, 5): pull reward: 0\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 35)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (7, 5) with action 2 to next state (7, 6): pull reward: 0\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 35)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (7, 6) with action 0 to next state (7, 6): pull reward: 0\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 35)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (7, 6) with action 2 to next state (7, 7): pull reward: 0\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 35)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (7, 7) with action 3 to next state (6, 7): pull reward: 0\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 35)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 7) with action 1 to next state (6, 6): pull reward: 0\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 35)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 35)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 35)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 6 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 35)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 6 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 35)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f9f820>, <__main__.Case object at 0x7f34c3f93ac0>, <__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3fb3ca0>, <__main__.Case object at 0x7f34c3fb2740>, <__main__.Case object at 0x7f34c3fc9150>, <__main__.Case object at 0x7f34c3fc9240>, <__main__.Case object at 0x7f34c3fc9ff0>, <__main__.Case object at 0x7f34c3fcb400>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fc9e10>, <__main__.Case object at 0x7f34c3fca140>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fca560>, <__main__.Case object at 0x7f34c3fca3e0>, <__main__.Case object at 0x7f34c3fc9120>, <__main__.Case object at 0x7f34c3fc94b0>, <__main__.Case object at 0x7f34c3fc86a0>, <__main__.Case object at 0x7f34c3fcb910>, <__main__.Case object at 0x7f34c3fc83a0>, <__main__.Case object at 0x7f34c3fcaad0>, <__main__.Case object at 0x7f34c3fca110>, <__main__.Case object at 0x7f34c3fc9a50>, <__main__.Case object at 0x7f34c3fc8a90>, <__main__.Case object at 0x7f34c3fc8790>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f9f6a0>, <__main__.Case object at 0x7f34c3f88a30>, <__main__.Case object at 0x7f34c3f916c0>, <__main__.Case object at 0x7f34c3f9f640>, <__main__.Case object at 0x7f34c3fb3ee0>, <__main__.Case object at 0x7f34c3fb3bb0>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fc9810>, <__main__.Case object at 0x7f34c3fc89d0>, <__main__.Case object at 0x7f34c3fcb190>, <__main__.Case object at 0x7f34c3fcb7f0>, <__main__.Case object at 0x7f34c3fcb1c0>, <__main__.Case object at 0x7f34c3fc97b0>, <__main__.Case object at 0x7f34c3fc9bd0>, <__main__.Case object at 0x7f34c3fc8910>, <__main__.Case object at 0x7f34c3fc8f10>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c3fca770>, <__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3fc80a0>, <__main__.Case object at 0x7f34c3fc9870>, <__main__.Case object at 0x7f34c3fcb490>, <__main__.Case object at 0x7f34c3fb3f70>, <__main__.Case object at 0x7f34c3fca3b0>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3fc84f0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 35\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 35\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.6, time steps: 35\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.6, time steps: 35\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.4, time steps: 29\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 1, tv: 0.4, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (2, 1), solution: 1, tv: 0.4, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.6, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.6, time steps: 6\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 7) is empty. Temporary case base stored to the case base: ((6, 7), 1, 0.5)\n",
      "Episode succeeded, case (7, 7) is empty. Temporary case base stored to the case base: ((7, 7), 3, 0.5)\n",
      "Episode succeeded, case (7, 6) is empty. Temporary case base stored to the case base: ((7, 6), 2, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 2, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 6) is empty. Temporary case base stored to the case base: ((8, 6), 3, 0.5)\n",
      "Episode succeeded, case (7, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 3), 2, 0.5, 29)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 2), 2, 0.5, 29)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 1), 2, 0.5, 29)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 0), 2, 0.5, 29)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.8999999999999999)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6, time steps: 6\n",
      "cases content after RETAIN, problem: (6, 7), solution: 1, tv: 0.5, time steps: 24\n",
      "cases content after RETAIN, problem: (7, 7), solution: 3, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (7, 6), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 2, tv: 0.5, time steps: 20\n",
      "cases content after RETAIN, problem: (8, 6), solution: 3, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f89b40>, <__main__.Case object at 0x7f34c3f90340>, <__main__.Case object at 0x7f34c3fb3d90>, <__main__.Case object at 0x7f34c3fb3b80>, <__main__.Case object at 0x7f34c3fb3f10>, <__main__.Case object at 0x7f34c3fc94e0>, <__main__.Case object at 0x7f34c3fca0e0>, <__main__.Case object at 0x7f34c3fcb940>, <__main__.Case object at 0x7f34c3fc8670>, <__main__.Case object at 0x7f34c3fcaa10>, <__main__.Case object at 0x7f34c3fcbb50>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fc92a0>, <__main__.Case object at 0x7f34c3fcace0>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fcb2b0>, <__main__.Case object at 0x7f34c3fc8100>, <__main__.Case object at 0x7f34c3fc8430>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fcaef0>, <__main__.Case object at 0x7f34c3fca7d0>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fcbf70>, <__main__.Case object at 0x7f34c3fc80d0>, <__main__.Case object at 0x7f34c3fca710>, <__main__.Case object at 0x7f34c3fc9c90>, <__main__.Case object at 0x7f34c3fc9570>, <__main__.Case object at 0x7f34c3fc8bb0>, <__main__.Case object at 0x7f34c3fca950>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3f903a0>, <__main__.Case object at 0x7f34c3f70e50>, <__main__.Case object at 0x7f34c3fcafb0>, <__main__.Case object at 0x7f34c3fc9000>, <__main__.Case object at 0x7f34c3fc90f0>, <__main__.Case object at 0x7f34c3fc9450>, <__main__.Case object at 0x7f34c3fc9600>, <__main__.Case object at 0x7f34c3fcbf10>, <__main__.Case object at 0x7f34c3fc8190>, <__main__.Case object at 0x7f34c3fc9210>, <__main__.Case object at 0x7f34c3fcacb0>, <__main__.Case object at 0x7f34c3fcb520>, <__main__.Case object at 0x7f34c3fc9690>, <__main__.Case object at 0x7f34c3fcbd30>, <__main__.Case object at 0x7f34c3fcb7c0>, <__main__.Case object at 0x7f34c3fcb6d0>, <__main__.Case object at 0x7f34c3fcb070>, <__main__.Case object at 0x7f34c3fcadd0>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fcb130>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fc9f90>, <__main__.Case object at 0x7f34c3fc9840>, <__main__.Case object at 0x7f34c3fc8cd0>, <__main__.Case object at 0x7f34c3fcb040>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.7999999999999999, time steps: 20\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.5)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.5)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.5)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.5)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.5)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.7999999999999999, time steps: 20\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5, time steps: 6\n",
      "Episode: 6, Total Steps: 29, Total Rewards: [72, 88], Status Episode: True\n",
      "------------------------------------------End of episode 6 loop--------------------\n",
      "----- starting point of Episode 7 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.6, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.6, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.6, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.6, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.6, 21)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 0.5, 29)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 0.5, 29)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.5, 29)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 29)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 0.9999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 4) with action 4 to next state (7, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 0.9999999999999999, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (7, 4) with action 2 to next state (7, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 0.9999999999999999, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 5), 2, 0.5, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 5), 2, 0.5, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 5), 2, 0.5, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 5), 2, 0.5, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 5), 2, 0.5, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7999999999999999, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 6) with action 4 to next state (6, 6): pull reward: 0\n",
      "comm next state for agent 1: ((7, 5), 2, 0.5, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7999999999999999, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 6) with action 1 to next state (6, 5): pull reward: 0\n",
      "comm next state for agent 1: ((7, 5), 2, 0.5, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 5), 2, 0.5, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 5), 2, 0.5, 20)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 7 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 5), 2, 0.5, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 5), 2, 0.5, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 7 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((7, 5), 2, 0.5, 20)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f89b40>, <__main__.Case object at 0x7f34c3fb0340>, <__main__.Case object at 0x7f34c3fb2920>, <__main__.Case object at 0x7f34c3fc95a0>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fcb1c0>, <__main__.Case object at 0x7f34c3fc8f10>, <__main__.Case object at 0x7f34c3fc8c40>, <__main__.Case object at 0x7f34c3fca3b0>, <__main__.Case object at 0x7f34c3fc8190>, <__main__.Case object at 0x7f34c3fca050>, <__main__.Case object at 0x7f34c3fcadd0>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fc9150>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fc93f0>, <__main__.Case object at 0x7f34c3fc99c0>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3f9f820>, <__main__.Case object at 0x7f34c3fcab00>, <__main__.Case object at 0x7f34c3fcba90>, <__main__.Case object at 0x7f34c3fc9300>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3f9f640>, <__main__.Case object at 0x7f34c3fb3f10>, <__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c3fcb7f0>, <__main__.Case object at 0x7f34c3fc8910>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3fcbf10>, <__main__.Case object at 0x7f34c3fcb520>, <__main__.Case object at 0x7f34c3fcb070>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fc8cd0>, <__main__.Case object at 0x7f34c3fcb400>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fca3e0>, <__main__.Case object at 0x7f34c3fca110>, <__main__.Case object at 0x7f34c3fc88b0>, <__main__.Case object at 0x7f34c3fc9900>, <__main__.Case object at 0x7f34c3fc8dc0>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fca890>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 29\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 29\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.6, time steps: 29\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.6, time steps: 29\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.7, time steps: 21\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.7, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.7, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.7, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (6, 7), solution: 1, tv: 0.6, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (7, 7), solution: 3, tv: 0.6, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (7, 6), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 2, tv: 0.6, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (8, 6), solution: 3, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.6, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 7) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 7) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) is empty. Temporary case base stored to the case base: ((7, 4), 2, 0.5)\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 3), 2, 0.5, 24)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 2), 2, 0.5, 24)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 1), 2, 0.5, 24)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 0), 2, 0.5, 24)\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.9999999999999999)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 24\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 24\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 24\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 24\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.7, time steps: 21\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7, time steps: 6\n",
      "cases content after RETAIN, problem: (6, 7), solution: 1, tv: 0.6, time steps: 24\n",
      "cases content after RETAIN, problem: (7, 7), solution: 3, tv: 0.6, time steps: 23\n",
      "cases content after RETAIN, problem: (7, 6), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 2, tv: 0.6, time steps: 20\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.6, time steps: 20\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.5, time steps: 11\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f90370>, <__main__.Case object at 0x7f34c3fb3bb0>, <__main__.Case object at 0x7f34c3fb3b80>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fcb190>, <__main__.Case object at 0x7f34c3fc9bd0>, <__main__.Case object at 0x7f34c3fca770>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fcb340>, <__main__.Case object at 0x7f34c3fcacb0>, <__main__.Case object at 0x7f34c3fcb6d0>, <__main__.Case object at 0x7f34c3fc8fd0>, <__main__.Case object at 0x7f34c3fc9690>, <__main__.Case object at 0x7f34c3fc9ff0>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fca140>, <__main__.Case object at 0x7f34c3fcab90>, <__main__.Case object at 0x7f34c3fcb280>, <__main__.Case object at 0x7f34c3fc8f70>, <__main__.Case object at 0x7f34c3fc85b0>, <__main__.Case object at 0x7f34c3fc8100>, <__main__.Case object at 0x7f34c3fc9750>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fc8220>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f93ac0>, <__main__.Case object at 0x7f34c3fb3ee0>, <__main__.Case object at 0x7f34c3fb3d90>, <__main__.Case object at 0x7f34c3fc9b70>, <__main__.Case object at 0x7f34c3fc89d0>, <__main__.Case object at 0x7f34c3fc97b0>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c3fcb760>, <__main__.Case object at 0x7f34c3fc9cc0>, <__main__.Case object at 0x7f34c3fc9210>, <__main__.Case object at 0x7f34c3fc8940>, <__main__.Case object at 0x7f34c3fc9240>, <__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fc9e10>, <__main__.Case object at 0x7f34c3fc8250>, <__main__.Case object at 0x7f34c3fc8310>, <__main__.Case object at 0x7f34c3fcbc70>, <__main__.Case object at 0x7f34c3fc9480>, <__main__.Case object at 0x7f34c3fc8af0>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fc8a30>, <__main__.Case object at 0x7f34c3fc9030>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.8999999999999999, time steps: 20\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.4, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.4, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.4, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.4, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.4, time steps: 6\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 2, 0.5)\n",
      "Integrated case process. comm case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 2, 0.5)\n",
      "Integrated case process. comm case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 2, 0.5)\n",
      "Integrated case process. comm case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 2, 0.5)\n",
      "Integrated case process. comm case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 2, 0.5)\n",
      "Integrated case process. comm case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 2, 0.5)\n",
      "Integrated case process. comm case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 2, 0.5)\n",
      "Integrated case process. comm case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 2, 0.5)\n",
      "Integrated case process. comm case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 2, 0.5)\n",
      "Integrated case process. comm case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 2, 0.5)\n",
      "Integrated case process. comm case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 2, 0.5)\n",
      "Integrated case process. comm case (7, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((7, 5), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.6)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.6)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.6)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.6)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.6)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.8999999999999999, time steps: 20\n",
      "cases content after RETAIN, problem: (7, 5), solution: 2, tv: 0.5, time steps: 20\n",
      "Episode: 7, Total Steps: 24, Total Rewards: [77, 88], Status Episode: True\n",
      "------------------------------------------End of episode 7 loop--------------------\n",
      "----- starting point of Episode 8 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.7, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.7, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.7, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.7, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.7, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.7, 21)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 0) with action 3 to next state (5, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.7, 21)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 5) with action 4 to next state (7, 5): pull reward: 0\n",
      "----- starting point of Episode 8 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 5), 2, 0.5, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 0.5, 24)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 0.5, 24)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 6) with action 1 to next state (7, 5): pull reward: 0\n",
      "----- starting point of Episode 8 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 5), 2, 0.5, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.5, 24)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 24)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 6) with action 3 to next state (6, 6): pull reward: 0\n",
      "----- starting point of Episode 8 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 0.6, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 0.7999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 0.7999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 6), 3, 0.7999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.8999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 6), 3, 0.7999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 8 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.8999999999999999, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 6), 3, 0.7999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3fb3ee0>, <__main__.Case object at 0x7f34c3fb3bb0>, <__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c3fc9870>, <__main__.Case object at 0x7f34c3fca980>, <__main__.Case object at 0x7f34c3fc9c30>, <__main__.Case object at 0x7f34c3fcb940>, <__main__.Case object at 0x7f34c3fc9570>, <__main__.Case object at 0x7f34c3fca470>, <__main__.Case object at 0x7f34c3fc83d0>, <__main__.Case object at 0x7f34c3fcb850>, <__main__.Case object at 0x7f34c3fc9480>, <__main__.Case object at 0x7f34c3fc95a0>, <__main__.Case object at 0x7f34c3fc8c40>, <__main__.Case object at 0x7f34c3fca050>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fc8e50>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3f9f820>, <__main__.Case object at 0x7f34c3f90370>, <__main__.Case object at 0x7f34c3fb2920>, <__main__.Case object at 0x7f34c3fb2740>, <__main__.Case object at 0x7f34c3f9f640>, <__main__.Case object at 0x7f34c3fcbd30>, <__main__.Case object at 0x7f34c3fca1a0>, <__main__.Case object at 0x7f34c3fcbf70>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fcbac0>, <__main__.Case object at 0x7f34c3fc8a30>, <__main__.Case object at 0x7f34c3fc8f10>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fca5f0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.6, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.6, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.7999999999999999, time steps: 21\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.7999999999999999, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.7999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (6, 7), solution: 1, tv: 0.5, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (7, 7), solution: 3, tv: 0.5, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (7, 6), solution: 2, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 2, tv: 0.5, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.5, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.7, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 2, tv: 0.4, time steps: 11\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 3), 2, 0.5, 19)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 2), 2, 0.5, 19)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 1), 2, 0.5, 19)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 0), 2, 0.5, 19)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 0), 4, 0.5, 19)\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 2, 0.5)\n",
      "Integrated case process. comm case (7, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 5), 2, 0.5)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.7999999999999999, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.7999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7999999999999999, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (6, 7), solution: 1, tv: 0.5, time steps: 24\n",
      "cases content after RETAIN, problem: (7, 7), solution: 3, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (7, 6), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (7, 5), solution: 2, tv: 0.5, time steps: 20\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5, time steps: 20\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7, time steps: 15\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f90340>, <__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3fb0340>, <__main__.Case object at 0x7f34c3fb3df0>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fc9390>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fcb610>, <__main__.Case object at 0x7f34c3fc8a90>, <__main__.Case object at 0x7f34c3fcbbe0>, <__main__.Case object at 0x7f34c3fc8850>, <__main__.Case object at 0x7f34c3fcbaf0>, <__main__.Case object at 0x7f34c3fc8310>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fcb1c0>, <__main__.Case object at 0x7f34c3fc8190>, <__main__.Case object at 0x7f34c3fcb2b0>, <__main__.Case object at 0x7f34c3fc99c0>, <__main__.Case object at 0x7f34c3fc80d0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f70e50>, <__main__.Case object at 0x7f34c3f89b40>, <__main__.Case object at 0x7f34c3fb3d90>, <__main__.Case object at 0x7f34c3fcb250>, <__main__.Case object at 0x7f34c3fc9450>, <__main__.Case object at 0x7f34c3fc9f90>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fc98d0>, <__main__.Case object at 0x7f34c3fc9a80>, <__main__.Case object at 0x7f34c3fc8700>, <__main__.Case object at 0x7f34c3fc91e0>, <__main__.Case object at 0x7f34c3fc8af0>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fca3b0>, <__main__.Case object at 0x7f34c3fc9f30>, <__main__.Case object at 0x7f34c3fc93f0>, <__main__.Case object at 0x7f34c3fcaef0>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.9999999999999999, time steps: 20\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 2, tv: 0.6, time steps: 20\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 4), 1, 0.5, 19)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 6) is empty. Temporary case base stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 5), 2, 0.5, 19)\n",
      "Episode succeeded, case (7, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 0.6)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 0.7)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.7)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.7)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.7)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 0.7)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.7)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.7)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (7, 5), solution: 2, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.7, time steps: 21\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.7, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.7, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7, time steps: 6\n",
      "Episode: 8, Total Steps: 19, Total Rewards: [82, 84], Status Episode: True\n",
      "------------------------------------------End of episode 8 loop--------------------\n",
      "----- starting point of Episode 9 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.7999999999999999, 6)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 0) with action 3 to next state (8, 0): pull reward: 0\n",
      "----- starting point of Episode 9 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.7999999999999999, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.7999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.7999999999999999, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.7999999999999999, 18)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 0.9, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 19)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 0.7, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 0.8999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 0.8999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 0.8999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 0.8999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 9 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 0.8999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f9f820>, <__main__.Case object at 0x7f34c3f88970>, <__main__.Case object at 0x7f34c3fb3d30>, <__main__.Case object at 0x7f34c3fcaef0>, <__main__.Case object at 0x7f34c3fcbd30>, <__main__.Case object at 0x7f34c3fc8a30>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fc8700>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fc9870>, <__main__.Case object at 0x7f34c3fc9570>, <__main__.Case object at 0x7f34c3fc9480>, <__main__.Case object at 0x7f34c3fca050>, <__main__.Case object at 0x7f34c3fcb070>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c3fc8280>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3fb3df0>, <__main__.Case object at 0x7f34c3f72170>, <__main__.Case object at 0x7f34c3fcbac0>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fc9a80>, <__main__.Case object at 0x7f34c3fca3b0>, <__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c3fcb940>, <__main__.Case object at 0x7f34c3fcb850>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fca710>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.8999999999999999, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.8999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.8999999999999999, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.8999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (6, 7), solution: 1, tv: 0.4, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (7, 7), solution: 3, tv: 0.4, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (7, 6), solution: 2, tv: 0.4, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 2, tv: 0.4, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.4, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6, time steps: 15\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 3), 2, 0.5, 16)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 2), 2, 0.5, 16)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 1), 2, 0.5, 16)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 0), 2, 0.5, 16)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 0), 4, 0.5, 16)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 0), 4, 0.5, 16)\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 0), 4, 0.5, 16)\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 0.9)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.7999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.8999999999999999, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.8999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6, time steps: 15\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f90370>, <__main__.Case object at 0x7f34c3fb2920>, <__main__.Case object at 0x7f34c3fb0340>, <__main__.Case object at 0x7f34c3fcb040>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fc98d0>, <__main__.Case object at 0x7f34c3fc8af0>, <__main__.Case object at 0x7f34c3fcbf40>, <__main__.Case object at 0x7f34c3fc9c30>, <__main__.Case object at 0x7f34c3fc83d0>, <__main__.Case object at 0x7f34c3fc8c40>, <__main__.Case object at 0x7f34c3fc9900>, <__main__.Case object at 0x7f34c3fc88b0>, <__main__.Case object at 0x7f34c3fc9e10>, <__main__.Case object at 0x7f34c3fcae90>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3fc80d0>, <__main__.Case object at 0x7f34c3fcbf70>, <__main__.Case object at 0x7f34c3fc8f10>, <__main__.Case object at 0x7f34c3fc9f90>, <__main__.Case object at 0x7f34c3fc91e0>, <__main__.Case object at 0x7f34c3fcb9d0>, <__main__.Case object at 0x7f34c3fca980>, <__main__.Case object at 0x7f34c3fca470>, <__main__.Case object at 0x7f34c3fc95a0>, <__main__.Case object at 0x7f34c3fcba90>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fc9210>, <__main__.Case object at 0x7f34c3fcb7c0>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1.0, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 2, tv: 0.4, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 0.4, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6, time steps: 6\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 4), 1, 0.5, 16)\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 0.7)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.7999999999999999)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1.0, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6, time steps: 21\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6, time steps: 6\n",
      "Episode: 9, Total Steps: 16, Total Rewards: [85, 88], Status Episode: True\n",
      "------------------------------------------End of episode 9 loop--------------------\n",
      "----- starting point of Episode 10 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.8999999999999999, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8999999999999999, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.8999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.5, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.5, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.5, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 0.5, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 0.5, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1.0, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.5, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 0.5, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 0.7999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 0.9999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 0.9999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 0.9999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 10 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 0.9999999999999999, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 10 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.5, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 0.9999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f9f820>, <__main__.Case object at 0x7f34c3fb2740>, <__main__.Case object at 0x7f34c3fb3f10>, <__main__.Case object at 0x7f34c3fc99c0>, <__main__.Case object at 0x7f34c3fcab00>, <__main__.Case object at 0x7f34c3fc8e80>, <__main__.Case object at 0x7f34c3fc8190>, <__main__.Case object at 0x7f34c3fc97b0>, <__main__.Case object at 0x7f34c3fca110>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fcb400>, <__main__.Case object at 0x7f34c3fcb760>, <__main__.Case object at 0x7f34c3fcb610>, <__main__.Case object at 0x7f34c3fc86a0>, <__main__.Case object at 0x7f34c3fcb7f0>, <__main__.Case object at 0x7f34c3fc9630>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c3f90370>, <__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3fb3f70>, <__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fc89d0>, <__main__.Case object at 0x7f34c3fc8850>, <__main__.Case object at 0x7f34c3fc9150>, <__main__.Case object at 0x7f34c3fc9240>, <__main__.Case object at 0x7f34c3fc8a90>, <__main__.Case object at 0x7f34c3fcb490>, <__main__.Case object at 0x7f34c3fc8cd0>, <__main__.Case object at 0x7f34c3fcbaf0>, <__main__.Case object at 0x7f34c3fcace0>, <__main__.Case object at 0x7f34c3fc9a50>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.9999999999999999, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.9999999999999999, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5, time steps: 15\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.5)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1.0)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.6, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.6, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.8999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.9999999999999999, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.9999999999999999, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5, time steps: 16\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3fb2980>, <__main__.Case object at 0x7f34c3fc9360>, <__main__.Case object at 0x7f34c3fc9ab0>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fc8940>, <__main__.Case object at 0x7f34c3fc8670>, <__main__.Case object at 0x7f34c3fca1a0>, <__main__.Case object at 0x7f34c3fc9b70>, <__main__.Case object at 0x7f34c3fc93f0>, <__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c3fc9720>, <__main__.Case object at 0x7f34c3fcb730>, <__main__.Case object at 0x7f34c3fca0b0>, <__main__.Case object at 0x7f34c3fcb1c0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3fb3df0>, <__main__.Case object at 0x7f34c3fb3ee0>, <__main__.Case object at 0x7f34c3fca3e0>, <__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c3fc8e50>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3fca7d0>, <__main__.Case object at 0x7f34c3fcba90>, <__main__.Case object at 0x7f34c3fca950>, <__main__.Case object at 0x7f34c3fc94e0>, <__main__.Case object at 0x7f34c3fc9990>, <__main__.Case object at 0x7f34c3fcb040>, <__main__.Case object at 0x7f34c3fc9cc0>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fcbbe0>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.5, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.5, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.5, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.5, time steps: 6\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.8999999999999999)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.6, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5, time steps: 6\n",
      "Episode: 10, Total Steps: 16, Total Rewards: [85, 88], Status Episode: True\n",
      "------------------------------------------End of episode 10 loop--------------------\n",
      "----- starting point of Episode 11 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.9999999999999999, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.9999999999999999, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.9999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.6, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 0) with action 3 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 11 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 4, 0.5, 21)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.6, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 0.6, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 0.6, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.6, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 0.6, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 0.8999999999999999, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 6) with action 2 to next state (6, 7): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 6) with action 3 to next state (4, 6): pull reward: 0\n",
      "----- starting point of Episode 11 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 7) with action 4 to next state (7, 7): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (7, 7) with action 1 to next state (7, 6): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (7, 6) with action 3 to next state (6, 6): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 11 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.6, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f72170>, <__main__.Case object at 0x7f34c3fb3df0>, <__main__.Case object at 0x7f34c3fb3d30>, <__main__.Case object at 0x7f34c3fb2920>, <__main__.Case object at 0x7f34c3fc89d0>, <__main__.Case object at 0x7f34c3fc8700>, <__main__.Case object at 0x7f34c3fca3e0>, <__main__.Case object at 0x7f34c3fca7d0>, <__main__.Case object at 0x7f34c3fc94e0>, <__main__.Case object at 0x7f34c3fc99c0>, <__main__.Case object at 0x7f34c3fc97b0>, <__main__.Case object at 0x7f34c3fcb760>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fc9360>, <__main__.Case object at 0x7f34c3fc9f90>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c3fca620>, <__main__.Case object at 0x7f34c3fcb4c0>, <__main__.Case object at 0x7f34c3fc83a0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3f88970>, <__main__.Case object at 0x7f34c3fb0340>, <__main__.Case object at 0x7f34c3fc80d0>, <__main__.Case object at 0x7f34c3fcaef0>, <__main__.Case object at 0x7f34c3fc98d0>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3fc9990>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fc8190>, <__main__.Case object at 0x7f34c3fcb400>, <__main__.Case object at 0x7f34c3fc9b10>, <__main__.Case object at 0x7f34c3fca710>, <__main__.Case object at 0x7f34c3fc9210>, <__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3fc8280>, <__main__.Case object at 0x7f34c3fc8310>, <__main__.Case object at 0x7f34c3fc8130>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.7, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.7, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.7, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.7, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.7, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.7, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.7, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.4, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.4, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.4, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.4, time steps: 16\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 6) is empty. Temporary case base stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (7, 7) is empty. Temporary case base stored to the case base: ((7, 7), 1, 0.5)\n",
      "Episode succeeded, case (6, 7) is empty. Temporary case base stored to the case base: ((6, 7), 4, 0.5)\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.7, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.7, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.7, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.9999999999999999, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.7, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.7, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.7, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 7), solution: 1, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 7), solution: 4, tv: 0.5, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f90340>, <__main__.Case object at 0x7f34c3fb3ca0>, <__main__.Case object at 0x7f34c3f9f640>, <__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fc8730>, <__main__.Case object at 0x7f34c3fcb2b0>, <__main__.Case object at 0x7f34c3fc8e50>, <__main__.Case object at 0x7f34c3fca950>, <__main__.Case object at 0x7f34c3fca3b0>, <__main__.Case object at 0x7f34c3fc8e80>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fc86a0>, <__main__.Case object at 0x7f34c3fc8160>, <__main__.Case object at 0x7f34c3fcb3d0>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fca4a0>, <__main__.Case object at 0x7f34c3fca0b0>, <__main__.Case object at 0x7f34c3fca4d0>, <__main__.Case object at 0x7f34c3fc84f0>, <__main__.Case object at 0x7f34c3fcb430>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f90370>, <__main__.Case object at 0x7f34c3fb3ee0>, <__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3fc9690>, <__main__.Case object at 0x7f34c3fc9240>, <__main__.Case object at 0x7f34c3fc9480>, <__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c3fcba90>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fcab00>, <__main__.Case object at 0x7f34c3fca110>, <__main__.Case object at 0x7f34c3fcb610>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.7, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.4, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.4, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.4, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.4, time steps: 6\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 0), 4, 0.5, 20)\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.6)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.6)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.6)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.6)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.6)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.6)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.9999999999999999)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.7, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 20\n",
      "Episode: 11, Total Steps: 20, Total Rewards: [81, 86], Status Episode: True\n",
      "------------------------------------------End of episode 11 loop--------------------\n",
      "----- starting point of Episode 12 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.7, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.7, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.7, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 0.7, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 0.7, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.7, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 0.7, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 4) with action 2 to next state (6, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 12 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3fb3bb0>, <__main__.Case object at 0x7f34c3fb3ca0>, <__main__.Case object at 0x7f34c3f93d00>, <__main__.Case object at 0x7f34c3fcaef0>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fca710>, <__main__.Case object at 0x7f34c3fc8eb0>, <__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c3fca110>, <__main__.Case object at 0x7f34c3fc9600>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fcb7c0>, <__main__.Case object at 0x7f34c3fc87f0>, <__main__.Case object at 0x7f34c3fc8e50>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f90340>, <__main__.Case object at 0x7f34c3f70e50>, <__main__.Case object at 0x7f34c3f90370>, <__main__.Case object at 0x7f34c3fb2920>, <__main__.Case object at 0x7f34c3f9f6d0>, <__main__.Case object at 0x7f34c3f9f6a0>, <__main__.Case object at 0x7f34c3fc9990>, <__main__.Case object at 0x7f34c3fc9b10>, <__main__.Case object at 0x7f34c3fc94b0>, <__main__.Case object at 0x7f34c3fc9480>, <__main__.Case object at 0x7f34c3fcab00>, <__main__.Case object at 0x7f34c3fc9300>, <__main__.Case object at 0x7f34c3fc9cc0>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fcabc0>, <__main__.Case object at 0x7f34c3fcb2b0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.7999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.7999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.7999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.7999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.7999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.7999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 6), solution: 3, tv: 0.4, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (7, 7), solution: 1, tv: 0.4, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 7), solution: 4, tv: 0.4, time steps: 13\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.7999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.7999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.7999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.7999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.7999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.7999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.7, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3f889a0>, <__main__.Case object at 0x7f34c3fb2740>, <__main__.Case object at 0x7f34c3fb3d90>, <__main__.Case object at 0x7f34c3fc8130>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3fcb400>, <__main__.Case object at 0x7f34c3fc9c30>, <__main__.Case object at 0x7f34c3fc80d0>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fcbd60>, <__main__.Case object at 0x7f34c3fcb940>, <__main__.Case object at 0x7f34c3fc9e10>, <__main__.Case object at 0x7f34c3fcbc70>, <__main__.Case object at 0x7f34c3fc8730>, <__main__.Case object at 0x7f34c3fca3b0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f916c0>, <__main__.Case object at 0x7f34c3f88a30>, <__main__.Case object at 0x7f34c3fb3f10>, <__main__.Case object at 0x7f34c3fb3cd0>, <__main__.Case object at 0x7f34c3fcb430>, <__main__.Case object at 0x7f34c3fc98d0>, <__main__.Case object at 0x7f34c3fc8190>, <__main__.Case object at 0x7f34c3fc9210>, <__main__.Case object at 0x7f34c3fc9f30>, <__main__.Case object at 0x7f34c3fcba90>, <__main__.Case object at 0x7f34c3fcbaf0>, <__main__.Case object at 0x7f34c3fc97b0>, <__main__.Case object at 0x7f34c3fc9900>, <__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fca950>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.7999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.4, time steps: 20\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.7)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.7)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.7)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.7)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.7)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.7)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.7)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.7999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.7, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.7, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "Episode: 12, Total Steps: 16, Total Rewards: [85, 88], Status Episode: True\n",
      "------------------------------------------End of episode 12 loop--------------------\n",
      "----- starting point of Episode 13 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 6)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 0) with action 2 to next state (8, 1): pull reward: 0\n",
      "----- starting point of Episode 13 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 9)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 1) with action 3 to next state (7, 1): pull reward: 0\n",
      "----- starting point of Episode 13 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 1) with action 4 to next state (8, 1): pull reward: 0\n",
      "----- starting point of Episode 13 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.7999999999999999, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 1) with action 2 to next state (8, 2): pull reward: 0\n",
      "----- starting point of Episode 13 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.7999999999999999, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 2) with action 4 to next state (9, 2): pull reward: 0\n",
      "----- starting point of Episode 13 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.7999999999999999, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 2) with action 2 to next state (9, 3): pull reward: 0\n",
      "----- starting point of Episode 13 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 0.7999999999999999, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 3) with action 3 to next state (8, 3): pull reward: 0\n",
      "----- starting point of Episode 13 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 1) with action 3 to next state (5, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 3) with action 0 to next state (8, 3): pull reward: 0\n",
      "----- starting point of Episode 13 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 1) with action 2 to next state (5, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 3) with action 2 to next state (8, 4): pull reward: 0\n",
      "----- starting point of Episode 13 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 2) with action 0 to next state (5, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 4) with action 0 to next state (8, 4): pull reward: 0\n",
      "----- starting point of Episode 13 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 2) with action 3 to next state (4, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 4) with action 3 to next state (7, 4): pull reward: 0\n",
      "----- starting point of Episode 13 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 2) with action 3 to next state (3, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 4) with action 2 to next state (7, 5): pull reward: 0\n",
      "----- starting point of Episode 13 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 1 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 5) with action 3 to next state (6, 5): pull reward: 0\n",
      "----- starting point of Episode 13 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 2 to next state (3, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 2 to next state (3, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 3) with action 1 to next state (3, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 4 to next state (4, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 13 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 2) with action 1 to next state (4, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f71060>, <__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3fb2740>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3fb3d90>, <__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c3fb29b0>, <__main__.Case object at 0x7f34c3fc9210>, <__main__.Case object at 0x7f34c3fcb760>, <__main__.Case object at 0x7f34c3fc97b0>, <__main__.Case object at 0x7f34c3fc9240>, <__main__.Case object at 0x7f34c3fc8eb0>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fc80d0>, <__main__.Case object at 0x7f34c3fcb940>, <__main__.Case object at 0x7f34c3fc8730>, <__main__.Case object at 0x7f34c3fc96f0>, <__main__.Case object at 0x7f34c3fcb280>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f93d00>, <__main__.Case object at 0x7f34c3fca7a0>, <__main__.Case object at 0x7f34c3fcbd60>, <__main__.Case object at 0x7f34c3f93ac0>, <__main__.Case object at 0x7f34c3fc8fa0>, <__main__.Case object at 0x7f34c3fc85b0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.7999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.7999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.5999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.5999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.5999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.5999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.8, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.8, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.7, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.7999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.7999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.8, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.8, time steps: 6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.7, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fb2920>, <__main__.Case object at 0x7f34c3fb3d60>, <__main__.Case object at 0x7f34c3f9f6a0>, <__main__.Case object at 0x7f34c3fcb070>, <__main__.Case object at 0x7f34c3fc94b0>, <__main__.Case object at 0x7f34c3fc9cc0>, <__main__.Case object at 0x7f34c3fc98d0>, <__main__.Case object at 0x7f34c3fcba90>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fcb9d0>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fc91e0>, <__main__.Case object at 0x7f34c3fc9ab0>, <__main__.Case object at 0x7f34c3fc8130>, <__main__.Case object at 0x7f34c3fcbc70>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fc8f70>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fb3df0>, <__main__.Case object at 0x7f34c3f9f820>, <__main__.Case object at 0x7f34c3fca950>, <__main__.Case object at 0x7f34c3fc9ba0>, <__main__.Case object at 0x7f34c3fc9300>, <__main__.Case object at 0x7f34c3fcabc0>, <__main__.Case object at 0x7f34c3fc9f30>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.7, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.9, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.9, time steps: 6\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 3, 0.5)\n",
      "Episode succeeded, case (7, 4) is empty. Temporary case base stored to the case base: ((7, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 3) is empty. Temporary case base stored to the case base: ((9, 3), 3, 0.5)\n",
      "Episode succeeded, case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 2, 0.5)\n",
      "Episode succeeded, case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 4, 0.5)\n",
      "Episode succeeded, case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 4, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.7, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.9, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.9, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 5), solution: 3, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 4), solution: 2, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (8, 4), solution: 3, tv: 0.5, time steps: 11\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.5, time steps: 9\n",
      "cases content after RETAIN, problem: (9, 3), solution: 3, tv: 0.5, time steps: 7\n",
      "cases content after RETAIN, problem: (9, 2), solution: 2, tv: 0.5, time steps: 6\n",
      "cases content after RETAIN, problem: (8, 2), solution: 4, tv: 0.5, time steps: 5\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.5, time steps: 4\n",
      "cases content after RETAIN, problem: (7, 1), solution: 4, tv: 0.5, time steps: 3\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.7999999999999999, time steps: 16\n",
      "Episode: 13, Total Steps: 19, Total Rewards: [-118, 82], Status Episode: False\n",
      "------------------------------------------End of episode 13 loop--------------------\n",
      "----- starting point of Episode 14 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.8, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.9, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.9, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.8, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.9, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.5999999999999999, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.9, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.5999999999999999, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.9, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.5999999999999999, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.9, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 0) with action 1 to next state (6, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 0.9, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 0.5999999999999999, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 0.7999999999999999, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.7999999999999999, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 0.7999999999999999, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 14 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 14 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f71060>, <__main__.Case object at 0x7f34c3fb3df0>, <__main__.Case object at 0x7f34c3fb3bb0>, <__main__.Case object at 0x7f34c3fca7a0>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fc8670>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fc9870>, <__main__.Case object at 0x7f34c3fc9660>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fcb250>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fc83d0>, <__main__.Case object at 0x7f34c3fca4a0>, <__main__.Case object at 0x7f34c3fcb910>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f889a0>, <__main__.Case object at 0x7f34c3f9f6d0>, <__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c3fb29b0>, <__main__.Case object at 0x7f34c3fc8cd0>, <__main__.Case object at 0x7f34c3fcbf40>, <__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fc8a30>, <__main__.Case object at 0x7f34c3fc9c90>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fca170>, <__main__.Case object at 0x7f34c3fc9f00>, <__main__.Case object at 0x7f34c3fc8dc0>, <__main__.Case object at 0x7f34c3fc9390>, <__main__.Case object at 0x7f34c3fcae30>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.8999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.8999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.6999999999999998, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.6999999999999998, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.6999999999999998, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.6999999999999998, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.9, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.9, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 0.9)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.9)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.9)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.9)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.9)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.9)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.9)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.8999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.8999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.6999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.9, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.9, time steps: 6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.6, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f9f820>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3fc8f70>, <__main__.Case object at 0x7f34c3fc9780>, <__main__.Case object at 0x7f34c3fc85b0>, <__main__.Case object at 0x7f34c3fc8a90>, <__main__.Case object at 0x7f34c3fc9330>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fc9990>, <__main__.Case object at 0x7f34c3fcbc70>, <__main__.Case object at 0x7f34c3fc9840>, <__main__.Case object at 0x7f34c3fca230>, <__main__.Case object at 0x7f34c3fcb4c0>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fc8940>, <__main__.Case object at 0x7f34c3fc92a0>, <__main__.Case object at 0x7f34c3fc8e50>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f93d00>, <__main__.Case object at 0x7f34c3fb3ca0>, <__main__.Case object at 0x7f34c3fb3f70>, <__main__.Case object at 0x7f34c3fcbd60>, <__main__.Case object at 0x7f34c3fc9300>, <__main__.Case object at 0x7f34c3fcbbe0>, <__main__.Case object at 0x7f34c3fcb400>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fc8130>, <__main__.Case object at 0x7f34c3fcb520>, <__main__.Case object at 0x7f34c3fc9a50>, <__main__.Case object at 0x7f34c3fc9420>, <__main__.Case object at 0x7f34c3fca1a0>, <__main__.Case object at 0x7f34c3fc8160>, <__main__.Case object at 0x7f34c3fc9720>, <__main__.Case object at 0x7f34c3fcbdf0>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1.0, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1.0, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1.0, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1.0, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1.0, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1.0, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1.0, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.7999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.5, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.5, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.8, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.8, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 3, tv: 0.4, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 4), solution: 2, tv: 0.4, time steps: 12\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 3, tv: 0.4, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 0.4, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 3, tv: 0.4, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 2, tv: 0.4, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 4, tv: 0.4, time steps: 5\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 0.4, time steps: 4\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 4, tv: 0.4, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.7, time steps: 16\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5999999999999999)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5999999999999999)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5999999999999999)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.8)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.8)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1.0, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1.0, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1.0, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1.0, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1.0, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1.0, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1.0, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.7999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.8, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.8, time steps: 6\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.7, time steps: 16\n",
      "Episode: 14, Total Steps: 17, Total Rewards: [84, 88], Status Episode: True\n",
      "------------------------------------------End of episode 14 loop--------------------\n",
      "----- starting point of Episode 15 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.9, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1.0, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.9, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1.0, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.9, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1.0, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.6999999999999998, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1.0, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.6999999999999998, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1.0, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.6999999999999998, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 0.6999999999999998, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 3) with action 4 to next state (7, 3): pull reward: 0\n",
      "----- starting point of Episode 15 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 0.8999999999999999, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 3) with action 4 to next state (8, 3): pull reward: 0\n",
      "----- starting point of Episode 15 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.8999999999999999, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 3) with action 4 to next state (9, 3): pull reward: 0\n",
      "----- starting point of Episode 15 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 0.8999999999999999, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 3) with action 1 to next state (9, 2): pull reward: 0\n",
      "----- starting point of Episode 15 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 2) with action 3 to next state (8, 2): pull reward: 0\n",
      "----- starting point of Episode 15 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 2) with action 1 to next state (8, 1): pull reward: 0\n",
      "----- starting point of Episode 15 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 1) with action 0 to next state (8, 1): pull reward: 0\n",
      "----- starting point of Episode 15 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 1) with action 0 to next state (8, 1): pull reward: 0\n",
      "----- starting point of Episode 15 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 1) with action 3 to next state (7, 1): pull reward: 0\n",
      "----- starting point of Episode 15 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 1) with action 4 to next state (8, 1): pull reward: 0\n",
      "----- starting point of Episode 15 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.6, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 1) with action 0 to next state (8, 1): pull reward: 0\n",
      "----- starting point of Episode 15 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 1) with action 0 to next state (8, 1): pull reward: 0\n",
      "----- starting point of Episode 15 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.6, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 1) with action 3 to next state (7, 1): pull reward: 0\n",
      "----- starting point of Episode 15 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.6, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 1) with action 0 to next state (7, 1): pull reward: 0\n",
      "----- starting point of Episode 15 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.6, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 1) with action 3 to next state (6, 1): pull reward: 0\n",
      "----- starting point of Episode 15 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.6, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.6, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.6, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.6, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.6, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.6, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.6, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 15 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.6, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f93ac0>, <__main__.Case object at 0x7f34c3fb29b0>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3fcace0>, <__main__.Case object at 0x7f34c3fc9240>, <__main__.Case object at 0x7f34c3fc9fc0>, <__main__.Case object at 0x7f34c3fca0b0>, <__main__.Case object at 0x7f34c3fcbbe0>, <__main__.Case object at 0x7f34c3fcae90>, <__main__.Case object at 0x7f34c3fc8280>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fcab00>, <__main__.Case object at 0x7f34c3fcb280>, <__main__.Case object at 0x7f34c3fc80d0>, <__main__.Case object at 0x7f34c3fc96f0>, <__main__.Case object at 0x7f34c3fcbf70>, <__main__.Case object at 0x7f34c3fc97b0>, <__main__.Case object at 0x7f34c3fc8c40>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fcad70>, <__main__.Case object at 0x7f34c3f93d00>, <__main__.Case object at 0x7f34c3fcb970>, <__main__.Case object at 0x7f34c3fca1d0>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fca440>, <__main__.Case object at 0x7f34c3fcb5e0>, <__main__.Case object at 0x7f34c3fc8310>, <__main__.Case object at 0x7f34c3fcb9d0>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fc9c90>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3fb3bb0>, <__main__.Case object at 0x7f34c3fb2980>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fcbb50>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.9999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.9999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.9999999999999999, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.7999999999999998, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.7999999999999998, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.7999999999999998, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.7999999999999998, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1.0, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1.0, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1.0, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.7, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.8, time steps: 15\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1.0)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1.0)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1.0)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1.0)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1.0)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.9999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.9999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.9999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.7999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.7999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.7999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.7999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1.0, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1.0, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1.0, time steps: 6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.7, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.8, time steps: 15\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f9f820>, <__main__.Case object at 0x7f34c3fb3df0>, <__main__.Case object at 0x7f34c3fc8e50>, <__main__.Case object at 0x7f34c3fc8700>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fc9f90>, <__main__.Case object at 0x7f34c3fcbd60>, <__main__.Case object at 0x7f34c3fcb940>, <__main__.Case object at 0x7f34c3fc9a50>, <__main__.Case object at 0x7f34c3fc86a0>, <__main__.Case object at 0x7f34c3fc94e0>, <__main__.Case object at 0x7f34c3fc8eb0>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fc9150>, <__main__.Case object at 0x7f34c3fca4d0>, <__main__.Case object at 0x7f34c3fca950>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fcbc70>, <__main__.Case object at 0x7f34c3fc9360>, <__main__.Case object at 0x7f34c3fc84f0>, <__main__.Case object at 0x7f34c3fcb4f0>, <__main__.Case object at 0x7f34c3fcbb80>, <__main__.Case object at 0x7f34c3fc9840>, <__main__.Case object at 0x7f34c3fcb190>, <__main__.Case object at 0x7f34c3fcb340>, <__main__.Case object at 0x7f34c3fc8a00>, <__main__.Case object at 0x7f34c3fca620>, <__main__.Case object at 0x7f34c3fc98d0>, <__main__.Case object at 0x7f34c3fcbac0>, <__main__.Case object at 0x7f34c3fcb3a0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f9f6d0>, <__main__.Case object at 0x7f34c3fb3d90>, <__main__.Case object at 0x7f34c3f71060>, <__main__.Case object at 0x7f34c3fc9690>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fc9e40>, <__main__.Case object at 0x7f34c3fcb6d0>, <__main__.Case object at 0x7f34c3fcb760>, <__main__.Case object at 0x7f34c3fcaef0>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fc9030>, <__main__.Case object at 0x7f34c3fc9210>, <__main__.Case object at 0x7f34c3fc9b10>, <__main__.Case object at 0x7f34c3fc88b0>, <__main__.Case object at 0x7f34c3fc9a80>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fcb730>, <__main__.Case object at 0x7f34c3fcba60>, <__main__.Case object at 0x7f34c3fca9e0>, <__main__.Case object at 0x7f34c3fc9bd0>, <__main__.Case object at 0x7f34c3fc9c00>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3fca7d0>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c3fc9ff0>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.7, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.4, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.4, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.7000000000000001, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.7000000000000001, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.7000000000000001, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6, time steps: 16\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 3, 0.5)\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 3, 0.5)\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 1, 0.5)\n",
      "Episode succeeded, case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 3, 0.5)\n",
      "Episode succeeded, case (9, 3) is empty. Temporary case base stored to the case base: ((9, 3), 1, 0.5)\n",
      "Episode succeeded, case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 4, 0.5)\n",
      "Episode succeeded, case (7, 3) is empty. Temporary case base stored to the case base: ((7, 3), 4, 0.5)\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.6)\n",
      "Integrated case process. comm case (4, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.6999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.6999999999999998)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.6999999999999998)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.6999999999999998)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.9)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.9)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.9)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.7, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.7000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7000000000000001, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7000000000000001, time steps: 6\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 1), solution: 3, tv: 0.5, time steps: 20\n",
      "cases content after RETAIN, problem: (8, 1), solution: 3, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (8, 2), solution: 1, tv: 0.5, time steps: 11\n",
      "cases content after RETAIN, problem: (9, 2), solution: 3, tv: 0.5, time steps: 10\n",
      "cases content after RETAIN, problem: (9, 3), solution: 1, tv: 0.5, time steps: 9\n",
      "cases content after RETAIN, problem: (8, 3), solution: 4, tv: 0.5, time steps: 8\n",
      "cases content after RETAIN, problem: (7, 3), solution: 4, tv: 0.5, time steps: 7\n",
      "Episode: 15, Total Steps: 30, Total Rewards: [85, 71], Status Episode: True\n",
      "------------------------------------------End of episode 15 loop--------------------\n",
      "----- starting point of Episode 16 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1.0, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1.0, 9)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 0) with action 3 to next state (7, 0): pull reward: 0\n",
      "----- starting point of Episode 16 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1.0, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.7999999999999998, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.7999999999999998, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.7999999999999998, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 0.7999999999999998, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 3) with action 1 to next state (6, 2): pull reward: 0\n",
      "----- starting point of Episode 16 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 0.9999999999999999, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 0.9999999999999999, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 3) with action 3 to next state (5, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 3) with action 2 to next state (5, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 4) with action 1 to next state (5, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 3) with action 4 to next state (6, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 0.9999999999999999, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 5) with action 0 to next state (6, 5): pull reward: 0\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 6) with action 3 to next state (5, 6): pull reward: 0\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 16 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3f93d00>, <__main__.Case object at 0x7f34c3fc8100>, <__main__.Case object at 0x7f34c3fc9e40>, <__main__.Case object at 0x7f34c3fc9210>, <__main__.Case object at 0x7f34c3fc88b0>, <__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c3fc8c70>, <__main__.Case object at 0x7f34c3fc8e80>, <__main__.Case object at 0x7f34c3fc9fc0>, <__main__.Case object at 0x7f34c3fca1a0>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fca4a0>, <__main__.Case object at 0x7f34c3fc92a0>, <__main__.Case object at 0x7f34c3fca440>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fc8700>, <__main__.Case object at 0x7f34c3fc9150>, <__main__.Case object at 0x7f34c3fcbc70>, <__main__.Case object at 0x7f34c3fcb190>, <__main__.Case object at 0x7f34c3fca620>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f71060>, <__main__.Case object at 0x7f34c3f9f640>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fc9b10>, <__main__.Case object at 0x7f34c3fcba60>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fcbf40>, <__main__.Case object at 0x7f34c3fc8910>, <__main__.Case object at 0x7f34c3fc89d0>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fcb9d0>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fc9840>, <__main__.Case object at 0x7f34c3f72350>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.8999999999999998, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.8999999999999998, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.8999999999999998, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.8999999999999998, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.6, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) is empty. Temporary case base stored to the case base: ((5, 3), 4, 0.5)\n",
      "Episode succeeded, case (5, 4) is empty. Temporary case base stored to the case base: ((5, 4), 1, 0.5)\n",
      "Episode succeeded, case (5, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.8999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.8999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.8999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.8999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.6, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 3), solution: 4, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 4), solution: 1, tv: 0.5, time steps: 11\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f9f6d0>, <__main__.Case object at 0x7f34c3fcb2e0>, <__main__.Case object at 0x7f34c3fc9690>, <__main__.Case object at 0x7f34c3fcb760>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fcacb0>, <__main__.Case object at 0x7f34c3fc9330>, <__main__.Case object at 0x7f34c3fcae90>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fcb400>, <__main__.Case object at 0x7f34c3fcbbe0>, <__main__.Case object at 0x7f34c3fca1d0>, <__main__.Case object at 0x7f34c3fc8310>, <__main__.Case object at 0x7f34c3fc9c90>, <__main__.Case object at 0x7f34c3fc80a0>, <__main__.Case object at 0x7f34c3fca4d0>, <__main__.Case object at 0x7f34c3fcbb80>, <__main__.Case object at 0x7f34c3fc8a00>, <__main__.Case object at 0x7f34c3fc94b0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3fc9f00>, <__main__.Case object at 0x7f34c3fcb6d0>, <__main__.Case object at 0x7f34c3fcaef0>, <__main__.Case object at 0x7f34c3fc9a80>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fc98a0>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c3fcb970>, <__main__.Case object at 0x7f34c3fcb5e0>, <__main__.Case object at 0x7f34c3fcb610>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fc8970>, <__main__.Case object at 0x7f34c3fc84f0>, <__main__.Case object at 0x7f34c3fcb340>, <__main__.Case object at 0x7f34c3fcb070>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.7999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6000000000000001, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6000000000000001, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.5, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 3, tv: 0.4, time steps: 20\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 3, tv: 0.4, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 1, tv: 0.4, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 3, tv: 0.4, time steps: 10\n",
      "case content after REVISE for agent 1, problem: (9, 3), solution: 1, tv: 0.4, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 4, tv: 0.4, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (7, 3), solution: 4, tv: 0.4, time steps: 7\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.7999999999999998)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 0.7999999999999998)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 0.7999999999999998)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 1.0)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1.0)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1.0)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.7999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6000000000000001, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6000000000000001, time steps: 6\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.7999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.7999999999999998, time steps: 16\n",
      "Episode: 16, Total Steps: 21, Total Rewards: [80, 86], Status Episode: True\n",
      "------------------------------------------End of episode 16 loop--------------------\n",
      "----- starting point of Episode 17 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.8999999999999998, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.8999999999999998, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.8999999999999998, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 0.8999999999999998, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7999999999999999, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7999999999999999, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 17 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7999999999999999, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f93d00>, <__main__.Case object at 0x7f34c3f9f820>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fcbf40>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fc9f00>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c3fcb610>, <__main__.Case object at 0x7f34c3fcb3a0>, <__main__.Case object at 0x7f34c3fcb910>, <__main__.Case object at 0x7f34c3fc96c0>, <__main__.Case object at 0x7f34c3fcab90>, <__main__.Case object at 0x7f34c3fc8fd0>, <__main__.Case object at 0x7f34c3fc8250>, <__main__.Case object at 0x7f34c3fc99f0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fcbac0>, <__main__.Case object at 0x7f34c3fc98a0>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fcb340>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fcb550>, <__main__.Case object at 0x7f34c3fcb640>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fcba00>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.9999999999999998, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.9999999999999998, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.9999999999999998, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.9999999999999998, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.5, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 4, tv: 0.4, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 1, tv: 0.4, time steps: 11\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.9999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.9999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.9999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.9999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f9f640>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fcba60>, <__main__.Case object at 0x7f34c3fc89d0>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fc9840>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fcb5e0>, <__main__.Case object at 0x7f34c3fca7d0>, <__main__.Case object at 0x7f34c3fc8fa0>, <__main__.Case object at 0x7f34c3fc9bd0>, <__main__.Case object at 0x7f34c3fcad10>, <__main__.Case object at 0x7f34c3fc8c40>, <__main__.Case object at 0x7f34c3fc8cd0>, <__main__.Case object at 0x7f34c3fc8730>, <__main__.Case object at 0x7f34c3fc8280>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3fcb460>, <__main__.Case object at 0x7f34c3fc9b10>, <__main__.Case object at 0x7f34c3fc8910>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fc9a80>, <__main__.Case object at 0x7f34c3fcb970>, <__main__.Case object at 0x7f34c3fc8a90>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fcab00>, <__main__.Case object at 0x7f34c3fc9150>, <__main__.Case object at 0x7f34c3fca590>, <__main__.Case object at 0x7f34c3fc9930>, <__main__.Case object at 0x7f34c3fc9240>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.8999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.5000000000000001, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.5000000000000001, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.4, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6999999999999998, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6999999999999998, time steps: 16\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.8999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.8999999999999998)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.8999999999999998)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.8999999999999998)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.8999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5000000000000001, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5000000000000001, time steps: 6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6999999999999998, time steps: 16\n",
      "Episode: 17, Total Steps: 16, Total Rewards: [85, 88], Status Episode: True\n",
      "------------------------------------------End of episode 17 loop--------------------\n",
      "----- starting point of Episode 18 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.9999999999999998, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.9999999999999998, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.9999999999999998, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 0.9999999999999998, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.8999999999999999, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 6) with action 1 to next state (5, 5): pull reward: 0\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.8999999999999999, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 5) with action 4 to next state (6, 5): pull reward: 0\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.8999999999999999, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.8999999999999999, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.8999999999999999, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.8999999999999999, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 18 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.8999999999999999, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3fc8280>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fcb460>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fc9150>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fc8bb0>, <__main__.Case object at 0x7f34c3fc92a0>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fc9f90>, <__main__.Case object at 0x7f34c3fc9ab0>, <__main__.Case object at 0x7f34c3fcb190>, <__main__.Case object at 0x7f34c3fca530>, <__main__.Case object at 0x7f34c3fc8d00>, <__main__.Case object at 0x7f34c3fc90f0>, <__main__.Case object at 0x7f34c3fcb430>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c3f9f640>, <__main__.Case object at 0x7f34c3f93ac0>, <__main__.Case object at 0x7f34c3fc98a0>, <__main__.Case object at 0x7f34c3fcb550>, <__main__.Case object at 0x7f34c3fcba00>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fc8a90>, <__main__.Case object at 0x7f34c3fca590>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fc85b0>, <__main__.Case object at 0x7f34c3fc8c70>, <__main__.Case object at 0x7f34c3fcae90>, <__main__.Case object at 0x7f34c3fc8430>, <__main__.Case object at 0x7f34c3fc8460>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3f889a0>, <__main__.Case object at 0x7f34c3fc9810>, <__main__.Case object at 0x7f34c3fcb580>, <__main__.Case object at 0x7f34c3fcaad0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.4, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) is empty. Temporary case base stored to the case base: ((5, 5), 4, 0.5)\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.5, time steps: 14\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f9f6a0>, <__main__.Case object at 0x7f34c3fc94b0>, <__main__.Case object at 0x7f34c3fcbac0>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fc8910>, <__main__.Case object at 0x7f34c3fcb970>, <__main__.Case object at 0x7f34c3fcab00>, <__main__.Case object at 0x7f34c3fcb790>, <__main__.Case object at 0x7f34c3fca230>, <__main__.Case object at 0x7f34c3fc9e40>, <__main__.Case object at 0x7f34c3fcb2e0>, <__main__.Case object at 0x7f34c3fc9f30>, <__main__.Case object at 0x7f34c3fc9ff0>, <__main__.Case object at 0x7f34c3fc88b0>, <__main__.Case object at 0x7f34c3fcacb0>, <__main__.Case object at 0x7f34c3fc8700>, <__main__.Case object at 0x7f34c3fca560>, <__main__.Case object at 0x7f34c3fc9090>, <__main__.Case object at 0x7f34c3fc93f0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f71060>, <__main__.Case object at 0x7f34c3fc9240>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fcb340>, <__main__.Case object at 0x7f34c3fca620>, <__main__.Case object at 0x7f34c3fc9b10>, <__main__.Case object at 0x7f34c3fc9a80>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fcb9d0>, <__main__.Case object at 0x7f34c3fc84f0>, <__main__.Case object at 0x7f34c3fc8f70>, <__main__.Case object at 0x7f34c3fcba60>, <__main__.Case object at 0x7f34c3fc9ed0>, <__main__.Case object at 0x7f34c3fc8e50>, <__main__.Case object at 0x7f34c3fcb760>, <__main__.Case object at 0x7f34c3fc9cf0>, <__main__.Case object at 0x7f34c3fcafb0>, <__main__.Case object at 0x7f34c3fcbe20>, <__main__.Case object at 0x7f34c3fc8160>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.9999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.40000000000000013, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.40000000000000013, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.40000000000000013, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.5999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.5999999999999999, time steps: 16\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.9999999999999998)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 0.9999999999999998)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.9999999999999998)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.9999999999999998)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.9999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.9999999999999998, time steps: 16\n",
      "Episode: 18, Total Steps: 20, Total Rewards: [81, 88], Status Episode: True\n",
      "------------------------------------------End of episode 18 loop--------------------\n",
      "----- starting point of Episode 19 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 1) with action 4 to next state (7, 1): pull reward: 0\n",
      "----- starting point of Episode 19 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 1) with action 0 to next state (7, 1): pull reward: 0\n",
      "----- starting point of Episode 19 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 1) with action 0 to next state (7, 1): pull reward: 0\n",
      "----- starting point of Episode 19 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 1) with action 3 to next state (6, 1): pull reward: 0\n",
      "----- starting point of Episode 19 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 19 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f9f6d0>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fcbb50>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fc9840>, <__main__.Case object at 0x7f34c3fcb700>, <__main__.Case object at 0x7f34c3fc98d0>, <__main__.Case object at 0x7f34c3fcaad0>, <__main__.Case object at 0x7f34c3fca2c0>, <__main__.Case object at 0x7f34c3fcba60>, <__main__.Case object at 0x7f34c3fc9cf0>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fcb460>, <__main__.Case object at 0x7f34c3fc8bb0>, <__main__.Case object at 0x7f34c3fcb190>, <__main__.Case object at 0x7f34c3fcb430>, <__main__.Case object at 0x7f34c3fcb970>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f9f820>, <__main__.Case object at 0x7f34c3f9f640>, <__main__.Case object at 0x7f34c3fc8970>, <__main__.Case object at 0x7f34c3fc9930>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fc8f70>, <__main__.Case object at 0x7f34c3fcb760>, <__main__.Case object at 0x7f34c3fc8280>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fc9f90>, <__main__.Case object at 0x7f34c3fc90f0>, <__main__.Case object at 0x7f34c3fc8640>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.40000000000000013, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.40000000000000013, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.40000000000000013, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 4, tv: 0.4, time steps: 14\n",
      "Episode succeeded, case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 2, 0.5)\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 2, tv: 0.5, time steps: 16\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fc9990>, <__main__.Case object at 0x7f34c3fcbd00>, <__main__.Case object at 0x7f34c3fc9210>, <__main__.Case object at 0x7f34c3fcab90>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fcb580>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fc9a80>, <__main__.Case object at 0x7f34c3fc9690>, <__main__.Case object at 0x7f34c3fc8e50>, <__main__.Case object at 0x7f34c3fcbe20>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fc8d00>, <__main__.Case object at 0x7f34c3fcbac0>, <__main__.Case object at 0x7f34c3fcaa70>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fc93f0>, <__main__.Case object at 0x7f34c3fca770>, <__main__.Case object at 0x7f34c3fcb610>, <__main__.Case object at 0x7f34c3fc9390>, <__main__.Case object at 0x7f34c3fc9810>, <__main__.Case object at 0x7f34c3fc9240>, <__main__.Case object at 0x7f34c3fca620>, <__main__.Case object at 0x7f34c3fc9f00>, <__main__.Case object at 0x7f34c3fc9ed0>, <__main__.Case object at 0x7f34c3fcafb0>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fc8e80>, <__main__.Case object at 0x7f34c3fc92a0>, <__main__.Case object at 0x7f34c3fca530>, <__main__.Case object at 0x7f34c3fc94b0>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.8999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.4999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.4999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.8999999999999998, time steps: 16\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 3, 0.5)\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.8999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.4999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.4999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.8999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 1), solution: 3, tv: 0.5, time steps: 7\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "Episode: 19, Total Steps: 17, Total Rewards: [85, 84], Status Episode: True\n",
      "------------------------------------------End of episode 19 loop--------------------\n",
      "----- starting point of Episode 20 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 0) with action 3 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 20 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 4, 0.8999999999999998, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 1) with action 0 to next state (6, 1): pull reward: 0\n",
      "----- starting point of Episode 20 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 6) with action 3 to next state (4, 6): pull reward: 0\n",
      "----- starting point of Episode 20 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 20 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f9f6a0>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fc92d0>, <__main__.Case object at 0x7f34c3fcb3a0>, <__main__.Case object at 0x7f34c3fc8850>, <__main__.Case object at 0x7f34c3fcb910>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fcace0>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fca2c0>, <__main__.Case object at 0x7f34c3fcb460>, <__main__.Case object at 0x7f34c3fc9990>, <__main__.Case object at 0x7f34c3fcab90>, <__main__.Case object at 0x7f34c3fc9690>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fcb880>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3f9f820>, <__main__.Case object at 0x7f34c3fc89d0>, <__main__.Case object at 0x7f34c3fc8cd0>, <__main__.Case object at 0x7f34c3fc91e0>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fc9840>, <__main__.Case object at 0x7f34c3fcaad0>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fcb430>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fc95a0>, <__main__.Case object at 0x7f34c3fcbac0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 2, tv: 0.4, time steps: 16\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.8999999999999998)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fcadd0>, <__main__.Case object at 0x7f34c3fcb9d0>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fcb520>, <__main__.Case object at 0x7f34c3fc8970>, <__main__.Case object at 0x7f34c3fc9e70>, <__main__.Case object at 0x7f34c3fca050>, <__main__.Case object at 0x7f34c3fcbb50>, <__main__.Case object at 0x7f34c3fc8460>, <__main__.Case object at 0x7f34c3fc9cf0>, <__main__.Case object at 0x7f34c3fcb190>, <__main__.Case object at 0x7f34c3fc9210>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fcbe20>, <__main__.Case object at 0x7f34c3fc8d00>, <__main__.Case object at 0x7f34c3fca4a0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fcaa70>, <__main__.Case object at 0x7f34c3fc85b0>, <__main__.Case object at 0x7f34c3fc83d0>, <__main__.Case object at 0x7f34c3fc8730>, <__main__.Case object at 0x7f34c3fc9240>, <__main__.Case object at 0x7f34c3fcad10>, <__main__.Case object at 0x7f34c3fcb070>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fcbd90>, <__main__.Case object at 0x7f34c3fcba60>, <__main__.Case object at 0x7f34c3fc8bb0>, <__main__.Case object at 0x7f34c3fcbd00>, <__main__.Case object at 0x7f34c3fcb370>, <__main__.Case object at 0x7f34c3fc8e50>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fca170>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.7999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.3999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.3999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.9999999999999998, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 3, tv: 0.4, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.9, time steps: 6\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.7999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.9999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.9, time steps: 6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "Episode: 20, Total Steps: 16, Total Rewards: [85, 85], Status Episode: True\n",
      "------------------------------------------End of episode 20 loop--------------------\n",
      "----- starting point of Episode 21 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 6) with action 3 to next state (4, 6): pull reward: 0\n",
      "----- starting point of Episode 21 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 6) with action 0 to next state (6, 6): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 21 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7999999999999999, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7999999999999999, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 21 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.7999999999999999, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fca170>, <__main__.Case object at 0x7f34c3fc9840>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fc83d0>, <__main__.Case object at 0x7f34c3fcad10>, <__main__.Case object at 0x7f34c3fc8bb0>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fc90f0>, <__main__.Case object at 0x7f34c3fc92a0>, <__main__.Case object at 0x7f34c3fcba00>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fc9f90>, <__main__.Case object at 0x7f34c3fc8e80>, <__main__.Case object at 0x7f34c3fc8460>, <__main__.Case object at 0x7f34c3fc8430>, <__main__.Case object at 0x7f34c3fca1a0>, <__main__.Case object at 0x7f34c3fcb790>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c3fc8ee0>, <__main__.Case object at 0x7f34c3fcb430>, <__main__.Case object at 0x7f34c3fcaa70>, <__main__.Case object at 0x7f34c3fcb070>, <__main__.Case object at 0x7f34c3fcba60>, <__main__.Case object at 0x7f34c3fc8e50>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fcb550>, <__main__.Case object at 0x7f34c3fc8fd0>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fc94b0>, <__main__.Case object at 0x7f34c3fc9b70>, <__main__.Case object at 0x7f34c3fca9e0>, <__main__.Case object at 0x7f34c3f9f640>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.7999999999999999, time steps: 16\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fca980>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fcbac0>, <__main__.Case object at 0x7f34c3fc9240>, <__main__.Case object at 0x7f34c3fc9db0>, <__main__.Case object at 0x7f34c3fcb370>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fcafb0>, <__main__.Case object at 0x7f34c3fc9810>, <__main__.Case object at 0x7f34c3fc87c0>, <__main__.Case object at 0x7f34c3fc98a0>, <__main__.Case object at 0x7f34c3fc9f00>, <__main__.Case object at 0x7f34c3fc9570>, <__main__.Case object at 0x7f34c3fc8100>, <__main__.Case object at 0x7f34c3fc8250>, <__main__.Case object at 0x7f34c3fcbc70>, <__main__.Case object at 0x7f34c3fc96c0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fca4a0>, <__main__.Case object at 0x7f34c3fcaad0>, <__main__.Case object at 0x7f34c3fc95a0>, <__main__.Case object at 0x7f34c3fc8730>, <__main__.Case object at 0x7f34c3fc8a90>, <__main__.Case object at 0x7f34c3fcbd00>, <__main__.Case object at 0x7f34c3fc8910>, <__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fca2c0>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fcb4c0>, <__main__.Case object at 0x7f34c3fc9390>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.8999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.8999999999999998, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.8, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.9, time steps: 9\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.8999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.8999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.8, time steps: 6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.9, time steps: 9\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "Episode: 21, Total Steps: 17, Total Rewards: [84, 88], Status Episode: True\n",
      "------------------------------------------End of episode 21 loop--------------------\n",
      "----- starting point of Episode 22 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 0) with action 3 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 22 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 4, 0.8999999999999998, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 6) with action 3 to next state (4, 6): pull reward: 0\n",
      "----- starting point of Episode 22 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 5) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 22 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.8999999999999999, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 5) with action 0 to next state (4, 5): pull reward: 0\n",
      "comm next state for agent 1: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 22 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.8999999999999999, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fc9ed0>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fc98d0>, <__main__.Case object at 0x7f34c3fc8c70>, <__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fc9e70>, <__main__.Case object at 0x7f34c3fcb610>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fc8970>, <__main__.Case object at 0x7f34c3fcbd60>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fcb3a0>, <__main__.Case object at 0x7f34c3fcb880>, <__main__.Case object at 0x7f34c3fc9cf0>, <__main__.Case object at 0x7f34c3fc8c10>, <__main__.Case object at 0x7f34c3fc9bd0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f9f6a0>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fc8850>, <__main__.Case object at 0x7f34c3fc9150>, <__main__.Case object at 0x7f34c3fc92d0>, <__main__.Case object at 0x7f34c3fcb9d0>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fcb910>, <__main__.Case object at 0x7f34c3fcadd0>, <__main__.Case object at 0x7f34c3fcbe20>, <__main__.Case object at 0x7f34c3fcb400>, <__main__.Case object at 0x7f34c3fcb760>, <__main__.Case object at 0x7f34c3fc8dc0>, <__main__.Case object at 0x7f34c3fc9330>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.7, time steps: 16\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.8999999999999998)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.7, time steps: 16\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fcb2e0>, <__main__.Case object at 0x7f34c3fca290>, <__main__.Case object at 0x7f34c3fcb520>, <__main__.Case object at 0x7f34c3fca230>, <__main__.Case object at 0x7f34c3fc8ee0>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fc84f0>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fca530>, <__main__.Case object at 0x7f34c3fcb190>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fc91b0>, <__main__.Case object at 0x7f34c3fc9810>, <__main__.Case object at 0x7f34c3fca050>, <__main__.Case object at 0x7f34c3fc8d00>, <__main__.Case object at 0x7f34c3fc8a30>, <__main__.Case object at 0x7f34c3fc91e0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fc96c0>, <__main__.Case object at 0x7f34c3fc95d0>, <__main__.Case object at 0x7f34c3fcab90>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fcae90>, <__main__.Case object at 0x7f34c3fcb460>, <__main__.Case object at 0x7f34c3fca170>, <__main__.Case object at 0x7f34c3fcbd90>, <__main__.Case object at 0x7f34c3fcba00>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fc9f30>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fc9210>, <__main__.Case object at 0x7f34c3fcba90>, <__main__.Case object at 0x7f34c3fca650>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.9999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.9999999999999998, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.7000000000000001, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.7000000000000001, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.8, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.9, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.9, time steps: 16\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.9999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.9999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.7000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7000000000000001, time steps: 6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.8, time steps: 9\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.9, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.9, time steps: 16\n",
      "Episode: 22, Total Steps: 17, Total Rewards: [84, 86], Status Episode: True\n",
      "------------------------------------------End of episode 22 loop--------------------\n",
      "----- starting point of Episode 23 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 6) with action 3 to next state (4, 6): pull reward: 0\n",
      "----- starting point of Episode 23 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 6) with action 3 to next state (4, 6): pull reward: 0\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 23 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9999999999999999, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 23 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9999999999999999, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3fca650>, <__main__.Case object at 0x7f34c3fca1d0>, <__main__.Case object at 0x7f34c3fcb5e0>, <__main__.Case object at 0x7f34c3fc9240>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fcb4c0>, <__main__.Case object at 0x7f34c3fc8430>, <__main__.Case object at 0x7f34c3fc9990>, <__main__.Case object at 0x7f34c3fc8e50>, <__main__.Case object at 0x7f34c3fcbd00>, <__main__.Case object at 0x7f34c3fc9690>, <__main__.Case object at 0x7f34c3fc9db0>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fcaad0>, <__main__.Case object at 0x7f34c3fc90f0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c3f88a30>, <__main__.Case object at 0x7f34c3fc8fd0>, <__main__.Case object at 0x7f34c3fcad10>, <__main__.Case object at 0x7f34c3fc98a0>, <__main__.Case object at 0x7f34c3fcaa70>, <__main__.Case object at 0x7f34c3fc8910>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fc9570>, <__main__.Case object at 0x7f34c3fcb580>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fcace0>, <__main__.Case object at 0x7f34c3fc9e40>, <__main__.Case object at 0x7f34c3fc83d0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.6, time steps: 16\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.6, time steps: 16\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3fcba60>, <__main__.Case object at 0x7f34c3fc8cd0>, <__main__.Case object at 0x7f34c3fcb790>, <__main__.Case object at 0x7f34c3fc91e0>, <__main__.Case object at 0x7f34c3fca4a0>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fcb370>, <__main__.Case object at 0x7f34c3fc95a0>, <__main__.Case object at 0x7f34c3fca9e0>, <__main__.Case object at 0x7f34c3fc9840>, <__main__.Case object at 0x7f34c3fca980>, <__main__.Case object at 0x7f34c3fc92a0>, <__main__.Case object at 0x7f34c3fcb550>, <__main__.Case object at 0x7f34c3fc9390>, <__main__.Case object at 0x7f34c3fca1a0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f72170>, <__main__.Case object at 0x7f34c3fcb430>, <__main__.Case object at 0x7f34c3fca2c0>, <__main__.Case object at 0x7f34c3fc8e80>, <__main__.Case object at 0x7f34c3fc9330>, <__main__.Case object at 0x7f34c3fc94b0>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fcba90>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fc8460>, <__main__.Case object at 0x7f34c3fc9cf0>, <__main__.Case object at 0x7f34c3fcb070>, <__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fc9f90>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.8999999999999998, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6000000000000001, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.7000000000000001, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.8, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.8, time steps: 16\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.8999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6000000000000001, time steps: 6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7000000000000001, time steps: 9\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.8, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.8, time steps: 16\n",
      "Episode: 23, Total Steps: 16, Total Rewards: [85, 88], Status Episode: True\n",
      "------------------------------------------End of episode 23 loop--------------------\n",
      "----- starting point of Episode 24 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 6) with action 3 to next state (4, 6): pull reward: 0\n",
      "----- starting point of Episode 24 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 24 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f9f6a0>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fcaa70>, <__main__.Case object at 0x7f34c3fcb580>, <__main__.Case object at 0x7f34c3fc9e40>, <__main__.Case object at 0x7f34c3fc9330>, <__main__.Case object at 0x7f34c3fcba90>, <__main__.Case object at 0x7f34c3fc9cf0>, <__main__.Case object at 0x7f34c3fca650>, <__main__.Case object at 0x7f34c3fcb4c0>, <__main__.Case object at 0x7f34c3fc8e50>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fcb460>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fca230>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f88a30>, <__main__.Case object at 0x7f34c3f88970>, <__main__.Case object at 0x7f34c3fc98a0>, <__main__.Case object at 0x7f34c3fc9570>, <__main__.Case object at 0x7f34c3fc83d0>, <__main__.Case object at 0x7f34c3fc8e80>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fc8460>, <__main__.Case object at 0x7f34c3fca1d0>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fc8bb0>, <__main__.Case object at 0x7f34c3fc9150>, <__main__.Case object at 0x7f34c3fc95d0>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fc9bd0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.5, time steps: 16\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5, time steps: 16\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3fcad10>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fcace0>, <__main__.Case object at 0x7f34c3fc91b0>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fc8dc0>, <__main__.Case object at 0x7f34c3fc9990>, <__main__.Case object at 0x7f34c3fc9690>, <__main__.Case object at 0x7f34c3fc90f0>, <__main__.Case object at 0x7f34c3fc9210>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fcb3a0>, <__main__.Case object at 0x7f34c3fc80d0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f70e50>, <__main__.Case object at 0x7f34c3fc8fd0>, <__main__.Case object at 0x7f34c3fc8910>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fc8850>, <__main__.Case object at 0x7f34c3fc94b0>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fcb070>, <__main__.Case object at 0x7f34c3fcbe20>, <__main__.Case object at 0x7f34c3fc8430>, <__main__.Case object at 0x7f34c3fcbd00>, <__main__.Case object at 0x7f34c3fcaad0>, <__main__.Case object at 0x7f34c3fcb790>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fc8520>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.7999999999999998, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.5000000000000001, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6000000000000001, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.7000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.7000000000000001, time steps: 16\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.7999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5000000000000001, time steps: 6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6000000000000001, time steps: 9\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.7000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.7000000000000001, time steps: 16\n",
      "Episode: 24, Total Steps: 16, Total Rewards: [85, 88], Status Episode: True\n",
      "------------------------------------------End of episode 24 loop--------------------\n",
      "----- starting point of Episode 25 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 3 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 3) with action 1 to next state (6, 2): pull reward: 0\n",
      "----- starting point of Episode 25 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 3) with action 3 to next state (5, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 3) with action 0 to next state (5, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 3) with action 1 to next state (5, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 5) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 25 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 2) with action 2 to next state (5, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 3) with action 3 to next state (4, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 3) with action 3 to next state (3, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 3) with action 0 to next state (3, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 3) with action 4 to next state (4, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 25 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 3) with action 1 to next state (4, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 2) with action 2 to next state (4, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 3) with action 0 to next state (4, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 3) with action 4 to next state (5, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 3) with action 0 to next state (5, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 3) with action 2 to next state (5, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 25 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 4) with action 4 to next state (6, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 25 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 25 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3fcab90>, <__main__.Case object at 0x7f34c3fc96c0>, <__main__.Case object at 0x7f34c3fcb5e0>, <__main__.Case object at 0x7f34c3fc9390>, <__main__.Case object at 0x7f34c3fc8910>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fcb190>, <__main__.Case object at 0x7f34c3fca170>, <__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c3fc9e70>, <__main__.Case object at 0x7f34c3fcb370>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fcace0>, <__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fc8dc0>, <__main__.Case object at 0x7f34c3fcb550>, <__main__.Case object at 0x7f34c3fc9810>, <__main__.Case object at 0x7f34c3fc8100>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fcb250>, <__main__.Case object at 0x7f34c3fcacb0>, <__main__.Case object at 0x7f34c3fc9090>, <__main__.Case object at 0x7f34c3fcb940>, <__main__.Case object at 0x7f34c3fcb700>, <__main__.Case object at 0x7f34c3fcb880>, <__main__.Case object at 0x7f34c3fcbbe0>, <__main__.Case object at 0x7f34c3f88970>, <__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3fb3d60>, <__main__.Case object at 0x7f34c3fb3cd0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f70e50>, <__main__.Case object at 0x7f34c3f72170>, <__main__.Case object at 0x7f34c3fc98d0>, <__main__.Case object at 0x7f34c3fc9ed0>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fc8fd0>, <__main__.Case object at 0x7f34c3fcb730>, <__main__.Case object at 0x7f34c3fc91e0>, <__main__.Case object at 0x7f34c3fc92d0>, <__main__.Case object at 0x7f34c3fcae90>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fca980>, <__main__.Case object at 0x7f34c3fc9f90>, <__main__.Case object at 0x7f34c3fc9b70>, <__main__.Case object at 0x7f34c3fc8250>, <__main__.Case object at 0x7f34c3fca9e0>, <__main__.Case object at 0x7f34c3fcbac0>, <__main__.Case object at 0x7f34c3f71060>, <__main__.Case object at 0x7f34c3fc9690>, <__main__.Case object at 0x7f34c3fca7d0>, <__main__.Case object at 0x7f34c3fcb4f0>, <__main__.Case object at 0x7f34c3fc9420>, <__main__.Case object at 0x7f34c3fcabc0>, <__main__.Case object at 0x7f34c3fc9720>, <__main__.Case object at 0x7f34c3f889a0>, <__main__.Case object at 0x7f34c3f93cd0>, <__main__.Case object at 0x7f34c3fb2740>, <__main__.Case object at 0x7f34c3fb3bb0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.9, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.4, time steps: 16\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) is empty. Temporary case base stored to the case base: ((5, 4), 4, 0.5)\n",
      "Episode succeeded, case (5, 3) is empty. Temporary case base stored to the case base: ((5, 3), 2, 0.5)\n",
      "Episode succeeded, case (5, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 4, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 2, 0.5)\n",
      "Episode succeeded, case (4, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 3) is empty. Temporary case base stored to the case base: ((3, 3), 4, 0.5)\n",
      "Episode succeeded, case (3, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) is empty. Temporary case base stored to the case base: ((5, 2), 2, 0.5)\n",
      "Episode succeeded, case (5, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.9, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 4), solution: 4, tv: 0.5, time steps: 26\n",
      "cases content after RETAIN, problem: (5, 3), solution: 2, tv: 0.5, time steps: 25\n",
      "cases content after RETAIN, problem: (4, 3), solution: 4, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (4, 2), solution: 2, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (3, 3), solution: 4, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (5, 2), solution: 2, tv: 0.5, time steps: 15\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fcbb20>, <__main__.Case object at 0x7f34c3fcb640>, <__main__.Case object at 0x7f34c3fcb430>, <__main__.Case object at 0x7f34c3fcb2e0>, <__main__.Case object at 0x7f34c3fc98a0>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fcb610>, <__main__.Case object at 0x7f34c3fcbd60>, <__main__.Case object at 0x7f34c3fca530>, <__main__.Case object at 0x7f34c3fca2c0>, <__main__.Case object at 0x7f34c3fc9240>, <__main__.Case object at 0x7f34c3fcb760>, <__main__.Case object at 0x7f34c3fc9780>, <__main__.Case object at 0x7f34c3fcb9d0>, <__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c3fc91b0>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fc83a0>, <__main__.Case object at 0x7f34c3fc9870>, <__main__.Case object at 0x7f34c3fca4a0>, <__main__.Case object at 0x7f34c3fc9750>, <__main__.Case object at 0x7f34c3fc8d00>, <__main__.Case object at 0x7f34c3fcbf40>, <__main__.Case object at 0x7f34c3fc9990>, <__main__.Case object at 0x7f34c3fcbcd0>, <__main__.Case object at 0x7f34c3fcb280>, <__main__.Case object at 0x7f34c3fcbe50>, <__main__.Case object at 0x7f34c3fc97b0>, <__main__.Case object at 0x7f34c3f90370>, <__main__.Case object at 0x7f34c3f916c0>, <__main__.Case object at 0x7f34c3fb2920>, <__main__.Case object at 0x7f34c3fb3e80>, <__main__.Case object at 0x7f34c3fca680>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fc80d0>, <__main__.Case object at 0x7f34c3fcbd90>, <__main__.Case object at 0x7f34c3fcb910>, <__main__.Case object at 0x7f34c3fcba00>, <__main__.Case object at 0x7f34c3fc9ab0>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fc8970>, <__main__.Case object at 0x7f34c3fcb520>, <__main__.Case object at 0x7f34c3fca320>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6999999999999998, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.40000000000000013, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.40000000000000013, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.5000000000000001, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6000000000000001, time steps: 16\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6999999999999998, time steps: 16\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5000000000000001, time steps: 9\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6000000000000001, time steps: 16\n",
      "Episode: 25, Total Steps: 33, Total Rewards: [68, 86], Status Episode: True\n",
      "------------------------------------------End of episode 25 loop--------------------\n",
      "----- starting point of Episode 26 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 6) with action 3 to next state (4, 6): pull reward: 0\n",
      "----- starting point of Episode 26 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 6) with action 0 to next state (4, 6): pull reward: 0\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 26 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3f90370>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fc8430>, <__main__.Case object at 0x7f34c3fcbc70>, <__main__.Case object at 0x7f34c3fcba60>, <__main__.Case object at 0x7f34c3fca7d0>, <__main__.Case object at 0x7f34c3fc9720>, <__main__.Case object at 0x7f34c3fca1a0>, <__main__.Case object at 0x7f34c3fcb4c0>, <__main__.Case object at 0x7f34c3fc96c0>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fca170>, <__main__.Case object at 0x7f34c3fcace0>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fcbb20>, <__main__.Case object at 0x7f34c3fcb2e0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f72170>, <__main__.Case object at 0x7f34c3f70e50>, <__main__.Case object at 0x7f34c3f90340>, <__main__.Case object at 0x7f34c3fc9f30>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fc9690>, <__main__.Case object at 0x7f34c3fcabc0>, <__main__.Case object at 0x7f34c3fcb070>, <__main__.Case object at 0x7f34c3fc9330>, <__main__.Case object at 0x7f34c3fc9db0>, <__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fc9810>, <__main__.Case object at 0x7f34c3fcbbe0>, <__main__.Case object at 0x7f34c3fcadd0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1.0, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.40000000000000013, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.40000000000000013, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.40000000000000013, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 4, tv: 0.4, time steps: 26\n",
      "case content after REVISE for agent 0, problem: (5, 3), solution: 2, tv: 0.4, time steps: 25\n",
      "case content after REVISE for agent 0, problem: (4, 3), solution: 4, tv: 0.4, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 2, tv: 0.4, time steps: 21\n",
      "case content after REVISE for agent 0, problem: (3, 3), solution: 4, tv: 0.4, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 2, tv: 0.4, time steps: 15\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1.0, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f93d00>, <__main__.Case object at 0x7f34c3fca230>, <__main__.Case object at 0x7f34c3fc9840>, <__main__.Case object at 0x7f34c3fcb580>, <__main__.Case object at 0x7f34c3fc9570>, <__main__.Case object at 0x7f34c3fcb970>, <__main__.Case object at 0x7f34c3fc9420>, <__main__.Case object at 0x7f34c3fc9150>, <__main__.Case object at 0x7f34c3fcbb80>, <__main__.Case object at 0x7f34c3fcab90>, <__main__.Case object at 0x7f34c3fc9390>, <__main__.Case object at 0x7f34c3fcb190>, <__main__.Case object at 0x7f34c3fc8130>, <__main__.Case object at 0x7f34c3fcb550>, <__main__.Case object at 0x7f34c3fcb940>, <__main__.Case object at 0x7f34c3fcb430>, <__main__.Case object at 0x7f34c3fcbe20>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f88970>, <__main__.Case object at 0x7f34c3f916c0>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fc95a0>, <__main__.Case object at 0x7f34c3fc9cf0>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fcb4f0>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fcb460>, <__main__.Case object at 0x7f34c3fcb5e0>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fc8dc0>, <__main__.Case object at 0x7f34c3fca290>, <__main__.Case object at 0x7f34c3fcb640>, <__main__.Case object at 0x7f34c3fcb610>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.5999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.40000000000000013, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.5000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.5000000000000001, time steps: 16\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.9)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) is empty. Temporary case base stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "Episode: 26, Total Steps: 17, Total Rewards: [84, 88], Status Episode: True\n",
      "------------------------------------------End of episode 26 loop--------------------\n",
      "----- starting point of Episode 27 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 1.0, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 5) with action 4 to next state (7, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (7, 5) with action 4 to next state (8, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (8, 5) with action 2 to next state (8, 6): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (8, 6) with action 4 to next state (9, 6): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (9, 6) with action 4 to next state (9, 6): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 27 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (9, 6) with action 3 to next state (8, 6): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (8, 6) with action 0 to next state (8, 6): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (8, 6) with action 1 to next state (8, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (8, 5) with action 0 to next state (8, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (8, 5) with action 1 to next state (8, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 27 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (8, 4) with action 1 to next state (8, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (8, 3) with action 2 to next state (8, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (8, 4) with action 4 to next state (9, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (9, 4) with action 4 to next state (9, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (9, 4) with action 2 to next state (9, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (9, 5) with action 3 to next state (8, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (8, 5) with action 3 to next state (7, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (7, 5) with action 1 to next state (7, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (7, 4) with action 4 to next state (8, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (8, 4) with action 3 to next state (7, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (7, 4) with action 3 to next state (6, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 6) with action 3 to next state (5, 6): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 27 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fcb070>, <__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c3fc9cf0>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fcb640>, <__main__.Case object at 0x7f34c3fcbac0>, <__main__.Case object at 0x7f34c3fcab00>, <__main__.Case object at 0x7f34c3fc8cd0>, <__main__.Case object at 0x7f34c3fc8e80>, <__main__.Case object at 0x7f34c3fc8250>, <__main__.Case object at 0x7f34c3fcba00>, <__main__.Case object at 0x7f34c3fc9f00>, <__main__.Case object at 0x7f34c3fc9ab0>, <__main__.Case object at 0x7f34c3fcbe50>, <__main__.Case object at 0x7f34c3fcbc10>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3f70e50>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fcb760>, <__main__.Case object at 0x7f34c3fc9930>, <__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fc8f70>, <__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3fb3bb0>, <__main__.Case object at 0x7f34c3fb3b80>, <__main__.Case object at 0x7f34c3fb0340>, <__main__.Case object at 0x7f34c3fbee90>, <__main__.Case object at 0x7f34c3fbcd90>, <__main__.Case object at 0x7f34c3fbf8b0>, <__main__.Case object at 0x7f34c3fbc9d0>, <__main__.Case object at 0x7f34c3fbcee0>, <__main__.Case object at 0x7f34c3fbf280>, <__main__.Case object at 0x7f34c3fbca90>, <__main__.Case object at 0x7f34c3fbd960>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f889a0>, <__main__.Case object at 0x7f34c3f93d00>, <__main__.Case object at 0x7f34c3f72170>, <__main__.Case object at 0x7f34c3fcabc0>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fc95a0>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fc9f90>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fcb370>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fc8e20>, <__main__.Case object at 0x7f34c3f88a30>, <__main__.Case object at 0x7f34c3fc95d0>, <__main__.Case object at 0x7f34c3fcb280>, <__main__.Case object at 0x7f34c3fc8d00>, <__main__.Case object at 0x7f34c3fc8ee0>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fc8280>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fc9090>, <__main__.Case object at 0x7f34c3fca770>, <__main__.Case object at 0x7f34c3f9f640>, <__main__.Case object at 0x7f34c3fc9810>, <__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3fb2980>, <__main__.Case object at 0x7f34c3fb29b0>, <__main__.Case object at 0x7f34c3fca230>, <__main__.Case object at 0x7f34c3fbedd0>, <__main__.Case object at 0x7f34c3fbc850>, <__main__.Case object at 0x7f34c3fbdff0>, <__main__.Case object at 0x7f34c3fcafb0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.9, time steps: 16\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 4) is empty. Temporary case base stored to the case base: ((7, 4), 3, 0.5)\n",
      "Episode succeeded, case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 3, 0.5)\n",
      "Episode succeeded, case (7, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 1, 0.5)\n",
      "Episode succeeded, case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (9, 5) is empty. Temporary case base stored to the case base: ((9, 5), 3, 0.5)\n",
      "Episode succeeded, case (9, 4) is empty. Temporary case base stored to the case base: ((9, 4), 2, 0.5)\n",
      "Episode succeeded, case (9, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 6) is empty. Temporary case base stored to the case base: ((8, 6), 1, 0.5)\n",
      "Episode succeeded, case (8, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 6) is empty. Temporary case base stored to the case base: ((9, 6), 3, 0.5)\n",
      "Episode succeeded, case (9, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.9, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 4), solution: 3, tv: 0.5, time steps: 31\n",
      "cases content after RETAIN, problem: (8, 4), solution: 3, tv: 0.5, time steps: 30\n",
      "cases content after RETAIN, problem: (7, 5), solution: 1, tv: 0.5, time steps: 28\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (9, 5), solution: 3, tv: 0.5, time steps: 26\n",
      "cases content after RETAIN, problem: (9, 4), solution: 2, tv: 0.5, time steps: 25\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (8, 6), solution: 1, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (9, 6), solution: 3, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f916c0>, <__main__.Case object at 0x7f34c3fcbd60>, <__main__.Case object at 0x7f34c3fc9690>, <__main__.Case object at 0x7f34c3fc9db0>, <__main__.Case object at 0x7f34c3fcaef0>, <__main__.Case object at 0x7f34c3fcb4f0>, <__main__.Case object at 0x7f34c3fcb5e0>, <__main__.Case object at 0x7f34c3fca290>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fcbd90>, <__main__.Case object at 0x7f34c3fcbd00>, <__main__.Case object at 0x7f34c3fcb130>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fc8070>, <__main__.Case object at 0x7f34c3fc9390>, <__main__.Case object at 0x7f34c3fc8460>, <__main__.Case object at 0x7f34c3fc8eb0>, <__main__.Case object at 0x7f34c3fcb6d0>, <__main__.Case object at 0x7f34c3fcb580>, <__main__.Case object at 0x7f34c3fcbf40>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fc9780>, <__main__.Case object at 0x7f34c3fc8c70>, <__main__.Case object at 0x7f34c3fc94e0>, <__main__.Case object at 0x7f34c3fcbee0>, <__main__.Case object at 0x7f34c3fc93f0>, <__main__.Case object at 0x7f34c3fc8a30>, <__main__.Case object at 0x7f34c3fc8700>, <__main__.Case object at 0x7f34c3fc8d60>, <__main__.Case object at 0x7f34c3fc9240>, <__main__.Case object at 0x7f34c3fb3f10>, <__main__.Case object at 0x7f34c3fca140>, <__main__.Case object at 0x7f34c3fbdd80>, <__main__.Case object at 0x7f34c3fbf2e0>, <__main__.Case object at 0x7f34c3fca950>, <__main__.Case object at 0x7f34c3fbf460>, <__main__.Case object at 0x7f34c3fbe380>, <__main__.Case object at 0x7f34c3fbf6d0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f90340>, <__main__.Case object at 0x7f34c3fca530>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fc9330>, <__main__.Case object at 0x7f34c3fcbbe0>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fcb460>, <__main__.Case object at 0x7f34c3fc8dc0>, <__main__.Case object at 0x7f34c3fc8430>, <__main__.Case object at 0x7f34c3fcbfd0>, <__main__.Case object at 0x7f34c3fc8bb0>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.4999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.40000000000000013, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.40000000000000013, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.9, time steps: 6\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1.0)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.4999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.9, time steps: 6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "Episode: 27, Total Steps: 38, Total Rewards: [63, 88], Status Episode: True\n",
      "------------------------------------------End of episode 27 loop--------------------\n",
      "----- starting point of Episode 28 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 6)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 0) with action 1 to next state (9, 0): pull reward: 0\n",
      "----- starting point of Episode 28 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 0) with action 3 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 28 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 4, 0.4999999999999999, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 5) with action 2 to next state (6, 6): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 28 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f88a30>, <__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3fb3df0>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fcabc0>, <__main__.Case object at 0x7f34c3fc8e20>, <__main__.Case object at 0x7f34c3fc9750>, <__main__.Case object at 0x7f34c3fc9660>, <__main__.Case object at 0x7f34c3fc9330>, <__main__.Case object at 0x7f34c3fc8dc0>, <__main__.Case object at 0x7f34c3fcb610>, <__main__.Case object at 0x7f34c3fc9f30>, <__main__.Case object at 0x7f34c3fcbac0>, <__main__.Case object at 0x7f34c3fc8250>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3fcbd60>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f70e50>, <__main__.Case object at 0x7f34c3f916c0>, <__main__.Case object at 0x7f34c3fb0340>, <__main__.Case object at 0x7f34c3fcad70>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fc9990>, <__main__.Case object at 0x7f34c3fb3f70>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fcb460>, <__main__.Case object at 0x7f34c3fc8bb0>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fcb640>, <__main__.Case object at 0x7f34c3fc8e80>, <__main__.Case object at 0x7f34c3fcbe50>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.8, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (7, 4), solution: 3, tv: 0.4, time steps: 31\n",
      "case content after REVISE for agent 0, problem: (8, 4), solution: 3, tv: 0.4, time steps: 30\n",
      "case content after REVISE for agent 0, problem: (7, 5), solution: 1, tv: 0.4, time steps: 28\n",
      "case content after REVISE for agent 0, problem: (8, 5), solution: 3, tv: 0.4, time steps: 27\n",
      "case content after REVISE for agent 0, problem: (9, 5), solution: 3, tv: 0.4, time steps: 26\n",
      "case content after REVISE for agent 0, problem: (9, 4), solution: 2, tv: 0.4, time steps: 25\n",
      "case content after REVISE for agent 0, problem: (8, 3), solution: 2, tv: 0.4, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (8, 6), solution: 1, tv: 0.4, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (9, 6), solution: 3, tv: 0.4, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.4999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.8, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9, time steps: 15\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c3fb2980>, <__main__.Case object at 0x7f34c3fcafb0>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fcac80>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fc90f0>, <__main__.Case object at 0x7f34c3fcba90>, <__main__.Case object at 0x7f34c3fc8310>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fcbfd0>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fc8fa0>, <__main__.Case object at 0x7f34c3fc8cd0>, <__main__.Case object at 0x7f34c3fc9f00>, <__main__.Case object at 0x7f34c3fcb9d0>, <__main__.Case object at 0x7f34c3fc80d0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f9f640>, <__main__.Case object at 0x7f34c3fb3d60>, <__main__.Case object at 0x7f34c3fb2920>, <__main__.Case object at 0x7f34c3fc95a0>, <__main__.Case object at 0x7f34c3fcb370>, <__main__.Case object at 0x7f34c3fcb940>, <__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c3fca770>, <__main__.Case object at 0x7f34c3fcbbe0>, <__main__.Case object at 0x7f34c3fc8430>, <__main__.Case object at 0x7f34c3fca9e0>, <__main__.Case object at 0x7f34c3fcab00>, <__main__.Case object at 0x7f34c3fcba00>, <__main__.Case object at 0x7f34c3fcb3a0>, <__main__.Case object at 0x7f34c3fcb700>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.9, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.5999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.8, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.9, time steps: 9\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.9, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.8, time steps: 6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.9, time steps: 9\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "Episode: 28, Total Steps: 17, Total Rewards: [84, 85], Status Episode: True\n",
      "------------------------------------------End of episode 28 loop--------------------\n",
      "----- starting point of Episode 29 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 0) with action 0 to next state (7, 0): pull reward: 0\n",
      "----- starting point of Episode 29 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 29 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 29 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3f9f6a0>, <__main__.Case object at 0x7f34c3f9f6d0>, <__main__.Case object at 0x7f34c3fb3d30>, <__main__.Case object at 0x7f34c3fc8f10>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fc9720>, <__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c3fcba00>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fc9750>, <__main__.Case object at 0x7f34c3fcb610>, <__main__.Case object at 0x7f34c3fcafb0>, <__main__.Case object at 0x7f34c3fcac80>, <__main__.Case object at 0x7f34c3fcba90>, <__main__.Case object at 0x7f34c3fca320>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c3f916c0>, <__main__.Case object at 0x7f34c3fb3b80>, <__main__.Case object at 0x7f34c3f93ac0>, <__main__.Case object at 0x7f34c3fc98a0>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fcb940>, <__main__.Case object at 0x7f34c3fca9e0>, <__main__.Case object at 0x7f34c3fcb3a0>, <__main__.Case object at 0x7f34c3fc8e20>, <__main__.Case object at 0x7f34c3fc8dc0>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fc9db0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.7000000000000001, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.8, time steps: 15\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.7000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.8, time steps: 15\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3fb3f70>, <__main__.Case object at 0x7f34c3fb3f10>, <__main__.Case object at 0x7f34c3fc9e70>, <__main__.Case object at 0x7f34c3fc9990>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fc8a00>, <__main__.Case object at 0x7f34c3fcbbe0>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fcabc0>, <__main__.Case object at 0x7f34c3fc9330>, <__main__.Case object at 0x7f34c3fcbac0>, <__main__.Case object at 0x7f34c3fc9090>, <__main__.Case object at 0x7f34c3fc90f0>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fc8cd0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3fb3e80>, <__main__.Case object at 0x7f34c3fcbc40>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fc9a50>, <__main__.Case object at 0x7f34c3fc9420>, <__main__.Case object at 0x7f34c3fca770>, <__main__.Case object at 0x7f34c3fc8430>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fc9660>, <__main__.Case object at 0x7f34c3fc9f30>, <__main__.Case object at 0x7f34c3fc8250>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fc8310>, <__main__.Case object at 0x7f34c3fc8fa0>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 1.0, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.4999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.7000000000000001, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.7000000000000001, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.8, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.9, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.9, time steps: 16\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1.0, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.4999999999999999, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.7000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7000000000000001, time steps: 6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.8, time steps: 9\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.9, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.9, time steps: 16\n",
      "Episode: 29, Total Steps: 16, Total Rewards: [85, 87], Status Episode: True\n",
      "------------------------------------------End of episode 29 loop--------------------\n",
      "----- starting point of Episode 30 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 2) with action 1 to next state (6, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 30 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f70e50>, <__main__.Case object at 0x7f34c3fb29b0>, <__main__.Case object at 0x7f34c3fb0340>, <__main__.Case object at 0x7f34c3fcbcd0>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fc8dc0>, <__main__.Case object at 0x7f34c3fcbc40>, <__main__.Case object at 0x7f34c3fca770>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fc8310>, <__main__.Case object at 0x7f34c3fcbe50>, <__main__.Case object at 0x7f34c3fc9810>, <__main__.Case object at 0x7f34c3fcb970>, <__main__.Case object at 0x7f34c3fcb640>, <__main__.Case object at 0x7f34c3fc95d0>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fcb910>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3f9f6d0>, <__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3fb3bb0>, <__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c3fc8e20>, <__main__.Case object at 0x7f34c3fc9db0>, <__main__.Case object at 0x7f34c3fc9420>, <__main__.Case object at 0x7f34c3fc9f30>, <__main__.Case object at 0x7f34c3fcad70>, <__main__.Case object at 0x7f34c3fc8bb0>, <__main__.Case object at 0x7f34c3fc9f90>, <__main__.Case object at 0x7f34c3fc88b0>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fc9930>, <__main__.Case object at 0x7f34c3fcad10>, <__main__.Case object at 0x7f34c3f90340>, <__main__.Case object at 0x7f34c3fcb4f0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.6000000000000001, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.6000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f9f640>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3fb2920>, <__main__.Case object at 0x7f34c3fc98a0>, <__main__.Case object at 0x7f34c3fcb3a0>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fc9a50>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fcbaf0>, <__main__.Case object at 0x7f34c3fcbb80>, <__main__.Case object at 0x7f34c3fcbb20>, <__main__.Case object at 0x7f34c3fc9690>, <__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c3fca1a0>, <__main__.Case object at 0x7f34c3fcab90>, <__main__.Case object at 0x7f34c3fcae90>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fca1d0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f93ac0>, <__main__.Case object at 0x7f34c3fb3b80>, <__main__.Case object at 0x7f34c3fb3d30>, <__main__.Case object at 0x7f34c3fcb730>, <__main__.Case object at 0x7f34c3fca9e0>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fc8430>, <__main__.Case object at 0x7f34c3fc80d0>, <__main__.Case object at 0x7f34c3fca230>, <__main__.Case object at 0x7f34c3fc8100>, <__main__.Case object at 0x7f34c3fc9e70>, <__main__.Case object at 0x7f34c3fcb280>, <__main__.Case object at 0x7f34c3fcbc70>, <__main__.Case object at 0x7f34c3fca170>, <__main__.Case object at 0x7f34c3fca2c0>, <__main__.Case object at 0x7f34c3fca290>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.3999999999999999, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6000000000000001, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.7000000000000001, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.8, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.8, time steps: 16\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6000000000000001, time steps: 6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.7000000000000001, time steps: 9\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.8, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.8, time steps: 16\n",
      "Episode: 30, Total Steps: 18, Total Rewards: [83, 88], Status Episode: True\n",
      "------------------------------------------End of episode 30 loop--------------------\n",
      "----- starting point of Episode 31 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 2), 2, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 5) with action 0 to next state (6, 5): pull reward: 0\n",
      "----- starting point of Episode 31 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 3), 2, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 4) with action 3 to next state (5, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 4) with action 2 to next state (5, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 5) with action 3 to next state (4, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 31 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 5), 1, 1, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f93ac0>, <__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3fb29b0>, <__main__.Case object at 0x7f34c3fca1d0>, <__main__.Case object at 0x7f34c3fc8e20>, <__main__.Case object at 0x7f34c3fc8bb0>, <__main__.Case object at 0x7f34c3fc9930>, <__main__.Case object at 0x7f34c3fcb460>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fcac80>, <__main__.Case object at 0x7f34c3fc8160>, <__main__.Case object at 0x7f34c3fcab00>, <__main__.Case object at 0x7f34c3fc8f10>, <__main__.Case object at 0x7f34c3fca320>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c3f916c0>, <__main__.Case object at 0x7f34c3f9f6a0>, <__main__.Case object at 0x7f34c3fb2920>, <__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3fcad70>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fcb5e0>, <__main__.Case object at 0x7f34c3fc9c30>, <__main__.Case object at 0x7f34c3fc9090>, <__main__.Case object at 0x7f34c3fc96c0>, <__main__.Case object at 0x7f34c3fc8130>, <__main__.Case object at 0x7f34c3fc8400>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.5000000000000001, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 5), 1, 0.5, 14)\n",
      "Episode succeeded, case (5, 5) is empty. Temporary case base stored to the case base: ((5, 5), 3, 0.5)\n",
      "Episode succeeded, case (5, 4) is empty. Temporary case base stored to the case base: ((5, 4), 2, 0.5)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 3, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 3), 2, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 2), 2, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 1), 2, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 0), 2, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 0), 4, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 0), 4, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 0), 4, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((2, 0), 4, 0.5, 14)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 1, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 1, time steps: 6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 5), solution: 3, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 11\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f70e50>, <__main__.Case object at 0x7f34c3fb3b80>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3fc8fa0>, <__main__.Case object at 0x7f34c3fc9f30>, <__main__.Case object at 0x7f34c3fc88b0>, <__main__.Case object at 0x7f34c3fca560>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fca230>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fc8940>, <__main__.Case object at 0x7f34c3fc9c00>, <__main__.Case object at 0x7f34c3fc9c90>, <__main__.Case object at 0x7f34c3fcb2b0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3fb3df0>, <__main__.Case object at 0x7f34c3fb0340>, <__main__.Case object at 0x7f34c3fca290>, <__main__.Case object at 0x7f34c3fc8670>, <__main__.Case object at 0x7f34c3fc9f90>, <__main__.Case object at 0x7f34c3fcad10>, <__main__.Case object at 0x7f34c3fc9b70>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fc9990>, <__main__.Case object at 0x7f34c3fcafb0>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.9, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.5000000000000001, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.6000000000000001, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.7000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.7000000000000001, time steps: 16\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 5), 1, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 6), 1, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 6), 3, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 6), 3, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 5), 2, 0.5, 14)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 2, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 3), 2, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 2), 2, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 1), 2, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 0), 2, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 0), 3, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((8, 0), 3, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((9, 0), 3, 0.5, 14)\n",
      "Integrated case process. comm case (4, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 1)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 4, 1)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 1)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 1)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 1)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.9, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 0), solution: 4, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5000000000000001, time steps: 6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6000000000000001, time steps: 9\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.7000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.7000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 1, time steps: 16\n",
      "Episode: 31, Total Steps: 14, Total Rewards: [87, 87], Status Episode: True\n",
      "------------------------------------------End of episode 31 loop--------------------\n",
      "----- starting point of Episode 32 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.5, 14)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 1, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.5, 14)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 1, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.5, 14)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 0), 4, 0.5, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.5, 14)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 0), 4, 0.5, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.5, 14)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 0), 4, 0.5, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.5, 14)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((5, 0), 4, 0.5, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.5, 14)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 0), 2, 0.5, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 0.5, 14)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((6, 1), 2, 0.5, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 0.5, 14)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 2) with action 3 to next state (5, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 0.5, 14)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 2) with action 1 to next state (5, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 0.5, 14)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 1) with action 3 to next state (5, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 0.5, 14)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 1) with action 3 to next state (5, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 32 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((6, 6), 3, 0.5, 14)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 1) with action 3 to next state (5, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f70e50>, <__main__.Case object at 0x7f34c3fb3d30>, <__main__.Case object at 0x7f34c3fb3bb0>, <__main__.Case object at 0x7f34c3fc95d0>, <__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c3fc8dc0>, <__main__.Case object at 0x7f34c3fc8250>, <__main__.Case object at 0x7f34c3fcbc70>, <__main__.Case object at 0x7f34c3fcb610>, <__main__.Case object at 0x7f34c3fcb280>, <__main__.Case object at 0x7f34c3fcb070>, <__main__.Case object at 0x7f34c3fcb730>, <__main__.Case object at 0x7f34c3fca770>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3fb3e80>, <__main__.Case object at 0x7f34c3fb0340>, <__main__.Case object at 0x7f34c3f90340>, <__main__.Case object at 0x7f34c3fc8280>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fca2c0>, <__main__.Case object at 0x7f34c3fcb970>, <__main__.Case object at 0x7f34c3fc8a00>, <__main__.Case object at 0x7f34c3fca170>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.3, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.3, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 4, tv: 0.3, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 4, tv: 0.3, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 4, tv: 0.3, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (2, 0), solution: 4, tv: 0.3, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.8, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.8, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.5000000000000001, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 3, tv: 0.5, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.5, time steps: 11\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.5)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.5)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.5)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.5)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 0.5)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.8, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.8, time steps: 6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 5), solution: 3, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 11\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f9f6a0>, <__main__.Case object at 0x7f34c3fb3df0>, <__main__.Case object at 0x7f34c3fb3f10>, <__main__.Case object at 0x7f34c3fcb700>, <__main__.Case object at 0x7f34c3fc9e70>, <__main__.Case object at 0x7f34c3fc9810>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fc9420>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fcbd30>, <__main__.Case object at 0x7f34c3fcbcd0>, <__main__.Case object at 0x7f34c3fc9bd0>, <__main__.Case object at 0x7f34c3fc8430>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3fb3cd0>, <__main__.Case object at 0x7f34c3fb3f70>, <__main__.Case object at 0x7f34c3fcb640>, <__main__.Case object at 0x7f34c3fc9c30>, <__main__.Case object at 0x7f34c3fcaad0>, <__main__.Case object at 0x7f34c3fcb4f0>, <__main__.Case object at 0x7f34c3fcbd90>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.8, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (2, 0), solution: 4, tv: 0.40000000000000013, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.40000000000000013, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.5000000000000001, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.6000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.6000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.9, time steps: 16\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 5), 1, 0.5, 13)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 6), 1, 0.5, 13)\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 6), 3, 0.5, 13)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 6), 3, 0.5, 13)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 5), 2, 0.5, 13)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 4), 2, 0.5, 13)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 3), 2, 0.5, 13)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 2), 2, 0.5, 13)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 1), 2, 0.5, 13)\n",
      "Episode succeeded, updated case base with fewer steps: ((6, 0), 2, 0.5, 13)\n",
      "Episode succeeded, updated case base with fewer steps: ((7, 0), 3, 0.5, 13)\n",
      "Episode succeeded, updated case base with fewer steps: ((8, 0), 3, 0.5, 13)\n",
      "Episode succeeded, updated case base with fewer steps: ((9, 0), 3, 0.5, 13)\n",
      "Integrated case process. comm case (6, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5)\n",
      "Integrated case process. comm case (6, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((5, 0), 4, 0.5)\n",
      "Integrated case process. comm case (4, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 0), 4, 0.5)\n",
      "Integrated case process. comm case (3, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 0), 4, 0.5)\n",
      "Integrated case process. comm case (2, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 0), 4, 0.5)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.8, time steps: 16\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5000000000000001, time steps: 9\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.6000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.6000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.9, time steps: 16\n",
      "Episode: 32, Total Steps: 13, Total Rewards: [-109, 88], Status Episode: False\n",
      "------------------------------------------End of episode 32 loop--------------------\n",
      "----- starting point of Episode 33 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.5, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.8, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.5, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 0) with action 1 to next state (7, 0): pull reward: 0\n",
      "----- starting point of Episode 33 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.5, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.8, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 0.5, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.8, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 4 to next state (4, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.8, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.8, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 2 to next state (4, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 1) with action 1 to next state (4, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 33 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.8, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.8, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.8, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 2 to next state (4, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.8, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 1) with action 3 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 33 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.8, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 3 to next state (2, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3fb3d60>, <__main__.Case object at 0x7f34c3fb3b80>, <__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3f88a30>, <__main__.Case object at 0x7f34c3fcafb0>, <__main__.Case object at 0x7f34c3fca1d0>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fc9090>, <__main__.Case object at 0x7f34c3fcba60>, <__main__.Case object at 0x7f34c3fc9b70>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fca230>, <__main__.Case object at 0x7f34c3fc9810>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fc9bd0>, <__main__.Case object at 0x7f34c3fc9750>, <__main__.Case object at 0x7f34c3fcb460>, <__main__.Case object at 0x7f34c3fc90f0>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fc87c0>, <__main__.Case object at 0x7f34c3fcbee0>, <__main__.Case object at 0x7f34c3fcaa70>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f70e50>, <__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3fb3ee0>, <__main__.Case object at 0x7f34c3f9f640>, <__main__.Case object at 0x7f34c3f93cd0>, <__main__.Case object at 0x7f34c3fcad10>, <__main__.Case object at 0x7f34c3fc9f30>, <__main__.Case object at 0x7f34c3fca290>, <__main__.Case object at 0x7f34c3fca170>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fc9c00>, <__main__.Case object at 0x7f34c3fc9330>, <__main__.Case object at 0x7f34c3fc9420>, <__main__.Case object at 0x7f34c3fcbcd0>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c3fcbfd0>, <__main__.Case object at 0x7f34c3fcb790>, <__main__.Case object at 0x7f34c3f9f6d0>, <__main__.Case object at 0x7f34c3fc8940>, <__main__.Case object at 0x7f34c3fcb250>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.6000000000000001, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.6000000000000001, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.5000000000000001, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 3, tv: 0.5, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.5, time steps: 11\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.8)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.8)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.8)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.8)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.8)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.8)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.8)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.8)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 0.5)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 0.5)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 0.5)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.5)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 0.5)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 0.5)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.5)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.5)\n",
      "Integrated case process. comm case (6, 1) is empty. Temporary case base stored to the case base: ((6, 1), 2, 0.5)\n",
      "Integrated case process. comm case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 2, 0.5)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6000000000000001, time steps: 9\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6000000000000001, time steps: 6\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 5), solution: 3, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 11\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fb29b0>, <__main__.Case object at 0x7f34c3fb2920>, <__main__.Case object at 0x7f34c3fb3fd0>, <__main__.Case object at 0x7f34c3f90340>, <__main__.Case object at 0x7f34c3fc9c90>, <__main__.Case object at 0x7f34c3f889d0>, <__main__.Case object at 0x7f34c3fc8160>, <__main__.Case object at 0x7f34c3fb2980>, <__main__.Case object at 0x7f34c3fcad70>, <__main__.Case object at 0x7f34c3fcbd90>, <__main__.Case object at 0x7f34c3fca9e0>, <__main__.Case object at 0x7f34c3fc8e20>, <__main__.Case object at 0x7f34c3fc88b0>, <__main__.Case object at 0x7f34c3fc8850>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fcbd30>, <__main__.Case object at 0x7f34c3fca0e0>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fca1a0>, <__main__.Case object at 0x7f34c3fc8a90>, <__main__.Case object at 0x7f34c3fc9720>, <__main__.Case object at 0x7f34c3fcb040>, <__main__.Case object at 0x7f34c3fca080>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3fc99f0>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.9, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.40000000000000013, time steps: 9\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.5000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.5000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.8, time steps: 16\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.8)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 4, 0.8)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.9, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 0), solution: 4, tv: 0.5000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 0), solution: 4, tv: 0.5000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.8, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.8, time steps: 6\n",
      "Episode: 33, Total Steps: 23, Total Rewards: [-122, 87], Status Episode: False\n",
      "------------------------------------------End of episode 33 loop--------------------\n",
      "----- starting point of Episode 34 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.6, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 4, 0.6000000000000001, 6)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.6, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.6000000000000001, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.6, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.6000000000000001, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 3) with action 1 to next state (6, 2): pull reward: 0\n",
      "----- starting point of Episode 34 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 2) with action 1 to next state (6, 1): pull reward: 0\n",
      "----- starting point of Episode 34 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 0.6, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 0), 4, 0.6000000000000001, 9)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 0.6, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 34 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 2 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 3 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 34 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 2 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 34 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 34 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 3 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 34 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 2 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 2 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 82 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 83 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 84 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 85 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 3 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 86 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 87 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 88 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 89 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 90 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 91 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 92 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 93 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 34 in steps 94 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 95 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 96 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 2 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 34 in steps 97 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 98 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 99 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 100 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 101 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 102 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 34 in steps 103 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 34 in steps 104 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 4 to next state (4, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 105 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 106 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 107 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 108 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 109 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 3 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 110 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 111 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 112 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 2 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 113 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 34 in steps 114 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 115 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 116 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 117 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 34 in steps 118 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 119 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 3 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 120 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 121 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 122 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 123 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 124 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 125 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 126 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 127 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 34 in steps 128 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 129 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 130 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 131 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 132 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 133 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 4 to next state (4, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 134 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 4 to next state (5, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 135 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 0) with action 0 to next state (5, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 136 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 0) with action 3 to next state (4, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 137 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 138 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 139 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 34 in steps 140 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 141 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 0) with action 4 to next state (5, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 142 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 0) with action 1 to next state (5, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 143 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 0) with action 0 to next state (5, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 144 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 0) with action 1 to next state (5, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 34 in steps 145 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 0) with action 2 to next state (5, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3fb3cd0>, <__main__.Case object at 0x7f34c3f90340>, <__main__.Case object at 0x7f34c3fca080>, <__main__.Case object at 0x7f34c3fca290>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fc84f0>, <__main__.Case object at 0x7f34c3fc9000>, <__main__.Case object at 0x7f34c3fca140>, <__main__.Case object at 0x7f34c3fcafb0>, <__main__.Case object at 0x7f34c3fc9090>, <__main__.Case object at 0x7f34c3fc9b70>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fcb460>, <__main__.Case object at 0x7f34c3fc87c0>, <__main__.Case object at 0x7f34c3fc9c90>, <__main__.Case object at 0x7f34c3fcbd90>, <__main__.Case object at 0x7f34c3fc8e20>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fca0e0>, <__main__.Case object at 0x7f34c3fc9720>, <__main__.Case object at 0x7f34c3fcb2e0>, <__main__.Case object at 0x7f34c3fcb6d0>, <__main__.Case object at 0x7f34c3fc94e0>, <__main__.Case object at 0x7f34c3fc89d0>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fc92a0>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fcbaf0>, <__main__.Case object at 0x7f34c3fcb9a0>, <__main__.Case object at 0x7f34c3fcb8e0>, <__main__.Case object at 0x7f34c3fc9690>, <__main__.Case object at 0x7f34c3fbc0a0>, <__main__.Case object at 0x7f34c3fbe0b0>, <__main__.Case object at 0x7f34c3fbedd0>, <__main__.Case object at 0x7f34c3fbdff0>, <__main__.Case object at 0x7f34c3fbf280>, <__main__.Case object at 0x7f34c3fbd960>, <__main__.Case object at 0x7f34c3fbc8b0>, <__main__.Case object at 0x7f34c3fbf460>, <__main__.Case object at 0x7f34c3fbf430>, <__main__.Case object at 0x7f34c3fbfd90>, <__main__.Case object at 0x7f34c3fbf0a0>, <__main__.Case object at 0x7f34c3fbe3e0>, <__main__.Case object at 0x7f34c3fbd3c0>, <__main__.Case object at 0x7f34c3fbc460>, <__main__.Case object at 0x7f34c3fbe890>, <__main__.Case object at 0x7f34c3fbd2d0>, <__main__.Case object at 0x7f34c3fbd630>, <__main__.Case object at 0x7f34c3fbd4e0>, <__main__.Case object at 0x7f34c3fbe080>, <__main__.Case object at 0x7f34c3fcb430>, <__main__.Case object at 0x7f34c3fbe3b0>, <__main__.Case object at 0x7f34c3fbceb0>, <__main__.Case object at 0x7f34c3fbe6b0>, <__main__.Case object at 0x7f34c3fbdf60>, <__main__.Case object at 0x7f34c3fbc250>, <__main__.Case object at 0x7f34c3fbc5b0>, <__main__.Case object at 0x7f34c3fbd0c0>, <__main__.Case object at 0x7f34c3fbd750>, <__main__.Case object at 0x7f34c3fbe020>, <__main__.Case object at 0x7f34c3fbe8f0>, <__main__.Case object at 0x7f34c3fbf820>, <__main__.Case object at 0x7f34c3fbf4f0>, <__main__.Case object at 0x7f34c3fbde70>, <__main__.Case object at 0x7f34c3fbd450>, <__main__.Case object at 0x7f34c3fbc190>, <__main__.Case object at 0x7f34c3fbc2e0>, <__main__.Case object at 0x7f34c3fbf880>, <__main__.Case object at 0x7f34c3fbfa30>, <__main__.Case object at 0x7f34c3fbf0d0>, <__main__.Case object at 0x7f34c3fbf730>, <__main__.Case object at 0x7f34c3fbca30>, <__main__.Case object at 0x7f34c3fbd570>, <__main__.Case object at 0x7f34c3fbdba0>, <__main__.Case object at 0x7f34c3fbe0e0>, <__main__.Case object at 0x7f34c3fbe980>, <__main__.Case object at 0x7f34c3fbfd60>, <__main__.Case object at 0x7f34c3fa8a00>, <__main__.Case object at 0x7f34c3fe7eb0>, <__main__.Case object at 0x7f34c3fe42b0>, <__main__.Case object at 0x7f34c3fe46a0>, <__main__.Case object at 0x7f34c3fe4550>, <__main__.Case object at 0x7f34c3fe4cd0>, <__main__.Case object at 0x7f34c3fe51e0>, <__main__.Case object at 0x7f34c3fe55d0>, <__main__.Case object at 0x7f34c3fe5b70>, <__main__.Case object at 0x7f34c3fe5d80>, <__main__.Case object at 0x7f34c3fe6200>, <__main__.Case object at 0x7f34c3fe6560>, <__main__.Case object at 0x7f34c3fe68f0>, <__main__.Case object at 0x7f34c3fe6bc0>, <__main__.Case object at 0x7f34c3fe5ea0>, <__main__.Case object at 0x7f34c3fe7280>, <__main__.Case object at 0x7f34c3fe4070>, <__main__.Case object at 0x7f34c3fbe7d0>, <__main__.Case object at 0x7f34c3fe6d10>, <__main__.Case object at 0x7f34c3fe4970>, <__main__.Case object at 0x7f34c3fe4d30>, <__main__.Case object at 0x7f34c3fe4f70>, <__main__.Case object at 0x7f34c3fe53f0>, <__main__.Case object at 0x7f34c3fe5630>, <__main__.Case object at 0x7f34c3fe45e0>, <__main__.Case object at 0x7f34c3fe52d0>, <__main__.Case object at 0x7f34c3fe5e70>, <__main__.Case object at 0x7f34c3fe61d0>, <__main__.Case object at 0x7f34c3fe6530>, <__main__.Case object at 0x7f34c3fe6a70>, <__main__.Case object at 0x7f34c3fe6b90>, <__main__.Case object at 0x7f34c3fe6ef0>, <__main__.Case object at 0x7f34c3fe7250>, <__main__.Case object at 0x7f34c3fe7490>, <__main__.Case object at 0x7f34c3fe5f90>, <__main__.Case object at 0x7f34c3fe4640>, <__main__.Case object at 0x7f34c3fe49a0>, <__main__.Case object at 0x7f34c3fe4f40>, <__main__.Case object at 0x7f34c3fe5300>, <__main__.Case object at 0x7f34c3fe5210>, <__main__.Case object at 0x7f34c3fe5570>, <__main__.Case object at 0x7f34c3fe56f0>, <__main__.Case object at 0x7f34c3fe58d0>, <__main__.Case object at 0x7f34c3fe5ed0>, <__main__.Case object at 0x7f34c3fe6230>, <__main__.Case object at 0x7f34c3fe64a0>, <__main__.Case object at 0x7f34c3fe6a40>, <__main__.Case object at 0x7f34c3fe6ec0>, <__main__.Case object at 0x7f34c3fe7100>, <__main__.Case object at 0x7f34c3fe5ba0>, <__main__.Case object at 0x7f34c3fec250>, <__main__.Case object at 0x7f34c3fec100>, <__main__.Case object at 0x7f34c3fec430>, <__main__.Case object at 0x7f34c3fec4f0>, <__main__.Case object at 0x7f34c3fec670>, <__main__.Case object at 0x7f34c3fec790>, <__main__.Case object at 0x7f34c3fec8b0>, <__main__.Case object at 0x7f34c3fec970>, <__main__.Case object at 0x7f34c3fecaf0>, <__main__.Case object at 0x7f34c3fecc10>, <__main__.Case object at 0x7f34c3feccd0>, <__main__.Case object at 0x7f34c3fec9d0>, <__main__.Case object at 0x7f34c3fecf10>, <__main__.Case object at 0x7f34c3fed030>, <__main__.Case object at 0x7f34c3fed150>, <__main__.Case object at 0x7f34c3fed210>, <__main__.Case object at 0x7f34c3fed390>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3f9f6a0>, <__main__.Case object at 0x7f34c3fb3f10>, <__main__.Case object at 0x7f34c3fb2980>, <__main__.Case object at 0x7f34c3f70e50>, <__main__.Case object at 0x7f34c3fc9f30>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c3fcb4f0>, <__main__.Case object at 0x7f34c3fcb970>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fcb2b0>, <__main__.Case object at 0x7f34c3fc9750>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fcbb20>, <__main__.Case object at 0x7f34c3fcab90>, <__main__.Case object at 0x7f34c3f9f6d0>, <__main__.Case object at 0x7f34c3fc8d60>, <__main__.Case object at 0x7f34c3fc9390>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fc8c70>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fcb250>, <__main__.Case object at 0x7f34c3fc80d0>, <__main__.Case object at 0x7f34c3fca650>, <__main__.Case object at 0x7f34c3fc96f0>, <__main__.Case object at 0x7f34c3fcb730>, <__main__.Case object at 0x7f34c3fbc7f0>, <__main__.Case object at 0x7f34c3fbcb50>, <__main__.Case object at 0x7f34c3fc8160>, <__main__.Case object at 0x7f34c3fbcd90>, <__main__.Case object at 0x7f34c3fc8850>, <__main__.Case object at 0x7f34c3fcabc0>, <__main__.Case object at 0x7f34c3fbf8b0>, <__main__.Case object at 0x7f34c3fbfe80>, <__main__.Case object at 0x7f34c3fbf340>, <__main__.Case object at 0x7f34c3fc83a0>, <__main__.Case object at 0x7f34c3fbe380>, <__main__.Case object at 0x7f34c3fbc730>, <__main__.Case object at 0x7f34c3fbfa90>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fbdea0>, <__main__.Case object at 0x7f34c3fbf700>, <__main__.Case object at 0x7f34c3fbcbe0>, <__main__.Case object at 0x7f34c3fbfca0>, <__main__.Case object at 0x7f34c3fbfb20>, <__main__.Case object at 0x7f34c3fbf040>, <__main__.Case object at 0x7f34c3fbd810>, <__main__.Case object at 0x7f34c3fbebc0>, <__main__.Case object at 0x7f34c3fbd7e0>, <__main__.Case object at 0x7f34c3fbf940>, <__main__.Case object at 0x7f34c3fbd9c0>, <__main__.Case object at 0x7f34c3fbf220>, <__main__.Case object at 0x7f34c3fbe200>, <__main__.Case object at 0x7f34c3fbf550>, <__main__.Case object at 0x7f34c3fbcdc0>, <__main__.Case object at 0x7f34c3fbf490>, <__main__.Case object at 0x7f34c3fbf9d0>, <__main__.Case object at 0x7f34c3fbde10>, <__main__.Case object at 0x7f34c3fbca00>, <__main__.Case object at 0x7f34c3fbeb90>, <__main__.Case object at 0x7f34c3fbff10>, <__main__.Case object at 0x7f34c3fbc070>, <__main__.Case object at 0x7f34c3fbcfa0>, <__main__.Case object at 0x7f34c3fbd870>, <__main__.Case object at 0x7f34c3fbe1a0>, <__main__.Case object at 0x7f34c3fbd540>, <__main__.Case object at 0x7f34c3fbfa00>, <__main__.Case object at 0x7f34c3fbdb40>, <__main__.Case object at 0x7f34c3fbf790>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fe45b0>, <__main__.Case object at 0x7f34c3fe48e0>, <__main__.Case object at 0x7f34c3fbc640>, <__main__.Case object at 0x7f34c3fe4100>, <__main__.Case object at 0x7f34c3fe54b0>, <__main__.Case object at 0x7f34c3fe5960>, <__main__.Case object at 0x7f34c3fbffd0>, <__main__.Case object at 0x7f34c3fe4eb0>, <__main__.Case object at 0x7f34c3fe6470>, <__main__.Case object at 0x7f34c3fe67a0>, <__main__.Case object at 0x7f34c3fbc610>, <__main__.Case object at 0x7f34c3fbfee0>, <__main__.Case object at 0x7f34c3fe74c0>, <__main__.Case object at 0x7f34c3fe7190>, <__main__.Case object at 0x7f34c3fe48b0>, <__main__.Case object at 0x7f34c3fe4c10>, <__main__.Case object at 0x7f34c3fe5090>, <__main__.Case object at 0x7f34c3fe4430>, <__main__.Case object at 0x7f34c3fe5bd0>, <__main__.Case object at 0x7f34c3fe5db0>, <__main__.Case object at 0x7f34c3fe60b0>, <__main__.Case object at 0x7f34c3fe63b0>, <__main__.Case object at 0x7f34c3fe6830>, <__main__.Case object at 0x7f34c3fe49d0>, <__main__.Case object at 0x7f34c3fe6dd0>, <__main__.Case object at 0x7f34c3fe70d0>, <__main__.Case object at 0x7f34c3fe7550>, <__main__.Case object at 0x7f34c3fe59f0>, <__main__.Case object at 0x7f34c3fe4880>, <__main__.Case object at 0x7f34c3fe4d00>, <__main__.Case object at 0x7f34c3fe4520>, <__main__.Case object at 0x7f34c3fe5450>, <__main__.Case object at 0x7f34c3fe5870>, <__main__.Case object at 0x7f34c3fe44c0>, <__main__.Case object at 0x7f34c3fe4d90>, <__main__.Case object at 0x7f34c3fe6110>, <__main__.Case object at 0x7f34c3fe6590>, <__main__.Case object at 0x7f34c3fe5120>, <__main__.Case object at 0x7f34c3fe6c80>, <__main__.Case object at 0x7f34c3fe7460>, <__main__.Case object at 0x7f34c3fe6950>, <__main__.Case object at 0x7f34c3fec190>, <__main__.Case object at 0x7f34c3fec3d0>, <__main__.Case object at 0x7f34c3fe7670>, <__main__.Case object at 0x7f34c3fec2e0>, <__main__.Case object at 0x7f34c3fec730>, <__main__.Case object at 0x7f34c3fec850>, <__main__.Case object at 0x7f34c3fe4c70>, <__main__.Case object at 0x7f34c3fec550>, <__main__.Case object at 0x7f34c3fecbb0>, <__main__.Case object at 0x7f34c3fe7340>, <__main__.Case object at 0x7f34c3fe5a20>, <__main__.Case object at 0x7f34c3fecfd0>, <__main__.Case object at 0x7f34c3fed0f0>, <__main__.Case object at 0x7f34c3fe6740>, <__main__.Case object at 0x7f34c3feceb0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 4, tv: 0.4000000000000001, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 4, tv: 0.4000000000000001, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.5000000000000001, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 3, tv: 0.5, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.5, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.5, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.5, time steps: 13\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 0.6)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 0.6)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 0.6)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.6)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 0.6)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 0.6)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.6)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.6)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.6)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.6)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.6)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.6)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.6)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.6)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.6)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 5), solution: 3, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 11\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f889d0>, <__main__.Case object at 0x7f34c3fb3d30>, <__main__.Case object at 0x7f34c3fb3be0>, <__main__.Case object at 0x7f34c3fb3df0>, <__main__.Case object at 0x7f34c3fc8130>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3fc9c00>, <__main__.Case object at 0x7f34c3fc9420>, <__main__.Case object at 0x7f34c3fcae90>, <__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fca1d0>, <__main__.Case object at 0x7f34c3fcba60>, <__main__.Case object at 0x7f34c3fc9e70>, <__main__.Case object at 0x7f34c3fcad10>, <__main__.Case object at 0x7f34c3fcaa70>, <__main__.Case object at 0x7f34c3fc9bd0>, <__main__.Case object at 0x7f34c3fcaad0>, <__main__.Case object at 0x7f34c3fca9e0>, <__main__.Case object at 0x7f34c3fc88b0>, <__main__.Case object at 0x7f34c3fcbd30>, <__main__.Case object at 0x7f34c3fc90f0>, <__main__.Case object at 0x7f34c3fcb190>, <__main__.Case object at 0x7f34c3fc8730>, <__main__.Case object at 0x7f34c3fc86a0>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fc8a30>, <__main__.Case object at 0x7f34c3fc9ab0>, <__main__.Case object at 0x7f34c3fc8cd0>, <__main__.Case object at 0x7f34c3fc8e80>, <__main__.Case object at 0x7f34c3fc93f0>, <__main__.Case object at 0x7f34c3fc8b80>, <__main__.Case object at 0x7f34c3fcb130>, <__main__.Case object at 0x7f34c3fcbd00>, <__main__.Case object at 0x7f34c3fbfbb0>, <__main__.Case object at 0x7f34c3fc9cc0>, <__main__.Case object at 0x7f34c3fbc850>, <__main__.Case object at 0x7f34c3fbf7f0>, <__main__.Case object at 0x7f34c3fbca90>, <__main__.Case object at 0x7f34c3fbc400>, <__main__.Case object at 0x7f34c3fbc340>, <__main__.Case object at 0x7f34c3fbfcd0>, <__main__.Case object at 0x7f34c3fbe350>, <__main__.Case object at 0x7f34c3fbf970>, <__main__.Case object at 0x7f34c3fc8fd0>, <__main__.Case object at 0x7f34c3fbdbd0>, <__main__.Case object at 0x7f34c3fbcc40>, <__main__.Case object at 0x7f34c3fbecb0>, <__main__.Case object at 0x7f34c3fbefe0>, <__main__.Case object at 0x7f34c3fbe290>, <__main__.Case object at 0x7f34c3fbe470>, <__main__.Case object at 0x7f34c3fbdc60>, <__main__.Case object at 0x7f34c3fbc040>, <__main__.Case object at 0x7f34c3fbe680>, <__main__.Case object at 0x7f34c3fbfe50>, <__main__.Case object at 0x7f34c3fbccd0>, <__main__.Case object at 0x7f34c3fbce20>, <__main__.Case object at 0x7f34c3fcb040>, <__main__.Case object at 0x7f34c3fbc970>, <__main__.Case object at 0x7f34c3fbf310>, <__main__.Case object at 0x7f34c3fbd360>, <__main__.Case object at 0x7f34c3fbdcc0>, <__main__.Case object at 0x7f34c3fbe4a0>, <__main__.Case object at 0x7f34c3fbef20>, <__main__.Case object at 0x7f34c3fbd510>, <__main__.Case object at 0x7f34c3fbd2a0>, <__main__.Case object at 0x7f34c3fbd5a0>, <__main__.Case object at 0x7f34c3fbd330>, <__main__.Case object at 0x7f34c3fbde40>, <__main__.Case object at 0x7f34c3fbfdc0>, <__main__.Case object at 0x7f34c3fbd120>, <__main__.Case object at 0x7f34c3fbea10>, <__main__.Case object at 0x7f34c3fbf400>, <__main__.Case object at 0x7f34c3fbc940>, <__main__.Case object at 0x7f34c3fbcfd0>, <__main__.Case object at 0x7f34c3fbd840>, <__main__.Case object at 0x7f34c3fbfac0>, <__main__.Case object at 0x7f34c3fbec80>, <__main__.Case object at 0x7f34c3fbf670>, <__main__.Case object at 0x7f34c3fbe860>, <__main__.Case object at 0x7f34c3fbdde0>, <__main__.Case object at 0x7f34c3fe75b0>, <__main__.Case object at 0x7f34c3fe43d0>, <__main__.Case object at 0x7f34c3fe47c0>, <__main__.Case object at 0x7f34c3fe4bb0>, <__main__.Case object at 0x7f34c3fbfc40>, <__main__.Case object at 0x7f34c3fe4e50>, <__main__.Case object at 0x7f34c3fbc370>, <__main__.Case object at 0x7f34c3fe4580>, <__main__.Case object at 0x7f34c3fe5750>, <__main__.Case object at 0x7f34c3fe6320>, <__main__.Case object at 0x7f34c3fe6680>, <__main__.Case object at 0x7f34c3fe6aa0>, <__main__.Case object at 0x7f34c3fe5030>, <__main__.Case object at 0x7f34c3fe7040>, <__main__.Case object at 0x7f34c3fe73a0>, <__main__.Case object at 0x7f34c3fe73d0>, <__main__.Case object at 0x7f34c3fe4310>, <__main__.Case object at 0x7f34c3fe4790>, <__main__.Case object at 0x7f34c3fe4af0>, <__main__.Case object at 0x7f34c3fe4df0>, <__main__.Case object at 0x7f34c3fe41f0>, <__main__.Case object at 0x7f34c3fe5510>, <__main__.Case object at 0x7f34c3fe5810>, <__main__.Case object at 0x7f34c3fca620>, <__main__.Case object at 0x7f34c3fe5cc0>, <__main__.Case object at 0x7f34c3fe6050>, <__main__.Case object at 0x7f34c3fe62f0>, <__main__.Case object at 0x7f34c3fe6650>, <__main__.Case object at 0x7f34c3fe5930>, <__main__.Case object at 0x7f34c3fe6d70>, <__main__.Case object at 0x7f34c3fe7010>, <__main__.Case object at 0x7f34c3fe5240>, <__main__.Case object at 0x7f34c3fe41c0>, <__main__.Case object at 0x7f34c3fe4400>, <__main__.Case object at 0x7f34c3fe4760>, <__main__.Case object at 0x7f34c3fe7370>, <__main__.Case object at 0x7f34c3fe4be0>, <__main__.Case object at 0x7f34c3fe5060>, <__main__.Case object at 0x7f34c3fe6e90>, <__main__.Case object at 0x7f34c3fe5690>, <__main__.Case object at 0x7f34c3fe5ae0>, <__main__.Case object at 0x7f34c3fe5c60>, <__main__.Case object at 0x7f34c3fe6350>, <__main__.Case object at 0x7f34c3fe5c90>, <__main__.Case object at 0x7f34c3fe6860>, <__main__.Case object at 0x7f34c3fe6b60>, <__main__.Case object at 0x7f34c3fe6fe0>, <__main__.Case object at 0x7f34c3fe5f30>, <__main__.Case object at 0x7f34c3fec220>, <__main__.Case object at 0x7f34c3fe5330>, <__main__.Case object at 0x7f34c3fec0a0>, <__main__.Case object at 0x7f34c3fec490>, <__main__.Case object at 0x7f34c3fe69e0>, <__main__.Case object at 0x7f34c3fc9120>, <__main__.Case object at 0x7f34c3fec7f0>, <__main__.Case object at 0x7f34c3fec6d0>, <__main__.Case object at 0x7f34c3feca90>, <__main__.Case object at 0x7f34c3fecb50>, <__main__.Case object at 0x7f34c3fecc70>, <__main__.Case object at 0x7f34c3fecd30>, <__main__.Case object at 0x7f34c3fec910>, <__main__.Case object at 0x7f34c3fecf70>, <__main__.Case object at 0x7f34c3fed090>, <__main__.Case object at 0x7f34c3fed1b0>, <__main__.Case object at 0x7f34c3fc8100>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f88a30>, <__main__.Case object at 0x7f34c3fb3d90>, <__main__.Case object at 0x7f34c3fca170>, <__main__.Case object at 0x7f34c3fcbee0>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.7, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 1.0, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 4, tv: 0.40000000000000013, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 4, tv: 0.40000000000000013, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.7000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.7000000000000001, time steps: 6\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 4, 0.6000000000000001)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.6000000000000001)\n",
      "Integrated case process. comm case (1, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 0), 4, 0.6000000000000001)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 4, 0.6000000000000001)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1.0, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.7000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.7000000000000001, time steps: 6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.6000000000000001, time steps: 9\n",
      "Episode: 34, Total Steps: 146, Total Rewards: [-245, 84], Status Episode: False\n",
      "------------------------------------------End of episode 34 loop--------------------\n",
      "----- starting point of Episode 35 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 0.7, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 35 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 0 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 4 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 2 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 0 to next state (3, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 1) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 35 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 0) with action 3 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 3 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 35 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 35 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 35 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 82 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 35 in steps 83 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 84 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 85 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 86 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 35 in steps 87 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 35 in steps 88 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3fb3f10>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3fb29b0>, <__main__.Case object at 0x7f34c3fa8820>, <__main__.Case object at 0x7f34c3fbcb50>, <__main__.Case object at 0x7f34c3fbe530>, <__main__.Case object at 0x7f34c3fbeb00>, <__main__.Case object at 0x7f34c3fbc580>, <__main__.Case object at 0x7f34c3fbc910>, <__main__.Case object at 0x7f34c3fbd810>, <__main__.Case object at 0x7f34c3fbf940>, <__main__.Case object at 0x7f34c3fbf220>, <__main__.Case object at 0x7f34c3fbc6d0>, <__main__.Case object at 0x7f34c3fbeb90>, <__main__.Case object at 0x7f34c3fbcfa0>, <__main__.Case object at 0x7f34c3fbe1a0>, <__main__.Case object at 0x7f34c3fbffd0>, <__main__.Case object at 0x7f34c3fbe830>, <__main__.Case object at 0x7f34c3fbcee0>, <__main__.Case object at 0x7f34c3fbece0>, <__main__.Case object at 0x7f34c3fbf0a0>, <__main__.Case object at 0x7f34c3fbc460>, <__main__.Case object at 0x7f34c3fbe7a0>, <__main__.Case object at 0x7f34c3fbd1b0>, <__main__.Case object at 0x7f34c3fbdf60>, <__main__.Case object at 0x7f34c3fbd0c0>, <__main__.Case object at 0x7f34c3fbe8f0>, <__main__.Case object at 0x7f34c3fbe170>, <__main__.Case object at 0x7f34c3fbfdf0>, <__main__.Case object at 0x7f34c3fbf3d0>, <__main__.Case object at 0x7f34c3fbd3f0>, <__main__.Case object at 0x7f34c3fbd9f0>, <__main__.Case object at 0x7f34c3fbda20>, <__main__.Case object at 0x7f34c3fbc850>, <__main__.Case object at 0x7f34c3fbc400>, <__main__.Case object at 0x7f34c3fbfcd0>, <__main__.Case object at 0x7f34c3fbce50>, <__main__.Case object at 0x7f34c3fbdf30>, <__main__.Case object at 0x7f34c3fbda80>, <__main__.Case object at 0x7f34c3fbd1e0>, <__main__.Case object at 0x7f34c3fbc970>, <__main__.Case object at 0x7f34c3fbdcc0>, <__main__.Case object at 0x7f34c3fbd510>, <__main__.Case object at 0x7f34c3fbd5a0>, <__main__.Case object at 0x7f34c3fbd120>, <__main__.Case object at 0x7f34c3fbc940>, <__main__.Case object at 0x7f34c3fbfac0>, <__main__.Case object at 0x7f34c3fbd450>, <__main__.Case object at 0x7f34c3fe67d0>, <__main__.Case object at 0x7f34c3fe48e0>, <__main__.Case object at 0x7f34c3fe5660>, <__main__.Case object at 0x7f34c3fe4eb0>, <__main__.Case object at 0x7f34c3fe74c0>, <__main__.Case object at 0x7f34c3fe4c10>, <__main__.Case object at 0x7f34c3fe5bd0>, <__main__.Case object at 0x7f34c3fbe9b0>, <__main__.Case object at 0x7f34c3fe6470>, <__main__.Case object at 0x7f34c3fe7550>, <__main__.Case object at 0x7f34c3fe4d00>, <__main__.Case object at 0x7f34c3fe5450>, <__main__.Case object at 0x7f34c3fe6110>, <__main__.Case object at 0x7f34c3fe6c80>, <__main__.Case object at 0x7f34c3fe4c70>, <__main__.Case object at 0x7f34c3fe6a10>, <__main__.Case object at 0x7f34c3fe4c40>, <__main__.Case object at 0x7f34c3fe57b0>, <__main__.Case object at 0x7f34c3fe6560>, <__main__.Case object at 0x7f34c3fe6bc0>, <__main__.Case object at 0x7f34c3fe44f0>, <__main__.Case object at 0x7f34c3fe4f10>, <__main__.Case object at 0x7f34c3fe57e0>, <__main__.Case object at 0x7f34c3fe5d50>, <__main__.Case object at 0x7f34c3fe4e80>, <__main__.Case object at 0x7f34c3fe7400>, <__main__.Case object at 0x7f34c3fe4370>, <__main__.Case object at 0x7f34c3fe6770>, <__main__.Case object at 0x7f34c3fe5840>, <__main__.Case object at 0x7f34c3fe5e40>, <__main__.Case object at 0x7f34c3fe6980>, <__main__.Case object at 0x7f34c3fe6f50>, <__main__.Case object at 0x7f34c3fe43d0>, <__main__.Case object at 0x7f34c3fe4bb0>, <__main__.Case object at 0x7f34c3fe4580>, <__main__.Case object at 0x7f34c3fe6320>, <__main__.Case object at 0x7f34c3fe7040>, <__main__.Case object at 0x7f34c3fe4310>, <__main__.Case object at 0x7f34c3fe4af0>, <__main__.Case object at 0x7f34c3fe5510>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f70e50>, <__main__.Case object at 0x7f34c3f889d0>, <__main__.Case object at 0x7f34c3f9f6a0>, <__main__.Case object at 0x7f34c3fb3ee0>, <__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3faafe0>, <__main__.Case object at 0x7f34c3fbcd00>, <__main__.Case object at 0x7f34c3fbec50>, <__main__.Case object at 0x7f34c3fbcc70>, <__main__.Case object at 0x7f34c3fbe260>, <__main__.Case object at 0x7f34c3fbf040>, <__main__.Case object at 0x7f34c3fbd7e0>, <__main__.Case object at 0x7f34c3fbc1f0>, <__main__.Case object at 0x7f34c3fbf760>, <__main__.Case object at 0x7f34c3fbca00>, <__main__.Case object at 0x7f34c3fbc070>, <__main__.Case object at 0x7f34c3fbf7c0>, <__main__.Case object at 0x7f34c3fbf790>, <__main__.Case object at 0x7f34c3fbe800>, <__main__.Case object at 0x7f34c3f9f6d0>, <__main__.Case object at 0x7f34c3fbd540>, <__main__.Case object at 0x7f34c3fbd3c0>, <__main__.Case object at 0x7f34c3fbf850>, <__main__.Case object at 0x7f34c3fa8a00>, <__main__.Case object at 0x7f34c3fbf460>, <__main__.Case object at 0x7f34c3fbc5b0>, <__main__.Case object at 0x7f34c3fbe020>, <__main__.Case object at 0x7f34c3fbc550>, <__main__.Case object at 0x7f34c3fbe3b0>, <__main__.Case object at 0x7f34c3fbeef0>, <__main__.Case object at 0x7f34c3fbc310>, <__main__.Case object at 0x7f34c3fbdea0>, <__main__.Case object at 0x7f34c3fbdf00>, <__main__.Case object at 0x7f34c3fbca90>, <__main__.Case object at 0x7f34c3fbe350>, <__main__.Case object at 0x7f34c3fbf550>, <__main__.Case object at 0x7f34c3fbe950>, <__main__.Case object at 0x7f34c3fbdcf0>, <__main__.Case object at 0x7f34c3fbe8c0>, <__main__.Case object at 0x7f34c3fbfa00>, <__main__.Case object at 0x7f34c3fbd360>, <__main__.Case object at 0x7f34c3fbef20>, <__main__.Case object at 0x7f34c3fbd330>, <__main__.Case object at 0x7f34c3fbdab0>, <__main__.Case object at 0x7f34c3fbf400>, <__main__.Case object at 0x7f34c3fbd840>, <__main__.Case object at 0x7f34c3fb3d90>, <__main__.Case object at 0x7f34c3fbdde0>, <__main__.Case object at 0x7f34c3fe45b0>, <__main__.Case object at 0x7f34c3fe5390>, <__main__.Case object at 0x7f34c3fbe980>, <__main__.Case object at 0x7f34c3fe7220>, <__main__.Case object at 0x7f34c3fe48b0>, <__main__.Case object at 0x7f34c3fe4430>, <__main__.Case object at 0x7f34c3fbe2c0>, <__main__.Case object at 0x7f34c3fe70d0>, <__main__.Case object at 0x7f34c3fe4880>, <__main__.Case object at 0x7f34c3fbccd0>, <__main__.Case object at 0x7f34c3fe63b0>, <__main__.Case object at 0x7f34c3fe5120>, <__main__.Case object at 0x7f34c3fe6950>, <__main__.Case object at 0x7f34c3fbf190>, <__main__.Case object at 0x7f34c3fe5870>, <__main__.Case object at 0x7f34c3fe5420>, <__main__.Case object at 0x7f34c3fe60e0>, <__main__.Case object at 0x7f34c3fbfc40>, <__main__.Case object at 0x7f34c3fe6410>, <__main__.Case object at 0x7f34c3fe4b80>, <__main__.Case object at 0x7f34c3fe55a0>, <__main__.Case object at 0x7f34c3fe6020>, <__main__.Case object at 0x7f34c3fe5ea0>, <__main__.Case object at 0x7f34c3fe70a0>, <__main__.Case object at 0x7f34c3fe4250>, <__main__.Case object at 0x7f34c3fe65f0>, <__main__.Case object at 0x7f34c3fe4a90>, <__main__.Case object at 0x7f34c3fe71c0>, <__main__.Case object at 0x7f34c3fe65c0>, <__main__.Case object at 0x7f34c3fe6500>, <__main__.Case object at 0x7f34c3fe6680>, <__main__.Case object at 0x7f34c3fe4fd0>, <__main__.Case object at 0x7f34c3fe73d0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.5000000000000001, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 3, tv: 0.5, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.5, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.5, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.5, time steps: 13\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 0.7)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 0.7)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 0.7)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.7)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 0.7)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 0.7)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.7)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.7)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.7)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.7)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.7)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.7)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.7)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 5), solution: 3, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 11\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f89b40>, <__main__.Case object at 0x7f34c3f88a30>, <__main__.Case object at 0x7f34c3fb3f70>, <__main__.Case object at 0x7f34c3fb2980>, <__main__.Case object at 0x7f34c3fb3df0>, <__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3fbc9d0>, <__main__.Case object at 0x7f34c3fbc730>, <__main__.Case object at 0x7f34c3fbe380>, <__main__.Case object at 0x7f34c3fbcca0>, <__main__.Case object at 0x7f34c3fbfd30>, <__main__.Case object at 0x7f34c3fbf700>, <__main__.Case object at 0x7f34c3fbd9c0>, <__main__.Case object at 0x7f34c3fbcc10>, <__main__.Case object at 0x7f34c3fbff40>, <__main__.Case object at 0x7f34c3fbff10>, <__main__.Case object at 0x7f34c3fbd870>, <__main__.Case object at 0x7f34c3fbdb40>, <__main__.Case object at 0x7f34c3fbfee0>, <__main__.Case object at 0x7f34c3fbe5f0>, <__main__.Case object at 0x7f34c3fbf610>, <__main__.Case object at 0x7f34c3fbfd90>, <__main__.Case object at 0x7f34c3fbd2d0>, <__main__.Case object at 0x7f34c3fbe890>, <__main__.Case object at 0x7f34c3fbce80>, <__main__.Case object at 0x7f34c3fbdb70>, <__main__.Case object at 0x7f34c3fbc250>, <__main__.Case object at 0x7f34c3fbf4f0>, <__main__.Case object at 0x7f34c3fbf820>, <__main__.Case object at 0x7f34c3fbcdf0>, <__main__.Case object at 0x7f34c3fbd270>, <__main__.Case object at 0x7f34c3fbd750>, <__main__.Case object at 0x7f34c3fbdae0>, <__main__.Case object at 0x7f34c3fbfd60>, <__main__.Case object at 0x7f34c3fbe3e0>, <__main__.Case object at 0x7f34c3fbcdc0>, <__main__.Case object at 0x7f34c3fbc340>, <__main__.Case object at 0x7f34c3fbebc0>, <__main__.Case object at 0x7f34c3fbfe20>, <__main__.Case object at 0x7f34c3fbefb0>, <__main__.Case object at 0x7f34c3fbf5b0>, <__main__.Case object at 0x7f34c3fb3cd0>, <__main__.Case object at 0x7f34c3fbf310>, <__main__.Case object at 0x7f34c3fbe4a0>, <__main__.Case object at 0x7f34c3fbd2a0>, <__main__.Case object at 0x7f34c3fbd210>, <__main__.Case object at 0x7f34c3fbea10>, <__main__.Case object at 0x7f34c3fbcfd0>, <__main__.Case object at 0x7f34c3fbec80>, <__main__.Case object at 0x7f34c3fbf670>, <__main__.Case object at 0x7f34c3fe76a0>, <__main__.Case object at 0x7f34c3fbe6e0>, <__main__.Case object at 0x7f34c3fe4fa0>, <__main__.Case object at 0x7f34c3fe5ab0>, <__main__.Case object at 0x7f34c3fe6bf0>, <__main__.Case object at 0x7f34c3fe5090>, <__main__.Case object at 0x7f34c3fe5db0>, <__main__.Case object at 0x7f34c3fe60b0>, <__main__.Case object at 0x7f34c3fe6dd0>, <__main__.Case object at 0x7f34c3fe59f0>, <__main__.Case object at 0x7f34c3fe4520>, <__main__.Case object at 0x7f34c3fe7190>, <__main__.Case object at 0x7f34c3fe6590>, <__main__.Case object at 0x7f34c3fe7460>, <__main__.Case object at 0x7f34c3fe5a20>, <__main__.Case object at 0x7f34c3fe40d0>, <__main__.Case object at 0x7f34c3fe50c0>, <__main__.Case object at 0x7f34c3fe5cf0>, <__main__.Case object at 0x7f34c3fe68f0>, <__main__.Case object at 0x7f34c3fe7430>, <__main__.Case object at 0x7f34c3fe4820>, <__main__.Case object at 0x7f34c3fe46a0>, <__main__.Case object at 0x7f34c3fe59c0>, <__main__.Case object at 0x7f34c3fe6530>, <__main__.Case object at 0x7f34c3fe6d40>, <__main__.Case object at 0x7f34c3fe4130>, <__main__.Case object at 0x7f34c3fe46d0>, <__main__.Case object at 0x7f34c3fe5210>, <__main__.Case object at 0x7f34c3fe5a50>, <__main__.Case object at 0x7f34c3fe5d20>, <__main__.Case object at 0x7f34c3fe6ad0>, <__main__.Case object at 0x7f34c3fe4220>, <__main__.Case object at 0x7f34c3fe74f0>, <__main__.Case object at 0x7f34c3fe5540>, <__main__.Case object at 0x7f34c3fe5750>, <__main__.Case object at 0x7f34c3fe6fb0>, <__main__.Case object at 0x7f34c3fbce20>, <__main__.Case object at 0x7f34c3fe4790>, <__main__.Case object at 0x7f34c3fe5a80>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6000000000000001, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.5000000000000001, time steps: 9\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6000000000000001, time steps: 6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5000000000000001, time steps: 9\n",
      "Episode: 35, Total Steps: 89, Total Rewards: [-188, 88], Status Episode: False\n",
      "------------------------------------------End of episode 35 loop--------------------\n",
      "----- starting point of Episode 36 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 36 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 2) with action 3 to next state (5, 2): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 2) with action 3 to next state (4, 2): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 2) with action 1 to next state (4, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 1) with action 3 to next state (3, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (3, 1) with action 1 to next state (3, 0): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (3, 0) with action 1 to next state (3, 0): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (3, 0) with action 0 to next state (3, 0): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (3, 0) with action 3 to next state (2, 0): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 hit the obstacle!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 0) with action 2 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 4 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 1 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 2 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 3 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 2 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 4 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 1 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 1 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 2 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 2 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 3 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 1 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 4 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 2 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 4 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 3 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 3 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 3 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 3 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 4 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 2 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 2 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 1 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 2 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 2 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 4 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 2 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0\n",
      "----- starting point of Episode 36 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 0) with action 2 to next state (2, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (2, 1) with action 2 to next state (2, 1): pull reward: 0\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fa8820>, <__main__.Case object at 0x7f34c3f89b40>, <__main__.Case object at 0x7f34c3fb3d60>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3fb3d30>, <__main__.Case object at 0x7f34c3fb2980>, <__main__.Case object at 0x7f34c3fbcc70>, <__main__.Case object at 0x7f34c3fbe1d0>, <__main__.Case object at 0x7f34c3fbd990>, <__main__.Case object at 0x7f34c3fbe800>, <__main__.Case object at 0x7f34c3fbd3c0>, <__main__.Case object at 0x7f34c3fbf520>, <__main__.Case object at 0x7f34c3fbc550>, <__main__.Case object at 0x7f34c3fbf730>, <__main__.Case object at 0x7f34c3fbdf00>, <__main__.Case object at 0x7f34c3fbd480>, <__main__.Case object at 0x7f34c3fbc040>, <__main__.Case object at 0x7f34c3fbdcf0>, <__main__.Case object at 0x7f34c3fbc700>, <__main__.Case object at 0x7f34c3fbd330>, <__main__.Case object at 0x7f34c3fa8a00>, <__main__.Case object at 0x7f34c3fbc370>, <__main__.Case object at 0x7f34c3fbe950>, <__main__.Case object at 0x7f34c3fbcb50>, <__main__.Case object at 0x7f34c3fb3df0>, <__main__.Case object at 0x7f34c3fbc910>, <__main__.Case object at 0x7f34c3fbd840>, <__main__.Case object at 0x7f34c3fbc6d0>, <__main__.Case object at 0x7f34c3fbe200>, <__main__.Case object at 0x7f34c3fbc0a0>, <__main__.Case object at 0x7f34c3fbc580>, <__main__.Case object at 0x7f34c3fbd390>, <__main__.Case object at 0x7f34c3fbe6b0>, <__main__.Case object at 0x7f34c3fbc4f0>, <__main__.Case object at 0x7f34c3fbf7c0>, <__main__.Case object at 0x7f34c3fbfa30>, <__main__.Case object at 0x7f34c3fbd660>, <__main__.Case object at 0x7f34c3fbfbb0>, <__main__.Case object at 0x7f34c3fbd600>, <__main__.Case object at 0x7f34c3fbecb0>, <__main__.Case object at 0x7f34c3fbfaf0>, <__main__.Case object at 0x7f34c3fbd8d0>, <__main__.Case object at 0x7f34c3fbe8c0>, <__main__.Case object at 0x7f34c3fbc1c0>, <__main__.Case object at 0x7f34c3fbe9b0>, <__main__.Case object at 0x7f34c3fbf2e0>, <__main__.Case object at 0x7f34c3fbdde0>, <__main__.Case object at 0x7f34c3fbfd30>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3faafe0>, <__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3f889d0>, <__main__.Case object at 0x7f34c3f70e50>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.5000000000000001, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 3, tv: 0.5, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.5, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.5, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.5, time steps: 13\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.7999999999999999)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 5), solution: 3, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 11\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 13\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3f916c0>, <__main__.Case object at 0x7f34c3f90340>, <__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3fb29b0>, <__main__.Case object at 0x7f34c3fb3b80>, <__main__.Case object at 0x7f34c3fbcd00>, <__main__.Case object at 0x7f34c3fbe260>, <__main__.Case object at 0x7f34c3fbc1f0>, <__main__.Case object at 0x7f34c3fbca00>, <__main__.Case object at 0x7f34c3fb3be0>, <__main__.Case object at 0x7f34c3fbf850>, <__main__.Case object at 0x7f34c3fbceb0>, <__main__.Case object at 0x7f34c3fbf010>, <__main__.Case object at 0x7f34c3fbedd0>, <__main__.Case object at 0x7f34c3fbca90>, <__main__.Case object at 0x7f34c3fbe350>, <__main__.Case object at 0x7f34c3fbe710>, <__main__.Case object at 0x7f34c3fbf550>, <__main__.Case object at 0x7f34c3fbef20>, <__main__.Case object at 0x7f34c3fbdab0>, <__main__.Case object at 0x7f34c3fbf400>, <__main__.Case object at 0x7f34c3fbe2c0>, <__main__.Case object at 0x7f34c3fbfa00>, <__main__.Case object at 0x7f34c3fbe530>, <__main__.Case object at 0x7f34c3fbeb00>, <__main__.Case object at 0x7f34c3fbeb90>, <__main__.Case object at 0x7f34c3fbf220>, <__main__.Case object at 0x7f34c3fbc160>, <__main__.Case object at 0x7f34c3fbdfc0>, <__main__.Case object at 0x7f34c3fbcee0>, <__main__.Case object at 0x7f34c3fbed10>, <__main__.Case object at 0x7f34c3fbf160>, <__main__.Case object at 0x7f34c3fbe080>, <__main__.Case object at 0x7f34c3fbe8f0>, <__main__.Case object at 0x7f34c3fbd150>, <__main__.Case object at 0x7f34c3fbca30>, <__main__.Case object at 0x7f34c3fbdba0>, <__main__.Case object at 0x7f34c3fbc400>, <__main__.Case object at 0x7f34c3fbd7b0>, <__main__.Case object at 0x7f34c3fbe470>, <__main__.Case object at 0x7f34c3fbe680>, <__main__.Case object at 0x7f34c3fbfc40>, <__main__.Case object at 0x7f34c3fbc310>, <__main__.Case object at 0x7f34c3fbdd50>, <__main__.Case object at 0x7f34c3fbf7f0>, <__main__.Case object at 0x7f34c3fbe380>, <__main__.Case object at 0x7f34c3fbcc10>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.6000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.6000000000000001, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.5000000000000001, time steps: 9\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.7999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.6000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.6000000000000001, time steps: 6\n",
      "cases content after RETAIN, problem: (1, 0), solution: 4, tv: 0.5000000000000001, time steps: 9\n",
      "Episode: 36, Total Steps: 48, Total Rewards: [-147, -113], Status Episode: False\n",
      "------------------------------------------End of episode 36 loop--------------------\n",
      "----- starting point of Episode 37 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.5999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.5999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.5999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.5999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 1) with action 1 to next state (6, 0): pull reward: 0\n",
      "----- starting point of Episode 37 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.5999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.5999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 4) with action 0 to next state (6, 4): pull reward: 0\n",
      "----- starting point of Episode 37 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 0.7999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 2 to next state (0, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 1 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 37 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 2 to next state (0, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 2 to next state (0, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 5) with action 3 to next state (0, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 37 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 5) with action 1 to next state (0, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 4 to next state (1, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 4) with action 3 to next state (0, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 4 to next state (1, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 4) with action 4 to next state (2, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 4) with action 3 to next state (1, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 4) with action 0 to next state (1, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 4) with action 1 to next state (1, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 3) with action 1 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 2 to next state (1, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 3) with action 3 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 37 in steps 82 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 83 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 84 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 85 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 86 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 87 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 37 in steps 88 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 89 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 90 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 91 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 92 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 93 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 94 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 95 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 96 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 97 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 98 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 99 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 100 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 101 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 102 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 103 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 104 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 105 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 106 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 107 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 108 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 109 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 110 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 37 in steps 111 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 112 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 113 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 114 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 115 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 116 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 117 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 118 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 119 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 120 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 121 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 122 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 123 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 37 in steps 124 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 125 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 126 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 127 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 37 in steps 128 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 37 in steps 129 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 130 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 131 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 132 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 37 in steps 133 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 134 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 37 in steps 135 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 136 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 137 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 138 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 139 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 140 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 141 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 142 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 143 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 144 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 145 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 146 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 147 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 148 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 149 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 150 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 151 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 2 to next state (0, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 152 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 153 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 154 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 2 to next state (0, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 155 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 5) with action 2 to next state (0, 6): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 156 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 6) with action 0 to next state (0, 6): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 157 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 6) with action 4 to next state (1, 6): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 158 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 6) with action 4 to next state (2, 6): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 159 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 6) with action 0 to next state (2, 6): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 37 in steps 160 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 6) with action 3 to next state (1, 6): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 37 in steps 161 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 6) with action 1 to next state (1, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f70e50>, <__main__.Case object at 0x7f34c3f93ac0>, <__main__.Case object at 0x7f34c3fb3cd0>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3fb3f70>, <__main__.Case object at 0x7f34c3fbc850>, <__main__.Case object at 0x7f34c3fbcc70>, <__main__.Case object at 0x7f34c3fbe800>, <__main__.Case object at 0x7f34c3fbdf00>, <__main__.Case object at 0x7f34c3fbe110>, <__main__.Case object at 0x7f34c3fbd360>, <__main__.Case object at 0x7f34c3fbc370>, <__main__.Case object at 0x7f34c3fbc6d0>, <__main__.Case object at 0x7f34c3fbe200>, <__main__.Case object at 0x7f34c3fbd390>, <__main__.Case object at 0x7f34c3fbf7c0>, <__main__.Case object at 0x7f34c3fbd660>, <__main__.Case object at 0x7f34c3fbfaf0>, <__main__.Case object at 0x7f34c3fbc1c0>, <__main__.Case object at 0x7f34c3fbdde0>, <__main__.Case object at 0x7f34c3fbe2f0>, <__main__.Case object at 0x7f34c3fbca00>, <__main__.Case object at 0x7f34c3fbc070>, <__main__.Case object at 0x7f34c3fbe290>, <__main__.Case object at 0x7f34c3fbd6c0>, <__main__.Case object at 0x7f34c3fbee90>, <__main__.Case object at 0x7f34c3fbf940>, <__main__.Case object at 0x7f34c3fbffd0>, <__main__.Case object at 0x7f34c3fbf0a0>, <__main__.Case object at 0x7f34c3fbfdf0>, <__main__.Case object at 0x7f34c3fbc2b0>, <__main__.Case object at 0x7f34c3fbc970>, <__main__.Case object at 0x7f34c3fbfa90>, <__main__.Case object at 0x7f34c3fbfb80>, <__main__.Case object at 0x7f34c3fbfe50>, <__main__.Case object at 0x7f34c3fbfb50>, <__main__.Case object at 0x7f34c3fbcdc0>, <__main__.Case object at 0x7f34c3fbd270>, <__main__.Case object at 0x7f34c3fbf4f0>, <__main__.Case object at 0x7f34c3fbead0>, <__main__.Case object at 0x7f34c3fbe890>, <__main__.Case object at 0x7f34c3fbf280>, <__main__.Case object at 0x7f34c3fbd870>, <__main__.Case object at 0x7f34c3fbe4a0>, <__main__.Case object at 0x7f34c3fbd2a0>, <__main__.Case object at 0x7f34c3fbe6e0>, <__main__.Case object at 0x7f34c3fbef20>, <__main__.Case object at 0x7f34c3fe58a0>, <__main__.Case object at 0x7f34c3fe42b0>, <__main__.Case object at 0x7f34c3fe4160>, <__main__.Case object at 0x7f34c3fe70d0>, <__main__.Case object at 0x7f34c3fe63e0>, <__main__.Case object at 0x7f34c3fe6e30>, <__main__.Case object at 0x7f34c3fe7280>, <__main__.Case object at 0x7f34c3fe45e0>, <__main__.Case object at 0x7f34c3fe7490>, <__main__.Case object at 0x7f34c3fe5180>, <__main__.Case object at 0x7f34c3fe6170>, <__main__.Case object at 0x7f34c3fe67d0>, <__main__.Case object at 0x7f34c3fe4eb0>, <__main__.Case object at 0x7f34c3fbf610>, <__main__.Case object at 0x7f34c3fe7550>, <__main__.Case object at 0x7f34c3fe66b0>, <__main__.Case object at 0x7f34c3fe51e0>, <__main__.Case object at 0x7f34c3fe5d80>, <__main__.Case object at 0x7f34c3fe52d0>, <__main__.Case object at 0x7f34c3fe5f90>, <__main__.Case object at 0x7f34c3fe58d0>, <__main__.Case object at 0x7f34c3fe6230>, <__main__.Case object at 0x7f34c3fe5030>, <__main__.Case object at 0x7f34c3fe5360>, <__main__.Case object at 0x7f34c3fe54b0>, <__main__.Case object at 0x7f34c3fe5ab0>, <__main__.Case object at 0x7f34c3fe60b0>, <__main__.Case object at 0x7f34c3fe4520>, <__main__.Case object at 0x7f34c3fe7460>, <__main__.Case object at 0x7f34c3fe40d0>, <__main__.Case object at 0x7f34c3fe7430>, <__main__.Case object at 0x7f34c3fe59c0>, <__main__.Case object at 0x7f34c3fe4130>, <__main__.Case object at 0x7f34c3fe6ad0>, <__main__.Case object at 0x7f34c3fe71c0>, <__main__.Case object at 0x7f34c3fe74f0>, <__main__.Case object at 0x7f34c3fe5000>, <__main__.Case object at 0x7f34c3fe61d0>, <__main__.Case object at 0x7f34c3fe6da0>, <__main__.Case object at 0x7f34c3fe6350>, <__main__.Case object at 0x7f34c3fe5c00>, <__main__.Case object at 0x7f34c3fe5540>, <__main__.Case object at 0x7f34c3fe4760>, <__main__.Case object at 0x7f34c3fe7520>, <__main__.Case object at 0x7f34c3fe5930>, <__main__.Case object at 0x7f34c3fe43d0>, <__main__.Case object at 0x7f34c3fc8c10>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fcb2b0>, <__main__.Case object at 0x7f34c3fcbb20>, <__main__.Case object at 0x7f34c3fc9f00>, <__main__.Case object at 0x7f34c3fc9ff0>, <__main__.Case object at 0x7f34c3fcba00>, <__main__.Case object at 0x7f34c3fc9b40>, <__main__.Case object at 0x7f34c3fc8160>, <__main__.Case object at 0x7f34c3fca170>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fcb790>, <__main__.Case object at 0x7f34c3fc8dc0>, <__main__.Case object at 0x7f34c3fcbc10>, <__main__.Case object at 0x7f34c3fcad70>, <__main__.Case object at 0x7f34c3fc8a90>, <__main__.Case object at 0x7f34c3fc8460>, <__main__.Case object at 0x7f34c3fc91b0>, <__main__.Case object at 0x7f34c3fc98a0>, <__main__.Case object at 0x7f34c3fcace0>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fcba90>, <__main__.Case object at 0x7f34c3fca950>, <__main__.Case object at 0x7f34c3fca770>, <__main__.Case object at 0x7f34c3fc9240>, <__main__.Case object at 0x7f34c3fcb550>, <__main__.Case object at 0x7f34c3fc8f70>, <__main__.Case object at 0x7f34c3fcbe50>, <__main__.Case object at 0x7f34c3fcb4c0>, <__main__.Case object at 0x7f34c3fc83d0>, <__main__.Case object at 0x7f34c3fcaec0>, <__main__.Case object at 0x7f34c3fecdf0>, <__main__.Case object at 0x7f34c3feceb0>, <__main__.Case object at 0x7f34c3fca620>, <__main__.Case object at 0x7f34c3fec2e0>, <__main__.Case object at 0x7f34c3fecb20>, <__main__.Case object at 0x7f34c3fed060>, <__main__.Case object at 0x7f34c3fcbaf0>, <__main__.Case object at 0x7f34c3fec4c0>, <__main__.Case object at 0x7f34c3fec790>, <__main__.Case object at 0x7f34c3fec970>, <__main__.Case object at 0x7f34c3feccd0>, <__main__.Case object at 0x7f34c3fed150>, <__main__.Case object at 0x7f34c3fec1c0>, <__main__.Case object at 0x7f34c3fec490>, <__main__.Case object at 0x7f34c3fec7f0>, <__main__.Case object at 0x7f34c3fecc70>, <__main__.Case object at 0x7f34c3fecf70>, <__main__.Case object at 0x7f34c3fedd20>, <__main__.Case object at 0x7f34c3fedcc0>, <__main__.Case object at 0x7f34c3fedb40>, <__main__.Case object at 0x7f34c3fed990>, <__main__.Case object at 0x7f34c3fed8d0>, <__main__.Case object at 0x7f34c3fed810>, <__main__.Case object at 0x7f34c3fed690>, <__main__.Case object at 0x7f34c3fed540>, <__main__.Case object at 0x7f34c3fed450>, <__main__.Case object at 0x7f34c3fede10>, <__main__.Case object at 0x7f34c3fedf90>, <__main__.Case object at 0x7f34c3fee0b0>, <__main__.Case object at 0x7f34c3fee1d0>, <__main__.Case object at 0x7f34c3fee2f0>, <__main__.Case object at 0x7f34c3fee470>, <__main__.Case object at 0x7f34c3fee5f0>, <__main__.Case object at 0x7f34c3fee710>, <__main__.Case object at 0x7f34c3fecaf0>, <__main__.Case object at 0x7f34c3fee950>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3fa8a00>, <__main__.Case object at 0x7f34c3f88970>, <__main__.Case object at 0x7f34c3f9f6d0>, <__main__.Case object at 0x7f34c3faafe0>, <__main__.Case object at 0x7f34c3fb3f40>, <__main__.Case object at 0x7f34c3fb0340>, <__main__.Case object at 0x7f34c3fbd990>, <__main__.Case object at 0x7f34c3fbc550>, <__main__.Case object at 0x7f34c3fbc040>, <__main__.Case object at 0x7f34c3fbd690>, <__main__.Case object at 0x7f34c3fbc910>, <__main__.Case object at 0x7f34c3fbdcc0>, <__main__.Case object at 0x7f34c3fbc580>, <__main__.Case object at 0x7f34c3fbc4f0>, <__main__.Case object at 0x7f34c3fbfbb0>, <__main__.Case object at 0x7f34c3fbc9d0>, <__main__.Case object at 0x7f34c3fbe8c0>, <__main__.Case object at 0x7f34c3fbf2e0>, <__main__.Case object at 0x7f34c3fa8820>, <__main__.Case object at 0x7f34c3fbd0c0>, <__main__.Case object at 0x7f34c3fbe020>, <__main__.Case object at 0x7f34c3fbf970>, <__main__.Case object at 0x7f34c3fb29b0>, <__main__.Case object at 0x7f34c3fbcd00>, <__main__.Case object at 0x7f34c3fbc7f0>, <__main__.Case object at 0x7f34c3fbcfa0>, <__main__.Case object at 0x7f34c3fbe7a0>, <__main__.Case object at 0x7f34c3fbef80>, <__main__.Case object at 0x7f34c3fbda20>, <__main__.Case object at 0x7f34c3fbda80>, <__main__.Case object at 0x7f34c3fbfac0>, <__main__.Case object at 0x7f34c3fbf1f0>, <__main__.Case object at 0x7f34c3fbc880>, <__main__.Case object at 0x7f34c3fbfe20>, <__main__.Case object at 0x7f34c3fbf4c0>, <__main__.Case object at 0x7f34c3fbdae0>, <__main__.Case object at 0x7f34c3fbcdf0>, <__main__.Case object at 0x7f34c3fbc250>, <__main__.Case object at 0x7f34c3fbfd90>, <__main__.Case object at 0x7f34c3fbc8b0>, <__main__.Case object at 0x7f34c3fbdb40>, <__main__.Case object at 0x7f34c3fbe620>, <__main__.Case object at 0x7f34c3fbea10>, <__main__.Case object at 0x7f34c3fbe860>, <__main__.Case object at 0x7f34c3fe5de0>, <__main__.Case object at 0x7f34c3fbed10>, <__main__.Case object at 0x7f34c3fe5390>, <__main__.Case object at 0x7f34c3fe4dc0>, <__main__.Case object at 0x7f34c3fe4880>, <__main__.Case object at 0x7f34c3fbc310>, <__main__.Case object at 0x7f34c3fe5b70>, <__main__.Case object at 0x7f34c3fe6d10>, <__main__.Case object at 0x7f34c3fe6380>, <__main__.Case object at 0x7f34c3fbe0e0>, <__main__.Case object at 0x7f34c3fe65c0>, <__main__.Case object at 0x7f34c3fe6aa0>, <__main__.Case object at 0x7f34c3fe48e0>, <__main__.Case object at 0x7f34c3fe5270>, <__main__.Case object at 0x7f34c3fe4d90>, <__main__.Case object at 0x7f34c3fe4850>, <__main__.Case object at 0x7f34c3fbe5c0>, <__main__.Case object at 0x7f34c3fe4f10>, <__main__.Case object at 0x7f34c3fe66e0>, <__main__.Case object at 0x7f34c3fe49a0>, <__main__.Case object at 0x7f34c3fe75b0>, <__main__.Case object at 0x7f34c3fe4a00>, <__main__.Case object at 0x7f34c3fe4ca0>, <__main__.Case object at 0x7f34c3fe4730>, <__main__.Case object at 0x7f34c3fe6bf0>, <__main__.Case object at 0x7f34c3fe4b20>, <__main__.Case object at 0x7f34c3fe59f0>, <__main__.Case object at 0x7f34c3fe6590>, <__main__.Case object at 0x7f34c3fe50c0>, <__main__.Case object at 0x7f34c3fe4cd0>, <__main__.Case object at 0x7f34c3fe46a0>, <__main__.Case object at 0x7f34c3fe6d40>, <__main__.Case object at 0x7f34c3fe5a50>, <__main__.Case object at 0x7f34c3fe6a40>, <__main__.Case object at 0x7f34c3fe6fb0>, <__main__.Case object at 0x7f34c3fe5f30>, <__main__.Case object at 0x7f34c3fe6470>, <__main__.Case object at 0x7f34c3fe5c90>, <__main__.Case object at 0x7f34c3fe5600>, <__main__.Case object at 0x7f34c3fe44f0>, <__main__.Case object at 0x7f34c3fe42e0>, <__main__.Case object at 0x7f34c3fe6e60>, <__main__.Case object at 0x7f34c3fe6050>, <__main__.Case object at 0x7f34c3fe5480>, <__main__.Case object at 0x7f34c3fc9270>, <__main__.Case object at 0x7f34c3fcb970>, <__main__.Case object at 0x7f34c3fe6200>, <__main__.Case object at 0x7f34c3fca6e0>, <__main__.Case object at 0x7f34c3fca1a0>, <__main__.Case object at 0x7f34c3fc8eb0>, <__main__.Case object at 0x7f34c3fe64a0>, <__main__.Case object at 0x7f34c3fc9750>, <__main__.Case object at 0x7f34c3fc9b10>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fe6fe0>, <__main__.Case object at 0x7f34c3fcb910>, <__main__.Case object at 0x7f34c3fca2c0>, <__main__.Case object at 0x7f34c3fcbb80>, <__main__.Case object at 0x7f34c3fe4be0>, <__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c3fe52a0>, <__main__.Case object at 0x7f34c3fe7130>, <__main__.Case object at 0x7f34c3fcb280>, <__main__.Case object at 0x7f34c3fc8130>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fcb610>, <__main__.Case object at 0x7f34c3fc9a50>, <__main__.Case object at 0x7f34c3fc95d0>, <__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c3fcb580>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fc9030>, <__main__.Case object at 0x7f34c3fc8fd0>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fe41f0>, <__main__.Case object at 0x7f34c3feca30>, <__main__.Case object at 0x7f34c3fecf40>, <__main__.Case object at 0x7f34c3fec730>, <__main__.Case object at 0x7f34c3fec460>, <__main__.Case object at 0x7f34c3fcba60>, <__main__.Case object at 0x7f34c3fec100>, <__main__.Case object at 0x7f34c3fed390>, <__main__.Case object at 0x7f34c3fec0a0>, <__main__.Case object at 0x7f34c3fc86a0>, <__main__.Case object at 0x7f34c3fec9d0>, <__main__.Case object at 0x7f34c3fec910>, <__main__.Case object at 0x7f34c3fed1b0>, <__main__.Case object at 0x7f34c3fc9120>, <__main__.Case object at 0x7f34c3fec6d0>, <__main__.Case object at 0x7f34c3feda80>, <__main__.Case object at 0x7f34c3fed930>, <__main__.Case object at 0x7f34c3fed7b0>, <__main__.Case object at 0x7f34c3fedc90>, <__main__.Case object at 0x7f34c3fed5d0>, <__main__.Case object at 0x7f34c3fed4b0>, <__main__.Case object at 0x7f34c3fede70>, <__main__.Case object at 0x7f34c3fec0d0>, <__main__.Case object at 0x7f34c3fee050>, <__main__.Case object at 0x7f34c3fee170>, <__main__.Case object at 0x7f34c3fee350>, <__main__.Case object at 0x7f34c3fee3e0>, <__main__.Case object at 0x7f34c3fee590>, <__main__.Case object at 0x7f34c3fee770>, <__main__.Case object at 0x7f34c3fee8c0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.5000000000000001, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 3, tv: 0.5, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.5, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.5, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.5, time steps: 13\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.5999999999999999)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5999999999999999)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.5999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5999999999999999)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5999999999999999)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5999999999999999)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 5), solution: 3, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 11\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f889d0>, <__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3f90340>, <__main__.Case object at 0x7f34c3fb3ee0>, <__main__.Case object at 0x7f34c3fb3d30>, <__main__.Case object at 0x7f34c3fbd510>, <__main__.Case object at 0x7f34c3fbcc10>, <__main__.Case object at 0x7f34c3fbe1d0>, <__main__.Case object at 0x7f34c3fbd3c0>, <__main__.Case object at 0x7f34c3fbf520>, <__main__.Case object at 0x7f34c3fbd480>, <__main__.Case object at 0x7f34c3fbcb50>, <__main__.Case object at 0x7f34c3fbe950>, <__main__.Case object at 0x7f34c3fbd4b0>, <__main__.Case object at 0x7f34c3fbde40>, <__main__.Case object at 0x7f34c3fbe6b0>, <__main__.Case object at 0x7f34c3fbfa30>, <__main__.Case object at 0x7f34c3fbdf30>, <__main__.Case object at 0x7f34c3fbc0a0>, <__main__.Case object at 0x7f34c3fbe9b0>, <__main__.Case object at 0x7f34c3fbfd30>, <__main__.Case object at 0x7f34c3fbf9d0>, <__main__.Case object at 0x7f34c3fbff70>, <__main__.Case object at 0x7f34c3fbf460>, <__main__.Case object at 0x7f34c3fbd5d0>, <__main__.Case object at 0x7f34c3fbd8d0>, <__main__.Case object at 0x7f34c3fbfe80>, <__main__.Case object at 0x7f34c3fbf2b0>, <__main__.Case object at 0x7f34c3fbc190>, <__main__.Case object at 0x7f34c3fbf160>, <__main__.Case object at 0x7f34c3fbd3f0>, <__main__.Case object at 0x7f34c3fbd720>, <__main__.Case object at 0x7f34c3fbe3b0>, <__main__.Case object at 0x7f34c3fbc520>, <__main__.Case object at 0x7f34c3fbce50>, <__main__.Case object at 0x7f34c3fbdbd0>, <__main__.Case object at 0x7f34c3fbfd60>, <__main__.Case object at 0x7f34c3fbea70>, <__main__.Case object at 0x7f34c3fbf820>, <__main__.Case object at 0x7f34c3fbcb20>, <__main__.Case object at 0x7f34c3fbd2d0>, <__main__.Case object at 0x7f34c3fbfcd0>, <__main__.Case object at 0x7f34c3fbc640>, <__main__.Case object at 0x7f34c3fbd9c0>, <__main__.Case object at 0x7f34c3fbd210>, <__main__.Case object at 0x7f34c3fbc130>, <__main__.Case object at 0x7f34c3fe47c0>, <__main__.Case object at 0x7f34c3fbd960>, <__main__.Case object at 0x7f34c3fe5ba0>, <__main__.Case object at 0x7f34c3fe4df0>, <__main__.Case object at 0x7f34c3fe5c30>, <__main__.Case object at 0x7f34c3fe44c0>, <__main__.Case object at 0x7f34c3fbf5b0>, <__main__.Case object at 0x7f34c3fe45b0>, <__main__.Case object at 0x7f34c3fe4f70>, <__main__.Case object at 0x7f34c3fe6b90>, <__main__.Case object at 0x7f34c3fe56f0>, <__main__.Case object at 0x7f34c3fbf7f0>, <__main__.Case object at 0x7f34c3fe4040>, <__main__.Case object at 0x7f34c3fe51b0>, <__main__.Case object at 0x7f34c3fe4c10>, <__main__.Case object at 0x7f34c3fe5660>, <__main__.Case object at 0x7f34c3fe6740>, <__main__.Case object at 0x7f34c3fe53c0>, <__main__.Case object at 0x7f34c3fe6b30>, <__main__.Case object at 0x7f34c3fe6830>, <__main__.Case object at 0x7f34c3fe6ef0>, <__main__.Case object at 0x7f34c3fe6a10>, <__main__.Case object at 0x7f34c3fe4580>, <__main__.Case object at 0x7f34c3fe6ec0>, <__main__.Case object at 0x7f34c3fe43a0>, <__main__.Case object at 0x7f34c3fe7640>, <__main__.Case object at 0x7f34c3fe73a0>, <__main__.Case object at 0x7f34c3fe6260>, <__main__.Case object at 0x7f34c3fe6dd0>, <__main__.Case object at 0x7f34c3fe7190>, <__main__.Case object at 0x7f34c3fe4fa0>, <__main__.Case object at 0x7f34c3fe5870>, <__main__.Case object at 0x7f34c3fe4820>, <__main__.Case object at 0x7f34c3fe6530>, <__main__.Case object at 0x7f34c3fe46d0>, <__main__.Case object at 0x7f34c3fe5210>, <__main__.Case object at 0x7f34c3fe4220>, <__main__.Case object at 0x7f34c3fe7340>, <__main__.Case object at 0x7f34c3fe56c0>, <__main__.Case object at 0x7f34c3fe4940>, <__main__.Case object at 0x7f34c3fe6b60>, <__main__.Case object at 0x7f34c3fe5f60>, <__main__.Case object at 0x7f34c3fe5900>, <__main__.Case object at 0x7f34c3fe4b50>, <__main__.Case object at 0x7f34c3fe4490>, <__main__.Case object at 0x7f34c3fe6620>, <__main__.Case object at 0x7f34c3fe7010>, <__main__.Case object at 0x7f34c3fe64d0>, <__main__.Case object at 0x7f34c3fca980>, <__main__.Case object at 0x7f34c3fcb4f0>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fe6800>, <__main__.Case object at 0x7f34c3fcab90>, <__main__.Case object at 0x7f34c3fc9150>, <__main__.Case object at 0x7f34c3fbdab0>, <__main__.Case object at 0x7f34c3fc9db0>, <__main__.Case object at 0x7f34c3fc8e50>, <__main__.Case object at 0x7f34c3fcabc0>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3fc9810>, <__main__.Case object at 0x7f34c3fcb9d0>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fcaef0>, <__main__.Case object at 0x7f34c3fcbf40>, <__main__.Case object at 0x7f34c3fcbac0>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fc84f0>, <__main__.Case object at 0x7f34c3fc8670>, <__main__.Case object at 0x7f34c3fc8430>, <__main__.Case object at 0x7f34c3fc9570>, <__main__.Case object at 0x7f34c3fbe770>, <__main__.Case object at 0x7f34c3fc9930>, <__main__.Case object at 0x7f34c3fc8070>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fcb3a0>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fcbd00>, <__main__.Case object at 0x7f34c3fc98d0>, <__main__.Case object at 0x7f34c3fca6b0>, <__main__.Case object at 0x7f34c3feca00>, <__main__.Case object at 0x7f34c3fca530>, <__main__.Case object at 0x7f34c3fec5e0>, <__main__.Case object at 0x7f34c3fec850>, <__main__.Case object at 0x7f34c3fecc40>, <__main__.Case object at 0x7f34c3fed180>, <__main__.Case object at 0x7f34c3fec250>, <__main__.Case object at 0x7f34c3fec670>, <__main__.Case object at 0x7f34c3fec280>, <__main__.Case object at 0x7f34c3fecc10>, <__main__.Case object at 0x7f34c3fecfa0>, <__main__.Case object at 0x7f34c3fc9c30>, <__main__.Case object at 0x7f34c3fec130>, <__main__.Case object at 0x7f34c3fcb430>, <__main__.Case object at 0x7f34c3fecbe0>, <__main__.Case object at 0x7f34c3fecd30>, <__main__.Case object at 0x7f34c3fed090>, <__main__.Case object at 0x7f34c3fedcf0>, <__main__.Case object at 0x7f34c3fedb70>, <__main__.Case object at 0x7f34c3fedab0>, <__main__.Case object at 0x7f34c3fe5750>, <__main__.Case object at 0x7f34c3fed870>, <__main__.Case object at 0x7f34c3fed750>, <__main__.Case object at 0x7f34c3fed630>, <__main__.Case object at 0x7f34c3fe54e0>, <__main__.Case object at 0x7f34c3feddb0>, <__main__.Case object at 0x7f34c3fedf00>, <__main__.Case object at 0x7f34c3fe5bd0>, <__main__.Case object at 0x7f34c3fee290>, <__main__.Case object at 0x7f34c3fee2c0>, <__main__.Case object at 0x7f34c3fee560>, <__main__.Case object at 0x7f34c3fee6b0>, <__main__.Case object at 0x7f34c3fee6e0>, <__main__.Case object at 0x7f34c3fee530>, <__main__.Case object at 0x7f34c3feea40>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.5000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.5000000000000001, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (1, 0), solution: 4, tv: 0.40000000000000013, time steps: 9\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.8999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 0), solution: 4, tv: 0.5000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (0, 0), solution: 4, tv: 0.5000000000000001, time steps: 6\n",
      "Episode: 37, Total Steps: 162, Total Rewards: [-261, 85], Status Episode: False\n",
      "------------------------------------------End of episode 37 loop--------------------\n",
      "----- starting point of Episode 38 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.6999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.6999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.6999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.6999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.6999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 4) with action 0 to next state (6, 4): pull reward: 0\n",
      "----- starting point of Episode 38 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 6) with action 3 to next state (4, 6): pull reward: 0\n",
      "----- starting point of Episode 38 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 0.8999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 2 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 38 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 4 to next state (2, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 2) with action 4 to next state (3, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 4 to next state (4, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 2) with action 0 to next state (4, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 2) with action 4 to next state (5, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 2) with action 4 to next state (6, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 2) with action 4 to next state (7, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 38 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (7, 2) with action 1 to next state (7, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 38 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (7, 1) with action 3 to next state (6, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 38 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 38 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 5) with action 1 to next state (4, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3f916c0>, <__main__.Case object at 0x7f34c3fb3bb0>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3fb3d60>, <__main__.Case object at 0x7f34c3fbd990>, <__main__.Case object at 0x7f34c3fbd690>, <__main__.Case object at 0x7f34c3fbdcc0>, <__main__.Case object at 0x7f34c3fbdcf0>, <__main__.Case object at 0x7f34c3fbe260>, <__main__.Case object at 0x7f34c3fbe350>, <__main__.Case object at 0x7f34c3fbc7f0>, <__main__.Case object at 0x7f34c3fbf3d0>, <__main__.Case object at 0x7f34c3fbefb0>, <__main__.Case object at 0x7f34c3fbdae0>, <__main__.Case object at 0x7f34c3fbc250>, <__main__.Case object at 0x7f34c3fbc8b0>, <__main__.Case object at 0x7f34c3fbed10>, <__main__.Case object at 0x7f34c3fbcaf0>, <__main__.Case object at 0x7f34c3fbd360>, <__main__.Case object at 0x7f34c3fbc6d0>, <__main__.Case object at 0x7f34c3fbecb0>, <__main__.Case object at 0x7f34c3fbdde0>, <__main__.Case object at 0x7f34c3fbdb40>, <__main__.Case object at 0x7f34c3fbe290>, <__main__.Case object at 0x7f34c3fbd150>, <__main__.Case object at 0x7f34c3fbe680>, <__main__.Case object at 0x7f34c3fbda50>, <__main__.Case object at 0x7f34c3fbd750>, <__main__.Case object at 0x7f34c3fbe5f0>, <__main__.Case object at 0x7f34c3fbe740>, <__main__.Case object at 0x7f34c3fbd510>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3fa8820>, <__main__.Case object at 0x7f34c3f9f6a0>, <__main__.Case object at 0x7f34c3f88a30>, <__main__.Case object at 0x7f34c3fb3cd0>, <__main__.Case object at 0x7f34c3f72350>, <__main__.Case object at 0x7f34c3fb3be0>, <__main__.Case object at 0x7f34c3fbc040>, <__main__.Case object at 0x7f34c3fbc9d0>, <__main__.Case object at 0x7f34c3fbf2e0>, <__main__.Case object at 0x7f34c3fbf010>, <__main__.Case object at 0x7f34c3fbdba0>, <__main__.Case object at 0x7f34c3fbf6d0>, <__main__.Case object at 0x7f34c3fbf4c0>, <__main__.Case object at 0x7f34c3fbe620>, <__main__.Case object at 0x7f34c3fbe860>, <__main__.Case object at 0x7f34c3fbff40>, <__main__.Case object at 0x7f34c3fbe110>, <__main__.Case object at 0x7f34c3faafe0>, <__main__.Case object at 0x7f34c3fbd9f0>, <__main__.Case object at 0x7f34c3fb3ee0>, <__main__.Case object at 0x7f34c3fbfeb0>, <__main__.Case object at 0x7f34c3fbf940>, <__main__.Case object at 0x7f34c3fbc2e0>, <__main__.Case object at 0x7f34c3fbee90>, <__main__.Case object at 0x7f34c3fbe830>, <__main__.Case object at 0x7f34c3fbef20>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.40000000000000013, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.4, time steps: 13\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 3, 0.5)\n",
      "Episode succeeded, case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 1, 0.5)\n",
      "Episode succeeded, case (6, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) is empty. Temporary case base stored to the case base: ((5, 2), 4, 0.5)\n",
      "Episode succeeded, case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 4, 0.5)\n",
      "Episode succeeded, case (4, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (2, 2) is empty. Temporary case base stored to the case base: ((2, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 4, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 2, 0.5)\n",
      "Episode succeeded, case (1, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 4, 0.5)\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.6999999999999998)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.6999999999999998)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.6999999999999998)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.6999999999999998)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.6999999999999998)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 5), solution: 3, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 11\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 1), solution: 3, tv: 0.5, time steps: 24\n",
      "cases content after RETAIN, problem: (7, 2), solution: 1, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.5, time steps: 20\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.5, time steps: 11\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.5, time steps: 7\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f9f6d0>, <__main__.Case object at 0x7f34c3f89b40>, <__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3fb0340>, <__main__.Case object at 0x7f34c3fb3f70>, <__main__.Case object at 0x7f34c3fb3d30>, <__main__.Case object at 0x7f34c3fbc550>, <__main__.Case object at 0x7f34c3fbc4f0>, <__main__.Case object at 0x7f34c3fbc580>, <__main__.Case object at 0x7f34c3fbd450>, <__main__.Case object at 0x7f34c3fbdb10>, <__main__.Case object at 0x7f34c3fbe7a0>, <__main__.Case object at 0x7f34c3fbcfa0>, <__main__.Case object at 0x7f34c3fbc340>, <__main__.Case object at 0x7f34c3fbded0>, <__main__.Case object at 0x7f34c3fbcdf0>, <__main__.Case object at 0x7f34c3fbcb80>, <__main__.Case object at 0x7f34c3fbc850>, <__main__.Case object at 0x7f34c3fbfc70>, <__main__.Case object at 0x7f34c3fbd4e0>, <__main__.Case object at 0x7f34c3fbec20>, <__main__.Case object at 0x7f34c3fbf340>, <__main__.Case object at 0x7f34c3fbca00>, <__main__.Case object at 0x7f34c3fbccd0>, <__main__.Case object at 0x7f34c3fbe200>, <__main__.Case object at 0x7f34c3fbc400>, <__main__.Case object at 0x7f34c3fbdd50>, <__main__.Case object at 0x7f34c3fbdd80>, <__main__.Case object at 0x7f34c3fbead0>, <__main__.Case object at 0x7f34c3fbc730>, <__main__.Case object at 0x7f34c3fbec80>, <__main__.Case object at 0x7f34c3fbe1d0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 4, tv: 0.40000000000000013, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 4, tv: 0.40000000000000013, time steps: 6\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "Episode: 38, Total Steps: 32, Total Rewards: [69, 87], Status Episode: True\n",
      "------------------------------------------End of episode 38 loop--------------------\n",
      "----- starting point of Episode 39 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.7999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 2, 0.5, 7)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.7999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 1), 4, 0.5, 11)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 1), 2, 0.5, 13)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 0) with action 1 to next state (7, 0): pull reward: 0\n",
      "----- starting point of Episode 39 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.7999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 4, 0.5, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 hit the obstacle!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.7999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 0.5, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.7999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 0.5, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.7999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 0.5, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.7999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 2) with action 4 to next state (6, 2): pull reward: 0\n",
      "comm next state for agent 1: ((2, 2), 4, 0.5, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.7999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (6, 2) with action 4 to next state (7, 2): pull reward: 0\n",
      "comm next state for agent 1: ((2, 2), 4, 0.5, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.7999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 0.5, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 39 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.7999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 2), 4, 0.5, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f88970>, <__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3f70e50>, <__main__.Case object at 0x7f34c3fb3ee0>, <__main__.Case object at 0x7f34c3fb3f70>, <__main__.Case object at 0x7f34c3fbc040>, <__main__.Case object at 0x7f34c3fbdba0>, <__main__.Case object at 0x7f34c3fbe860>, <__main__.Case object at 0x7f34c3fbd390>, <__main__.Case object at 0x7f34c3fbee90>, <__main__.Case object at 0x7f34c3fbdcf0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3fa8a00>, <__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c3f916c0>, <__main__.Case object at 0x7f34c3fa8820>, <__main__.Case object at 0x7f34c3fb3d30>, <__main__.Case object at 0x7f34c3fbf010>, <__main__.Case object at 0x7f34c3fbe620>, <__main__.Case object at 0x7f34c3fbfaf0>, <__main__.Case object at 0x7f34c3fbc2e0>, <__main__.Case object at 0x7f34c3fbd690>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 1), solution: 3, tv: 0.3, time steps: 24\n",
      "case content after REVISE for agent 0, problem: (7, 2), solution: 1, tv: 0.3, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.3, time steps: 21\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 0.3, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.3, time steps: 18\n",
      "case content after REVISE for agent 0, problem: (2, 2), solution: 4, tv: 0.3, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 4, tv: 0.3, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 2, tv: 0.3, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 4, tv: 0.3, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.3, time steps: 7\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.7999999999999998)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.7999999999999998)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.7999999999999998)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.7999999999999998)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.7999999999999998)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.7999999999999998)\n",
      "Integrated case process. comm case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 2, 0.7999999999999998)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.7999999999999998)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 5), solution: 3, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 11\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.7999999999999998, time steps: 13\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f89b40>, <__main__.Case object at 0x7f34c3f9f640>, <__main__.Case object at 0x7f34c3fb2920>, <__main__.Case object at 0x7f34c3fb3f10>, <__main__.Case object at 0x7f34c3fbd3c0>, <__main__.Case object at 0x7f34c3fbf2e0>, <__main__.Case object at 0x7f34c3fbf4c0>, <__main__.Case object at 0x7f34c3fbe110>, <__main__.Case object at 0x7f34c3fbf8b0>, <__main__.Case object at 0x7f34c3fbd990>, <__main__.Case object at 0x7f34c3fbe350>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3f89d20>, <__main__.Case object at 0x7f34c3f9f6d0>, <__main__.Case object at 0x7f34c3fb3cd0>, <__main__.Case object at 0x7f34c3fb3b80>, <__main__.Case object at 0x7f34c3fbf4f0>, <__main__.Case object at 0x7f34c3fbc9d0>, <__main__.Case object at 0x7f34c3fbf6d0>, <__main__.Case object at 0x7f34c3fbd5a0>, <__main__.Case object at 0x7f34c3fbf850>, <__main__.Case object at 0x7f34c3fbe830>, <__main__.Case object at 0x7f34c3fbe260>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (2, 2) is empty. Temporary case base stored to the case base: ((2, 2), 4, 0.5)\n",
      "Integrated case process. comm case (2, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Integrated case process. comm case (2, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Integrated case process. comm case (2, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Integrated case process. comm case (2, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Integrated case process. comm case (2, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Integrated case process. comm case (2, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 2), 4, 0.5)\n",
      "Integrated case process. comm case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 4, 0.5)\n",
      "Integrated case process. comm case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 2, 0.5)\n",
      "Integrated case process. comm case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 4, 0.5)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.9999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (2, 2), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 2), solution: 4, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (1, 1), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (0, 1), solution: 4, tv: 0.5, time steps: 11\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.5, time steps: 7\n",
      "Episode: 39, Total Steps: 11, Total Rewards: [-110, -104], Status Episode: False\n",
      "------------------------------------------End of episode 39 loop--------------------\n",
      "----- starting point of Episode 40 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.5999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 0) with action 1 to next state (8, 0): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.5999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.5999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 0) with action 3 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 0) with action 0 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 0) with action 2 to next state (5, 1): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 1) with action 3 to next state (4, 1): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 1) with action 4 to next state (5, 1): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 1) with action 2 to next state (5, 2): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 2) with action 2 to next state (5, 3): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 3) with action 3 to next state (4, 3): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 3) with action 4 to next state (5, 3): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 3) with action 0 to next state (5, 3): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 3) with action 1 to next state (5, 2): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 2) with action 1 to next state (5, 1): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 1) with action 1 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 0) with action 3 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 2 to next state (4, 1): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 1) with action 0 to next state (4, 1): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 1) with action 2 to next state (4, 2): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 2) with action 4 to next state (5, 2): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 2) with action 0 to next state (5, 2): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 2) with action 4 to next state (6, 2): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 2) with action 0 to next state (6, 2): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 5) with action 3 to next state (5, 5): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 5) with action 1 to next state (5, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 4) with action 1 to next state (5, 3): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 3) with action 2 to next state (5, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 4) with action 4 to next state (6, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 5), 1, 0.9999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 82 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 83 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 84 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 85 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 86 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 87 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 88 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 89 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 90 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 91 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 92 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 93 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 94 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 95 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 96 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 97 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 98 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 99 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 100 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 101 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 102 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 103 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 104 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 105 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 106 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 107 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 108 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 109 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 110 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 111 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 112 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 113 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 114 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 115 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 116 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 117 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 118 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 119 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 120 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 121 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 122 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 123 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 124 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 125 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 126 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 127 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 128 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 129 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 130 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 131 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 132 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 133 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 134 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 135 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 136 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 137 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 138 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 139 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 140 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 141 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 142 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 143 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 144 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 145 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 146 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 147 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 148 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 149 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 150 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 151 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 152 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 153 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 154 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 155 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 156 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 157 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 158 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 159 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 160 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 161 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 162 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 163 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 164 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 165 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 166 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 167 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 168 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 169 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 170 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 171 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 172 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 173 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 174 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 175 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 176 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 177 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 178 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 179 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 180 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 181 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 182 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 183 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 184 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 185 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 186 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 187 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 188 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 189 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 190 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 191 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 192 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 193 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 194 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 195 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 196 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 197 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 198 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 199 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 200 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 201 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 202 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 203 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 204 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 205 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 206 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 207 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 208 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 209 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 210 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 211 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 212 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 213 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 214 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 215 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 216 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 217 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 218 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 219 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 220 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 221 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 222 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 223 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 224 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 225 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 226 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 227 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 228 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 229 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 230 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 231 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 232 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 233 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 234 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 235 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 236 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 237 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 238 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 239 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 240 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 241 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 242 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 243 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 244 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 245 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 246 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 247 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 248 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 249 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 250 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 251 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 252 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 253 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 254 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 255 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 256 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 257 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 258 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 259 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 260 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 261 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 262 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 263 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 264 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 265 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 266 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 267 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 268 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 269 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 270 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 271 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 272 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 273 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 274 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 275 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 276 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 277 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 278 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 279 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 280 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 281 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 282 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 283 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 284 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 285 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 286 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 287 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 288 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 289 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 290 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 291 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 292 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 293 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 294 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 295 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 296 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 297 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 298 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 299 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 300 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 301 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 302 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 303 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 304 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 305 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 306 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 307 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 308 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 309 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 310 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 311 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 312 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 313 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 314 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 315 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 316 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 317 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 318 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 319 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 320 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 321 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 322 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 323 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 324 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 325 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 326 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 327 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 328 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 2 to next state (0, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 329 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 330 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 331 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 3 to next state (0, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 332 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 0 to next state (0, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 333 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 1 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 334 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 335 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 336 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 337 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 338 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 1 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 339 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 340 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 341 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 342 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 343 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 344 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 345 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 346 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 347 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 348 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 349 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 350 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 351 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 352 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 353 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 354 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 355 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 356 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 357 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 358 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 359 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 360 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 361 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 362 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 363 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 364 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 365 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 366 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 367 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 368 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 369 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 370 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 371 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 372 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 373 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 374 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 375 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 376 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 377 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 378 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 379 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 380 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 381 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 382 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 383 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 384 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 385 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 386 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 387 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 388 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 389 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 390 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 391 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 392 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 393 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 394 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 395 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 396 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 397 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 398 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 399 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 400 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 401 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 402 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 403 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 404 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 405 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 406 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 407 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 408 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 409 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 410 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 411 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 412 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 413 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 414 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 415 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 416 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 417 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 418 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 419 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 420 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 421 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 422 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 423 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 424 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 425 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 426 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 427 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 428 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 429 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 430 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 431 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 432 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 433 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 434 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 435 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 436 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 437 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 438 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 439 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 440 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 441 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 442 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 443 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 444 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 445 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 446 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 447 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 448 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 449 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 450 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 451 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 452 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 453 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 454 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 455 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 456 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 457 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 458 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 459 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 460 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 461 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 462 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 463 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 464 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 465 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 466 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 467 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 468 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 469 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 470 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 471 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 472 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 473 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 474 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 475 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 476 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 477 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 478 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 479 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 480 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 481 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 482 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 483 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 484 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 485 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 486 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 487 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 488 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 489 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 490 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 491 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 492 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 493 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 494 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 495 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 496 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 497 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 498 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 499 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 500 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 501 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 502 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 503 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 2 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 504 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 3 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 505 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 0 to next state (0, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 506 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 3) with action 2 to next state (0, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 507 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 4) with action 4 to next state (1, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 508 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 4) with action 2 to next state (1, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 509 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 5) with action 1 to next state (1, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 510 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 4) with action 1 to next state (1, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 511 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 3) with action 0 to next state (1, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 512 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 3) with action 4 to next state (2, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 513 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 3) with action 1 to next state (2, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 514 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 2) with action 4 to next state (3, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 515 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 0 to next state (3, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 516 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 0 to next state (3, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 517 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 3 to next state (2, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 518 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 2) with action 4 to next state (3, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 519 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 3 to next state (2, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 40 in steps 520 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 2) with action 4 to next state (3, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 40 in steps 521 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 2) with action 2 to next state (3, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3f889d0>, <__main__.Case object at 0x7f34c3f90340>, <__main__.Case object at 0x7f34c3fb3dc0>, <__main__.Case object at 0x7f34c3fb3bb0>, <__main__.Case object at 0x7f34c3fa8a00>, <__main__.Case object at 0x7f34c3fbe620>, <__main__.Case object at 0x7f34c3fbfaf0>, <__main__.Case object at 0x7f34c3fbd930>, <__main__.Case object at 0x7f34c3faafe0>, <__main__.Case object at 0x7f34c3fbd0c0>, <__main__.Case object at 0x7f34c3fbcf10>, <__main__.Case object at 0x7f34c3fbcf70>, <__main__.Case object at 0x7f34c3fb3e80>, <__main__.Case object at 0x7f34c3fbd390>, <__main__.Case object at 0x7f34c3fbf370>, <__main__.Case object at 0x7f34c3fbdba0>, <__main__.Case object at 0x7f34c3fbf970>, <__main__.Case object at 0x7f34c3fbfdc0>, <__main__.Case object at 0x7f34c3fbfca0>, <__main__.Case object at 0x7f34c3fbc700>, <__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c3fbd330>, <__main__.Case object at 0x7f34c3fbffd0>, <__main__.Case object at 0x7f34c3fbd7e0>, <__main__.Case object at 0x7f34c3fbf0a0>, <__main__.Case object at 0x7f34c3fbdb40>, <__main__.Case object at 0x7f34c3fbdf00>, <__main__.Case object at 0x7f34c3fbfe20>, <__main__.Case object at 0x7f34c3fbece0>, <__main__.Case object at 0x7f34c3fbded0>, <__main__.Case object at 0x7f34c3fbcb80>, <__main__.Case object at 0x7f34c3fbf8e0>, <__main__.Case object at 0x7f34c3fbec20>, <__main__.Case object at 0x7f34c3fbe110>, <__main__.Case object at 0x7f34c3fbf340>, <__main__.Case object at 0x7f34c3fbfe50>, <__main__.Case object at 0x7f34c3fbe4a0>, <__main__.Case object at 0x7f34c3fbd7b0>, <__main__.Case object at 0x7f34c3fbcee0>, <__main__.Case object at 0x7f34c3fbe530>, <__main__.Case object at 0x7f34c3fbf460>, <__main__.Case object at 0x7f34c3fbc580>, <__main__.Case object at 0x7f34c3fbc0a0>, <__main__.Case object at 0x7f34c3fbde40>, <__main__.Case object at 0x7f34c3fbd480>, <__main__.Case object at 0x7f34c3fbd180>, <__main__.Case object at 0x7f34c3fbe3e0>, <__main__.Case object at 0x7f34c3fbce80>, <__main__.Case object at 0x7f34c3fbfee0>, <__main__.Case object at 0x7f34c3fbf670>, <__main__.Case object at 0x7f34c3fbe770>, <__main__.Case object at 0x7f34c3fe48b0>, <__main__.Case object at 0x7f34c3fe5120>, <__main__.Case object at 0x7f34c3fe67a0>, <__main__.Case object at 0x7f34c3fe6680>, <__main__.Case object at 0x7f34c3fe6c80>, <__main__.Case object at 0x7f34c3fe4f10>, <__main__.Case object at 0x7f34c3fe4a00>, <__main__.Case object at 0x7f34c3fe6bf0>, <__main__.Case object at 0x7f34c3fe6590>, <__main__.Case object at 0x7f34c3fe4cd0>, <__main__.Case object at 0x7f34c3fe6a40>, <__main__.Case object at 0x7f34c3fe6470>, <__main__.Case object at 0x7f34c3fe44f0>, <__main__.Case object at 0x7f34c3fe6e60>, <__main__.Case object at 0x7f34c3fe46a0>, <__main__.Case object at 0x7f34c3fe7130>, <__main__.Case object at 0x7f34c3fe4160>, <__main__.Case object at 0x7f34c3fe63e0>, <__main__.Case object at 0x7f34c3fe7490>, <__main__.Case object at 0x7f34c3fe67d0>, <__main__.Case object at 0x7f34c3fe5450>, <__main__.Case object at 0x7f34c3fe6770>, <__main__.Case object at 0x7f34c3fe6980>, <__main__.Case object at 0x7f34c3fe50f0>, <__main__.Case object at 0x7f34c3fe4910>, <__main__.Case object at 0x7f34c3fe4550>, <__main__.Case object at 0x7f34c3fe5570>, <__main__.Case object at 0x7f34c3fe5780>, <__main__.Case object at 0x7f34c3fe6920>, <__main__.Case object at 0x7f34c3fe6d70>, <__main__.Case object at 0x7f34c3fe62f0>, <__main__.Case object at 0x7f34c3fe5ba0>, <__main__.Case object at 0x7f34c3fe44c0>, <__main__.Case object at 0x7f34c3fe70a0>, <__main__.Case object at 0x7f34c3fe56f0>, <__main__.Case object at 0x7f34c3fe4c10>, <__main__.Case object at 0x7f34c3fe53c0>, <__main__.Case object at 0x7f34c3fe6830>, <__main__.Case object at 0x7f34c3fe6ec0>, <__main__.Case object at 0x7f34c3fe7640>, <__main__.Case object at 0x7f34c3fe6260>, <__main__.Case object at 0x7f34c3fe4fa0>, <__main__.Case object at 0x7f34c3fe46d0>, <__main__.Case object at 0x7f34c3fe4220>, <__main__.Case object at 0x7f34c3fe56c0>, <__main__.Case object at 0x7f34c3fe5f60>, <__main__.Case object at 0x7f34c3fe6620>, <__main__.Case object at 0x7f34c3fe54e0>, <__main__.Case object at 0x7f34c3fc9d50>, <__main__.Case object at 0x7f34c3fc9660>, <__main__.Case object at 0x7f34c3fc9270>, <__main__.Case object at 0x7f34c3fc8250>, <__main__.Case object at 0x7f34c3fc9750>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fc87c0>, <__main__.Case object at 0x7f34c3fcb280>, <__main__.Case object at 0x7f34c3fcb610>, <__main__.Case object at 0x7f34c3fc95d0>, <__main__.Case object at 0x7f34c3fc9030>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c3fcb2b0>, <__main__.Case object at 0x7f34c3fcba00>, <__main__.Case object at 0x7f34c3fca170>, <__main__.Case object at 0x7f34c3fc8dc0>, <__main__.Case object at 0x7f34c3fcad70>, <__main__.Case object at 0x7f34c3fc98a0>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fca950>, <__main__.Case object at 0x7f34c3fc8a90>, <__main__.Case object at 0x7f34c3fcb4c0>, <__main__.Case object at 0x7f34c3fca620>, <__main__.Case object at 0x7f34c3fcb4f0>, <__main__.Case object at 0x7f34c3fc8d60>, <__main__.Case object at 0x7f34c3fc8e50>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fc9810>, <__main__.Case object at 0x7f34c3fcb550>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fc8430>, <__main__.Case object at 0x7f34c3fc8730>, <__main__.Case object at 0x7f34c3fcb130>, <__main__.Case object at 0x7f34c3fcbd00>, <__main__.Case object at 0x7f34c3fc8e20>, <__main__.Case object at 0x7f34c3fee830>, <__main__.Case object at 0x7f34c3fec760>, <__main__.Case object at 0x7f34c3feca30>, <__main__.Case object at 0x7f34c3fec460>, <__main__.Case object at 0x7f34c3fec220>, <__main__.Case object at 0x7f34c3fec9d0>, <__main__.Case object at 0x7f34c3fedae0>, <__main__.Case object at 0x7f34c3fed720>, <__main__.Case object at 0x7f34c3fedd50>, <__main__.Case object at 0x7f34c3fedfc0>, <__main__.Case object at 0x7f34c3fee4a0>, <__main__.Case object at 0x7f34c3fed480>, <__main__.Case object at 0x7f34c3fecb20>, <__main__.Case object at 0x7f34c3fec430>, <__main__.Case object at 0x7f34c3fed030>, <__main__.Case object at 0x7f34c3fec880>, <__main__.Case object at 0x7f34c3fed240>, <__main__.Case object at 0x7f34c3fedba0>, <__main__.Case object at 0x7f34c3fed6c0>, <__main__.Case object at 0x7f34c3fedde0>, <__main__.Case object at 0x7f34c3fee140>, <__main__.Case object at 0x7f34c3fee410>, <__main__.Case object at 0x7f34c3fee8f0>, <__main__.Case object at 0x7f34c3fec3d0>, <__main__.Case object at 0x7f34c3fec850>, <__main__.Case object at 0x7f34c3fe4940>, <__main__.Case object at 0x7f34c3feca60>, <__main__.Case object at 0x7f34c3fec520>, <__main__.Case object at 0x7f34c3fecd30>, <__main__.Case object at 0x7f34c3fedcf0>, <__main__.Case object at 0x7f34c3fed120>, <__main__.Case object at 0x7f34c3fed5a0>, <__main__.Case object at 0x7f34c3fee1a0>, <__main__.Case object at 0x7f34c3fee440>, <__main__.Case object at 0x7f34c3fee920>, <__main__.Case object at 0x7f34c3fef400>, <__main__.Case object at 0x7f34c3fef2e0>, <__main__.Case object at 0x7f34c3fef1f0>, <__main__.Case object at 0x7f34c3fef070>, <__main__.Case object at 0x7f34c3feef50>, <__main__.Case object at 0x7f34c3feee00>, <__main__.Case object at 0x7f34c3feed70>, <__main__.Case object at 0x7f34c3feebc0>, <__main__.Case object at 0x7f34c3feeb30>, <__main__.Case object at 0x7f34c3fecac0>, <__main__.Case object at 0x7f34c3fef6d0>, <__main__.Case object at 0x7f34c3fef730>, <__main__.Case object at 0x7f34c3fef850>, <__main__.Case object at 0x7f34c3fef970>, <__main__.Case object at 0x7f34c3fefb50>, <__main__.Case object at 0x7f34c3fefbb0>, <__main__.Case object at 0x7f34c3fefcd0>, <__main__.Case object at 0x7f34c3fefdf0>, <__main__.Case object at 0x7f34c3fefeb0>, <__main__.Case object at 0x7f34c0f0c070>, <__main__.Case object at 0x7f34c0f0c190>, <__main__.Case object at 0x7f34c0f0c250>, <__main__.Case object at 0x7f34c3fef4f0>, <__main__.Case object at 0x7f34c0f0c3d0>, <__main__.Case object at 0x7f34c0f0c550>, <__main__.Case object at 0x7f34c0f0c610>, <__main__.Case object at 0x7f34c0f0c430>, <__main__.Case object at 0x7f34c0f0c850>, <__main__.Case object at 0x7f34c0f0c910>, <__main__.Case object at 0x7f34c0f0c730>, <__main__.Case object at 0x7f34c0f0ca30>, <__main__.Case object at 0x7f34c0f0cc10>, <__main__.Case object at 0x7f34c0f0cd30>, <__main__.Case object at 0x7f34c0f0ce50>, <__main__.Case object at 0x7f34c0f0cf10>, <__main__.Case object at 0x7f34c0f0d090>, <__main__.Case object at 0x7f34c0f0d1b0>, <__main__.Case object at 0x7f34c0f0d2d0>, <__main__.Case object at 0x7f34c0f0d390>, <__main__.Case object at 0x7f34c0f0d510>, <__main__.Case object at 0x7f34c0f0d630>, <__main__.Case object at 0x7f34c0f0d750>, <__main__.Case object at 0x7f34c0f0d810>, <__main__.Case object at 0x7f34c0f0d990>, <__main__.Case object at 0x7f34c0f0dab0>, <__main__.Case object at 0x7f34c0f0dbd0>, <__main__.Case object at 0x7f34c0f0ddb0>, <__main__.Case object at 0x7f34c0f0de10>, <__main__.Case object at 0x7f34c0f0df30>, <__main__.Case object at 0x7f34c0f0e050>, <__main__.Case object at 0x7f34c0f0e110>, <__main__.Case object at 0x7f34c0f0c760>, <__main__.Case object at 0x7f34c0f0e2f0>, <__main__.Case object at 0x7f34c0f0e410>, <__main__.Case object at 0x7f34c0f0e5f0>, <__main__.Case object at 0x7f34c0f0e650>, <__main__.Case object at 0x7f34c0f0e770>, <__main__.Case object at 0x7f34c0f0e890>, <__main__.Case object at 0x7f34c0f0ea70>, <__main__.Case object at 0x7f34c0f0ead0>, <__main__.Case object at 0x7f34c0f0ebf0>, <__main__.Case object at 0x7f34c0f0ed10>, <__main__.Case object at 0x7f34c0f0edd0>, <__main__.Case object at 0x7f34c0f0ef50>, <__main__.Case object at 0x7f34c0f0f070>, <__main__.Case object at 0x7f34c0f0f190>, <__main__.Case object at 0x7f34c0f0f250>, <__main__.Case object at 0x7f34c0f0f3d0>, <__main__.Case object at 0x7f34c0f0f4f0>, <__main__.Case object at 0x7f34c0f0f610>, <__main__.Case object at 0x7f34c0f0f6d0>, <__main__.Case object at 0x7f34c0f0f850>, <__main__.Case object at 0x7f34c0f0f970>, <__main__.Case object at 0x7f34c0f0fa90>, <__main__.Case object at 0x7f34c0f0fc70>, <__main__.Case object at 0x7f34c0f0fd00>, <__main__.Case object at 0x7f34c0f0fe20>, <__main__.Case object at 0x7f34c0f0ff40>, <__main__.Case object at 0x7f34c0f14040>, <__main__.Case object at 0x7f34c0f141c0>, <__main__.Case object at 0x7f34c0f142e0>, <__main__.Case object at 0x7f34c0f14400>, <__main__.Case object at 0x7f34c0f144c0>, <__main__.Case object at 0x7f34c0f14640>, <__main__.Case object at 0x7f34c0f14760>, <__main__.Case object at 0x7f34c0f14880>, <__main__.Case object at 0x7f34c0f14940>, <__main__.Case object at 0x7f34c0f14ac0>, <__main__.Case object at 0x7f34c0f14be0>, <__main__.Case object at 0x7f34c0f14d00>, <__main__.Case object at 0x7f34c0f14dc0>, <__main__.Case object at 0x7f34c0f14f40>, <__main__.Case object at 0x7f34c0f15060>, <__main__.Case object at 0x7f34c0f15120>, <__main__.Case object at 0x7f34c0f0e530>, <__main__.Case object at 0x7f34c0f152d0>, <__main__.Case object at 0x7f34c0f15420>, <__main__.Case object at 0x7f34c0f15540>, <__main__.Case object at 0x7f34c0f15600>, <__main__.Case object at 0x7f34c0f15780>, <__main__.Case object at 0x7f34c0f158a0>, <__main__.Case object at 0x7f34c0f159c0>, <__main__.Case object at 0x7f34c0f15a80>, <__main__.Case object at 0x7f34c0f15c00>, <__main__.Case object at 0x7f34c0f15d20>, <__main__.Case object at 0x7f34c0f15e40>, <__main__.Case object at 0x7f34c0f16020>, <__main__.Case object at 0x7f34c0f14550>, <__main__.Case object at 0x7f34c0f160e0>, <__main__.Case object at 0x7f34c0f16260>, <__main__.Case object at 0x7f34c0f16320>, <__main__.Case object at 0x7f34c0f164a0>, <__main__.Case object at 0x7f34c0f165c0>, <__main__.Case object at 0x7f34c0f166e0>, <__main__.Case object at 0x7f34c0f167a0>, <__main__.Case object at 0x7f34c0f16920>, <__main__.Case object at 0x7f34c0f16a40>, <__main__.Case object at 0x7f34c0f16b00>, <__main__.Case object at 0x7f34c0f16140>, <__main__.Case object at 0x7f34c0f16d40>, <__main__.Case object at 0x7f34c0f16e60>, <__main__.Case object at 0x7f34c0f16f20>, <__main__.Case object at 0x7f34c0f16c20>, <__main__.Case object at 0x7f34c0f17160>, <__main__.Case object at 0x7f34c0f17280>, <__main__.Case object at 0x7f34c0f173a0>, <__main__.Case object at 0x7f34c0f175b0>, <__main__.Case object at 0x7f34c0f17610>, <__main__.Case object at 0x7f34c0f17730>, <__main__.Case object at 0x7f34c0f177f0>, <__main__.Case object at 0x7f34c0f17040>, <__main__.Case object at 0x7f34c0f17a30>, <__main__.Case object at 0x7f34c0f17b50>, <__main__.Case object at 0x7f34c0f17c70>, <__main__.Case object at 0x7f34c0f17d30>, <__main__.Case object at 0x7f34c0f17eb0>, <__main__.Case object at 0x7f34c0f17fd0>, <__main__.Case object at 0x7f34c0f1c130>, <__main__.Case object at 0x7f34c0f1c1f0>, <__main__.Case object at 0x7f34c0f1c370>, <__main__.Case object at 0x7f34c0f1c490>, <__main__.Case object at 0x7f34c0f1c5b0>, <__main__.Case object at 0x7f34c0f1c670>, <__main__.Case object at 0x7f34c0f1c7f0>, <__main__.Case object at 0x7f34c0f1c910>, <__main__.Case object at 0x7f34c0f1ca30>, <__main__.Case object at 0x7f34c0f174f0>, <__main__.Case object at 0x7f34c0f1cbe0>, <__main__.Case object at 0x7f34c0f1cd30>, <__main__.Case object at 0x7f34c0f1ce50>, <__main__.Case object at 0x7f34c0f1cf10>, <__main__.Case object at 0x7f34c0f1d090>, <__main__.Case object at 0x7f34c0f1d1b0>, <__main__.Case object at 0x7f34c0f1d2d0>, <__main__.Case object at 0x7f34c0f1d390>, <__main__.Case object at 0x7f34c0f1d510>, <__main__.Case object at 0x7f34c0f1d630>, <__main__.Case object at 0x7f34c0f1d750>, <__main__.Case object at 0x7f34c0f1d930>, <__main__.Case object at 0x7f34c0f1d990>, <__main__.Case object at 0x7f34c0f1dab0>, <__main__.Case object at 0x7f34c0f1dbd0>, <__main__.Case object at 0x7f34c0f1ddb0>, <__main__.Case object at 0x7f34c0f1de10>, <__main__.Case object at 0x7f34c0f1df30>, <__main__.Case object at 0x7f34c0f1e050>, <__main__.Case object at 0x7f34c0f1e110>, <__main__.Case object at 0x7f34c0f1e290>, <__main__.Case object at 0x7f34c0f1e3b0>, <__main__.Case object at 0x7f34c0f1e4d0>, <__main__.Case object at 0x7f34c0f1e6b0>, <__main__.Case object at 0x7f34c0f1e710>, <__main__.Case object at 0x7f34c0f1e830>, <__main__.Case object at 0x7f34c0f1e950>, <__main__.Case object at 0x7f34c0f1ea10>, <__main__.Case object at 0x7f34c0f1eb90>, <__main__.Case object at 0x7f34c0f1ecb0>, <__main__.Case object at 0x7f34c0f1edd0>, <__main__.Case object at 0x7f34c0f1ee90>, <__main__.Case object at 0x7f34c0f1f010>, <__main__.Case object at 0x7f34c0f1f130>, <__main__.Case object at 0x7f34c0f1f250>, <__main__.Case object at 0x7f34c0f1f310>, <__main__.Case object at 0x7f34c0f1d8a0>, <__main__.Case object at 0x7f34c0f1f4f0>, <__main__.Case object at 0x7f34c0f1f670>, <__main__.Case object at 0x7f34c0f1f850>, <__main__.Case object at 0x7f34c0f1f8b0>, <__main__.Case object at 0x7f34c0f1f9d0>, <__main__.Case object at 0x7f34c0f1faf0>, <__main__.Case object at 0x7f34c0f1fcd0>, <__main__.Case object at 0x7f34c0f1fd30>, <__main__.Case object at 0x7f34c0f1fe50>, <__main__.Case object at 0x7f34c0f1ff70>, <__main__.Case object at 0x7f34c0f28070>, <__main__.Case object at 0x7f34c0f0fc40>, <__main__.Case object at 0x7f34c0f28250>, <__main__.Case object at 0x7f34c0f283d0>, <__main__.Case object at 0x7f34c0f28490>, <__main__.Case object at 0x7f34c0f28610>, <__main__.Case object at 0x7f34c0f286d0>, <__main__.Case object at 0x7f34c0f28790>, <__main__.Case object at 0x7f34c0f288b0>, <__main__.Case object at 0x7f34c0f284f0>, <__main__.Case object at 0x7f34c0f28a90>, <__main__.Case object at 0x7f34c0f28c10>, <__main__.Case object at 0x7f34c0f28cd0>, <__main__.Case object at 0x7f34c0f28e50>, <__main__.Case object at 0x7f34c0f28f70>, <__main__.Case object at 0x7f34c0f29090>, <__main__.Case object at 0x7f34c0f29150>, <__main__.Case object at 0x7f34c0f292d0>, <__main__.Case object at 0x7f34c0f293f0>, <__main__.Case object at 0x7f34c0f29510>, <__main__.Case object at 0x7f34c0f295d0>, <__main__.Case object at 0x7f34c0f29750>, <__main__.Case object at 0x7f34c0f29870>, <__main__.Case object at 0x7f34c0f29990>, <__main__.Case object at 0x7f34c0f29b70>, <__main__.Case object at 0x7f34c0f29bd0>, <__main__.Case object at 0x7f34c0f29cf0>, <__main__.Case object at 0x7f34c0f29e10>, <__main__.Case object at 0x7f34c0f29f30>, <__main__.Case object at 0x7f34c0f29ed0>, <__main__.Case object at 0x7f34c0f2a110>, <__main__.Case object at 0x7f34c0f2a230>, <__main__.Case object at 0x7f34c0f2a410>, <__main__.Case object at 0x7f34c0f2a470>, <__main__.Case object at 0x7f34c0f2a590>, <__main__.Case object at 0x7f34c0f2a650>, <__main__.Case object at 0x7f34c0f28520>, <__main__.Case object at 0x7f34c0f2a890>, <__main__.Case object at 0x7f34c0f2a9b0>, <__main__.Case object at 0x7f34c0f2aad0>, <__main__.Case object at 0x7f34c0f2ab90>, <__main__.Case object at 0x7f34c0f2ad10>, <__main__.Case object at 0x7f34c0f2ae30>, <__main__.Case object at 0x7f34c0f2af50>, <__main__.Case object at 0x7f34c0f2b010>, <__main__.Case object at 0x7f34c0f2b190>, <__main__.Case object at 0x7f34c0f2b2b0>, <__main__.Case object at 0x7f34c0f2b3d0>, <__main__.Case object at 0x7f34c0f2b5b0>, <__main__.Case object at 0x7f34c0f2b610>, <__main__.Case object at 0x7f34c0f2b730>, <__main__.Case object at 0x7f34c0f2b850>, <__main__.Case object at 0x7f34c0f2b910>, <__main__.Case object at 0x7f34c0f2ba90>, <__main__.Case object at 0x7f34c0f2bbb0>, <__main__.Case object at 0x7f34c0f2bcd0>, <__main__.Case object at 0x7f34c0f2bd90>, <__main__.Case object at 0x7f34c0f2bf10>, <__main__.Case object at 0x7f34c0f30070>, <__main__.Case object at 0x7f34c0f30190>, <__main__.Case object at 0x7f34c0f30250>, <__main__.Case object at 0x7f34c0f303d0>, <__main__.Case object at 0x7f34c0f30490>, <__main__.Case object at 0x7f34c0f30550>, <__main__.Case object at 0x7f34c0f30670>, <__main__.Case object at 0x7f34c0f307f0>, <__main__.Case object at 0x7f34c0f30910>, <__main__.Case object at 0x7f34c0f30a30>, <__main__.Case object at 0x7f34c0f30af0>, <__main__.Case object at 0x7f34c0f30c70>, <__main__.Case object at 0x7f34c0f30d90>, <__main__.Case object at 0x7f34c0f30eb0>, <__main__.Case object at 0x7f34c0f30f70>, <__main__.Case object at 0x7f34c0f310f0>, <__main__.Case object at 0x7f34c0f31210>, <__main__.Case object at 0x7f34c0f31330>, <__main__.Case object at 0x7f34c0f313f0>, <__main__.Case object at 0x7f34c0f30fd0>, <__main__.Case object at 0x7f34c0f315d0>, <__main__.Case object at 0x7f34c0f31750>, <__main__.Case object at 0x7f34c0f31810>, <__main__.Case object at 0x7f34c0f31990>, <__main__.Case object at 0x7f34c0f31ab0>, <__main__.Case object at 0x7f34c0f31bd0>, <__main__.Case object at 0x7f34c0f31c90>, <__main__.Case object at 0x7f34c0f31e10>, <__main__.Case object at 0x7f34c0f31f30>, <__main__.Case object at 0x7f34c0f32050>, <__main__.Case object at 0x7f34c0f32110>, <__main__.Case object at 0x7f34c0f32290>, <__main__.Case object at 0x7f34c0f32350>, <__main__.Case object at 0x7f34c0f32410>, <__main__.Case object at 0x7f34c0f32530>, <__main__.Case object at 0x7f34c0f326b0>, <__main__.Case object at 0x7f34c0f327d0>, <__main__.Case object at 0x7f34c0f328f0>, <__main__.Case object at 0x7f34c0f329b0>, <__main__.Case object at 0x7f34c0f32b30>, <__main__.Case object at 0x7f34c0f32bf0>, <__main__.Case object at 0x7f34c0f32cb0>, <__main__.Case object at 0x7f34c0f32dd0>, <__main__.Case object at 0x7f34c0f32f50>, <__main__.Case object at 0x7f34c0f33070>, <__main__.Case object at 0x7f34c0f33190>, <__main__.Case object at 0x7f34c0f33250>, <__main__.Case object at 0x7f34c0f333d0>, <__main__.Case object at 0x7f34c0f334f0>, <__main__.Case object at 0x7f34c0f33610>, <__main__.Case object at 0x7f34c0f337f0>, <__main__.Case object at 0x7f34c0f33850>, <__main__.Case object at 0x7f34c0f33970>, <__main__.Case object at 0x7f34c0f33a90>, <__main__.Case object at 0x7f34c0f33c70>, <__main__.Case object at 0x7f34c0f33cd0>, <__main__.Case object at 0x7f34c0f33df0>, <__main__.Case object at 0x7f34c0f33eb0>, <__main__.Case object at 0x7f34c0f1eaa0>, <__main__.Case object at 0x7f34c0f32a40>, <__main__.Case object at 0x7f34c0f3c190>, <__main__.Case object at 0x7f34c0f3c310>, <__main__.Case object at 0x7f34c0f3c3d0>, <__main__.Case object at 0x7f34c0f3c550>, <__main__.Case object at 0x7f34c0f3c670>, <__main__.Case object at 0x7f34c0f3c790>, <__main__.Case object at 0x7f34c0f332e0>, <__main__.Case object at 0x7f34c0f3c940>, <__main__.Case object at 0x7f34c0f3ca90>, <__main__.Case object at 0x7f34c0f3cbb0>, <__main__.Case object at 0x7f34c0f3cc70>, <__main__.Case object at 0x7f34c0f3c8b0>, <__main__.Case object at 0x7f34c0f3ce50>, <__main__.Case object at 0x7f34c0f3cfd0>, <__main__.Case object at 0x7f34c0f33be0>, <__main__.Case object at 0x7f34c0f3d180>, <__main__.Case object at 0x7f34c0f3d2d0>, <__main__.Case object at 0x7f34c0f3d3f0>, <__main__.Case object at 0x7f34c0f325c0>, <__main__.Case object at 0x7f34c0f3d5a0>, <__main__.Case object at 0x7f34c0f3d6f0>, <__main__.Case object at 0x7f34c0f3d810>, <__main__.Case object at 0x7f34c0f3d8d0>, <__main__.Case object at 0x7f34c0f3da50>, <__main__.Case object at 0x7f34c0f3db70>, <__main__.Case object at 0x7f34c0f3dc90>, <__main__.Case object at 0x7f34c0f3dd50>, <__main__.Case object at 0x7f34c0f3ded0>, <__main__.Case object at 0x7f34c0f3dff0>, <__main__.Case object at 0x7f34c0f3e110>, <__main__.Case object at 0x7f34c0f3e2f0>, <__main__.Case object at 0x7f34c0f3e3b0>, <__main__.Case object at 0x7f34c0f3e4d0>, <__main__.Case object at 0x7f34c0f3e5f0>, <__main__.Case object at 0x7f34c0f3e650>, <__main__.Case object at 0x7f34c0f3e830>, <__main__.Case object at 0x7f34c0f3e950>, <__main__.Case object at 0x7f34c0f3e9b0>, <__main__.Case object at 0x7f34c0f3d0f0>, <__main__.Case object at 0x7f34c0f3ec50>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f70e50>, <__main__.Case object at 0x7f34c3f903a0>, <__main__.Case object at 0x7f34c3f88a30>, <__main__.Case object at 0x7f34c3fbed10>, <__main__.Case object at 0x7f34c3fbf3d0>, <__main__.Case object at 0x7f34c3fbf010>, <__main__.Case object at 0x7f34c3fbd600>, <__main__.Case object at 0x7f34c3fbf610>, <__main__.Case object at 0x7f34c3fbf430>, <__main__.Case object at 0x7f34c3fbe080>, <__main__.Case object at 0x7f34c3fbd8d0>, <__main__.Case object at 0x7f34c3fbff70>, <__main__.Case object at 0x7f34c3fbefe0>, <__main__.Case object at 0x7f34c3fbe6b0>, <__main__.Case object at 0x7f34c3fbcb50>, <__main__.Case object at 0x7f34c3fbda50>, <__main__.Case object at 0x7f34c3fbebc0>, <__main__.Case object at 0x7f34c3fbde70>, <__main__.Case object at 0x7f34c3fbff10>, <__main__.Case object at 0x7f34c3fbc1c0>, <__main__.Case object at 0x7f34c3fbf7f0>, <__main__.Case object at 0x7f34c3fbe7a0>, <__main__.Case object at 0x7f34c3fbf490>, <__main__.Case object at 0x7f34c3fbc940>, <__main__.Case object at 0x7f34c3fe4d00>, <__main__.Case object at 0x7f34c3fbc520>, <__main__.Case object at 0x7f34c3fe5b70>, <__main__.Case object at 0x7f34c3fe4730>, <__main__.Case object at 0x7f34c3fe59f0>, <__main__.Case object at 0x7f34c3fbceb0>, <__main__.Case object at 0x7f34c3fe66e0>, <__main__.Case object at 0x7f34c3fe5f30>, <__main__.Case object at 0x7f34c3fe5600>, <__main__.Case object at 0x7f34c3fbf640>, <__main__.Case object at 0x7f34c3fe4340>, <__main__.Case object at 0x7f34c3fe42b0>, <__main__.Case object at 0x7f34c3fbc640>, <__main__.Case object at 0x7f34c3fe6050>, <__main__.Case object at 0x7f34c3fe6170>, <__main__.Case object at 0x7f34c3fe6f80>, <__main__.Case object at 0x7f34c3fe53f0>, <__main__.Case object at 0x7f34c3fe6e30>, <__main__.Case object at 0x7f34c3fe4310>, <__main__.Case object at 0x7f34c3fe5db0>, <__main__.Case object at 0x7f34c3fe68f0>, <__main__.Case object at 0x7f34c3fe6410>, <__main__.Case object at 0x7f34c3fe5150>, <__main__.Case object at 0x7f34c3fe6c50>, <__main__.Case object at 0x7f34c3fe7370>, <__main__.Case object at 0x7f34c3fe4370>, <__main__.Case object at 0x7f34c3fe5300>, <__main__.Case object at 0x7f34c3fe5c30>, <__main__.Case object at 0x7f34c3fe5b10>, <__main__.Case object at 0x7f34c3fe51b0>, <__main__.Case object at 0x7f34c3fe6740>, <__main__.Case object at 0x7f34c3fe6ef0>, <__main__.Case object at 0x7f34c3fe6140>, <__main__.Case object at 0x7f34c3fe6a70>, <__main__.Case object at 0x7f34c3fe5870>, <__main__.Case object at 0x7f34c3fe5420>, <__main__.Case object at 0x7f34c3fe6dd0>, <__main__.Case object at 0x7f34c3fe5900>, <__main__.Case object at 0x7f34c3fe52d0>, <__main__.Case object at 0x7f34c3fe6800>, <__main__.Case object at 0x7f34c3fe7430>, <__main__.Case object at 0x7f34c3fe4760>, <__main__.Case object at 0x7f34c3fc9f90>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fe6b90>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fe5840>, <__main__.Case object at 0x7f34c3fcb700>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fc8c10>, <__main__.Case object at 0x7f34c3fe5e70>, <__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c3fc8160>, <__main__.Case object at 0x7f34c3fcb790>, <__main__.Case object at 0x7f34c3fe47f0>, <__main__.Case object at 0x7f34c3fcbb20>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fb3f40>, <__main__.Case object at 0x7f34c3fcbe50>, <__main__.Case object at 0x7f34c3fcaec0>, <__main__.Case object at 0x7f34c3fca980>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fca4d0>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fcb910>, <__main__.Case object at 0x7f34c3fc8670>, <__main__.Case object at 0x7f34c3fcbd30>, <__main__.Case object at 0x7f34c3fcaef0>, <__main__.Case object at 0x7f34c3fc9c30>, <__main__.Case object at 0x7f34c3fca560>, <__main__.Case object at 0x7f34c3fcb2e0>, <__main__.Case object at 0x7f34c3fec8b0>, <__main__.Case object at 0x7f34c3fec730>, <__main__.Case object at 0x7f34c3fed1e0>, <__main__.Case object at 0x7f34c3fc9240>, <__main__.Case object at 0x7f34c3feea40>, <__main__.Case object at 0x7f34c3fed8a0>, <__main__.Case object at 0x7f34c3fed570>, <__main__.Case object at 0x7f34c3fc9150>, <__main__.Case object at 0x7f34c3fec910>, <__main__.Case object at 0x7f34c3fee890>, <__main__.Case object at 0x7f34c3fec2e0>, <__main__.Case object at 0x7f34c3fc89d0>, <__main__.Case object at 0x7f34c3fee0e0>, <__main__.Case object at 0x7f34c3fec400>, <__main__.Case object at 0x7f34c3fecee0>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fec700>, <__main__.Case object at 0x7f34c3fed4e0>, <__main__.Case object at 0x7f34c3fee020>, <__main__.Case object at 0x7f34c3fee500>, <__main__.Case object at 0x7f34c3feda20>, <__main__.Case object at 0x7f34c3fed3c0>, <__main__.Case object at 0x7f34c3fec250>, <__main__.Case object at 0x7f34c3fecfa0>, <__main__.Case object at 0x7f34c3fecbe0>, <__main__.Case object at 0x7f34c3fedb70>, <__main__.Case object at 0x7f34c3fec280>, <__main__.Case object at 0x7f34c3fedf00>, <__main__.Case object at 0x7f34c3fee5c0>, <__main__.Case object at 0x7f34c3fee170>, <__main__.Case object at 0x7f34c3fef430>, <__main__.Case object at 0x7f34c3fef310>, <__main__.Case object at 0x7f34c3fef1c0>, <__main__.Case object at 0x7f34c3fec790>, <__main__.Case object at 0x7f34c3feefb0>, <__main__.Case object at 0x7f34c3feee60>, <__main__.Case object at 0x7f34c3feed10>, <__main__.Case object at 0x7f34c3fed990>, <__main__.Case object at 0x7f34c3fed270>, <__main__.Case object at 0x7f34c3fef610>, <__main__.Case object at 0x7f34c3fee5f0>, <__main__.Case object at 0x7f34c3fef7f0>, <__main__.Case object at 0x7f34c3fef910>, <__main__.Case object at 0x7f34c3fefa90>, <__main__.Case object at 0x7f34c3fed180>, <__main__.Case object at 0x7f34c3fefc70>, <__main__.Case object at 0x7f34c3fefd90>, <__main__.Case object at 0x7f34c3feff10>, <__main__.Case object at 0x7f34c3feda50>, <__main__.Case object at 0x7f34c3fee6b0>, <__main__.Case object at 0x7f34c3fc90f0>, <__main__.Case object at 0x7f34c0f0c4f0>, <__main__.Case object at 0x7f34c3feece0>, <__main__.Case object at 0x7f34c0f0c370>, <__main__.Case object at 0x7f34c3fef640>, <__main__.Case object at 0x7f34c0f0c6d0>, <__main__.Case object at 0x7f34c0f0ccd0>, <__main__.Case object at 0x7f34c0f0cdf0>, <__main__.Case object at 0x7f34c3fefac0>, <__main__.Case object at 0x7f34c0f0ca90>, <__main__.Case object at 0x7f34c0f0d150>, <__main__.Case object at 0x7f34c0f0d270>, <__main__.Case object at 0x7f34c3feff40>, <__main__.Case object at 0x7f34c0f0cf70>, <__main__.Case object at 0x7f34c0f0d5d0>, <__main__.Case object at 0x7f34c0f0d6f0>, <__main__.Case object at 0x7f34c0f0d870>, <__main__.Case object at 0x7f34c0f0d3f0>, <__main__.Case object at 0x7f34c0f0da50>, <__main__.Case object at 0x7f34c0f0db70>, <__main__.Case object at 0x7f34c0f0dcf0>, <__main__.Case object at 0x7f34c0f0c310>, <__main__.Case object at 0x7f34c0f0ded0>, <__main__.Case object at 0x7f34c0f0dff0>, <__main__.Case object at 0x7f34c0f0e170>, <__main__.Case object at 0x7f34c0f0caf0>, <__main__.Case object at 0x7f34c0f0e4d0>, <__main__.Case object at 0x7f34c0f0cb20>, <__main__.Case object at 0x7f34c0f0e710>, <__main__.Case object at 0x7f34c0f0e830>, <__main__.Case object at 0x7f34c0f0e9b0>, <__main__.Case object at 0x7f34c0f0cfa0>, <__main__.Case object at 0x7f34c0f0eb90>, <__main__.Case object at 0x7f34c0f0ecb0>, <__main__.Case object at 0x7f34c0f0ee30>, <__main__.Case object at 0x7f34c0f0d420>, <__main__.Case object at 0x7f34c0f0f010>, <__main__.Case object at 0x7f34c0f0f130>, <__main__.Case object at 0x7f34c0f0f2b0>, <__main__.Case object at 0x7f34c0f0d8a0>, <__main__.Case object at 0x7f34c0f0f490>, <__main__.Case object at 0x7f34c0f0f5b0>, <__main__.Case object at 0x7f34c0f0f730>, <__main__.Case object at 0x7f34c0f0dd20>, <__main__.Case object at 0x7f34c0f0f910>, <__main__.Case object at 0x7f34c0f0fa30>, <__main__.Case object at 0x7f34c0f0fbb0>, <__main__.Case object at 0x7f34c0f0fca0>, <__main__.Case object at 0x7f34c0f0fdc0>, <__main__.Case object at 0x7f34c0f0fee0>, <__main__.Case object at 0x7f34c0f0e560>, <__main__.Case object at 0x7f34c0f0e9e0>, <__main__.Case object at 0x7f34c0f14280>, <__main__.Case object at 0x7f34c0f143a0>, <__main__.Case object at 0x7f34c0f0ee60>, <__main__.Case object at 0x7f34c0f14160>, <__main__.Case object at 0x7f34c0f14700>, <__main__.Case object at 0x7f34c0f14820>, <__main__.Case object at 0x7f34c0f0f2e0>, <__main__.Case object at 0x7f34c0f14520>, <__main__.Case object at 0x7f34c0f14b80>, <__main__.Case object at 0x7f34c0f14ca0>, <__main__.Case object at 0x7f34c0f0f760>, <__main__.Case object at 0x7f34c0f149a0>, <__main__.Case object at 0x7f34c0f15000>, <__main__.Case object at 0x7f34c0f0fbe0>, <__main__.Case object at 0x7f34c0f153c0>, <__main__.Case object at 0x7f34c0f154e0>, <__main__.Case object at 0x7f34c0f15660>, <__main__.Case object at 0x7f34c0f15240>, <__main__.Case object at 0x7f34c0f15840>, <__main__.Case object at 0x7f34c0f15960>, <__main__.Case object at 0x7f34c0f15ae0>, <__main__.Case object at 0x7f34c0f140a0>, <__main__.Case object at 0x7f34c0f15cc0>, <__main__.Case object at 0x7f34c0f15de0>, <__main__.Case object at 0x7f34c0f15f60>, <__main__.Case object at 0x7f34c0f15300>, <__main__.Case object at 0x7f34c0f16200>, <__main__.Case object at 0x7f34c0f16380>, <__main__.Case object at 0x7f34c0f149d0>, <__main__.Case object at 0x7f34c0f16560>, <__main__.Case object at 0x7f34c0f16680>, <__main__.Case object at 0x7f34c0f16800>, <__main__.Case object at 0x7f34c0f14e50>, <__main__.Case object at 0x7f34c0f169e0>, <__main__.Case object at 0x7f34c0f16bc0>, <__main__.Case object at 0x7f34c0f151e0>, <__main__.Case object at 0x7f34c0f16e00>, <__main__.Case object at 0x7f34c0f16fe0>, <__main__.Case object at 0x7f34c0f15690>, <__main__.Case object at 0x7f34c0f17220>, <__main__.Case object at 0x7f34c0f17340>, <__main__.Case object at 0x7f34c0f174c0>, <__main__.Case object at 0x7f34c0f17550>, <__main__.Case object at 0x7f34c0f176d0>, <__main__.Case object at 0x7f34c0f178b0>, <__main__.Case object at 0x7f34c0f15f90>, <__main__.Case object at 0x7f34c0f17af0>, <__main__.Case object at 0x7f34c0f17c10>, <__main__.Case object at 0x7f34c0f17d90>, <__main__.Case object at 0x7f34c0f163b0>, <__main__.Case object at 0x7f34c0f17f70>, <__main__.Case object at 0x7f34c0f16830>, <__main__.Case object at 0x7f34c0f16c50>, <__main__.Case object at 0x7f34c0f1c0d0>, <__main__.Case object at 0x7f34c0f1c430>, <__main__.Case object at 0x7f34c0f1c550>, <__main__.Case object at 0x7f34c0f17070>, <__main__.Case object at 0x7f34c0f1c250>, <__main__.Case object at 0x7f34c0f1c8b0>, <__main__.Case object at 0x7f34c0f1c9d0>, <__main__.Case object at 0x7f34c0f17910>, <__main__.Case object at 0x7f34c0f1ccd0>, <__main__.Case object at 0x7f34c0f1cdf0>, <__main__.Case object at 0x7f34c0f17940>, <__main__.Case object at 0x7f34c0f1cb50>, <__main__.Case object at 0x7f34c0f1d150>, <__main__.Case object at 0x7f34c0f1d270>, <__main__.Case object at 0x7f34c0f17dc0>, <__main__.Case object at 0x7f34c0f1cf70>, <__main__.Case object at 0x7f34c0f1d5d0>, <__main__.Case object at 0x7f34c0f1d6f0>, <__main__.Case object at 0x7f34c0f1d870>, <__main__.Case object at 0x7f34c0f1d3f0>, <__main__.Case object at 0x7f34c0f1da50>, <__main__.Case object at 0x7f34c0f1db70>, <__main__.Case object at 0x7f34c0f1dcf0>, <__main__.Case object at 0x7f34c0f1c280>, <__main__.Case object at 0x7f34c0f1ded0>, <__main__.Case object at 0x7f34c0f1dff0>, <__main__.Case object at 0x7f34c0f1e170>, <__main__.Case object at 0x7f34c0f1c700>, <__main__.Case object at 0x7f34c0f1e350>, <__main__.Case object at 0x7f34c0f1e470>, <__main__.Case object at 0x7f34c0f1e5f0>, <__main__.Case object at 0x7f34c0f1cb80>, <__main__.Case object at 0x7f34c0f1e7d0>, <__main__.Case object at 0x7f34c0f1e8f0>, <__main__.Case object at 0x7f34c0f1ea70>, <__main__.Case object at 0x7f34c0f1cfa0>, <__main__.Case object at 0x7f34c0f1ec50>, <__main__.Case object at 0x7f34c0f1ed70>, <__main__.Case object at 0x7f34c0f1eef0>, <__main__.Case object at 0x7f34c0f1d420>, <__main__.Case object at 0x7f34c0f1f0d0>, <__main__.Case object at 0x7f34c0f1f1f0>, <__main__.Case object at 0x7f34c0f1f370>, <__main__.Case object at 0x7f34c0f1c6d0>, <__main__.Case object at 0x7f34c0f1f610>, <__main__.Case object at 0x7f34c0f1f790>, <__main__.Case object at 0x7f34c0f1dd20>, <__main__.Case object at 0x7f34c0f1f970>, <__main__.Case object at 0x7f34c0f1fa90>, <__main__.Case object at 0x7f34c0f1fc10>, <__main__.Case object at 0x7f34c0f1e1a0>, <__main__.Case object at 0x7f34c0f1fdf0>, <__main__.Case object at 0x7f34c0f1ff10>, <__main__.Case object at 0x7f34c0f1e620>, <__main__.Case object at 0x7f34c0f1f550>, <__main__.Case object at 0x7f34c0f28370>, <__main__.Case object at 0x7f34c0f1ef20>, <__main__.Case object at 0x7f34c0f28190>, <__main__.Case object at 0x7f34c0f282b0>, <__main__.Case object at 0x7f34c0f1f3a0>, <__main__.Case object at 0x7f34c0f287f0>, <__main__.Case object at 0x7f34c0f28bb0>, <__main__.Case object at 0x7f34c0f1f7c0>, <__main__.Case object at 0x7f34c0f28910>, <__main__.Case object at 0x7f34c0f28f10>, <__main__.Case object at 0x7f34c0f29030>, <__main__.Case object at 0x7f34c0f1fc40>, <__main__.Case object at 0x7f34c0f28d30>, <__main__.Case object at 0x7f34c0f29390>, <__main__.Case object at 0x7f34c0f294b0>, <__main__.Case object at 0x7f34c0f29630>, <__main__.Case object at 0x7f34c0f291b0>, <__main__.Case object at 0x7f34c0f29810>, <__main__.Case object at 0x7f34c0f29930>, <__main__.Case object at 0x7f34c0f29ab0>, <__main__.Case object at 0x7f34c0f280d0>, <__main__.Case object at 0x7f34c0f29c90>, <__main__.Case object at 0x7f34c0f29db0>, <__main__.Case object at 0x7f34c0f28af0>, <__main__.Case object at 0x7f34c0f2a0b0>, <__main__.Case object at 0x7f34c0f2a1d0>, <__main__.Case object at 0x7f34c0f2a350>, <__main__.Case object at 0x7f34c0f28940>, <__main__.Case object at 0x7f34c0f2a530>, <__main__.Case object at 0x7f34c0f2a710>, <__main__.Case object at 0x7f34c0f28d60>, <__main__.Case object at 0x7f34c0f2a950>, <__main__.Case object at 0x7f34c0f2aa70>, <__main__.Case object at 0x7f34c0f2abf0>, <__main__.Case object at 0x7f34c0f291e0>, <__main__.Case object at 0x7f34c0f2add0>, <__main__.Case object at 0x7f34c0f2aef0>, <__main__.Case object at 0x7f34c0f2b070>, <__main__.Case object at 0x7f34c0f29660>, <__main__.Case object at 0x7f34c0f2b250>, <__main__.Case object at 0x7f34c0f2b370>, <__main__.Case object at 0x7f34c0f2b4f0>, <__main__.Case object at 0x7f34c0f29ae0>, <__main__.Case object at 0x7f34c0f2b6d0>, <__main__.Case object at 0x7f34c0f2b7f0>, <__main__.Case object at 0x7f34c0f2b970>, <__main__.Case object at 0x7f34c0f29f60>, <__main__.Case object at 0x7f34c0f2bb50>, <__main__.Case object at 0x7f34c0f2bc70>, <__main__.Case object at 0x7f34c0f2bdf0>, <__main__.Case object at 0x7f34c0f2a380>, <__main__.Case object at 0x7f34c0f2bfd0>, <__main__.Case object at 0x7f34c0f2a7a0>, <__main__.Case object at 0x7f34c0f2ac20>, <__main__.Case object at 0x7f34c0f30130>, <__main__.Case object at 0x7f34c0f2a770>, <__main__.Case object at 0x7f34c0f2b0a0>, <__main__.Case object at 0x7f34c0f302b0>, <__main__.Case object at 0x7f34c0f308b0>, <__main__.Case object at 0x7f34c0f309d0>, <__main__.Case object at 0x7f34c0f2b520>, <__main__.Case object at 0x7f34c0f306d0>, <__main__.Case object at 0x7f34c0f30d30>, <__main__.Case object at 0x7f34c0f30e50>, <__main__.Case object at 0x7f34c0f2b9a0>, <__main__.Case object at 0x7f34c0f30b50>, <__main__.Case object at 0x7f34c0f311b0>, <__main__.Case object at 0x7f34c0f312d0>, <__main__.Case object at 0x7f34c0f2be20>, <__main__.Case object at 0x7f34c0f305b0>, <__main__.Case object at 0x7f34c0f316f0>, <__main__.Case object at 0x7f34c0f31870>, <__main__.Case object at 0x7f34c0f31450>, <__main__.Case object at 0x7f34c0f31a50>, <__main__.Case object at 0x7f34c0f31b70>, <__main__.Case object at 0x7f34c0f31cf0>, <__main__.Case object at 0x7f34c0f302e0>, <__main__.Case object at 0x7f34c0f31ed0>, <__main__.Case object at 0x7f34c0f31ff0>, <__main__.Case object at 0x7f34c0f32170>, <__main__.Case object at 0x7f34c0f30700>, <__main__.Case object at 0x7f34c0f31630>, <__main__.Case object at 0x7f34c0f32590>, <__main__.Case object at 0x7f34c0f30b80>, <__main__.Case object at 0x7f34c0f32770>, <__main__.Case object at 0x7f34c0f32890>, <__main__.Case object at 0x7f34c0f32a10>, <__main__.Case object at 0x7f34c0f31000>, <__main__.Case object at 0x7f34c0f32470>, <__main__.Case object at 0x7f34c0f32e30>, <__main__.Case object at 0x7f34c0f31480>, <__main__.Case object at 0x7f34c0f33010>, <__main__.Case object at 0x7f34c0f33130>, <__main__.Case object at 0x7f34c0f332b0>, <__main__.Case object at 0x7f34c0f318a0>, <__main__.Case object at 0x7f34c0f33490>, <__main__.Case object at 0x7f34c0f335b0>, <__main__.Case object at 0x7f34c0f33730>, <__main__.Case object at 0x7f34c0f31d20>, <__main__.Case object at 0x7f34c0f33910>, <__main__.Case object at 0x7f34c0f33a30>, <__main__.Case object at 0x7f34c0f33bb0>, <__main__.Case object at 0x7f34c0f321a0>, <__main__.Case object at 0x7f34c0f33d90>, <__main__.Case object at 0x7f34c0f33f70>, <__main__.Case object at 0x7f34c0f33fd0>, <__main__.Case object at 0x7f34c0f3c2b0>, <__main__.Case object at 0x7f34c0f32e60>, <__main__.Case object at 0x7f34c0f3c0d0>, <__main__.Case object at 0x7f34c0f3c610>, <__main__.Case object at 0x7f34c0f3c730>, <__main__.Case object at 0x7f34c0f3c1f0>, <__main__.Case object at 0x7f34c0f3ca30>, <__main__.Case object at 0x7f34c0f3cb50>, <__main__.Case object at 0x7f34c0f33760>, <__main__.Case object at 0x7f34c0f3c430>, <__main__.Case object at 0x7f34c0f3cf70>, <__main__.Case object at 0x7f34c0f3ceb0>, <__main__.Case object at 0x7f34c0f3d270>, <__main__.Case object at 0x7f34c0f3d390>, <__main__.Case object at 0x7f34c0f3ccd0>, <__main__.Case object at 0x7f34c0f3d690>, <__main__.Case object at 0x7f34c0f3d7b0>, <__main__.Case object at 0x7f34c0f3d930>, <__main__.Case object at 0x7f34c0f3d510>, <__main__.Case object at 0x7f34c0f3db10>, <__main__.Case object at 0x7f34c0f3dc30>, <__main__.Case object at 0x7f34c0f3ddb0>, <__main__.Case object at 0x7f34c0f3c460>, <__main__.Case object at 0x7f34c0f3df90>, <__main__.Case object at 0x7f34c0f3e0b0>, <__main__.Case object at 0x7f34c0f3e230>, <__main__.Case object at 0x7f34c0f3e350>, <__main__.Case object at 0x7f34c0f3e410>, <__main__.Case object at 0x7f34c0f3e530>, <__main__.Case object at 0x7f34c0f3e710>, <__main__.Case object at 0x7f34c0f3e7a0>, <__main__.Case object at 0x7f34c0f3e890>, <__main__.Case object at 0x7f34c0f3ea70>, <__main__.Case object at 0x7f34c0f3ebc0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.7999999999999998, time steps: 13\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.5999999999999999)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.5999999999999999)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.5999999999999999)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 5), solution: 3, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 11\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3f93ca0>, <__main__.Case object at 0x7f34c3fb0340>, <__main__.Case object at 0x7f34c3f9d060>, <__main__.Case object at 0x7f34c3fbcd00>, <__main__.Case object at 0x7f34c3fb3d90>, <__main__.Case object at 0x7f34c3fbe350>, <__main__.Case object at 0x7f34c3fbc910>, <__main__.Case object at 0x7f34c3fbf280>, <__main__.Case object at 0x7f34c3fbef20>, <__main__.Case object at 0x7f34c3fbebf0>, <__main__.Case object at 0x7f34c3f89b40>, <__main__.Case object at 0x7f34c3fb3d60>, <__main__.Case object at 0x7f34c3fbe860>, <__main__.Case object at 0x7f34c3fbe020>, <__main__.Case object at 0x7f34c3fbc040>, <__main__.Case object at 0x7f34c3fbf0d0>, <__main__.Case object at 0x7f34c3fbf910>, <__main__.Case object at 0x7f34c3fbf190>, <__main__.Case object at 0x7f34c3fbf8b0>, <__main__.Case object at 0x7f34c3fbd3c0>, <__main__.Case object at 0x7f34c3fbfeb0>, <__main__.Case object at 0x7f34c3fbe740>, <__main__.Case object at 0x7f34c3fbe680>, <__main__.Case object at 0x7f34c3fbd870>, <__main__.Case object at 0x7f34c3fbf580>, <__main__.Case object at 0x7f34c3fbf100>, <__main__.Case object at 0x7f34c3fbd990>, <__main__.Case object at 0x7f34c3fbc8b0>, <__main__.Case object at 0x7f34c3fbdae0>, <__main__.Case object at 0x7f34c3fbc1f0>, <__main__.Case object at 0x7f34c3fbcdf0>, <__main__.Case object at 0x7f34c3fbf1f0>, <__main__.Case object at 0x7f34c3fbf7c0>, <__main__.Case object at 0x7f34c3fbe230>, <__main__.Case object at 0x7f34c3fbdd50>, <__main__.Case object at 0x7f34c3fbe200>, <__main__.Case object at 0x7f34c3fbe890>, <__main__.Case object at 0x7f34c3fbe380>, <__main__.Case object at 0x7f34c3fbd3f0>, <__main__.Case object at 0x7f34c3fbedd0>, <__main__.Case object at 0x7f34c3fbf400>, <__main__.Case object at 0x7f34c3fbfc70>, <__main__.Case object at 0x7f34c3fbe9b0>, <__main__.Case object at 0x7f34c3fbd1e0>, <__main__.Case object at 0x7f34c3fbc850>, <__main__.Case object at 0x7f34c3fbe1a0>, <__main__.Case object at 0x7f34c3fbda80>, <__main__.Case object at 0x7f34c3fbd570>, <__main__.Case object at 0x7f34c3fbcf40>, <__main__.Case object at 0x7f34c3fbd9c0>, <__main__.Case object at 0x7f34c3fbe830>, <__main__.Case object at 0x7f34c3fe5de0>, <__main__.Case object at 0x7f34c3fe40a0>, <__main__.Case object at 0x7f34c3fe6020>, <__main__.Case object at 0x7f34c3fe5990>, <__main__.Case object at 0x7f34c3fe6e00>, <__main__.Case object at 0x7f34c3fe57b0>, <__main__.Case object at 0x7f34c3fbfc10>, <__main__.Case object at 0x7f34c3fe4ca0>, <__main__.Case object at 0x7f34c3fe5e40>, <__main__.Case object at 0x7f34c3fe50c0>, <__main__.Case object at 0x7f34c3fe4f40>, <__main__.Case object at 0x7f34c3fe6fb0>, <__main__.Case object at 0x7f34c3fe4b20>, <__main__.Case object at 0x7f34c3fe42e0>, <__main__.Case object at 0x7f34c3fe6200>, <__main__.Case object at 0x7f34c3fe4be0>, <__main__.Case object at 0x7f34c3fbf2b0>, <__main__.Case object at 0x7f34c3fbfd90>, <__main__.Case object at 0x7f34c3fe4b80>, <__main__.Case object at 0x7f34c3fe5180>, <__main__.Case object at 0x7f34c3fe4eb0>, <__main__.Case object at 0x7f34c3fe4c70>, <__main__.Case object at 0x7f34c3fe6560>, <__main__.Case object at 0x7f34c3fe4e50>, <__main__.Case object at 0x7f34c3fe7310>, <__main__.Case object at 0x7f34c3fe69b0>, <__main__.Case object at 0x7f34c3fe59c0>, <__main__.Case object at 0x7f34c3fe7100>, <__main__.Case object at 0x7f34c3fe69e0>, <__main__.Case object at 0x7f34c3fe5ae0>, <__main__.Case object at 0x7f34c3fe6e90>, <__main__.Case object at 0x7f34c3fe61a0>, <__main__.Case object at 0x7f34c3fe4df0>, <__main__.Case object at 0x7f34c3fe6710>, <__main__.Case object at 0x7f34c3fe55a0>, <__main__.Case object at 0x7f34c3fe4040>, <__main__.Case object at 0x7f34c3fe5660>, <__main__.Case object at 0x7f34c3fe6b30>, <__main__.Case object at 0x7f34c3fe6f50>, <__main__.Case object at 0x7f34c3fe43a0>, <__main__.Case object at 0x7f34c3fe73a0>, <__main__.Case object at 0x7f34c3fe7190>, <__main__.Case object at 0x7f34c3fe4640>, <__main__.Case object at 0x7f34c3fe5210>, <__main__.Case object at 0x7f34c3fe7340>, <__main__.Case object at 0x7f34c3fe6b60>, <__main__.Case object at 0x7f34c3fe5240>, <__main__.Case object at 0x7f34c3fe7010>, <__main__.Case object at 0x7f34c3fc9330>, <__main__.Case object at 0x7f34c3fc94b0>, <__main__.Case object at 0x7f34c3fc8910>, <__main__.Case object at 0x7f34c3fcb970>, <__main__.Case object at 0x7f34c3fc9390>, <__main__.Case object at 0x7f34c3fc9b10>, <__main__.Case object at 0x7f34c3fca2c0>, <__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c3fc8130>, <__main__.Case object at 0x7f34c3fc9a50>, <__main__.Case object at 0x7f34c3fc9780>, <__main__.Case object at 0x7f34c3fc8fd0>, <__main__.Case object at 0x7f34c3fc86a0>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fc9b40>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fcbc10>, <__main__.Case object at 0x7f34c3fc94e0>, <__main__.Case object at 0x7f34c3fcace0>, <__main__.Case object at 0x7f34c3fcba90>, <__main__.Case object at 0x7f34c3fca770>, <__main__.Case object at 0x7f34c3fe70d0>, <__main__.Case object at 0x7f34c3fc83d0>, <__main__.Case object at 0x7f34c3fc8100>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fc80d0>, <__main__.Case object at 0x7f34c3fc8cd0>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3fcb9d0>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fc84f0>, <__main__.Case object at 0x7f34c3fc9570>, <__main__.Case object at 0x7f34c3fc8a30>, <__main__.Case object at 0x7f34c3fc8e80>, <__main__.Case object at 0x7f34c3fca6b0>, <__main__.Case object at 0x7f34c3fed9c0>, <__main__.Case object at 0x7f34c3fed210>, <__main__.Case object at 0x7f34c3fee980>, <__main__.Case object at 0x7f34c3fecf40>, <__main__.Case object at 0x7f34c3fecf10>, <__main__.Case object at 0x7f34c3fec640>, <__main__.Case object at 0x7f34c3fedc60>, <__main__.Case object at 0x7f34c3fed9f0>, <__main__.Case object at 0x7f34c3fed660>, <__main__.Case object at 0x7f34c3fcabc0>, <__main__.Case object at 0x7f34c3fee350>, <__main__.Case object at 0x7f34c3fee620>, <__main__.Case object at 0x7f34c3fec190>, <__main__.Case object at 0x7f34c3fed060>, <__main__.Case object at 0x7f34c3fe58a0>, <__main__.Case object at 0x7f34c3fed330>, <__main__.Case object at 0x7f34c3fecb50>, <__main__.Case object at 0x7f34c3fec970>, <__main__.Case object at 0x7f34c3fed840>, <__main__.Case object at 0x7f34c3fed600>, <__main__.Case object at 0x7f34c3fedf30>, <__main__.Case object at 0x7f34c3fee260>, <__main__.Case object at 0x7f34c3fee710>, <__main__.Case object at 0x7f34c3fee9e0>, <__main__.Case object at 0x7f34c3fedd80>, <__main__.Case object at 0x7f34c3fbd4b0>, <__main__.Case object at 0x7f34c3feeaa0>, <__main__.Case object at 0x7f34c3fecc10>, <__main__.Case object at 0x7f34c3fecc40>, <__main__.Case object at 0x7f34c3fed090>, <__main__.Case object at 0x7f34c3fed900>, <__main__.Case object at 0x7f34c3fed6f0>, <__main__.Case object at 0x7f34c3feddb0>, <__main__.Case object at 0x7f34c3fec5e0>, <__main__.Case object at 0x7f34c3feded0>, <__main__.Case object at 0x7f34c3fee6e0>, <__main__.Case object at 0x7f34c3fef3d0>, <__main__.Case object at 0x7f34c3fef280>, <__main__.Case object at 0x7f34c3fef130>, <__main__.Case object at 0x7f34c3fef010>, <__main__.Case object at 0x7f34c3fef460>, <__main__.Case object at 0x7f34c3feedd0>, <__main__.Case object at 0x7f34c3feeef0>, <__main__.Case object at 0x7f34c3feeb90>, <__main__.Case object at 0x7f34c3feea70>, <__main__.Case object at 0x7f34c3fee320>, <__main__.Case object at 0x7f34c3fec9a0>, <__main__.Case object at 0x7f34c3fef790>, <__main__.Case object at 0x7f34c3fef8b0>, <__main__.Case object at 0x7f34c3fbd960>, <__main__.Case object at 0x7f34c3fefa30>, <__main__.Case object at 0x7f34c3fefc10>, <__main__.Case object at 0x7f34c3fefd30>, <__main__.Case object at 0x7f34c3fefe50>, <__main__.Case object at 0x7f34c3feffa0>, <__main__.Case object at 0x7f34c0f0c0d0>, <__main__.Case object at 0x7f34c3fef9d0>, <__main__.Case object at 0x7f34c0f0c2b0>, <__main__.Case object at 0x7f34c0f0c130>, <__main__.Case object at 0x7f34c3fef5b0>, <__main__.Case object at 0x7f34c0f0c5b0>, <__main__.Case object at 0x7f34c0f0c670>, <__main__.Case object at 0x7f34c0f0c790>, <__main__.Case object at 0x7f34c0f0c8b0>, <__main__.Case object at 0x7f34c0f0c970>, <__main__.Case object at 0x7f34c0f0c9d0>, <__main__.Case object at 0x7f34c0f0cbb0>, <__main__.Case object at 0x7f34c0f0c490>, <__main__.Case object at 0x7f34c0f0cd90>, <__main__.Case object at 0x7f34c0f0ceb0>, <__main__.Case object at 0x7f34c0f0c1f0>, <__main__.Case object at 0x7f34c0f0d0f0>, <__main__.Case object at 0x7f34c0f0d210>, <__main__.Case object at 0x7f34c3fe5c90>, <__main__.Case object at 0x7f34c0f0d480>, <__main__.Case object at 0x7f34c0f0d570>, <__main__.Case object at 0x7f34c0f0d690>, <__main__.Case object at 0x7f34c0f0d7b0>, <__main__.Case object at 0x7f34c0f0cc70>, <__main__.Case object at 0x7f34c0f0d9f0>, <__main__.Case object at 0x7f34c3fef550>, <__main__.Case object at 0x7f34c0f0dc30>, <__main__.Case object at 0x7f34c0f0dc90>, <__main__.Case object at 0x7f34c0f0de70>, <__main__.Case object at 0x7f34c0f0df90>, <__main__.Case object at 0x7f34c0f0e0b0>, <__main__.Case object at 0x7f34c0f0e200>, <__main__.Case object at 0x7f34c3feecb0>, <__main__.Case object at 0x7f34c0f0e290>, <__main__.Case object at 0x7f34c0f0e470>, <__main__.Case object at 0x7f34c0f0e5c0>, <__main__.Case object at 0x7f34c0f0db10>, <__main__.Case object at 0x7f34c0f0e7d0>, <__main__.Case object at 0x7f34c0f0e8f0>, <__main__.Case object at 0x7f34c0f0e950>, <__main__.Case object at 0x7f34c0f0eb30>, <__main__.Case object at 0x7f34c0f0d000>, <__main__.Case object at 0x7f34c0f0ed70>, <__main__.Case object at 0x7f34c0f0eec0>, <__main__.Case object at 0x7f34c0f0ec50>, <__main__.Case object at 0x7f34c0f0e6b0>, <__main__.Case object at 0x7f34c0f0f1f0>, <__main__.Case object at 0x7f34c0f0f340>, <__main__.Case object at 0x7f34c0f0e3b0>, <__main__.Case object at 0x7f34c0f0f550>, <__main__.Case object at 0x7f34c0f0f670>, <__main__.Case object at 0x7f34c0f0f430>, <__main__.Case object at 0x7f34c0f0f8b0>, <__main__.Case object at 0x7f34c0f0f9d0>, <__main__.Case object at 0x7f34c0f0faf0>, <__main__.Case object at 0x7f34c0f0fb50>, <__main__.Case object at 0x7f34c0f0fd60>, <__main__.Case object at 0x7f34c0f0fe80>, <__main__.Case object at 0x7f34c0f0f0d0>, <__main__.Case object at 0x7f34c0f14100>, <__main__.Case object at 0x7f34c0f14220>, <__main__.Case object at 0x7f34c0f14340>, <__main__.Case object at 0x7f34c0f14460>, <__main__.Case object at 0x7f34c0f0efb0>, <__main__.Case object at 0x7f34c0f145b0>, <__main__.Case object at 0x7f34c0f147c0>, <__main__.Case object at 0x7f34c0f148e0>, <__main__.Case object at 0x7f34c0f0d900>, <__main__.Case object at 0x7f34c0f14b20>, <__main__.Case object at 0x7f34c0f14c40>, <__main__.Case object at 0x7f34c0f14d60>, <__main__.Case object at 0x7f34c0f14eb0>, <__main__.Case object at 0x7f34c0f14fa0>, <__main__.Case object at 0x7f34c0f150c0>, <__main__.Case object at 0x7f34c0f15180>, <__main__.Case object at 0x7f34c0f14e20>, <__main__.Case object at 0x7f34c0f15360>, <__main__.Case object at 0x7f34c0f15480>, <__main__.Case object at 0x7f34c0f155a0>, <__main__.Case object at 0x7f34c0f156f0>, <__main__.Case object at 0x7f34c0f157e0>, <__main__.Case object at 0x7f34c0f15900>, <__main__.Case object at 0x7f34c0f14a30>, <__main__.Case object at 0x7f34c0f15b70>, <__main__.Case object at 0x7f34c0f15c60>, <__main__.Case object at 0x7f34c0f146a0>, <__main__.Case object at 0x7f34c0f15ea0>, <__main__.Case object at 0x7f34c0f0d330>, <__main__.Case object at 0x7f34c0f16080>, <__main__.Case object at 0x7f34c0f161a0>, <__main__.Case object at 0x7f34c0f162c0>, <__main__.Case object at 0x7f34c0f16410>, <__main__.Case object at 0x7f34c0f16500>, <__main__.Case object at 0x7f34c0f16620>, <__main__.Case object at 0x7f34c0f15ff0>, <__main__.Case object at 0x7f34c0f16890>, <__main__.Case object at 0x7f34c0f16980>, <__main__.Case object at 0x7f34c0f16aa0>, <__main__.Case object at 0x7f34c0f16b60>, <__main__.Case object at 0x7f34c0f16740>, <__main__.Case object at 0x7f34c0f16da0>, <__main__.Case object at 0x7f34c0f15a20>, <__main__.Case object at 0x7f34c0f16f80>, <__main__.Case object at 0x7f34c0f170d0>, <__main__.Case object at 0x7f34c0f171c0>, <__main__.Case object at 0x7f34c0f172e0>, <__main__.Case object at 0x7f34c0f17400>, <__main__.Case object at 0x7f34c0f17460>, <__main__.Case object at 0x7f34c0f17670>, <__main__.Case object at 0x7f34c0f17790>, <__main__.Case object at 0x7f34c0f17850>, <__main__.Case object at 0x7f34c0f15d80>, <__main__.Case object at 0x7f34c0f0ffa0>, <__main__.Case object at 0x7f34c0f17bb0>, <__main__.Case object at 0x7f34c3fed510>, <__main__.Case object at 0x7f34c0f17cd0>, <__main__.Case object at 0x7f34c0f17f10>, <__main__.Case object at 0x7f34c0f0f7c0>, <__main__.Case object at 0x7f34c0f1c190>, <__main__.Case object at 0x7f34c0f1c070>, <__main__.Case object at 0x7f34c0f1c3d0>, <__main__.Case object at 0x7f34c0f1c4f0>, <__main__.Case object at 0x7f34c0f1c610>, <__main__.Case object at 0x7f34c0f17e20>, <__main__.Case object at 0x7f34c0f1c850>, <__main__.Case object at 0x7f34c0f1c970>, <__main__.Case object at 0x7f34c0f1ca90>, <__main__.Case object at 0x7f34c0f1caf0>, <__main__.Case object at 0x7f34c0f1cc70>, <__main__.Case object at 0x7f34c0f1cd90>, <__main__.Case object at 0x7f34c0f1ceb0>, <__main__.Case object at 0x7f34c0f1d000>, <__main__.Case object at 0x7f34c0f1d0f0>, <__main__.Case object at 0x7f34c0f1d210>, <__main__.Case object at 0x7f34c0f1d330>, <__main__.Case object at 0x7f34c0f1d480>, <__main__.Case object at 0x7f34c0f1d570>, <__main__.Case object at 0x7f34c0f1d690>, <__main__.Case object at 0x7f34c3fedff0>, <__main__.Case object at 0x7f34c0f1d810>, <__main__.Case object at 0x7f34c0f1d9f0>, <__main__.Case object at 0x7f34c0f1db10>, <__main__.Case object at 0x7f34c0f1dc30>, <__main__.Case object at 0x7f34c0f1d7b0>, <__main__.Case object at 0x7f34c0f1de70>, <__main__.Case object at 0x7f34c0f1df90>, <__main__.Case object at 0x7f34c0f1e0b0>, <__main__.Case object at 0x7f34c0f1e200>, <__main__.Case object at 0x7f34c0f1c760>, <__main__.Case object at 0x7f34c0f1e410>, <__main__.Case object at 0x7f34c0f1e530>, <__main__.Case object at 0x7f34c0f1e590>, <__main__.Case object at 0x7f34c0f1c2e0>, <__main__.Case object at 0x7f34c0f1e770>, <__main__.Case object at 0x7f34c0f17a90>, <__main__.Case object at 0x7f34c0f1e9b0>, <__main__.Case object at 0x7f34c0f1ebf0>, <__main__.Case object at 0x7f34c0f1ed10>, <__main__.Case object at 0x7f34c0f1ee30>, <__main__.Case object at 0x7f34c0f1ef80>, <__main__.Case object at 0x7f34c0f1e890>, <__main__.Case object at 0x7f34c0f16cb0>, <__main__.Case object at 0x7f34c0f1f2b0>, <__main__.Case object at 0x7f34c0f1f400>, <__main__.Case object at 0x7f34c0f1eb00>, <__main__.Case object at 0x7f34c0f1f5b0>, <__main__.Case object at 0x7f34c0f1f6d0>, <__main__.Case object at 0x7f34c0f1f730>, <__main__.Case object at 0x7f34c0f1f070>, <__main__.Case object at 0x7f34c0f1f910>, <__main__.Case object at 0x7f34c0f1fb50>, <__main__.Case object at 0x7f34c0f1f490>, <__main__.Case object at 0x7f34c0f1fd90>, <__main__.Case object at 0x7f34c0f16ec0>, <__main__.Case object at 0x7f34c0f1ffd0>, <__main__.Case object at 0x7f34c0f28130>, <__main__.Case object at 0x7f34c0f281f0>, <__main__.Case object at 0x7f34c0f28310>, <__main__.Case object at 0x7f34c0f28430>, <__main__.Case object at 0x7f34c0f1feb0>, <__main__.Case object at 0x7f34c0f28670>, <__main__.Case object at 0x7f34c0f28730>, <__main__.Case object at 0x7f34c0f28850>, <__main__.Case object at 0x7f34c0f1fbb0>, <__main__.Case object at 0x7f34c0f28a30>, <__main__.Case object at 0x7f34c0f179a0>, <__main__.Case object at 0x7f34c0f28c70>, <__main__.Case object at 0x7f34c0f28dc0>, <__main__.Case object at 0x7f34c0f28eb0>, <__main__.Case object at 0x7f34c0f28b50>, <__main__.Case object at 0x7f34c0f290f0>, <__main__.Case object at 0x7f34c0f29240>, <__main__.Case object at 0x7f34c0f28fd0>, <__main__.Case object at 0x7f34c0f29450>, <__main__.Case object at 0x7f34c0f29570>, <__main__.Case object at 0x7f34c0f296c0>, <__main__.Case object at 0x7f34c0f297b0>, <__main__.Case object at 0x7f34c0f298d0>, <__main__.Case object at 0x7f34c0f299f0>, <__main__.Case object at 0x7f34c0f29330>, <__main__.Case object at 0x7f34c0f29c30>, <__main__.Case object at 0x7f34c0f1fa30>, <__main__.Case object at 0x7f34c0f29e70>, <__main__.Case object at 0x7f34c0f29ff0>, <__main__.Case object at 0x7f34c0f2a050>, <__main__.Case object at 0x7f34c0f2a170>, <__main__.Case object at 0x7f34c0f2a290>, <__main__.Case object at 0x7f34c0f2a2f0>, <__main__.Case object at 0x7f34c0f2a4d0>, <__main__.Case object at 0x7f34c0f2a5f0>, <__main__.Case object at 0x7f34c0f29d50>, <__main__.Case object at 0x7f34c0f2a800>, <__main__.Case object at 0x7f34c0f2a8f0>, <__main__.Case object at 0x7f34c0f2a6b0>, <__main__.Case object at 0x7f34c0f1f190>, <__main__.Case object at 0x7f34c0f2ab30>, <__main__.Case object at 0x7f34c0f2ad70>, <__main__.Case object at 0x7f34c0f2ae90>, <__main__.Case object at 0x7f34c0f2afb0>, <__main__.Case object at 0x7f34c0f2b100>, <__main__.Case object at 0x7f34c0f2ac80>, <__main__.Case object at 0x7f34c0f2b310>, <__main__.Case object at 0x7f34c0f2b430>, <__main__.Case object at 0x7f34c0f289a0>, <__main__.Case object at 0x7f34c0f28580>, <__main__.Case object at 0x7f34c0f2b790>, <__main__.Case object at 0x7f34c0f2b670>, <__main__.Case object at 0x7f34c0f2ba00>, <__main__.Case object at 0x7f34c0f2baf0>, <__main__.Case object at 0x7f34c0f2b8b0>, <__main__.Case object at 0x7f34c0f2bd30>, <__main__.Case object at 0x7f34c0f2be80>, <__main__.Case object at 0x7f34c0f2bf70>, <__main__.Case object at 0x7f34c0f29a50>, <__main__.Case object at 0x7f34c0f301f0>, <__main__.Case object at 0x7f34c0f30370>, <__main__.Case object at 0x7f34c0f30430>, <__main__.Case object at 0x7f34c0f304f0>, <__main__.Case object at 0x7f34c0f30610>, <__main__.Case object at 0x7f34c0f30760>, <__main__.Case object at 0x7f34c0f30850>, <__main__.Case object at 0x7f34c0f30970>, <__main__.Case object at 0x7f34c0f30a90>, <__main__.Case object at 0x7f34c0f2aa10>, <__main__.Case object at 0x7f34c0f2bc10>, <__main__.Case object at 0x7f34c0f30df0>, <__main__.Case object at 0x7f34c0f30cd0>, <__main__.Case object at 0x7f34c0f31090>, <__main__.Case object at 0x7f34c0f31150>, <__main__.Case object at 0x7f34c0f31270>, <__main__.Case object at 0x7f34c0f31390>, <__main__.Case object at 0x7f34c0f314e0>, <__main__.Case object at 0x7f34c0f31570>, <__main__.Case object at 0x7f34c0f30f10>, <__main__.Case object at 0x7f34c0f317b0>, <__main__.Case object at 0x7f34c0f31900>, <__main__.Case object at 0x7f34c0f319f0>, <__main__.Case object at 0x7f34c0f300d0>, <__main__.Case object at 0x7f34c0f31c30>, <__main__.Case object at 0x7f34c0f31d80>, <__main__.Case object at 0x7f34c0f31e70>, <__main__.Case object at 0x7f34c0f31f90>, <__main__.Case object at 0x7f34c0f320b0>, <__main__.Case object at 0x7f34c0f32200>, <__main__.Case object at 0x7f34c0f322f0>, <__main__.Case object at 0x7f34c0f323b0>, <__main__.Case object at 0x7f34c0f30be0>, <__main__.Case object at 0x7f34c0f32620>, <__main__.Case object at 0x7f34c0f32710>, <__main__.Case object at 0x7f34c0f32830>, <__main__.Case object at 0x7f34c0f32950>, <__main__.Case object at 0x7f34c0f32aa0>, <__main__.Case object at 0x7f34c0f32b90>, <__main__.Case object at 0x7f34c0f32c50>, <__main__.Case object at 0x7f34c0f32d70>, <__main__.Case object at 0x7f34c0f32ec0>, <__main__.Case object at 0x7f34c0f32fb0>, <__main__.Case object at 0x7f34c0f330d0>, <__main__.Case object at 0x7f34c0f31690>, <__main__.Case object at 0x7f34c0f33340>, <__main__.Case object at 0x7f34c0f33430>, <__main__.Case object at 0x7f34c0f33550>, <__main__.Case object at 0x7f34c0f33670>, <__main__.Case object at 0x7f34c0f31b10>, <__main__.Case object at 0x7f34c0f338b0>, <__main__.Case object at 0x7f34c0f337c0>, <__main__.Case object at 0x7f34c0f33af0>, <__main__.Case object at 0x7f34c0f331f0>, <__main__.Case object at 0x7f34c0f33d30>, <__main__.Case object at 0x7f34c0f33e50>, <__main__.Case object at 0x7f34c0f33f10>, <__main__.Case object at 0x7f34c0f3c070>, <__main__.Case object at 0x7f34c0f3c130>, <__main__.Case object at 0x7f34c0f3c250>, <__main__.Case object at 0x7f34c0f3c370>, <__main__.Case object at 0x7f34c0f3c4c0>, <__main__.Case object at 0x7f34c0f3c5b0>, <__main__.Case object at 0x7f34c0f3c6d0>, <__main__.Case object at 0x7f34c0f33c40>, <__main__.Case object at 0x7f34c0f3c7f0>, <__main__.Case object at 0x7f34c0f3c9d0>, <__main__.Case object at 0x7f34c0f3caf0>, <__main__.Case object at 0x7f34c0f3cc10>, <__main__.Case object at 0x7f34c0f3c850>, <__main__.Case object at 0x7f34c0f3cdf0>, <__main__.Case object at 0x7f34c0f2b1f0>, <__main__.Case object at 0x7f34c0f3cf10>, <__main__.Case object at 0x7f34c0f3d090>, <__main__.Case object at 0x7f34c0f3d210>, <__main__.Case object at 0x7f34c0f3d030>, <__main__.Case object at 0x7f34c0f3d450>, <__main__.Case object at 0x7f34c0f3d4b0>, <__main__.Case object at 0x7f34c0f3d630>, <__main__.Case object at 0x7f34c0f1e2f0>, <__main__.Case object at 0x7f34c0f3d870>, <__main__.Case object at 0x7f34c0f3d9c0>, <__main__.Case object at 0x7f34c0f1dc90>, <__main__.Case object at 0x7f34c3fe5810>, <__main__.Case object at 0x7f34c3feea10>, <__main__.Case object at 0x7f34c0f3dcf0>, <__main__.Case object at 0x7f34c3fe6f20>, <__main__.Case object at 0x7f34c0f3e050>, <__main__.Case object at 0x7f34c0f3c8e0>, <__main__.Case object at 0x7f34c3fbcfd0>, <__main__.Case object at 0x7f34c3fbe0e0>, <__main__.Case object at 0x7f34c0f3e590>, <__main__.Case object at 0x7f34c0f3e6b0>, <__main__.Case object at 0x7f34c0f3e470>, <__main__.Case object at 0x7f34c0f3cd00>, <__main__.Case object at 0x7f34c0f3e8f0>, <__main__.Case object at 0x7f34c0f3ea10>, <__main__.Case object at 0x7f34c3fca050>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.4999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.4999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (2, 2), solution: 4, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 4, tv: 0.4, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (1, 1), solution: 2, tv: 0.4, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 4, tv: 0.4, time steps: 11\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.4, time steps: 7\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 4) is empty. Temporary case base stored to the case base: ((5, 4), 4, 0.5)\n",
      "Episode succeeded, case (5, 3) is empty. Temporary case base stored to the case base: ((5, 3), 2, 0.5)\n",
      "Episode succeeded, case (5, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) is empty. Temporary case base stored to the case base: ((5, 5), 1, 0.5)\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) is empty. Temporary case base stored to the case base: ((5, 2), 4, 0.5)\n",
      "Episode succeeded, case (5, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 4, 0.5)\n",
      "Episode succeeded, case (4, 1) is empty. Temporary case base stored to the case base: ((4, 1), 2, 0.5)\n",
      "Episode succeeded, case (4, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 2, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 3, 0.5)\n",
      "Episode succeeded, case (5, 1) is empty. Temporary case base stored to the case base: ((5, 1), 1, 0.5)\n",
      "Episode succeeded, case (5, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 3) is empty. Temporary case base stored to the case base: ((4, 3), 4, 0.5)\n",
      "Episode succeeded, case (5, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.4999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.4999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 4), solution: 4, tv: 0.5, time steps: 34\n",
      "cases content after RETAIN, problem: (5, 3), solution: 2, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 0.5, time steps: 31\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.5, time steps: 25\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (4, 1), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.5, time steps: 20\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 1), solution: 1, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 3), solution: 4, tv: 0.5, time steps: 12\n",
      "Episode: 40, Total Steps: 522, Total Rewards: [-621, 60], Status Episode: False\n",
      "------------------------------------------End of episode 40 loop--------------------\n",
      "----- starting point of Episode 41 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.6999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.6999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.6999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 hit the obstacle!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 1) with action 1 to next state (6, 1): pull reward: 0\n",
      "----- starting point of Episode 41 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 1) with action 0 to next state (6, 1): pull reward: 0\n",
      "----- starting point of Episode 41 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 63 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 64 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 65 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 66 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 67 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 68 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 69 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 70 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 71 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 72 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 73 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 1) with action 1 to next state (6, 1): pull reward: 0\n",
      "----- starting point of Episode 41 in steps 74 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 75 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 76 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 77 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 1) with action 1 to next state (6, 1): pull reward: 0\n",
      "----- starting point of Episode 41 in steps 78 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 79 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 80 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 81 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 82 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 83 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 84 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 85 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 86 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 87 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 88 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 89 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 90 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 91 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 92 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 93 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 94 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 1) with action 1 to next state (6, 1): pull reward: 0\n",
      "----- starting point of Episode 41 in steps 95 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 96 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 97 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 98 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 1) with action 1 to next state (6, 1): pull reward: 0\n",
      "----- starting point of Episode 41 in steps 99 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 100 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 101 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 102 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 103 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 104 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 105 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 106 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 107 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 108 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 109 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 110 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 111 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 112 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 113 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 114 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 115 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 116 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 117 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 118 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 119 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 120 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 121 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 122 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 123 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 124 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 125 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 1 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 126 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 127 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 128 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 129 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 4 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 130 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 131 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 132 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 0 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 133 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 4 to next state (2, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 134 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 135 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 136 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 0 to next state (2, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 41 in steps 137 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 hit the obstacle!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 1), 2, 0.4999999999999999, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 1) with action 1 to next state (2, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: False\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fca650>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fcba60>, <__main__.Case object at 0x7f34c3fc8940>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fcbaf0>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fc9090>, <__main__.Case object at 0x7f34c3fc8610>, <__main__.Case object at 0x7f34c3fc90f0>, <__main__.Case object at 0x7f34c3fc9270>, <__main__.Case object at 0x7f34c3fc8250>, <__main__.Case object at 0x7f34c3fc95d0>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c3fca170>, <__main__.Case object at 0x7f34c3fc8dc0>, <__main__.Case object at 0x7f34c3fc8f70>, <__main__.Case object at 0x7f34c3fc8a00>, <__main__.Case object at 0x7f34c3fcb460>, <__main__.Case object at 0x7f34c3fca0e0>, <__main__.Case object at 0x7f34c3fcb040>, <__main__.Case object at 0x7f34c3fcb670>, <__main__.Case object at 0x7f34c3fc9cf0>, <__main__.Case object at 0x7f34c3fcbc70>, <__main__.Case object at 0x7f34c3fc9ab0>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fcbc10>, <__main__.Case object at 0x7f34c3fc8340>, <__main__.Case object at 0x7f34c3fcb730>, <__main__.Case object at 0x7f34c3fcb6d0>, <__main__.Case object at 0x7f34c3fcb8e0>, <__main__.Case object at 0x7f34c3fcb430>, <__main__.Case object at 0x7f34c3fc87c0>, <__main__.Case object at 0x7f34c0f14310>, <__main__.Case object at 0x7f34c0f14700>, <__main__.Case object at 0x7f34c0f14d30>, <__main__.Case object at 0x7f34c0f15330>, <__main__.Case object at 0x7f34c0f156c0>, <__main__.Case object at 0x7f34c0f157b0>, <__main__.Case object at 0x7f34c0f15d50>, <__main__.Case object at 0x7f34c0f16170>, <__main__.Case object at 0x7f34c0f164d0>, <__main__.Case object at 0x7f34c0f165f0>, <__main__.Case object at 0x7f34c0f16c80>, <__main__.Case object at 0x7f34c0f170a0>, <__main__.Case object at 0x7f34c0f173d0>, <__main__.Case object at 0x7f34c0f17520>, <__main__.Case object at 0x7f34c0f17b80>, <__main__.Case object at 0x7f34c0f17ee0>, <__main__.Case object at 0x7f34c0f17940>, <__main__.Case object at 0x7f34c0f14040>, <__main__.Case object at 0x7f34c0f14640>, <__main__.Case object at 0x7f34c0f14940>, <__main__.Case object at 0x7f34c0f14ac0>, <__main__.Case object at 0x7f34c0f151b0>, <__main__.Case object at 0x7f34c0f152d0>, <__main__.Case object at 0x7f34c0f15600>, <__main__.Case object at 0x7f34c0f159c0>, <__main__.Case object at 0x7f34c0f15a80>, <__main__.Case object at 0x7f34c0f14550>, <__main__.Case object at 0x7f34c0f16320>, <__main__.Case object at 0x7f34c0f166e0>, <__main__.Case object at 0x7f34c0f167a0>, <__main__.Case object at 0x7f34c0f16d40>, <__main__.Case object at 0x7f34c0f16c20>, <__main__.Case object at 0x7f34c0f173a0>, <__main__.Case object at 0x7f34c0f175b0>, <__main__.Case object at 0x7f34c0f17a30>, <__main__.Case object at 0x7f34c0f17d30>, <__main__.Case object at 0x7f34c0f14070>, <__main__.Case object at 0x7f34c0f14100>, <__main__.Case object at 0x7f34c0f14610>, <__main__.Case object at 0x7f34c0f14850>, <__main__.Case object at 0x7f34c0f14d60>, <__main__.Case object at 0x7f34c0f14eb0>, <__main__.Case object at 0x7f34c0f152a0>, <__main__.Case object at 0x7f34c0f15360>, <__main__.Case object at 0x7f34c0f15900>, <__main__.Case object at 0x7f34c0f14a30>, <__main__.Case object at 0x7f34c0f16110>, <__main__.Case object at 0x7f34c0f16470>, <__main__.Case object at 0x7f34c0f167d0>, <__main__.Case object at 0x7f34c0f16d10>, <__main__.Case object at 0x7f34c0f16e30>, <__main__.Case object at 0x7f34c0f17130>, <__main__.Case object at 0x7f34c0f17490>, <__main__.Case object at 0x7f34c0f175e0>, <__main__.Case object at 0x7f34c0f17b20>, <__main__.Case object at 0x7f34c0f17fa0>, <__main__.Case object at 0x7f34c0f179a0>, <__main__.Case object at 0x7f34c0f339d0>, <__main__.Case object at 0x7f34c0f308b0>, <__main__.Case object at 0x7f34c0f309d0>, <__main__.Case object at 0x7f34c0f30ca0>, <__main__.Case object at 0x7f34c0f30b50>, <__main__.Case object at 0x7f34c0f31780>, <__main__.Case object at 0x7f34c0f318d0>, <__main__.Case object at 0x7f34c0f31ae0>, <__main__.Case object at 0x7f34c0f31e40>, <__main__.Case object at 0x7f34c0f324a0>, <__main__.Case object at 0x7f34c0f32800>, <__main__.Case object at 0x7f34c0f32b60>, <__main__.Case object at 0x7f34c0f32d40>, <__main__.Case object at 0x7f34c0f33310>, <__main__.Case object at 0x7f34c0f33640>, <__main__.Case object at 0x7f34c0f339a0>, <__main__.Case object at 0x7f34c0f33ac0>, <__main__.Case object at 0x7f34c0f33760>, <__main__.Case object at 0x7f34c0f30340>, <__main__.Case object at 0x7f34c0f30640>, <__main__.Case object at 0x7f34c0f30790>, <__main__.Case object at 0x7f34c0f30d00>, <__main__.Case object at 0x7f34c0f31060>, <__main__.Case object at 0x7f34c0f313c0>, <__main__.Case object at 0x7f34c0f31510>, <__main__.Case object at 0x7f34c0f31a20>, <__main__.Case object at 0x7f34c0f31db0>, <__main__.Case object at 0x7f34c0f320e0>, <__main__.Case object at 0x7f34c0f32650>, <__main__.Case object at 0x7f34c0f32740>, <__main__.Case object at 0x7f34c0f32ad0>, <__main__.Case object at 0x7f34c0f32da0>, <__main__.Case object at 0x7f34c0f32ef0>, <__main__.Case object at 0x7f34c0f33460>, <__main__.Case object at 0x7f34c0f33790>, <__main__.Case object at 0x7f34c0f33b20>, <__main__.Case object at 0x7f34c0f33c10>, <__main__.Case object at 0x7f34c0f325c0>, <__main__.Case object at 0x7f34c0f30370>, <__main__.Case object at 0x7f34c0f30610>, <__main__.Case object at 0x7f34c0f30760>, <__main__.Case object at 0x7f34c0f30df0>, <__main__.Case object at 0x7f34c0f31300>, <__main__.Case object at 0x7f34c0f31600>, <__main__.Case object at 0x7f34c0f31ba0>, <__main__.Case object at 0x7f34c0f31cc0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c3fca1a0>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fcb580>, <__main__.Case object at 0x7f34c3fca080>, <__main__.Case object at 0x7f34c3fc8460>, <__main__.Case object at 0x7f34c3fcbbe0>, <__main__.Case object at 0x7f34c3fc8fa0>, <__main__.Case object at 0x7f34c3fc8670>, <__main__.Case object at 0x7f34c3fc98d0>, <__main__.Case object at 0x7f34c3fc89d0>, <__main__.Case object at 0x7f34c3fc9d50>, <__main__.Case object at 0x7f34c3fc9b70>, <__main__.Case object at 0x7f34c3fcb610>, <__main__.Case object at 0x7f34c3fc9030>, <__main__.Case object at 0x7f34c3fcb2b0>, <__main__.Case object at 0x7f34c3fcbeb0>, <__main__.Case object at 0x7f34c3fca950>, <__main__.Case object at 0x7f34c3fc95a0>, <__main__.Case object at 0x7f34c3fc9db0>, <__main__.Case object at 0x7f34c3fa8a00>, <__main__.Case object at 0x7f34c3fc8730>, <__main__.Case object at 0x7f34c3fca530>, <__main__.Case object at 0x7f34c3fc9f30>, <__main__.Case object at 0x7f34c3fc9ed0>, <__main__.Case object at 0x7f34c3fc8130>, <__main__.Case object at 0x7f34c3fc86a0>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fcace0>, <__main__.Case object at 0x7f34c3fcb190>, <__main__.Case object at 0x7f34c3fc8280>, <__main__.Case object at 0x7f34c3fc9000>, <__main__.Case object at 0x7f34c3fc9930>, <__main__.Case object at 0x7f34c3fc8a30>, <__main__.Case object at 0x7f34c3fcbcd0>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c0f14b80>, <__main__.Case object at 0x7f34c0f149a0>, <__main__.Case object at 0x7f34c0f15450>, <__main__.Case object at 0x7f34c3fc8430>, <__main__.Case object at 0x7f34c0f15c30>, <__main__.Case object at 0x7f34c0f15e70>, <__main__.Case object at 0x7f34c0f16290>, <__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c0f169e0>, <__main__.Case object at 0x7f34c0f16d70>, <__main__.Case object at 0x7f34c0f17190>, <__main__.Case object at 0x7f34c3fc9bd0>, <__main__.Case object at 0x7f34c0f17970>, <__main__.Case object at 0x7f34c0f17ca0>, <__main__.Case object at 0x7f34c0f16830>, <__main__.Case object at 0x7f34c3fc9570>, <__main__.Case object at 0x7f34c0f144c0>, <__main__.Case object at 0x7f34c0f14760>, <__main__.Case object at 0x7f34c0f14ee0>, <__main__.Case object at 0x7f34c0f14160>, <__main__.Case object at 0x7f34c0f15420>, <__main__.Case object at 0x7f34c0f15780>, <__main__.Case object at 0x7f34c0f15d20>, <__main__.Case object at 0x7f34c0f15ed0>, <__main__.Case object at 0x7f34c0f160e0>, <__main__.Case object at 0x7f34c0f164a0>, <__main__.Case object at 0x7f34c0f16a40>, <__main__.Case object at 0x7f34c0f16b90>, <__main__.Case object at 0x7f34c0f16e60>, <__main__.Case object at 0x7f34c0f17160>, <__main__.Case object at 0x7f34c0f17730>, <__main__.Case object at 0x7f34c0f17880>, <__main__.Case object at 0x7f34c0f17b50>, <__main__.Case object at 0x7f34c0f17eb0>, <__main__.Case object at 0x7f34c0f14340>, <__main__.Case object at 0x7f34c0f14fd0>, <__main__.Case object at 0x7f34c0f14b20>, <__main__.Case object at 0x7f34c0f150c0>, <__main__.Case object at 0x7f34c0f14a90>, <__main__.Case object at 0x7f34c0f156f0>, <__main__.Case object at 0x7f34c0f15c60>, <__main__.Case object at 0x7f34c0f15f30>, <__main__.Case object at 0x7f34c0f16230>, <__main__.Case object at 0x7f34c0f16590>, <__main__.Case object at 0x7f34c0f16b30>, <__main__.Case object at 0x7f34c0f168f0>, <__main__.Case object at 0x7f34c0f16f50>, <__main__.Case object at 0x7f34c0f17250>, <__main__.Case object at 0x7f34c0f17820>, <__main__.Case object at 0x7f34c0f17850>, <__main__.Case object at 0x7f34c0f17bb0>, <__main__.Case object at 0x7f34c0f17e20>, <__main__.Case object at 0x7f34c0f177c0>, <__main__.Case object at 0x7f34c3faafe0>, <__main__.Case object at 0x7f34c0f155a0>, <__main__.Case object at 0x7f34c0f15210>, <__main__.Case object at 0x7f34c0f305b0>, <__main__.Case object at 0x7f34c0f30ee0>, <__main__.Case object at 0x7f34c0f15e10>, <__main__.Case object at 0x7f34c0f30700>, <__main__.Case object at 0x7f34c0f325f0>, <__main__.Case object at 0x7f34c0f32920>, <__main__.Case object at 0x7f34c0f16aa0>, <__main__.Case object at 0x7f34c0f331c0>, <__main__.Case object at 0x7f34c0f33400>, <__main__.Case object at 0x7f34c0f336d0>, <__main__.Case object at 0x7f34c0f17790>, <__main__.Case object at 0x7f34c0f33f70>, <__main__.Case object at 0x7f34c0f30100>, <__main__.Case object at 0x7f34c0f30460>, <__main__.Case object at 0x7f34c0f309a0>, <__main__.Case object at 0x7f34c0f30af0>, <__main__.Case object at 0x7f34c0f30e20>, <__main__.Case object at 0x7f34c0f31180>, <__main__.Case object at 0x7f34c0f316c0>, <__main__.Case object at 0x7f34c0f31810>, <__main__.Case object at 0x7f34c0f31b40>, <__main__.Case object at 0x7f34c0f31ea0>, <__main__.Case object at 0x7f34c0f323e0>, <__main__.Case object at 0x7f34c0f32230>, <__main__.Case object at 0x7f34c0f32860>, <__main__.Case object at 0x7f34c0f32bc0>, <__main__.Case object at 0x7f34c0f33100>, <__main__.Case object at 0x7f34c0f33250>, <__main__.Case object at 0x7f34c0f33580>, <__main__.Case object at 0x7f34c0f338e0>, <__main__.Case object at 0x7f34c0f33e80>, <__main__.Case object at 0x7f34c0f32a40>, <__main__.Case object at 0x7f34c0f30160>, <__main__.Case object at 0x7f34c0f30430>, <__main__.Case object at 0x7f34c0f30970>, <__main__.Case object at 0x7f34c0f30c40>, <__main__.Case object at 0x7f34c0f311e0>, <__main__.Case object at 0x7f34c0f31420>, <__main__.Case object at 0x7f34c0f31960>, <__main__.Case object at 0x7f34c0f31720>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.8, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 3, tv: 0.6, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.6, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 1), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 2, 0.4999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.6999999999999998)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.6999999999999998)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.6999999999999998)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.8, time steps: 15\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 5), solution: 3, tv: 0.6, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.6, time steps: 11\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.6, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fca050>, <__main__.Case object at 0x7f34c3fcbfd0>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fc93f0>, <__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c3fcbb20>, <__main__.Case object at 0x7f34c3fc9cc0>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fc8070>, <__main__.Case object at 0x7f34c3fcb2e0>, <__main__.Case object at 0x7f34c3fc9150>, <__main__.Case object at 0x7f34c3fc9660>, <__main__.Case object at 0x7f34c3fcb280>, <__main__.Case object at 0x7f34c3fc9750>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fcb070>, <__main__.Case object at 0x7f34c3fcad70>, <__main__.Case object at 0x7f34c3fc88b0>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fca290>, <__main__.Case object at 0x7f34c3fcba00>, <__main__.Case object at 0x7f34c3fc98a0>, <__main__.Case object at 0x7f34c3fcaef0>, <__main__.Case object at 0x7f34c3fca6e0>, <__main__.Case object at 0x7f34c3fcbb80>, <__main__.Case object at 0x7f34c3fc8970>, <__main__.Case object at 0x7f34c3fc8df0>, <__main__.Case object at 0x7f34c3fc9b40>, <__main__.Case object at 0x7f34c3fc94e0>, <__main__.Case object at 0x7f34c3fc9420>, <__main__.Case object at 0x7f34c3fcab90>, <__main__.Case object at 0x7f34c3fcbd90>, <__main__.Case object at 0x7f34c3fca1d0>, <__main__.Case object at 0x7f34c3fcad10>, <__main__.Case object at 0x7f34c3fc92d0>, <__main__.Case object at 0x7f34c0f14430>, <__main__.Case object at 0x7f34c0f14820>, <__main__.Case object at 0x7f34c3fcbac0>, <__main__.Case object at 0x7f34c0f15000>, <__main__.Case object at 0x7f34c0f15570>, <__main__.Case object at 0x7f34c0f158d0>, <__main__.Case object at 0x7f34c0f14a00>, <__main__.Case object at 0x7f34c3fcbf40>, <__main__.Case object at 0x7f34c0f163e0>, <__main__.Case object at 0x7f34c0f16710>, <__main__.Case object at 0x7f34c0f159f0>, <__main__.Case object at 0x7f34c0f16e90>, <__main__.Case object at 0x7f34c0f172b0>, <__main__.Case object at 0x7f34c0f17640>, <__main__.Case object at 0x7f34c0f16860>, <__main__.Case object at 0x7f34c0f17df0>, <__main__.Case object at 0x7f34c3fcabc0>, <__main__.Case object at 0x7f34c3fcb790>, <__main__.Case object at 0x7f34c0f17760>, <__main__.Case object at 0x7f34c0f14880>, <__main__.Case object at 0x7f34c0f14d00>, <__main__.Case object at 0x7f34c0f14be0>, <__main__.Case object at 0x7f34c0f142e0>, <__main__.Case object at 0x7f34c0f15540>, <__main__.Case object at 0x7f34c0f158a0>, <__main__.Case object at 0x7f34c0f15c00>, <__main__.Case object at 0x7f34c0f14dc0>, <__main__.Case object at 0x7f34c0f16260>, <__main__.Case object at 0x7f34c0f141f0>, <__main__.Case object at 0x7f34c0f16920>, <__main__.Case object at 0x7f34c0f141c0>, <__main__.Case object at 0x7f34c0f14520>, <__main__.Case object at 0x7f34c0f16f20>, <__main__.Case object at 0x7f34c0f17610>, <__main__.Case object at 0x7f34c0f15ae0>, <__main__.Case object at 0x7f34c0f17c70>, <__main__.Case object at 0x7f34c0f17fd0>, <__main__.Case object at 0x7f34c0f15f00>, <__main__.Case object at 0x7f34c0f178b0>, <__main__.Case object at 0x7f34c0f14970>, <__main__.Case object at 0x7f34c0f14c40>, <__main__.Case object at 0x7f34c0f14fa0>, <__main__.Case object at 0x7f34c0f14220>, <__main__.Case object at 0x7f34c0f15480>, <__main__.Case object at 0x7f34c0f14370>, <__main__.Case object at 0x7f34c0f15b70>, <__main__.Case object at 0x7f34c0f15060>, <__main__.Case object at 0x7f34c0f16350>, <__main__.Case object at 0x7f34c0f166b0>, <__main__.Case object at 0x7f34c0f17070>, <__main__.Case object at 0x7f34c0f15db0>, <__main__.Case object at 0x7f34c0f17010>, <__main__.Case object at 0x7f34c0f17370>, <__main__.Case object at 0x7f34c0f17700>, <__main__.Case object at 0x7f34c0f16ad0>, <__main__.Case object at 0x7f34c0f17e80>, <__main__.Case object at 0x7f34c0f16cb0>, <__main__.Case object at 0x7f34c0f301c0>, <__main__.Case object at 0x7f34c0f14e50>, <__main__.Case object at 0x7f34c0f30bb0>, <__main__.Case object at 0x7f34c0f30dc0>, <__main__.Case object at 0x7f34c0f311b0>, <__main__.Case object at 0x7f34c0f305e0>, <__main__.Case object at 0x7f34c0f319c0>, <__main__.Case object at 0x7f34c0f31c00>, <__main__.Case object at 0x7f34c0f31f60>, <__main__.Case object at 0x7f34c0f312d0>, <__main__.Case object at 0x7f34c0f302b0>, <__main__.Case object at 0x7f34c0f32a70>, <__main__.Case object at 0x7f34c0f32e90>, <__main__.Case object at 0x7f34c0f32080>, <__main__.Case object at 0x7f34c0f33520>, <__main__.Case object at 0x7f34c0f33880>, <__main__.Case object at 0x7f34c0f33b50>, <__main__.Case object at 0x7f34c0f32f80>, <__main__.Case object at 0x7f34c0f30220>, <__main__.Case object at 0x7f34c0f30520>, <__main__.Case object at 0x7f34c0f326e0>, <__main__.Case object at 0x7f34c0f33d00>, <__main__.Case object at 0x7f34c0f17280>, <__main__.Case object at 0x7f34c0f312a0>, <__main__.Case object at 0x7f34c0f315a0>, <__main__.Case object at 0x7f34c0f30130>, <__main__.Case object at 0x7f34c0f31c60>, <__main__.Case object at 0x7f34c0f31fc0>, <__main__.Case object at 0x7f34c0f32320>, <__main__.Case object at 0x7f34c0f31360>, <__main__.Case object at 0x7f34c0f32980>, <__main__.Case object at 0x7f34c0f165c0>, <__main__.Case object at 0x7f34c0f32fe0>, <__main__.Case object at 0x7f34c0f157e0>, <__main__.Case object at 0x7f34c0f30f40>, <__main__.Case object at 0x7f34c0f33a00>, <__main__.Case object at 0x7f34c0f33d60>, <__main__.Case object at 0x7f34c0f336a0>, <__main__.Case object at 0x7f34c0f301f0>, <__main__.Case object at 0x7f34c0f304f0>, <__main__.Case object at 0x7f34c0f30850>, <__main__.Case object at 0x7f34c0f30cd0>, <__main__.Case object at 0x7f34c0f31090>, <__main__.Case object at 0x7f34c0f31540>, <__main__.Case object at 0x7f34c0f31840>, <__main__.Case object at 0x7f34c0f31de0>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 1), solution: 2, tv: 0.2999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 2, tv: 0.2999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.49999999999999983, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.49999999999999983, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.49999999999999983, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 4, tv: 0.5, time steps: 34\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 2, tv: 0.5, time steps: 33\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 0.5, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.5, time steps: 25\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.5, time steps: 23\n",
      "case content after REVISE for agent 1, problem: (4, 1), solution: 2, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 2, tv: 0.5, time steps: 20\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 3, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (5, 1), solution: 1, tv: 0.5, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 4, tv: 0.5, time steps: 12\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.49999999999999983, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.49999999999999983, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.49999999999999983, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 4), solution: 4, tv: 0.5, time steps: 34\n",
      "cases content after RETAIN, problem: (5, 3), solution: 2, tv: 0.5, time steps: 33\n",
      "cases content after RETAIN, problem: (5, 5), solution: 1, tv: 0.5, time steps: 31\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.5, time steps: 25\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (4, 1), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.5, time steps: 20\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 1), solution: 1, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 3), solution: 4, tv: 0.5, time steps: 12\n",
      "Episode: 41, Total Steps: 138, Total Rewards: [-237, -103], Status Episode: False\n",
      "------------------------------------------End of episode 41 loop--------------------\n",
      "----- starting point of Episode 42 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.49999999999999983, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.49999999999999983, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.49999999999999983, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 0) with action 3 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 42 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 0) with action 1 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 42 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.5, 17)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 0), 2, 0.5, 20)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 4 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 1), 2, 0.5, 22)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 1 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 2), 4, 0.5, 23)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 0 to next state (1, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 2), 4, 0.5, 25)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 0) with action 2 to next state (1, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 3) with action 1 to next state (6, 2): pull reward: 0\n",
      "----- starting point of Episode 42 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 6), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 6), 1, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 5) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 42 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 2 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 42 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 42 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 42 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 1 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 42 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 1 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 3 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 0 to next state (0, 0): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 42 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 0) with action 2 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 43 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 3 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 44 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 2 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 45 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 46 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 47 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 48 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 42 in steps 49 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 50 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 51 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 52 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "----- starting point of Episode 42 in steps 53 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 2 to next state (1, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 54 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 3) with action 2 to next state (1, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 55 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 4) with action 0 to next state (1, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 56 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 4) with action 4 to next state (2, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 57 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (2, 4) with action 4 to next state (3, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 58 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 4) with action 2 to next state (3, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 59 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 5) with action 2 to next state (3, 6): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 60 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 6) with action 4 to next state (4, 6): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 61 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [False, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 42 in steps 62 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fc9990>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fcbd30>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fca7d0>, <__main__.Case object at 0x7f34c3fc8d60>, <__main__.Case object at 0x7f34c3fc94b0>, <__main__.Case object at 0x7f34c3fc8d00>, <__main__.Case object at 0x7f34c3fcb190>, <__main__.Case object at 0x7f34c3fcb3a0>, <__main__.Case object at 0x7f34c3fc8430>, <__main__.Case object at 0x7f34c3fc9bd0>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fc8160>, <__main__.Case object at 0x7f34c3fc8eb0>, <__main__.Case object at 0x7f34c3fcb640>, <__main__.Case object at 0x7f34c3fc8a00>, <__main__.Case object at 0x7f34c3fcb040>, <__main__.Case object at 0x7f34c3fcb670>, <__main__.Case object at 0x7f34c3fcbc70>, <__main__.Case object at 0x7f34c3fcbc10>, <__main__.Case object at 0x7f34c3fc8100>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fcaec0>, <__main__.Case object at 0x7f34c3fca4d0>, <__main__.Case object at 0x7f34c3fcb910>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fcaa70>, <__main__.Case object at 0x7f34c3fcad70>, <__main__.Case object at 0x7f34c3fcb4f0>, <__main__.Case object at 0x7f34c3fcb970>, <__main__.Case object at 0x7f34c3fca2c0>, <__main__.Case object at 0x7f34c3fc9420>, <__main__.Case object at 0x7f34c3fcab90>, <__main__.Case object at 0x7f34c3fca1d0>, <__main__.Case object at 0x7f34c0f16a10>, <__main__.Case object at 0x7f34c0f15450>, <__main__.Case object at 0x7f34c0f16200>, <__main__.Case object at 0x7f34c0f16d70>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c0f17190>, <__main__.Case object at 0x7f34c0f14ee0>, <__main__.Case object at 0x7f34c0f15780>, <__main__.Case object at 0x7f34c0f15d20>, <__main__.Case object at 0x7f34c0f16b90>, <__main__.Case object at 0x7f34c0f17730>, <__main__.Case object at 0x7f34c0f17eb0>, <__main__.Case object at 0x7f34c3fcb070>, <__main__.Case object at 0x7f34c0f14340>, <__main__.Case object at 0x7f34c0f15f30>, <__main__.Case object at 0x7f34c0f16b30>, <__main__.Case object at 0x7f34c3fc8df0>, <__main__.Case object at 0x7f34c0f168f0>, <__main__.Case object at 0x7f34c0f15210>, <__main__.Case object at 0x7f34c0f14790>, <__main__.Case object at 0x7f34c0f14c10>, <__main__.Case object at 0x7f34c0f16170>, <__main__.Case object at 0x7f34c0f170a0>, <__main__.Case object at 0x7f34c0f17ee0>, <__main__.Case object at 0x7f34c0f14640>, <__main__.Case object at 0x7f34c0f152d0>, <__main__.Case object at 0x7f34c0f15a80>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3faafe0>, <__main__.Case object at 0x7f34c3fc8c10>, <__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fc9720>, <__main__.Case object at 0x7f34c3fc9e70>, <__main__.Case object at 0x7f34c3fcbd00>, <__main__.Case object at 0x7f34c3fc86a0>, <__main__.Case object at 0x7f34c3fc9390>, <__main__.Case object at 0x7f34c3fcb9d0>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fc9f00>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fc9240>, <__main__.Case object at 0x7f34c3fca170>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fcb460>, <__main__.Case object at 0x7f34c3fa8a00>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fcb370>, <__main__.Case object at 0x7f34c3fcb250>, <__main__.Case object at 0x7f34c3fc9690>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fc96f0>, <__main__.Case object at 0x7f34c3fc8850>, <__main__.Case object at 0x7f34c3fcb130>, <__main__.Case object at 0x7f34c3fc91e0>, <__main__.Case object at 0x7f34c3fc94e0>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fcb700>, <__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c0f15b40>, <__main__.Case object at 0x7f34c0f16560>, <__main__.Case object at 0x7f34c3fcbac0>, <__main__.Case object at 0x7f34c0f144c0>, <__main__.Case object at 0x7f34c0f14160>, <__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c0f16770>, <__main__.Case object at 0x7f34c0f16e60>, <__main__.Case object at 0x7f34c0f17880>, <__main__.Case object at 0x7f34c0f14b80>, <__main__.Case object at 0x7f34c0f156f0>, <__main__.Case object at 0x7f34c0f16230>, <__main__.Case object at 0x7f34c0f160e0>, <__main__.Case object at 0x7f34c0f177c0>, <__main__.Case object at 0x7f34c0f143a0>, <__main__.Case object at 0x7f34c0f154e0>, <__main__.Case object at 0x7f34c0f15d50>, <__main__.Case object at 0x7f34c0f16c80>, <__main__.Case object at 0x7f34c0f17b80>, <__main__.Case object at 0x7f34c0f14940>, <__main__.Case object at 0x7f34c0f14670>, <__main__.Case object at 0x7f34c0f159c0>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.7000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.9, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.40000000000000013, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.40000000000000013, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.40000000000000013, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 3, tv: 0.5, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.5, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.5, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.9, time steps: 16\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 6) is empty. Temporary case base stored to the case base: ((3, 6), 4, 0.5)\n",
      "Episode succeeded, case (3, 5) is empty. Temporary case base stored to the case base: ((3, 5), 2, 0.5)\n",
      "Episode succeeded, case (3, 4) is empty. Temporary case base stored to the case base: ((3, 4), 2, 0.5)\n",
      "Episode succeeded, case (2, 4) is empty. Temporary case base stored to the case base: ((2, 4), 4, 0.5)\n",
      "Episode succeeded, case (1, 4) is empty. Temporary case base stored to the case base: ((1, 4), 4, 0.5)\n",
      "Episode succeeded, case (1, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 3) is empty. Temporary case base stored to the case base: ((1, 3), 2, 0.5)\n",
      "Episode succeeded, case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 2, 0.5)\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) is empty. Temporary case base stored to the case base: ((0, 2), 4, 0.5)\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 2, 0.5)\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 2, 0.5)\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 1) is empty. Temporary case base stored to the case base: ((1, 1), 3, 0.5)\n",
      "Episode succeeded, case (1, 0) is empty. Temporary case base stored to the case base: ((1, 0), 2, 0.5)\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (5, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (5, 2) is empty. Temporary case base stored to the case base: ((5, 2), 4, 0.5)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 4, 0.5)\n",
      "Integrated case process. comm case (4, 1) is empty. Temporary case base stored to the case base: ((4, 1), 2, 0.5)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 3, 0.5)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.49999999999999983)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.49999999999999983)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.49999999999999983)\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.7000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.7000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 4), solution: 3, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.9, time steps: 15\n",
      "cases content after RETAIN, problem: (5, 5), solution: 3, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (5, 4), solution: 2, tv: 0.5, time steps: 11\n",
      "cases content after RETAIN, problem: (6, 1), solution: 2, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.9, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.5, time steps: 60\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.5, time steps: 59\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.5, time steps: 58\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.5, time steps: 57\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.5, time steps: 56\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.5, time steps: 54\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.5, time steps: 53\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.5, time steps: 50\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.5, time steps: 44\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.5, time steps: 41\n",
      "cases content after RETAIN, problem: (1, 1), solution: 3, tv: 0.5, time steps: 10\n",
      "cases content after RETAIN, problem: (1, 0), solution: 2, tv: 0.5, time steps: 9\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.5, time steps: 25\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (4, 1), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.5, time steps: 20\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.5, time steps: 17\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fcb580>, <__main__.Case object at 0x7f34c3fca980>, <__main__.Case object at 0x7f34c3fca560>, <__main__.Case object at 0x7f34c3fc8bb0>, <__main__.Case object at 0x7f34c3fcb2b0>, <__main__.Case object at 0x7f34c3fc9810>, <__main__.Case object at 0x7f34c3fc8730>, <__main__.Case object at 0x7f34c3fcae90>, <__main__.Case object at 0x7f34c3fc8280>, <__main__.Case object at 0x7f34c3fca6b0>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fc9f90>, <__main__.Case object at 0x7f34c3fcbaf0>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fcbbb0>, <__main__.Case object at 0x7f34c3fc8940>, <__main__.Case object at 0x7f34c3fc8f70>, <__main__.Case object at 0x7f34c3fca0e0>, <__main__.Case object at 0x7f34c3fc9cf0>, <__main__.Case object at 0x7f34c3fc9ab0>, <__main__.Case object at 0x7f34c3fc9ff0>, <__main__.Case object at 0x7f34c3fc8cd0>, <__main__.Case object at 0x7f34c3fca200>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fc83a0>, <__main__.Case object at 0x7f34c3fc9c30>, <__main__.Case object at 0x7f34c3fcbee0>, <__main__.Case object at 0x7f34c3fc9120>, <__main__.Case object at 0x7f34c3fc9030>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fc9330>, <__main__.Case object at 0x7f34c3fc9b40>, <__main__.Case object at 0x7f34c3fc9a50>, <__main__.Case object at 0x7f34c3fc87c0>, <__main__.Case object at 0x7f34c3fcad10>, <__main__.Case object at 0x7f34c3fcabc0>, <__main__.Case object at 0x7f34c3fcb4c0>, <__main__.Case object at 0x7f34c0f15de0>, <__main__.Case object at 0x7f34c0f169e0>, <__main__.Case object at 0x7f34c0f17a60>, <__main__.Case object at 0x7f34c0f149a0>, <__main__.Case object at 0x7f34c0f14760>, <__main__.Case object at 0x7f34c0f15420>, <__main__.Case object at 0x7f34c3fcbd90>, <__main__.Case object at 0x7f34c0f17c10>, <__main__.Case object at 0x7f34c0f17160>, <__main__.Case object at 0x7f34c0f17b50>, <__main__.Case object at 0x7f34c0f14fd0>, <__main__.Case object at 0x7f34c0f15180>, <__main__.Case object at 0x7f34c3fc8c70>, <__main__.Case object at 0x7f34c0f16590>, <__main__.Case object at 0x7f34c0f16f50>, <__main__.Case object at 0x7f34c0f17bb0>, <__main__.Case object at 0x7f34c0f16aa0>, <__main__.Case object at 0x7f34c0f140d0>, <__main__.Case object at 0x7f34c0f157b0>, <__main__.Case object at 0x7f34c0f16a70>, <__main__.Case object at 0x7f34c0f17520>, <__main__.Case object at 0x7f34c0f14490>, <__main__.Case object at 0x7f34c3fcacb0>, <__main__.Case object at 0x7f34c0f15600>, <__main__.Case object at 0x7f34c0f14550>]\n",
      "agent1 comm temp case base: []\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 4), solution: 4, tv: 0.4, time steps: 34\n",
      "case content after REVISE for agent 1, problem: (5, 3), solution: 2, tv: 0.4, time steps: 33\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 1, tv: 0.4, time steps: 31\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.6, time steps: 25\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.6, time steps: 23\n",
      "case content after REVISE for agent 1, problem: (4, 1), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 2, tv: 0.6, time steps: 20\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 3, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (5, 1), solution: 1, tv: 0.4, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (4, 3), solution: 4, tv: 0.4, time steps: 12\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 3, 0.5)\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.6, time steps: 25\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.6, time steps: 23\n",
      "cases content after RETAIN, problem: (4, 1), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.6, time steps: 20\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.5, time steps: 3\n",
      "Episode: 42, Total Steps: 63, Total Rewards: [38, 81], Status Episode: True\n",
      "------------------------------------------End of episode 42 loop--------------------\n",
      "----- starting point of Episode 43 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.5999999999999999, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 2, 0.5, 41)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.5999999999999999, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 1), 2, 0.5, 44)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.5999999999999999, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 2), 4, 0.5, 50)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 2, 0.5, 53)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 0) with action 3 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 43 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.6, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 3), 2, 0.5, 54)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 0), 2, 0.6, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 4), 4, 0.5, 56)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 1), 2, 0.6, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 4), 4, 0.5, 57)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 2), 4, 0.6, 23)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 4), 2, 0.5, 58)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 2), 4, 0.6, 25)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 5), 2, 0.5, 59)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 6) with action 2 to next state (3, 7): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 2) with action 4 to next state (7, 2): pull reward: 0\n",
      "----- starting point of Episode 43 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 7) with action 4 to next state (4, 7): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 2) with action 4 to next state (8, 2): pull reward: 0\n",
      "----- starting point of Episode 43 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 7) with action 4 to next state (5, 7): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 2) with action 3 to next state (7, 2): pull reward: 0\n",
      "----- starting point of Episode 43 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (5, 7) with action 3 to next state (4, 7): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 2) with action 1 to next state (7, 1): pull reward: 0\n",
      "----- starting point of Episode 43 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 7) with action 2 to next state (4, 8): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 1) with action 4 to next state (8, 1): pull reward: 0\n",
      "----- starting point of Episode 43 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 8) with action 1 to next state (4, 7): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 1) with action 2 to next state (8, 2): pull reward: 0\n",
      "----- starting point of Episode 43 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 7) with action 3 to next state (3, 7): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 2) with action 2 to next state (8, 3): pull reward: 0\n",
      "----- starting point of Episode 43 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 7) with action 1 to next state (3, 6): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 3) with action 1 to next state (8, 2): pull reward: 0\n",
      "----- starting point of Episode 43 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 6), 4, 0.5, 60)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 2) with action 0 to next state (8, 2): pull reward: 0\n",
      "----- starting point of Episode 43 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 0.9, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 2) with action 0 to next state (8, 2): pull reward: 0\n",
      "----- starting point of Episode 43 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 5), 1, 0.7, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 2) with action 3 to next state (7, 2): pull reward: 0\n",
      "----- starting point of Episode 43 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 2) with action 0 to next state (7, 2): pull reward: 0\n",
      "----- starting point of Episode 43 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 2) with action 3 to next state (6, 2): pull reward: 0\n",
      "----- starting point of Episode 43 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 43 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fcba60>, <__main__.Case object at 0x7f34c3fc8dc0>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fcac80>, <__main__.Case object at 0x7f34c3fcafb0>, <__main__.Case object at 0x7f34c3fca140>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fc9690>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fc96c0>, <__main__.Case object at 0x7f34c3fcbf10>, <__main__.Case object at 0x7f34c3fcbb20>, <__main__.Case object at 0x7f34c3fcb340>, <__main__.Case object at 0x7f34c3fc9450>, <__main__.Case object at 0x7f34c3fcbc10>, <__main__.Case object at 0x7f34c3fcab90>, <__main__.Case object at 0x7f34c3fcbb80>, <__main__.Case object at 0x7f34c3fcb070>, <__main__.Case object at 0x7f34c3fca980>, <__main__.Case object at 0x7f34c3fa8a00>, <__main__.Case object at 0x7f34c3fc8280>, <__main__.Case object at 0x7f34c3fca650>, <__main__.Case object at 0x7f34c3fc8f70>, <__main__.Case object at 0x7f34c3fc9ff0>, <__main__.Case object at 0x7f34c3fcae90>, <__main__.Case object at 0x7f34c3fc9c30>, <__main__.Case object at 0x7f34c3fc9030>, <__main__.Case object at 0x7f34c3fc9a50>, <__main__.Case object at 0x7f34c3fc96f0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3f18f10>, <__main__.Case object at 0x7f34c3fc8a30>, <__main__.Case object at 0x7f34c3fc8250>, <__main__.Case object at 0x7f34c3fcb280>, <__main__.Case object at 0x7f34c3fca6e0>, <__main__.Case object at 0x7f34c3fcbac0>, <__main__.Case object at 0x7f34c3fcbd30>, <__main__.Case object at 0x7f34c3fcb610>]\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.4, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.4, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.7999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.6000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 3, tv: 0.4, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1.0, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (5, 5), solution: 3, tv: 0.4, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (5, 4), solution: 2, tv: 0.4, time steps: 11\n",
      "case content after REVISE for agent 0, problem: (6, 1), solution: 2, tv: 0.4, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 1.0, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 6), solution: 4, tv: 0.6, time steps: 60\n",
      "case content after REVISE for agent 0, problem: (3, 5), solution: 2, tv: 0.6, time steps: 59\n",
      "case content after REVISE for agent 0, problem: (3, 4), solution: 2, tv: 0.6, time steps: 58\n",
      "case content after REVISE for agent 0, problem: (2, 4), solution: 4, tv: 0.6, time steps: 57\n",
      "case content after REVISE for agent 0, problem: (1, 4), solution: 4, tv: 0.6, time steps: 56\n",
      "case content after REVISE for agent 0, problem: (1, 3), solution: 2, tv: 0.6, time steps: 54\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 2, tv: 0.6, time steps: 53\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 4, tv: 0.6, time steps: 50\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 2, tv: 0.6, time steps: 44\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.6, time steps: 41\n",
      "case content after REVISE for agent 0, problem: (1, 1), solution: 3, tv: 0.4, time steps: 10\n",
      "case content after REVISE for agent 0, problem: (1, 0), solution: 2, tv: 0.4, time steps: 9\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.4, time steps: 25\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 0.4, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (4, 1), solution: 2, tv: 0.4, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 2, tv: 0.4, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 3, tv: 0.4, time steps: 17\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 6), 4, 0.5, 30)\n",
      "Episode succeeded, case (3, 7) is empty. Temporary case base stored to the case base: ((3, 7), 1, 0.5)\n",
      "Episode succeeded, case (4, 7) is empty. Temporary case base stored to the case base: ((4, 7), 3, 0.5)\n",
      "Episode succeeded, case (4, 8) is empty. Temporary case base stored to the case base: ((4, 8), 1, 0.5)\n",
      "Episode succeeded, case (4, 7) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 7) is empty. Temporary case base stored to the case base: ((5, 7), 3, 0.5)\n",
      "Episode succeeded, case (4, 7) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 7) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 5), 2, 0.5, 30)\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 4), 2, 0.5, 30)\n",
      "Episode succeeded, updated case base with fewer steps: ((2, 4), 4, 0.5, 30)\n",
      "Episode succeeded, updated case base with fewer steps: ((1, 4), 4, 0.5, 30)\n",
      "Episode succeeded, updated case base with fewer steps: ((1, 3), 2, 0.5, 30)\n",
      "Episode succeeded, updated case base with fewer steps: ((1, 2), 2, 0.5, 30)\n",
      "Episode succeeded, updated case base with fewer steps: ((0, 2), 4, 0.5, 30)\n",
      "Episode succeeded, updated case base with fewer steps: ((0, 1), 2, 0.5, 30)\n",
      "Episode succeeded, updated case base with fewer steps: ((0, 0), 2, 0.5, 30)\n",
      "Integrated case process. comm case (5, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 0.6)\n",
      "Integrated case process. comm case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 4, 0.6)\n",
      "Integrated case process. comm case (4, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 1), 2, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 2, 0.6)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.6)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 0.5999999999999999)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 0.5999999999999999)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.5999999999999999)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.6000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1.0, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1.0, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.5, time steps: 30\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.5, time steps: 30\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.5, time steps: 30\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.5, time steps: 30\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.5, time steps: 30\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.5, time steps: 30\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.5, time steps: 30\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.5, time steps: 30\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.5, time steps: 30\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.5, time steps: 30\n",
      "cases content after RETAIN, problem: (3, 7), solution: 1, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (4, 7), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 8), solution: 1, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (5, 7), solution: 3, tv: 0.5, time steps: 12\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fc8670>, <__main__.Case object at 0x7f34c3fc8610>, <__main__.Case object at 0x7f34c3fc8e20>, <__main__.Case object at 0x7f34c3fcb250>, <__main__.Case object at 0x7f34c3fcba00>, <__main__.Case object at 0x7f34c3fcbf40>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fc8d60>, <__main__.Case object at 0x7f34c3fc9780>, <__main__.Case object at 0x7f34c3fc95d0>, <__main__.Case object at 0x7f34c3fca9e0>, <__main__.Case object at 0x7f34c3fc8100>, <__main__.Case object at 0x7f34c3fcb670>, <__main__.Case object at 0x7f34c3fcaa70>, <__main__.Case object at 0x7f34c3fcb910>, <__main__.Case object at 0x7f34c3fcb640>, <__main__.Case object at 0x7f34c3fcb790>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fc8bb0>, <__main__.Case object at 0x7f34c3fc8730>, <__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c3fcbaf0>, <__main__.Case object at 0x7f34c3fc8940>, <__main__.Case object at 0x7f34c3fc9cf0>, <__main__.Case object at 0x7f34c3fca050>, <__main__.Case object at 0x7f34c3fc83a0>, <__main__.Case object at 0x7f34c3fc9120>, <__main__.Case object at 0x7f34c3fc9330>, <__main__.Case object at 0x7f34c3fc92d0>, <__main__.Case object at 0x7f34c3fc8c70>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fc8460>, <__main__.Case object at 0x7f34c3fcbe50>, <__main__.Case object at 0x7f34c3fc8e50>, <__main__.Case object at 0x7f34c3fcb370>, <__main__.Case object at 0x7f34c3fc8850>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fca7d0>, <__main__.Case object at 0x7f34c3fc9bd0>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fcb580>, <__main__.Case object at 0x7f34c3fcb2b0>, <__main__.Case object at 0x7f34c3fcaec0>, <__main__.Case object at 0x7f34c3fc9c00>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fca0e0>, <__main__.Case object at 0x7f34c3fc9ab0>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fcbee0>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fc9b40>, <__main__.Case object at 0x7f34c3fcb4c0>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.9, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.7, time steps: 25\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.7, time steps: 23\n",
      "case content after REVISE for agent 1, problem: (4, 1), solution: 2, tv: 0.7, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 2, tv: 0.7, time steps: 20\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 3, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 3, tv: 0.6, time steps: 3\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) is empty. Temporary case base stored to the case base: ((7, 2), 3, 0.5)\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 3, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 1, 0.5)\n",
      "Episode succeeded, case (8, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 2, 0.5)\n",
      "Episode succeeded, case (7, 1) is empty. Temporary case base stored to the case base: ((7, 1), 4, 0.5)\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 0.7)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 0.9)\n",
      "Integrated case process. comm case (3, 6) is empty. Temporary case base stored to the case base: ((3, 6), 4, 0.5)\n",
      "Integrated case process. comm case (3, 5) is empty. Temporary case base stored to the case base: ((3, 5), 2, 0.5)\n",
      "Integrated case process. comm case (3, 4) is empty. Temporary case base stored to the case base: ((3, 4), 2, 0.5)\n",
      "Integrated case process. comm case (2, 4) is empty. Temporary case base stored to the case base: ((2, 4), 4, 0.5)\n",
      "Integrated case process. comm case (1, 4) is empty. Temporary case base stored to the case base: ((1, 4), 4, 0.5)\n",
      "Integrated case process. comm case (1, 3) is empty. Temporary case base stored to the case base: ((1, 3), 2, 0.5)\n",
      "Integrated case process. comm case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 2, 0.5)\n",
      "Integrated case process. comm case (0, 2) is empty. Temporary case base stored to the case base: ((0, 2), 4, 0.5)\n",
      "Integrated case process. comm case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 2, 0.5)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.9, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.7, time steps: 25\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.7, time steps: 23\n",
      "cases content after RETAIN, problem: (4, 1), solution: 2, tv: 0.7, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.6, time steps: 3\n",
      "cases content after RETAIN, problem: (7, 2), solution: 3, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (8, 2), solution: 3, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (8, 3), solution: 1, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (8, 1), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (7, 1), solution: 4, tv: 0.5, time steps: 13\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.5, time steps: 60\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.5, time steps: 59\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.5, time steps: 58\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.5, time steps: 57\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.5, time steps: 56\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.5, time steps: 54\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.5, time steps: 53\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.5, time steps: 50\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.5, time steps: 44\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.5, time steps: 41\n",
      "Episode: 43, Total Steps: 30, Total Rewards: [81, 71], Status Episode: True\n",
      "------------------------------------------End of episode 43 loop--------------------\n",
      "----- starting point of Episode 44 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.6999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 2, 0.5, 30)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.6999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 1), 2, 0.5, 30)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.6999999999999998, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 4 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 3, 0.6, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 2, 0.5, 30)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7, 17)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 3) with action 1 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 0), 2, 0.7, 20)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 2, 0.5, 30)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 1), 2, 0.7, 22)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 3), 2, 0.5, 30)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 2), 4, 0.7, 23)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 4), 4, 0.5, 30)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 2), 4, 0.7, 25)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 4), 4, 0.5, 30)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 4), 2, 0.5, 30)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 5), 2, 0.5, 30)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 6), 4, 0.5, 30)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 1.0, 15)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 5), 1, 0.7999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 44 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((6, 6), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fcb4c0>, <__main__.Case object at 0x7f34c3fc9990>, <__main__.Case object at 0x7f34c3fc8130>, <__main__.Case object at 0x7f34c3fca0e0>, <__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fcba60>, <__main__.Case object at 0x7f34c3fcafb0>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fcb340>, <__main__.Case object at 0x7f34c3fcadd0>, <__main__.Case object at 0x7f34c3fc9cc0>, <__main__.Case object at 0x7f34c3fcbd90>, <__main__.Case object at 0x7f34c3fcb8e0>, <__main__.Case object at 0x7f34c3fc9ed0>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fc95a0>, <__main__.Case object at 0x7f34c3fc8340>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3fa8a00>, <__main__.Case object at 0x7f34c3faafe0>, <__main__.Case object at 0x7f34c3fc99c0>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fc94e0>, <__main__.Case object at 0x7f34c3fcac80>, <__main__.Case object at 0x7f34c3fc9690>, <__main__.Case object at 0x7f34c3fcbb80>, <__main__.Case object at 0x7f34c3fca6b0>, <__main__.Case object at 0x7f34c3fc8cd0>, <__main__.Case object at 0x7f34c3fc87c0>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fc9d50>, <__main__.Case object at 0x7f34c3fca770>, <__main__.Case object at 0x7f34c3fc8fa0>, <__main__.Case object at 0x7f34c3fc93f0>]\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.8999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.5000000000000001, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.4999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 6), solution: 4, tv: 0.6, time steps: 30\n",
      "case content after REVISE for agent 0, problem: (3, 5), solution: 2, tv: 0.6, time steps: 30\n",
      "case content after REVISE for agent 0, problem: (3, 4), solution: 2, tv: 0.6, time steps: 30\n",
      "case content after REVISE for agent 0, problem: (2, 4), solution: 4, tv: 0.6, time steps: 30\n",
      "case content after REVISE for agent 0, problem: (1, 4), solution: 4, tv: 0.6, time steps: 30\n",
      "case content after REVISE for agent 0, problem: (1, 3), solution: 2, tv: 0.6, time steps: 30\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 2, tv: 0.6, time steps: 30\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 4, tv: 0.6, time steps: 30\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 2, tv: 0.6, time steps: 30\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.6, time steps: 30\n",
      "case content after REVISE for agent 0, problem: (3, 7), solution: 1, tv: 0.4, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (4, 7), solution: 3, tv: 0.4, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 8), solution: 1, tv: 0.4, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (5, 7), solution: 3, tv: 0.4, time steps: 12\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.4999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.4999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.4999999999999999, time steps: 13\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 6), 4, 0.5, 17)\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 5), 2, 0.5, 17)\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 4), 2, 0.5, 17)\n",
      "Episode succeeded, updated case base with fewer steps: ((2, 4), 4, 0.5, 17)\n",
      "Episode succeeded, updated case base with fewer steps: ((1, 4), 4, 0.5, 17)\n",
      "Episode succeeded, updated case base with fewer steps: ((1, 3), 2, 0.5, 17)\n",
      "Episode succeeded, updated case base with fewer steps: ((1, 2), 2, 0.5, 17)\n",
      "Episode succeeded, case (1, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((0, 2), 4, 0.5, 17)\n",
      "Episode succeeded, updated case base with fewer steps: ((0, 1), 2, 0.5, 17)\n",
      "Episode succeeded, updated case base with fewer steps: ((0, 0), 2, 0.5, 17)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 6) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 6), 3, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) is empty. Temporary case base stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) is empty. Temporary case base stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (5, 2) is empty. Temporary case base stored to the case base: ((5, 2), 4, 0.7)\n",
      "Integrated case process. comm case (4, 2) is empty. Temporary case base stored to the case base: ((4, 2), 4, 0.7)\n",
      "Integrated case process. comm case (4, 1) is empty. Temporary case base stored to the case base: ((4, 1), 2, 0.7)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 2, 0.7)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 3, 0.7)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 3, 0.6)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.6999999999999998)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.6999999999999998)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.6999999999999998)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.8999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.5000000000000001, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (6, 0), solution: 2, tv: 0.4999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.4999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.4999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.4999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.7, time steps: 25\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.7, time steps: 23\n",
      "cases content after RETAIN, problem: (4, 1), solution: 2, tv: 0.7, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.7, time steps: 20\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.7, time steps: 17\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fcbe20>, <__main__.Case object at 0x7f34c3fc94b0>, <__main__.Case object at 0x7f34c3fc9090>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fcbf10>, <__main__.Case object at 0x7f34c3fc8e80>, <__main__.Case object at 0x7f34c3fc9b10>, <__main__.Case object at 0x7f34c3fcaef0>, <__main__.Case object at 0x7f34c3fca170>, <__main__.Case object at 0x7f34c3fc9930>, <__main__.Case object at 0x7f34c3fc8a00>, <__main__.Case object at 0x7f34c3fc9420>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fcb6d0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fc8c70>, <__main__.Case object at 0x7f34c3fca230>, <__main__.Case object at 0x7f34c3fc9ab0>, <__main__.Case object at 0x7f34c3fc8dc0>, <__main__.Case object at 0x7f34c3fca140>, <__main__.Case object at 0x7f34c3fc96c0>, <__main__.Case object at 0x7f34c3fc89d0>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fcbf40>, <__main__.Case object at 0x7f34c3fc8160>, <__main__.Case object at 0x7f34c3fc8d00>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fc9120>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.8, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.7999999999999999, time steps: 25\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.7999999999999999, time steps: 23\n",
      "case content after REVISE for agent 1, problem: (4, 1), solution: 2, tv: 0.7999999999999999, time steps: 22\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 2, tv: 0.7999999999999999, time steps: 20\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 3, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 3, tv: 0.7, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (7, 2), solution: 3, tv: 0.4, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 3, tv: 0.4, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 1, tv: 0.4, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 2, tv: 0.4, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (7, 1), solution: 4, tv: 0.4, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (3, 6), solution: 4, tv: 0.4, time steps: 60\n",
      "case content after REVISE for agent 1, problem: (3, 5), solution: 2, tv: 0.4, time steps: 59\n",
      "case content after REVISE for agent 1, problem: (3, 4), solution: 2, tv: 0.4, time steps: 58\n",
      "case content after REVISE for agent 1, problem: (2, 4), solution: 4, tv: 0.4, time steps: 57\n",
      "case content after REVISE for agent 1, problem: (1, 4), solution: 4, tv: 0.4, time steps: 56\n",
      "case content after REVISE for agent 1, problem: (1, 3), solution: 2, tv: 0.4, time steps: 54\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 2, tv: 0.4, time steps: 53\n",
      "case content after REVISE for agent 1, problem: (0, 2), solution: 4, tv: 0.4, time steps: 50\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 2, tv: 0.4, time steps: 44\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.4, time steps: 41\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 2), 4, 0.5, 17)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 2), 4, 0.5, 17)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 1), 2, 0.5, 17)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 0), 2, 0.5, 17)\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1.0)\n",
      "Integrated case process. comm case (3, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 6), 4, 0.5)\n",
      "Integrated case process. comm case (3, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 5), 2, 0.5)\n",
      "Integrated case process. comm case (3, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 4), 2, 0.5)\n",
      "Integrated case process. comm case (2, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 4), 4, 0.5)\n",
      "Integrated case process. comm case (1, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 4), 4, 0.5)\n",
      "Integrated case process. comm case (1, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 3), 2, 0.5)\n",
      "Integrated case process. comm case (1, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 2), 2, 0.5)\n",
      "Integrated case process. comm case (1, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 2), 2, 0.5)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 2, 0.5)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.8, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 1), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.7999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.7, time steps: 3\n",
      "Episode: 44, Total Steps: 17, Total Rewards: [87, 84], Status Episode: True\n",
      "------------------------------------------End of episode 44 loop--------------------\n",
      "----- starting point of Episode 45 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.7999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 2, 0.5, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.7999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 1), 2, 0.5, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.7999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 2), 4, 0.5, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 3, 0.7, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 2, 0.5, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999999, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 3), 2, 0.5, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 4), 4, 0.5, 17)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 3 to next state (3, 0): pull reward: 0\n",
      "----- starting point of Episode 45 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 4), 4, 0.5, 17)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (3, 0) with action 2 to next state (3, 1): pull reward: 0\n",
      "----- starting point of Episode 45 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 4), 2, 0.5, 17)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (3, 1) with action 2 to next state (3, 2): pull reward: 0\n",
      "----- starting point of Episode 45 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 5), 2, 0.5, 17)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (3, 2) with action 4 to next state (4, 2): pull reward: 0\n",
      "----- starting point of Episode 45 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 2), 4, 0.5, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 6), 4, 0.5, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 2) with action 0 to next state (5, 2): pull reward: 0\n",
      "----- starting point of Episode 45 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 2), 4, 0.5, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 5), 1, 0.8999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 2), 4, 0.5, 17)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 2), 4, 0.5, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 2), 4, 0.5, 17)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 2), 4, 0.5, 17)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 2), 4, 0.5, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 2), 4, 0.5, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 2), 4, 0.5, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 45 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((5, 2), 4, 0.5, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fc9120>, <__main__.Case object at 0x7f34c3fc8fd0>, <__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c3fcb2e0>, <__main__.Case object at 0x7f34c3fcb730>, <__main__.Case object at 0x7f34c3fc8e20>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fc8130>, <__main__.Case object at 0x7f34c3fc8910>, <__main__.Case object at 0x7f34c3fcbd90>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fcb2b0>, <__main__.Case object at 0x7f34c3fcbf70>, <__main__.Case object at 0x7f34c3fc9b10>, <__main__.Case object at 0x7f34c3fc8a00>, <__main__.Case object at 0x7f34c3fc83d0>, <__main__.Case object at 0x7f34c3fc9390>, <__main__.Case object at 0x7f34c3fc8f10>, <__main__.Case object at 0x7f34c3fc9450>, <__main__.Case object at 0x7f34c3fcb790>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3faafe0>, <__main__.Case object at 0x7f34c3fab010>, <__main__.Case object at 0x7f34c3fcb670>, <__main__.Case object at 0x7f34c3fcb580>, <__main__.Case object at 0x7f34c3fca530>, <__main__.Case object at 0x7f34c3fc9030>, <__main__.Case object at 0x7f34c3fc94b0>, <__main__.Case object at 0x7f34c3fca1a0>, <__main__.Case object at 0x7f34c3fc8e80>, <__main__.Case object at 0x7f34c3fc9930>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fcbc40>, <__main__.Case object at 0x7f34c3fca2c0>, <__main__.Case object at 0x7f34c3fc8730>]\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 0.9999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (5, 6), solution: 3, tv: 0.40000000000000013, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 6), solution: 3, tv: 0.40000000000000013, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.40000000000000013, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 2, tv: 0.3999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 6), solution: 4, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 5), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 4), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (2, 4), solution: 4, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (1, 4), solution: 4, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (1, 3), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 4, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.3999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.3999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.3999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.6, time steps: 25\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 0.6, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (4, 1), solution: 2, tv: 0.6, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 2, tv: 0.6, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 3, tv: 0.6, time steps: 17\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 0.5)\n",
      "Integrated case process. comm case (5, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 0.5)\n",
      "Integrated case process. comm case (5, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 0.5)\n",
      "Integrated case process. comm case (5, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 0.5)\n",
      "Integrated case process. comm case (5, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 0.5)\n",
      "Integrated case process. comm case (5, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 0.5)\n",
      "Integrated case process. comm case (5, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 0.5)\n",
      "Integrated case process. comm case (5, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 0.5)\n",
      "Integrated case process. comm case (5, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 0.5)\n",
      "Integrated case process. comm case (4, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 2), 4, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 3, 0.7)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.7999999999999998)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 0.9999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.6, time steps: 25\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.6, time steps: 23\n",
      "cases content after RETAIN, problem: (4, 1), solution: 2, tv: 0.6, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.6, time steps: 20\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.6, time steps: 17\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fc92d0>, <__main__.Case object at 0x7f34c3fc9780>, <__main__.Case object at 0x7f34c3fcacb0>, <__main__.Case object at 0x7f34c3fca950>, <__main__.Case object at 0x7f34c3fc99c0>, <__main__.Case object at 0x7f34c3fca9e0>, <__main__.Case object at 0x7f34c3fc9990>, <__main__.Case object at 0x7f34c3fc9b40>, <__main__.Case object at 0x7f34c3fcadd0>, <__main__.Case object at 0x7f34c3fc9ed0>, <__main__.Case object at 0x7f34c3fc8340>, <__main__.Case object at 0x7f34c3fc88b0>, <__main__.Case object at 0x7f34c3fc8df0>, <__main__.Case object at 0x7f34c3fca170>, <__main__.Case object at 0x7f34c3fcbf10>, <__main__.Case object at 0x7f34c3fc9420>, <__main__.Case object at 0x7f34c3fcba00>, <__main__.Case object at 0x7f34c3fca1d0>, <__main__.Case object at 0x7f34c3fcbaf0>, <__main__.Case object at 0x7f34c3fcad70>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fcb6d0>, <__main__.Case object at 0x7f34c3fc91e0>, <__main__.Case object at 0x7f34c3fc9660>, <__main__.Case object at 0x7f34c3fcbfd0>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fcb280>, <__main__.Case object at 0x7f34c3fca200>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fcb8e0>, <__main__.Case object at 0x7f34c3fc95a0>, <__main__.Case object at 0x7f34c3fca620>, <__main__.Case object at 0x7f34c3fcaef0>, <__main__.Case object at 0x7f34c3fc80d0>, <__main__.Case object at 0x7f34c3fcab90>, <__main__.Case object at 0x7f34c3fcbb20>, <__main__.Case object at 0x7f34c3fcb910>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.7000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 1), solution: 2, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 2, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 3, tv: 0.8999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 3, tv: 0.7999999999999999, time steps: 3\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 0.5)\n",
      "Episode succeeded, case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 0.5)\n",
      "Episode succeeded, case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 2, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (3, 6) is empty. Temporary case base stored to the case base: ((3, 6), 4, 0.5)\n",
      "Integrated case process. comm case (3, 5) is empty. Temporary case base stored to the case base: ((3, 5), 2, 0.5)\n",
      "Integrated case process. comm case (3, 4) is empty. Temporary case base stored to the case base: ((3, 4), 2, 0.5)\n",
      "Integrated case process. comm case (2, 4) is empty. Temporary case base stored to the case base: ((2, 4), 4, 0.5)\n",
      "Integrated case process. comm case (1, 4) is empty. Temporary case base stored to the case base: ((1, 4), 4, 0.5)\n",
      "Integrated case process. comm case (1, 3) is empty. Temporary case base stored to the case base: ((1, 3), 2, 0.5)\n",
      "Integrated case process. comm case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 2, 0.5)\n",
      "Integrated case process. comm case (0, 2) is empty. Temporary case base stored to the case base: ((0, 2), 4, 0.5)\n",
      "Integrated case process. comm case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 2, 0.5)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.7000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.8999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.7999999999999999, time steps: 3\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.5, time steps: 8\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.5, time steps: 7\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.5, time steps: 6\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.5, time steps: 17\n",
      "Episode: 45, Total Steps: 20, Total Rewards: [89, 81], Status Episode: True\n",
      "------------------------------------------End of episode 45 loop--------------------\n",
      "----- starting point of Episode 46 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.8999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 2, 0.6, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.8999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 1), 2, 0.6, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.8999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 2), 4, 0.6, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 3, 0.7999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 2, 0.6, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 3), 2, 0.6, 17)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 0) with action 1 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 46 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 4), 4, 0.6, 17)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 0) with action 0 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 46 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.8999999999999999, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 4), 4, 0.6, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 4), 2, 0.6, 17)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 4 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 46 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.8999999999999999, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 5), 2, 0.6, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 6), 4, 0.6, 17)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 46 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 46 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 5), 1, 0.9999999999999999, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 4 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 46 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 46 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 46 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 46 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 46 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 3 to next state (3, 0): pull reward: 0\n",
      "----- starting point of Episode 46 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 46 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fcb910>, <__main__.Case object at 0x7f34c3fc9ff0>, <__main__.Case object at 0x7f34c3fcae90>, <__main__.Case object at 0x7f34c3fca4d0>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fc9d50>, <__main__.Case object at 0x7f34c3fc9a50>, <__main__.Case object at 0x7f34c3fc9120>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fc9db0>, <__main__.Case object at 0x7f34c3fc8bb0>, <__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fc9450>, <__main__.Case object at 0x7f34c3fcbac0>, <__main__.Case object at 0x7f34c3fc99c0>, <__main__.Case object at 0x7f34c3fcb190>, <__main__.Case object at 0x7f34c3fc9cf0>, <__main__.Case object at 0x7f34c3fcbe50>, <__main__.Case object at 0x7f34c3fcbaf0>, <__main__.Case object at 0x7f34c3fcab00>, <__main__.Case object at 0x7f34c3fc8a30>, <__main__.Case object at 0x7f34c3fcb130>, <__main__.Case object at 0x7f34c3fab010>, <__main__.Case object at 0x7f34c3fcaef0>, <__main__.Case object at 0x7f34c0f15750>, <__main__.Case object at 0x7f34c0f144c0>, <__main__.Case object at 0x7f34c3fcbd90>, <__main__.Case object at 0x7f34c0f16dd0>, <__main__.Case object at 0x7f34c0f14700>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3fcb670>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fc9570>, <__main__.Case object at 0x7f34c3fcb3a0>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fca980>]\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 6), solution: 4, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 5), solution: 2, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 4), solution: 2, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (2, 4), solution: 4, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (1, 4), solution: 4, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (1, 3), solution: 2, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 2, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 4, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 2, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.5, time steps: 25\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 0.5, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (4, 1), solution: 2, tv: 0.5, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 2, tv: 0.5, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 3, tv: 0.5, time steps: 17\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 0.8999999999999998)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 0.8999999999999998)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.8999999999999998)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.5, time steps: 25\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (4, 1), solution: 2, tv: 0.5, time steps: 22\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.5, time steps: 20\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.7999999999999999, time steps: 3\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fc9090>, <__main__.Case object at 0x7f34c3fc91b0>, <__main__.Case object at 0x7f34c3fcb970>, <__main__.Case object at 0x7f34c3fc80d0>, <__main__.Case object at 0x7f34c3fcba90>, <__main__.Case object at 0x7f34c3fca140>, <__main__.Case object at 0x7f34c3fcb4c0>, <__main__.Case object at 0x7f34c3fcb340>, <__main__.Case object at 0x7f34c3fc96f0>, <__main__.Case object at 0x7f34c3fc8f10>, <__main__.Case object at 0x7f34c3fc92d0>, <__main__.Case object at 0x7f34c3fc9f30>, <__main__.Case object at 0x7f34c3fcb640>, <__main__.Case object at 0x7f34c3fc86a0>, <__main__.Case object at 0x7f34c3fca050>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c3fc9e70>, <__main__.Case object at 0x7f34c3fc84f0>, <__main__.Case object at 0x7f34c3fc92a0>, <__main__.Case object at 0x7f34c3fc93f0>, <__main__.Case object at 0x7f34c3fc8250>, <__main__.Case object at 0x7f34c3fcaad0>, <__main__.Case object at 0x7f34c0f15690>, <__main__.Case object at 0x7f34c0f16e60>, <__main__.Case object at 0x7f34c0f16080>, <__main__.Case object at 0x7f34c0f156f0>, <__main__.Case object at 0x7f34c0f15240>, <__main__.Case object at 0x7f34c0f17400>, <__main__.Case object at 0x7f34c0f15e10>, <__main__.Case object at 0x7f34c0f16380>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fcad70>, <__main__.Case object at 0x7f34c3fc8610>, <__main__.Case object at 0x7f34c3fc8100>, <__main__.Case object at 0x7f34c3fc8070>, <__main__.Case object at 0x7f34c3fca290>, <__main__.Case object at 0x7f34c3fcb4f0>, <__main__.Case object at 0x7f34c3fc8c70>, <__main__.Case object at 0x7f34c3fc8160>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fc9390>, <__main__.Case object at 0x7f34c3fcb790>, <__main__.Case object at 0x7f34c3fcbf40>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fc8d60>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fcbeb0>, <__main__.Case object at 0x7f34c3fc8b80>, <__main__.Case object at 0x7f34c3fc8e50>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fcbd30>, <__main__.Case object at 0x7f34c3fa8820>, <__main__.Case object at 0x7f34c0f14160>, <__main__.Case object at 0x7f34c0f177f0>, <__main__.Case object at 0x7f34c0f160e0>, <__main__.Case object at 0x7f34c0f16290>, <__main__.Case object at 0x7f34c0f16020>, <__main__.Case object at 0x7f34c0f14040>, <__main__.Case object at 0x7f34c0f15840>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.9999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.9999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.9999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.6000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 3, tv: 0.9999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 3, tv: 0.8999999999999999, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.6, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.6, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 2, tv: 0.6, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (3, 6), solution: 4, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 5), solution: 2, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 4), solution: 2, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (2, 4), solution: 4, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (1, 4), solution: 4, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (1, 3), solution: 2, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 2, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 2), solution: 4, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 2, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.4, time steps: 17\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 3, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (3, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 6), 4, 0.6)\n",
      "Integrated case process. comm case (3, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 5), 2, 0.6)\n",
      "Integrated case process. comm case (3, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 4), 2, 0.6)\n",
      "Integrated case process. comm case (2, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 4), 4, 0.6)\n",
      "Integrated case process. comm case (1, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 4), 4, 0.6)\n",
      "Integrated case process. comm case (1, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 3), 2, 0.6)\n",
      "Integrated case process. comm case (1, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 2), 2, 0.6)\n",
      "Integrated case process. comm case (0, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 2), 4, 0.6)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 2, 0.6)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 0.6)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.9999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.9999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.6000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.9999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.8999999999999999, time steps: 3\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6, time steps: 8\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.6, time steps: 7\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.6, time steps: 6\n",
      "cases content after RETAIN, problem: (4, 0), solution: 3, tv: 0.5, time steps: 17\n",
      "Episode: 46, Total Steps: 31, Total Rewards: [89, 70], Status Episode: True\n",
      "------------------------------------------End of episode 46 loop--------------------\n",
      "----- starting point of Episode 47 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.9999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 2, 0.7, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 1), 2, 0.7, 17)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 0) with action 0 to next state (8, 0): pull reward: 0\n",
      "----- starting point of Episode 47 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.9999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 2), 4, 0.7, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.9999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 2, 0.7, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 3, 0.8999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 3), 2, 0.7, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.9999999999999999, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 4), 4, 0.7, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 0), 3, 0.5, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 4), 4, 0.7, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((3, 0), 2, 0.6, 6)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 4), 2, 0.7, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((3, 1), 2, 0.6, 7)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 5), 2, 0.7, 17)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((3, 2), 4, 0.6, 8)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 6) with action 1 to next state (3, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 47 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 5), 2, 0.7, 17)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 2) with action 2 to next state (4, 3): pull reward: 0\n",
      "----- starting point of Episode 47 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 6), 4, 0.7, 17)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 3) with action 0 to next state (4, 3): pull reward: 0\n",
      "----- starting point of Episode 47 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 hit the obstacle!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 3) with action 3 to next state (3, 3): pull reward: 0\n",
      "----- starting point of Episode 47 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is locked. Done status: True, win status: False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 1, 15)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (3, 3) with action 2 to next state (3, 3): pull reward: 0\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fca980>, <__main__.Case object at 0x7f34c3fa8820>, <__main__.Case object at 0x7f34c3fcb4f0>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fcbf40>, <__main__.Case object at 0x7f34c3fc8b80>, <__main__.Case object at 0x7f34c3fcbd30>, <__main__.Case object at 0x7f34c3fc8730>, <__main__.Case object at 0x7f34c3fcb2e0>, <__main__.Case object at 0x7f34c3fcaa70>, <__main__.Case object at 0x7f34c3fc9450>, <__main__.Case object at 0x7f34c3fc9cf0>, <__main__.Case object at 0x7f34c3fc8a30>, <__main__.Case object at 0x7f34c3fc8dc0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3fa8a00>, <__main__.Case object at 0x7f34c3fcbcd0>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fcbeb0>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fcb5e0>]\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 15\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 0.9, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 6), solution: 4, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 5), solution: 2, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (3, 4), solution: 2, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (2, 4), solution: 4, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (1, 4), solution: 4, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (1, 3), solution: 2, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 2, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 4, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 2, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.7000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.7000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.7000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.4, time steps: 25\n",
      "case content after REVISE for agent 0, problem: (4, 2), solution: 4, tv: 0.4, time steps: 23\n",
      "case content after REVISE for agent 0, problem: (4, 1), solution: 2, tv: 0.4, time steps: 22\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 2, tv: 0.4, time steps: 20\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 3, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 3, tv: 0.7, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 6), 1, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 6), 4, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 5), 2, 0.5, 14)\n",
      "Episode succeeded, case (3, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((3, 4), 2, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((2, 4), 4, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((1, 4), 4, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((1, 3), 2, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((1, 2), 2, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((0, 2), 4, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((0, 1), 2, 0.5, 14)\n",
      "Episode succeeded, updated case base with fewer steps: ((0, 0), 2, 0.5, 14)\n",
      "Integrated case process. comm case (3, 2) is empty. Temporary case base stored to the case base: ((3, 2), 4, 0.6)\n",
      "Integrated case process. comm case (3, 1) is empty. Temporary case base stored to the case base: ((3, 1), 2, 0.6)\n",
      "Integrated case process. comm case (3, 0) is empty. Temporary case base stored to the case base: ((3, 0), 2, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 3, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.9999999999999998)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.9999999999999998)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.9999999999999998)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.9, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.7000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.7000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.7, time steps: 3\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.6, time steps: 8\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.6, time steps: 7\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.6, time steps: 6\n",
      "win status of agent 1  before update the case base: False\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fc8160>, <__main__.Case object at 0x7f34c3fcb790>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fc94b0>, <__main__.Case object at 0x7f34c3fcbb20>, <__main__.Case object at 0x7f34c3fca1d0>, <__main__.Case object at 0x7f34c3fc9ed0>, <__main__.Case object at 0x7f34c3fc99c0>, <__main__.Case object at 0x7f34c3fc9240>, <__main__.Case object at 0x7f34c3fcb190>, <__main__.Case object at 0x7f34c3fcbc40>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c3fca1a0>, <__main__.Case object at 0x7f34c3fc8c70>, <__main__.Case object at 0x7f34c3fc9390>, <__main__.Case object at 0x7f34c3fc9420>, <__main__.Case object at 0x7f34c3fc8e50>, <__main__.Case object at 0x7f34c3fca530>, <__main__.Case object at 0x7f34c3fcb250>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fcbac0>, <__main__.Case object at 0x7f34c3fc8460>, <__main__.Case object at 0x7f34c3fcb700>, <__main__.Case object at 0x7f34c3fc8e80>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.6000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 3, tv: 0.7999999999999998, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 3, tv: 0.7, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (3, 2), solution: 4, tv: 0.39999999999999997, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (3, 1), solution: 2, tv: 0.39999999999999997, time steps: 7\n",
      "case content after REVISE for agent 1, problem: (3, 0), solution: 2, tv: 0.39999999999999997, time steps: 6\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 3, tv: 0.3, time steps: 17\n",
      "Episode not succeeded, temporary case base from own experience is not stored to the case base\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (3, 6) is empty. Temporary case base stored to the case base: ((3, 6), 4, 0.7)\n",
      "Integrated case process. comm case (3, 5) is empty. Temporary case base stored to the case base: ((3, 5), 2, 0.7)\n",
      "Integrated case process. comm case (3, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 5), 2, 0.7)\n",
      "Integrated case process. comm case (3, 4) is empty. Temporary case base stored to the case base: ((3, 4), 2, 0.7)\n",
      "Integrated case process. comm case (2, 4) is empty. Temporary case base stored to the case base: ((2, 4), 4, 0.7)\n",
      "Integrated case process. comm case (1, 4) is empty. Temporary case base stored to the case base: ((1, 4), 4, 0.7)\n",
      "Integrated case process. comm case (1, 3) is empty. Temporary case base stored to the case base: ((1, 3), 2, 0.7)\n",
      "Integrated case process. comm case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 2, 0.7)\n",
      "Integrated case process. comm case (0, 2) is empty. Temporary case base stored to the case base: ((0, 2), 4, 0.7)\n",
      "Integrated case process. comm case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 2, 0.7)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 2, 0.7)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.6000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.7999999999999998, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.7, time steps: 3\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.7, time steps: 17\n",
      "Episode: 47, Total Steps: 14, Total Rewards: [87, -112], Status Episode: False\n",
      "------------------------------------------End of episode 47 loop--------------------\n",
      "----- starting point of Episode 48 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.7999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 2, 0.5, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.7999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 1), 2, 0.5, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.7999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 2), 4, 0.5, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 3, 0.7, 3)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 2, 0.5, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 3), 2, 0.5, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 2 to next state (4, 1): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 4), 4, 0.5, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 1) with action 1 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 4), 4, 0.5, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 4 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 4), 2, 0.5, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 5), 2, 0.5, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 6), 4, 0.5, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 0.5, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 4 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 5), 1, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 2 to next state (4, 1): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 1) with action 0 to next state (4, 1): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 1) with action 0 to next state (4, 1): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 1) with action 0 to next state (4, 1): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 1) with action 1 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 4 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 2 to next state (4, 1): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 1) with action 2 to next state (4, 2): pull reward: 0\n",
      "----- starting point of Episode 48 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 48 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 0.9, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fc8e80>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fcb700>, <__main__.Case object at 0x7f34c3fc8310>, <__main__.Case object at 0x7f34c3fcb9a0>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fcb130>, <__main__.Case object at 0x7f34c3fc9ab0>, <__main__.Case object at 0x7f34c3fc9780>, <__main__.Case object at 0x7f34c3fc9ed0>, <__main__.Case object at 0x7f34c3fcb790>, <__main__.Case object at 0x7f34c3fc8250>, <__main__.Case object at 0x7f34c3fc94b0>, <__main__.Case object at 0x7f34c3fcb640>, <__main__.Case object at 0x7f34c3fc9270>, <__main__.Case object at 0x7f34c3fca140>, <__main__.Case object at 0x7f34c3fc91b0>, <__main__.Case object at 0x7f34c3fcadd0>, <__main__.Case object at 0x7f34c3fca7d0>, <__main__.Case object at 0x7f34c3fcb280>, <__main__.Case object at 0x7f34c3fcbcd0>, <__main__.Case object at 0x7f34c0f16380>, <__main__.Case object at 0x7f34c0f177c0>, <__main__.Case object at 0x7f34c0f143a0>, <__main__.Case object at 0x7f34c0f14f10>, <__main__.Case object at 0x7f34c0f14190>, <__main__.Case object at 0x7f34c0f156c0>, <__main__.Case object at 0x7f34c0f16da0>, <__main__.Case object at 0x7f34c0f17c40>, <__main__.Case object at 0x7f34c0f16d70>, <__main__.Case object at 0x7f34c0f14670>, <__main__.Case object at 0x7f34c0f153c0>, <__main__.Case object at 0x7f34c0f17a60>, <__main__.Case object at 0x7f34c0f17430>, <__main__.Case object at 0x7f34c0f146a0>, <__main__.Case object at 0x7f34c0f16aa0>, <__main__.Case object at 0x7f34c0f16a70>, <__main__.Case object at 0x7f34c0f17a00>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3fa8820>, <__main__.Case object at 0x7f34c3faafe0>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fcb730>, <__main__.Case object at 0x7f34c3fab010>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fc92a0>, <__main__.Case object at 0x7f34c3fca9e0>, <__main__.Case object at 0x7f34c3fc8f70>, <__main__.Case object at 0x7f34c3fc83d0>, <__main__.Case object at 0x7f34c3fc8eb0>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fcafb0>, <__main__.Case object at 0x7f34c3fc9090>, <__main__.Case object at 0x7f34c3fa8a00>, <__main__.Case object at 0x7f34c3fc9330>, <__main__.Case object at 0x7f34c3fc91e0>, <__main__.Case object at 0x7f34c3fc84f0>, <__main__.Case object at 0x7f34c3fc8730>, <__main__.Case object at 0x7f34c0f178e0>, <__main__.Case object at 0x7f34c0f16230>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fc8970>, <__main__.Case object at 0x7f34c3fca4d0>, <__main__.Case object at 0x7f34c0f16a10>, <__main__.Case object at 0x7f34c0f15a80>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c0f16b00>, <__main__.Case object at 0x7f34c0f14a90>, <__main__.Case object at 0x7f34c0f17bb0>, <__main__.Case object at 0x7f34c3fcac80>, <__main__.Case object at 0x7f34c0f15ba0>]\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 1.0, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 6), solution: 4, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 5), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 4), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (2, 4), solution: 4, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 4), solution: 4, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 3), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 4, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 3, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.5, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 0.5, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 0.5, time steps: 6\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 3, 0.7)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.7999999999999998)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1.0, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.6, time steps: 3\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (3, 2), solution: 4, tv: 0.5, time steps: 8\n",
      "cases content after RETAIN, problem: (3, 1), solution: 2, tv: 0.5, time steps: 7\n",
      "cases content after RETAIN, problem: (3, 0), solution: 2, tv: 0.5, time steps: 6\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.7999999999999998, time steps: 17\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fc80d0>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fc8100>, <__main__.Case object at 0x7f34c3fcad10>, <__main__.Case object at 0x7f34c3fc8a00>, <__main__.Case object at 0x7f34c3fcbe50>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fc9ff0>, <__main__.Case object at 0x7f34c3fc9240>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fc8850>, <__main__.Case object at 0x7f34c3fc8340>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fc8910>, <__main__.Case object at 0x7f34c3fc8fa0>, <__main__.Case object at 0x7f34c3fc9120>, <__main__.Case object at 0x7f34c3fca620>, <__main__.Case object at 0x7f34c3fca200>, <__main__.Case object at 0x7f34c3fcbfd0>, <__main__.Case object at 0x7f34c0f15a50>, <__main__.Case object at 0x7f34c0f17880>, <__main__.Case object at 0x7f34c0f14b50>, <__main__.Case object at 0x7f34c0f144c0>, <__main__.Case object at 0x7f34c0f15840>, <__main__.Case object at 0x7f34c0f159c0>, <__main__.Case object at 0x7f34c0f161d0>, <__main__.Case object at 0x7f34c0f17eb0>, <__main__.Case object at 0x7f34c0f15780>, <__main__.Case object at 0x7f34c0f15450>, <__main__.Case object at 0x7f34c0f16c50>, <__main__.Case object at 0x7f34c0f169e0>, <__main__.Case object at 0x7f34c0f17f70>, <__main__.Case object at 0x7f34c0f14bb0>, <__main__.Case object at 0x7f34c0f16f50>, <__main__.Case object at 0x7f34c0f157b0>, <__main__.Case object at 0x7f34c0f149a0>, <__main__.Case object at 0x7f34c0f17460>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fcbc40>, <__main__.Case object at 0x7f34c3fcbeb0>, <__main__.Case object at 0x7f34c3fca980>, <__main__.Case object at 0x7f34c3fcb2e0>, <__main__.Case object at 0x7f34c3fcbf10>, <__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c3fc8160>, <__main__.Case object at 0x7f34c3fc9a50>, <__main__.Case object at 0x7f34c3fc99c0>, <__main__.Case object at 0x7f34c3fcb190>, <__main__.Case object at 0x7f34c3fc98d0>, <__main__.Case object at 0x7f34c3fc86a0>, <__main__.Case object at 0x7f34c3fcbf70>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fc9c00>, <__main__.Case object at 0x7f34c3fcba60>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fcba90>, <__main__.Case object at 0x7f34c0f16560>, <__main__.Case object at 0x7f34c0f14af0>, <__main__.Case object at 0x7f34c0f15750>, <__main__.Case object at 0x7f34c0f15690>, <__main__.Case object at 0x7f34c0f14400>, <__main__.Case object at 0x7f34c0f17820>, <__main__.Case object at 0x7f34c0f150c0>, <__main__.Case object at 0x7f34c0f16b90>, <__main__.Case object at 0x7f34c0f16200>, <__main__.Case object at 0x7f34c0f14ac0>, <__main__.Case object at 0x7f34c0f15de0>, <__main__.Case object at 0x7f34c0f15720>, <__main__.Case object at 0x7f34c0f17e50>, <__main__.Case object at 0x7f34c0f16590>, <__main__.Case object at 0x7f34c0f140d0>, <__main__.Case object at 0x7f34c0f15120>, <__main__.Case object at 0x7f34c0f175e0>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.5000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 3, tv: 0.8999999999999998, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 3, tv: 0.7999999999999999, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (3, 6), solution: 4, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 5), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 4), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (2, 4), solution: 4, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (1, 4), solution: 4, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (1, 3), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 2), solution: 4, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.6, time steps: 17\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 1) is empty. Temporary case base stored to the case base: ((4, 1), 2, 0.5)\n",
      "Episode succeeded, case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 2, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 0.9)\n",
      "Integrated case process. comm case (4, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 0.5)\n",
      "Integrated case process. comm case (3, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 6), 4, 0.5)\n",
      "Integrated case process. comm case (3, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 5), 2, 0.5)\n",
      "Integrated case process. comm case (3, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 4), 2, 0.5)\n",
      "Integrated case process. comm case (2, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 4), 4, 0.5)\n",
      "Integrated case process. comm case (1, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 4), 4, 0.5)\n",
      "Integrated case process. comm case (1, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 3), 2, 0.5)\n",
      "Integrated case process. comm case (1, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 2), 2, 0.5)\n",
      "Integrated case process. comm case (0, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 2), 4, 0.5)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 2, 0.5)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 0.5)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.7999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.7999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.8999999999999998, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.7999999999999999, time steps: 3\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 1), solution: 2, tv: 0.5, time steps: 27\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.5, time steps: 26\n",
      "Episode: 48, Total Steps: 38, Total Rewards: [88, 63], Status Episode: True\n",
      "------------------------------------------End of episode 48 loop--------------------\n",
      "----- starting point of Episode 49 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 2, 0.6, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 0) with action 2 to next state (9, 1): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 1), 2, 0.6, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 1) with action 3 to next state (8, 1): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 2), 4, 0.6, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 1) with action 4 to next state (9, 1): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 0 to next state (1, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 1) with action 4 to next state (9, 1): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 2, 0.6, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 1) with action 4 to next state (9, 1): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 3), 2, 0.6, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 1) with action 0 to next state (9, 1): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 4), 4, 0.6, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 1) with action 4 to next state (9, 1): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 4), 4, 0.6, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 1) with action 0 to next state (9, 1): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 4), 2, 0.6, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 1) with action 2 to next state (9, 2): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 5), 2, 0.6, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 2) with action 4 to next state (9, 2): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 6), 4, 0.6, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 2) with action 0 to next state (9, 2): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 0.6, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 2) with action 0 to next state (9, 2): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 5), 1, 1, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 2) with action 0 to next state (9, 2): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 2) with action 4 to next state (9, 2): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 2) with action 4 to next state (9, 2): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 2) with action 3 to next state (8, 2): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 2) with action 2 to next state (8, 3): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 3) with action 0 to next state (8, 3): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 3) with action 0 to next state (8, 3): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 3) with action 0 to next state (8, 3): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 3) with action 2 to next state (8, 4): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 4) with action 2 to next state (8, 5): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 5) with action 0 to next state (8, 5): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (8, 5) with action 3 to next state (7, 5): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 5) with action 0 to next state (7, 5): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 5) with action 2 to next state (7, 6): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (7, 6) with action 3 to next state (6, 6): pull reward: 0\n",
      "----- starting point of Episode 49 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 49 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1.0, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fab010>, <__main__.Case object at 0x7f34c3faafe0>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fcbf40>, <__main__.Case object at 0x7f34c3fc9090>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fc8730>, <__main__.Case object at 0x7f34c3fcbc40>, <__main__.Case object at 0x7f34c3fc8430>, <__main__.Case object at 0x7f34c3fcb3d0>, <__main__.Case object at 0x7f34c3fcaad0>, <__main__.Case object at 0x7f34c3fcb4c0>, <__main__.Case object at 0x7f34c3fcb9d0>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fc9b10>, <__main__.Case object at 0x7f34c3fcacb0>, <__main__.Case object at 0x7f34c3fc9270>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3fcadd0>, <__main__.Case object at 0x7f34c3fcbcd0>, <__main__.Case object at 0x7f34c3fa8a00>, <__main__.Case object at 0x7f34c3fca110>, <__main__.Case object at 0x7f34c3fc8340>, <__main__.Case object at 0x7f34c3fc9c90>, <__main__.Case object at 0x7f34c3fc8d00>, <__main__.Case object at 0x7f34c3fc9e70>, <__main__.Case object at 0x7f34c3fc9540>, <__main__.Case object at 0x7f34c3fcaa10>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fc9a50>, <__main__.Case object at 0x7f34c0f14490>]\n",
      "agent0 comm temp case base: []\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 6), solution: 4, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 5), solution: 2, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 4), solution: 2, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (2, 4), solution: 4, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 4), solution: 4, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 3), solution: 2, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 2, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 4, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 2, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.5000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.5000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 3, tv: 0.5, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (3, 2), solution: 4, tv: 0.4, time steps: 8\n",
      "case content after REVISE for agent 0, problem: (3, 1), solution: 2, tv: 0.4, time steps: 7\n",
      "case content after REVISE for agent 0, problem: (3, 0), solution: 2, tv: 0.4, time steps: 6\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 3, tv: 0.6999999999999998, time steps: 17\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.5000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.5, time steps: 3\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.6999999999999998, time steps: 17\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fc8d60>, <__main__.Case object at 0x7f34c3fcbb20>, <__main__.Case object at 0x7f34c3fc96f0>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fc9330>, <__main__.Case object at 0x7f34c3fc8970>, <__main__.Case object at 0x7f34c3fcbeb0>, <__main__.Case object at 0x7f34c3fcbf10>, <__main__.Case object at 0x7f34c3fcb2e0>, <__main__.Case object at 0x7f34c3fcabc0>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fca230>, <__main__.Case object at 0x7f34c3fc8310>, <__main__.Case object at 0x7f34c3fc9cf0>, <__main__.Case object at 0x7f34c3fca1d0>, <__main__.Case object at 0x7f34c3fc93f0>, <__main__.Case object at 0x7f34c3fcbb80>, <__main__.Case object at 0x7f34c3fca7d0>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fcbd30>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fc8f10>, <__main__.Case object at 0x7f34c3fc9cc0>, <__main__.Case object at 0x7f34c3fc9900>, <__main__.Case object at 0x7f34c0f175e0>, <__main__.Case object at 0x7f34c0f14b80>, <__main__.Case object at 0x7f34c0f15ab0>, <__main__.Case object at 0x7f34c0f15e70>, <__main__.Case object at 0x7f34c0f17bb0>, <__main__.Case object at 0x7f34c0f14af0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fcba00>, <__main__.Case object at 0x7f34c3fc94e0>, <__main__.Case object at 0x7f34c3fc91e0>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c3fca980>, <__main__.Case object at 0x7f34c3fc9b40>, <__main__.Case object at 0x7f34c3fcab00>, <__main__.Case object at 0x7f34c3fca170>, <__main__.Case object at 0x7f34c3fca080>, <__main__.Case object at 0x7f34c3fc95d0>, <__main__.Case object at 0x7f34c3fc8b80>, <__main__.Case object at 0x7f34c3fc8610>, <__main__.Case object at 0x7f34c3fc9720>, <__main__.Case object at 0x7f34c3fc9810>, <__main__.Case object at 0x7f34c3fc91b0>, <__main__.Case object at 0x7f34c3fcb280>, <__main__.Case object at 0x7f34c3fc9db0>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fca050>, <__main__.Case object at 0x7f34c3fc88b0>, <__main__.Case object at 0x7f34c3fc9690>, <__main__.Case object at 0x7f34c3fc9450>, <__main__.Case object at 0x7f34c0f17ac0>, <__main__.Case object at 0x7f34c0f16c80>, <__main__.Case object at 0x7f34c0f16230>, <__main__.Case object at 0x7f34c0f16950>, <__main__.Case object at 0x7f34c0f14a90>, <__main__.Case object at 0x7f34c0f16560>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.40000000000000013, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 3, tv: 0.7999999999999998, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 3, tv: 0.7, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (3, 6), solution: 4, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 5), solution: 2, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 4), solution: 2, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (2, 4), solution: 4, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (1, 4), solution: 4, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (1, 3), solution: 2, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 2, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 2), solution: 4, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 2, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 1), solution: 2, tv: 0.4, time steps: 27\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 2, tv: 0.4, time steps: 26\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 6) is empty. Temporary case base stored to the case base: ((7, 6), 3, 0.5)\n",
      "Episode succeeded, case (7, 5) is empty. Temporary case base stored to the case base: ((7, 5), 2, 0.5)\n",
      "Episode succeeded, case (7, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 5) is empty. Temporary case base stored to the case base: ((8, 5), 3, 0.5)\n",
      "Episode succeeded, case (8, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 4) is empty. Temporary case base stored to the case base: ((8, 4), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) is empty. Temporary case base stored to the case base: ((8, 3), 2, 0.5)\n",
      "Episode succeeded, case (8, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 2) is empty. Temporary case base stored to the case base: ((8, 2), 2, 0.5)\n",
      "Episode succeeded, case (9, 2) is empty. Temporary case base stored to the case base: ((9, 2), 3, 0.5)\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) is empty. Temporary case base stored to the case base: ((9, 1), 2, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 1) is empty. Temporary case base stored to the case base: ((8, 1), 4, 0.5)\n",
      "Episode succeeded, case (9, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1.0)\n",
      "Integrated case process. comm case (4, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 0.6)\n",
      "Integrated case process. comm case (3, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 6), 4, 0.6)\n",
      "Integrated case process. comm case (3, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 5), 2, 0.6)\n",
      "Integrated case process. comm case (3, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 4), 2, 0.6)\n",
      "Integrated case process. comm case (2, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 4), 4, 0.6)\n",
      "Integrated case process. comm case (1, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 4), 4, 0.6)\n",
      "Integrated case process. comm case (1, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 3), 2, 0.6)\n",
      "Integrated case process. comm case (1, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 2), 2, 0.6)\n",
      "Integrated case process. comm case (0, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 2), 4, 0.6)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 2, 0.6)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 0.6)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.7999999999999998, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.7, time steps: 3\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (7, 6), solution: 3, tv: 0.5, time steps: 26\n",
      "cases content after RETAIN, problem: (7, 5), solution: 2, tv: 0.5, time steps: 25\n",
      "cases content after RETAIN, problem: (8, 5), solution: 3, tv: 0.5, time steps: 23\n",
      "cases content after RETAIN, problem: (8, 4), solution: 2, tv: 0.5, time steps: 21\n",
      "cases content after RETAIN, problem: (8, 3), solution: 2, tv: 0.5, time steps: 20\n",
      "cases content after RETAIN, problem: (8, 2), solution: 2, tv: 0.5, time steps: 16\n",
      "cases content after RETAIN, problem: (9, 2), solution: 3, tv: 0.5, time steps: 15\n",
      "cases content after RETAIN, problem: (9, 1), solution: 2, tv: 0.5, time steps: 8\n",
      "cases content after RETAIN, problem: (8, 1), solution: 4, tv: 0.5, time steps: 2\n",
      "Episode: 49, Total Steps: 31, Total Rewards: [88, 70], Status Episode: True\n",
      "------------------------------------------End of episode 49 loop--------------------\n",
      "----- starting point of Episode 50 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.7999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 2, 0.7, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.7999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 1), 2, 0.7, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.7999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 2), 4, 0.7, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 3, 0.7, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 2, 0.7, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 3), 2, 0.7, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 4), 4, 0.7, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 4 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.7999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 4), 4, 0.7, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 4), 2, 0.7, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 5), 2, 0.7, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 6), 4, 0.7, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 0.7, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 5), 1, 1, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 4 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 4 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 4 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 1 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 2 to next state (4, 1): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 20 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 1) with action 4 to next state (5, 1): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 21 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 1) with action 0 to next state (5, 1): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 22 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 1) with action 0 to next state (5, 1): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 23 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 1) with action 3 to next state (4, 1): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 24 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 1) with action 0 to next state (4, 1): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 25 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 1) with action 0 to next state (4, 1): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 26 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 1) with action 4 to next state (5, 1): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 27 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 1) with action 0 to next state (5, 1): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 28 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 1) with action 0 to next state (5, 1): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 29 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 1) with action 2 to next state (5, 2): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 30 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 31 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 32 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 33 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 4) with action 4 to next state (4, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 34 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 35 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 36 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 6) with action 1 to next state (5, 5): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 37 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (5, 5) with action 4 to next state (6, 5): pull reward: 0\n",
      "----- starting point of Episode 50 in steps 38 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 39 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 40 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 41 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 50 in steps 42 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fcaef0>, <__main__.Case object at 0x7f34c3fc99c0>, <__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c3fc94b0>, <__main__.Case object at 0x7f34c3fc8a00>, <__main__.Case object at 0x7f34c3fc9450>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fc8430>, <__main__.Case object at 0x7f34c3fcb970>, <__main__.Case object at 0x7f34c3fcb340>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fcb2b0>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fcbcd0>, <__main__.Case object at 0x7f34c3fcbb20>, <__main__.Case object at 0x7f34c3fcb4f0>, <__main__.Case object at 0x7f34c3fcbf70>, <__main__.Case object at 0x7f34c3fc9270>, <__main__.Case object at 0x7f34c3fcb9a0>, <__main__.Case object at 0x7f34c3fa8a00>, <__main__.Case object at 0x7f34c3fcb5e0>, <__main__.Case object at 0x7f34c3fcbeb0>, <__main__.Case object at 0x7f34c0f15b40>, <__main__.Case object at 0x7f34c3fc8fa0>, <__main__.Case object at 0x7f34c3fa8820>, <__main__.Case object at 0x7f34c0f14490>, <__main__.Case object at 0x7f34c0f15d50>, <__main__.Case object at 0x7f34c3fc8bb0>, <__main__.Case object at 0x7f34c0f164a0>, <__main__.Case object at 0x7f34c3fcbe20>, <__main__.Case object at 0x7f34c0f160b0>, <__main__.Case object at 0x7f34c3fca110>, <__main__.Case object at 0x7f34c0f16bf0>, <__main__.Case object at 0x7f34c0f14400>, <__main__.Case object at 0x7f34c0f161d0>, <__main__.Case object at 0x7f34c3fcbf10>, <__main__.Case object at 0x7f34c0f16a70>, <__main__.Case object at 0x7f34c3fcb940>, <__main__.Case object at 0x7f34c0f16dd0>, <__main__.Case object at 0x7f34c3fc9bd0>, <__main__.Case object at 0x7f34c0f17430>, <__main__.Case object at 0x7f34c0f155a0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3fab010>, <__main__.Case object at 0x7f34c3faafe0>, <__main__.Case object at 0x7f34c3fc8a30>, <__main__.Case object at 0x7f34c3fcba90>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fca620>]\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.7999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 6), solution: 4, tv: 0.7999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 5), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 4), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (2, 4), solution: 4, tv: 0.7999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 4), solution: 4, tv: 0.7999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 3), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 4, tv: 0.7999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.40000000000000013, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.40000000000000013, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.40000000000000013, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 3, tv: 0.4, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.4999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.4999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.4999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 3, tv: 0.5999999999999999, time steps: 17\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 3, 0.7)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.7999999999999998)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.7999999999999998)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.4999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.4999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.4999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.5999999999999999, time steps: 17\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fc92a0>, <__main__.Case object at 0x7f34c3fcb910>, <__main__.Case object at 0x7f34c3fc9c00>, <__main__.Case object at 0x7f34c3fc9ed0>, <__main__.Case object at 0x7f34c3fc83d0>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fc8f70>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fcaa70>, <__main__.Case object at 0x7f34c3fcb9d0>, <__main__.Case object at 0x7f34c3fcacb0>, <__main__.Case object at 0x7f34c3fcadd0>, <__main__.Case object at 0x7f34c3fc8340>, <__main__.Case object at 0x7f34c3fc9660>, <__main__.Case object at 0x7f34c3fc8d60>, <__main__.Case object at 0x7f34c3fc9330>, <__main__.Case object at 0x7f34c3fcabc0>, <__main__.Case object at 0x7f34c3fc8e80>, <__main__.Case object at 0x7f34c3fcb790>, <__main__.Case object at 0x7f34c3fcbe50>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c0f16770>, <__main__.Case object at 0x7f34c0f17130>, <__main__.Case object at 0x7f34c0f14760>, <__main__.Case object at 0x7f34c0f15a80>, <__main__.Case object at 0x7f34c0f16a10>, <__main__.Case object at 0x7f34c0f16da0>, <__main__.Case object at 0x7f34c0f156c0>, <__main__.Case object at 0x7f34c0f16020>, <__main__.Case object at 0x7f34c0f14550>, <__main__.Case object at 0x7f34c0f14fd0>, <__main__.Case object at 0x7f34c0f17220>, <__main__.Case object at 0x7f34c0f150c0>, <__main__.Case object at 0x7f34c3fcb070>, <__main__.Case object at 0x7f34c0f17730>, <__main__.Case object at 0x7f34c0f153c0>, <__main__.Case object at 0x7f34c0f170d0>, <__main__.Case object at 0x7f34c0f17d60>, <__main__.Case object at 0x7f34c0f150f0>, <__main__.Case object at 0x7f34c0f15e10>, <__main__.Case object at 0x7f34c0f169e0>, <__main__.Case object at 0x7f34c0f15180>, <__main__.Case object at 0x7f34c0f16440>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fca770>, <__main__.Case object at 0x7f34c3fca4d0>, <__main__.Case object at 0x7f34c3fc86a0>, <__main__.Case object at 0x7f34c3fcb130>, <__main__.Case object at 0x7f34c3fcb280>, <__main__.Case object at 0x7f34c3fc9f00>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fcb6d0>, <__main__.Case object at 0x7f34c3fcaad0>, <__main__.Case object at 0x7f34c3fcb4c0>, <__main__.Case object at 0x7f34c3fc9b10>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3fc8910>, <__main__.Case object at 0x7f34c3fc9120>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fc9540>, <__main__.Case object at 0x7f34c3fcba60>, <__main__.Case object at 0x7f34c3fc9ab0>, <__main__.Case object at 0x7f34c3fc8100>, <__main__.Case object at 0x7f34c3fcb700>, <__main__.Case object at 0x7f34c0f14940>, <__main__.Case object at 0x7f34c0f14040>, <__main__.Case object at 0x7f34c0f17100>, <__main__.Case object at 0x7f34c0f16b30>, <__main__.Case object at 0x7f34c0f14a60>, <__main__.Case object at 0x7f34c0f15f90>, <__main__.Case object at 0x7f34c0f152d0>, <__main__.Case object at 0x7f34c0f14670>, <__main__.Case object at 0x7f34c0f14160>, <__main__.Case object at 0x7f34c0f17850>, <__main__.Case object at 0x7f34c0f17970>, <__main__.Case object at 0x7f34c0f14ac0>, <__main__.Case object at 0x7f34c0f15690>, <__main__.Case object at 0x7f34c0f15c30>, <__main__.Case object at 0x7f34c0f17a60>, <__main__.Case object at 0x7f34c0f147f0>, <__main__.Case object at 0x7f34c0f160e0>, <__main__.Case object at 0x7f34c0f16080>, <__main__.Case object at 0x7f34c0f174c0>, <__main__.Case object at 0x7f34c0f17c10>, <__main__.Case object at 0x7f34c0f16e00>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1.0, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 3, tv: 0.8999999999999998, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 3, tv: 0.7999999999999999, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (3, 6), solution: 4, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 5), solution: 2, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 4), solution: 2, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (2, 4), solution: 4, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (1, 4), solution: 4, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (1, 3), solution: 2, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 2, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 2), solution: 4, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 2, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (7, 6), solution: 3, tv: 0.4, time steps: 26\n",
      "case content after REVISE for agent 1, problem: (7, 5), solution: 2, tv: 0.4, time steps: 25\n",
      "case content after REVISE for agent 1, problem: (8, 5), solution: 3, tv: 0.4, time steps: 23\n",
      "case content after REVISE for agent 1, problem: (8, 4), solution: 2, tv: 0.4, time steps: 21\n",
      "case content after REVISE for agent 1, problem: (8, 3), solution: 2, tv: 0.4, time steps: 20\n",
      "case content after REVISE for agent 1, problem: (8, 2), solution: 2, tv: 0.4, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (9, 2), solution: 3, tv: 0.4, time steps: 15\n",
      "case content after REVISE for agent 1, problem: (9, 1), solution: 2, tv: 0.4, time steps: 8\n",
      "case content after REVISE for agent 1, problem: (8, 1), solution: 4, tv: 0.4, time steps: 2\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 5) is empty. Temporary case base stored to the case base: ((5, 5), 4, 0.5)\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) is empty. Temporary case base stored to the case base: ((5, 1), 2, 0.5)\n",
      "Episode succeeded, case (5, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 1) is empty. Temporary case base stored to the case base: ((4, 1), 4, 0.5)\n",
      "Episode succeeded, case (4, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 2, 0.5)\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 0.7)\n",
      "Integrated case process. comm case (3, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 6), 4, 0.7)\n",
      "Integrated case process. comm case (3, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 5), 2, 0.7)\n",
      "Integrated case process. comm case (3, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 4), 2, 0.7)\n",
      "Integrated case process. comm case (2, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 4), 4, 0.7)\n",
      "Integrated case process. comm case (1, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 4), 4, 0.7)\n",
      "Integrated case process. comm case (1, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 3), 2, 0.7)\n",
      "Integrated case process. comm case (1, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 2), 2, 0.7)\n",
      "Integrated case process. comm case (0, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 2), 4, 0.7)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 2, 0.7)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 0.7)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.7999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.8999999999999998, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.7999999999999999, time steps: 3\n",
      "cases content after RETAIN, problem: (5, 5), solution: 4, tv: 0.5, time steps: 37\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (4, 1), solution: 4, tv: 0.5, time steps: 26\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "Episode: 50, Total Steps: 43, Total Rewards: [89, 58], Status Episode: True\n",
      "------------------------------------------End of episode 50 loop--------------------\n",
      "----- starting point of Episode 51 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.8999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 2, 0.7999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.8999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 1), 2, 0.7999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.8999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 2), 4, 0.7999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 3, 0.7999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 2, 0.7999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.8999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 3), 2, 0.7999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 4), 4, 0.7999999999999999, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (4, 0) with action 0 to next state (4, 0): pull reward: 0\n",
      "----- starting point of Episode 51 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 0), 2, 0.5, 19)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 4), 4, 0.7999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 1), 4, 0.5, 26)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 4), 2, 0.7999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 1), 2, 0.5, 29)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 5), 2, 0.7999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 2), 4, 0.7999999999999999, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 6), 4, 0.7999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1.0, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 0.7999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1.0, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 5), 1, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1.0, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1.0, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1.0, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1.0, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 4) with action 3 to next state (4, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1.0, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 51 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((6, 3), 2, 1.0, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fc8a30>, <__main__.Case object at 0x7f34c3fcb610>, <__main__.Case object at 0x7f34c3fca080>, <__main__.Case object at 0x7f34c3fcbf40>, <__main__.Case object at 0x7f34c3fcb8e0>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fc9ab0>, <__main__.Case object at 0x7f34c3fcaef0>, <__main__.Case object at 0x7f34c3fc94b0>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fc8d00>, <__main__.Case object at 0x7f34c3fca230>, <__main__.Case object at 0x7f34c3fcb5e0>, <__main__.Case object at 0x7f34c3fcb910>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fcb9d0>, <__main__.Case object at 0x7f34c3fcad10>, <__main__.Case object at 0x7f34c3fc8e80>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3fa8820>, <__main__.Case object at 0x7f34c3faafe0>, <__main__.Case object at 0x7f34c3fca980>, <__main__.Case object at 0x7f34c3fc88b0>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3fc96c0>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fca050>, <__main__.Case object at 0x7f34c3fcb3d0>, <__main__.Case object at 0x7f34c3fc9240>, <__main__.Case object at 0x7f34c3fca290>, <__main__.Case object at 0x7f34c3fc8bb0>, <__main__.Case object at 0x7f34c3fc92a0>, <__main__.Case object at 0x7f34c3fc83d0>, <__main__.Case object at 0x7f34c3fcaa70>, <__main__.Case object at 0x7f34c3fc9660>, <__main__.Case object at 0x7f34c3fcabc0>]\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.8999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 6), solution: 4, tv: 0.8999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 5), solution: 2, tv: 0.8999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 4), solution: 2, tv: 0.8999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (2, 4), solution: 4, tv: 0.8999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 4), solution: 4, tv: 0.8999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 3), solution: 2, tv: 0.8999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 2, tv: 0.8999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 4, tv: 0.8999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 2, tv: 0.8999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.8999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.3999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.3999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.3999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 3, tv: 0.4999999999999999, time steps: 17\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 3) is empty. Temporary case base stored to the case base: ((6, 3), 2, 1.0)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1.0)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1.0)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1.0)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1.0)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1.0)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1.0)\n",
      "Integrated case process. comm case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 2, 1.0)\n",
      "Integrated case process. comm case (5, 2) is empty. Temporary case base stored to the case base: ((5, 2), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 1) is empty. Temporary case base stored to the case base: ((5, 1), 2, 0.5)\n",
      "Integrated case process. comm case (4, 1) is empty. Temporary case base stored to the case base: ((4, 1), 4, 0.5)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.8999999999999998)\n",
      "Integrated case process. comm case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 3, 0.7999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 0.8999999999999998)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 0.8999999999999998)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 0.8999999999999998)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.8999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.8999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.8999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.8999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.8999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.8999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.8999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.8999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.8999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.8999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.8999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.4999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1.0, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.7999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.5, time steps: 29\n",
      "cases content after RETAIN, problem: (4, 1), solution: 4, tv: 0.5, time steps: 26\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.5, time steps: 19\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.7999999999999999, time steps: 3\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fcab00>, <__main__.Case object at 0x7f34c3fcba00>, <__main__.Case object at 0x7f34c3fc9db0>, <__main__.Case object at 0x7f34c3fc98d0>, <__main__.Case object at 0x7f34c3fc9120>, <__main__.Case object at 0x7f34c3fc9540>, <__main__.Case object at 0x7f34c3fcb700>, <__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fc9990>, <__main__.Case object at 0x7f34c3fcafb0>, <__main__.Case object at 0x7f34c3fcbd30>, <__main__.Case object at 0x7f34c3fcb940>, <__main__.Case object at 0x7f34c3fc9ed0>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fcadd0>, <__main__.Case object at 0x7f34c3fc9330>, <__main__.Case object at 0x7f34c3fcb8b0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fc91e0>, <__main__.Case object at 0x7f34c3fca6e0>, <__main__.Case object at 0x7f34c3fc9720>, <__main__.Case object at 0x7f34c3fc8730>, <__main__.Case object at 0x7f34c3fca5f0>, <__main__.Case object at 0x7f34c3fc81c0>, <__main__.Case object at 0x7f34c3fc8100>, <__main__.Case object at 0x7f34c3fc99c0>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c3fc9780>, <__main__.Case object at 0x7f34c3fc9a50>, <__main__.Case object at 0x7f34c3fca1d0>, <__main__.Case object at 0x7f34c3fcbf10>, <__main__.Case object at 0x7f34c3fc9c00>, <__main__.Case object at 0x7f34c3fc8f70>, <__main__.Case object at 0x7f34c3fca9e0>, <__main__.Case object at 0x7f34c3fcb790>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 0.9999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 0.9999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 0.9999999999999998, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.8999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 3, tv: 0.9999999999999998, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 3, tv: 0.8999999999999999, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (5, 5), solution: 4, tv: 0.4, time steps: 37\n",
      "case content after REVISE for agent 1, problem: (5, 1), solution: 2, tv: 0.6, time steps: 29\n",
      "case content after REVISE for agent 1, problem: (4, 1), solution: 4, tv: 0.6, time steps: 26\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 2, tv: 0.6, time steps: 19\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.9, time steps: 16\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 1), 2, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 1), 4, 0.5, 18)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 0), 2, 0.5, 18)\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (3, 6) is empty. Temporary case base stored to the case base: ((3, 6), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (3, 5) is empty. Temporary case base stored to the case base: ((3, 5), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (3, 4) is empty. Temporary case base stored to the case base: ((3, 4), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (2, 4) is empty. Temporary case base stored to the case base: ((2, 4), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (1, 4) is empty. Temporary case base stored to the case base: ((1, 4), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (1, 3) is empty. Temporary case base stored to the case base: ((1, 3), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (0, 2) is empty. Temporary case base stored to the case base: ((0, 2), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 2, 0.7999999999999999)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.9999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.9999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.8999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 2), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.9999999999999998, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.8999999999999999, time steps: 3\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 1), solution: 4, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.5, time steps: 18\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.9, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "Episode: 51, Total Steps: 18, Total Rewards: [89, 83], Status Episode: True\n",
      "------------------------------------------End of episode 51 loop--------------------\n",
      "----- starting point of Episode 52 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 0.9999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 2, 0.8999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 0.9999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 1), 2, 0.8999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 0.9999999999999998, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 2), 4, 0.8999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 3, 0.8999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 2, 0.8999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 0.9999999999999998, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 3), 2, 0.8999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 0), 2, 0.5, 18)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 4), 4, 0.8999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 1), 4, 0.5, 18)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 4), 4, 0.8999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 1), 2, 0.5, 18)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 4) with action 0 to next state (3, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 2), 4, 0.8999999999999999, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 4), 2, 0.8999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 5) with action 4 to next state (4, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 5), 1, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 52 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fcb790>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3fc83d0>, <__main__.Case object at 0x7f34c3fca1d0>, <__main__.Case object at 0x7f34c3fc8f70>, <__main__.Case object at 0x7f34c3fcbf40>, <__main__.Case object at 0x7f34c3fcaef0>, <__main__.Case object at 0x7f34c3fca230>, <__main__.Case object at 0x7f34c3fc8fa0>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fc9b10>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fcbf70>, <__main__.Case object at 0x7f34c3fc8340>, <__main__.Case object at 0x7f34c3fcbb80>, <__main__.Case object at 0x7f34c3fc9cc0>, <__main__.Case object at 0x7f34c3fcb370>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3fab010>, <__main__.Case object at 0x7f34c3fa8a00>, <__main__.Case object at 0x7f34c3fc92a0>, <__main__.Case object at 0x7f34c3fc9a50>, <__main__.Case object at 0x7f34c3fca9e0>, <__main__.Case object at 0x7f34c3fca080>, <__main__.Case object at 0x7f34c3fc9ab0>, <__main__.Case object at 0x7f34c3fc8d00>, <__main__.Case object at 0x7f34c3fc9090>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fcbeb0>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fc96f0>, <__main__.Case object at 0x7f34c3fcbee0>, <__main__.Case object at 0x7f34c3fc9390>]\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.7999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 6), solution: 4, tv: 0.7999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 5), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 4), solution: 2, tv: 0.9999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (2, 4), solution: 4, tv: 0.9999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 4), solution: 4, tv: 0.9999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 3), solution: 2, tv: 0.9999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 2, tv: 0.9999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 4, tv: 0.9999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 2, tv: 0.9999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 0.9999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 3, tv: 0.3999999999999999, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 2, tv: 0.4, time steps: 29\n",
      "case content after REVISE for agent 0, problem: (4, 1), solution: 4, tv: 0.4, time steps: 26\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 2, tv: 0.4, time steps: 19\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 3, tv: 0.7, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (5, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 1), 2, 0.5)\n",
      "Integrated case process. comm case (4, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 1), 4, 0.5)\n",
      "Integrated case process. comm case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 0.9999999999999998)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 3, 0.8999999999999999)\n",
      "Integrated case process. comm case (7, 0) is empty. Temporary case base stored to the case base: ((7, 0), 3, 0.9999999999999998)\n",
      "Integrated case process. comm case (8, 0) is empty. Temporary case base stored to the case base: ((8, 0), 3, 0.9999999999999998)\n",
      "Integrated case process. comm case (9, 0) is empty. Temporary case base stored to the case base: ((9, 0), 3, 0.9999999999999998)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.7999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.9999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.9999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.9999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.9999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.9999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.9999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.9999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.9999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.7, time steps: 3\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.9999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.9999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.9999999999999998, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fc87c0>, <__main__.Case object at 0x7f34c3fc8bb0>, <__main__.Case object at 0x7f34c3fc9660>, <__main__.Case object at 0x7f34c3fc9c00>, <__main__.Case object at 0x7f34c3fca980>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fcb910>, <__main__.Case object at 0x7f34c3fc80d0>, <__main__.Case object at 0x7f34c3fc8ee0>, <__main__.Case object at 0x7f34c3fc8310>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fc9450>, <__main__.Case object at 0x7f34c3fcb2e0>, <__main__.Case object at 0x7f34c3fc9ff0>, <__main__.Case object at 0x7f34c3fcad70>, <__main__.Case object at 0x7f34c3fcbe20>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fca290>, <__main__.Case object at 0x7f34c3fcaa70>, <__main__.Case object at 0x7f34c3fcbf10>, <__main__.Case object at 0x7f34c3fc8670>, <__main__.Case object at 0x7f34c3fcb8e0>, <__main__.Case object at 0x7f34c3fc94b0>, <__main__.Case object at 0x7f34c3fcad10>, <__main__.Case object at 0x7f34c3fcb730>, <__main__.Case object at 0x7f34c3fcb970>, <__main__.Case object at 0x7f34c3fc9ed0>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fc93f0>, <__main__.Case object at 0x7f34c3fca200>, <__main__.Case object at 0x7f34c3fc8070>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 0.9999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 2), solution: 4, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 3, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 3, tv: 0.9999999999999999, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (5, 1), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 1), solution: 4, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 2, tv: 0.6, time steps: 18\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.8, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 6), solution: 4, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (3, 5), solution: 2, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (3, 4), solution: 2, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (2, 4), solution: 4, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 4), solution: 4, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 3), solution: 2, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 2, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (0, 2), solution: 4, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 2, tv: 0.7, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.7, time steps: 14\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, updated case base with fewer steps: ((5, 1), 2, 0.5, 17)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 1), 4, 0.5, 17)\n",
      "Episode succeeded, updated case base with fewer steps: ((4, 0), 2, 0.5, 17)\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (3, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 4), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (2, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 4), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 4), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 3), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (1, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 2), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (0, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 2), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 0.8999999999999999)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.9999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.9999999999999999, time steps: 3\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 1), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.8, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.7, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.7, time steps: 14\n",
      "Episode: 52, Total Steps: 17, Total Rewards: [90, 84], Status Episode: True\n",
      "------------------------------------------End of episode 52 loop--------------------\n",
      "----- starting point of Episode 53 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 2, 0.9999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 1), 2, 0.9999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 2), 4, 0.9999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 3, 0.9999999999999999, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 2, 0.9999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 3), 2, 0.9999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 0), 2, 0.5, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 4), 4, 0.9999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 1), 4, 0.5, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 4), 4, 0.9999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 1), 2, 0.5, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 4), 2, 0.9999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 2), 4, 0.9999999999999999, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 5), 2, 0.7999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 6), 4, 0.7999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 0.7999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 5), 1, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 53 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fc8070>, <__main__.Case object at 0x7f34c3fc9ab0>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fc96f0>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fcb8e0>, <__main__.Case object at 0x7f34c3fcb970>, <__main__.Case object at 0x7f34c3fca200>, <__main__.Case object at 0x7f34c3fc83d0>, <__main__.Case object at 0x7f34c3fca230>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fc9cc0>, <__main__.Case object at 0x7f34c3fc8bb0>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fc8310>, <__main__.Case object at 0x7f34c3fc9ff0>, <__main__.Case object at 0x7f34c3fc9c90>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3faafe0>, <__main__.Case object at 0x7f34c3fca080>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fca290>, <__main__.Case object at 0x7f34c3fc8670>, <__main__.Case object at 0x7f34c3fcb730>, <__main__.Case object at 0x7f34c3fc93f0>, <__main__.Case object at 0x7f34c3fca1d0>, <__main__.Case object at 0x7f34c3fcaef0>, <__main__.Case object at 0x7f34c3fc9b10>, <__main__.Case object at 0x7f34c3fcbb80>, <__main__.Case object at 0x7f34c3fc9660>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fc8ee0>, <__main__.Case object at 0x7f34c3fcb2e0>, <__main__.Case object at 0x7f34c3fc91b0>]\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.8999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 6), solution: 4, tv: 0.8999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 5), solution: 2, tv: 0.8999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 4), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (2, 4), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 4), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 3), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 3, tv: 0.6, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 4) is empty. Temporary case base stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (5, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 1) is empty. Temporary case base stored to the case base: ((5, 1), 2, 0.5)\n",
      "Integrated case process. comm case (4, 1) is empty. Temporary case base stored to the case base: ((4, 1), 4, 0.5)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 2, 0.5)\n",
      "Integrated case process. comm case (5, 0) is empty. Temporary case base stored to the case base: ((5, 0), 3, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 3, 0.9999999999999999)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.8999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.8999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.8999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.6, time steps: 3\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.8999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 1), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 1, time steps: 17\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fcae90>, <__main__.Case object at 0x7f34c3fc9090>, <__main__.Case object at 0x7f34c3fcbeb0>, <__main__.Case object at 0x7f34c3fc9390>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fcad10>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3fcac80>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fc8340>, <__main__.Case object at 0x7f34c3fc87c0>, <__main__.Case object at 0x7f34c3fcb9d0>, <__main__.Case object at 0x7f34c3fc80d0>, <__main__.Case object at 0x7f34c3fc9450>, <__main__.Case object at 0x7f34c3fcb670>, <__main__.Case object at 0x7f34c3fcb940>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fcbe20>, <__main__.Case object at 0x7f34c3fc8d00>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fcbee0>, <__main__.Case object at 0x7f34c3fc8d60>, <__main__.Case object at 0x7f34c3fc94b0>, <__main__.Case object at 0x7f34c3fc9ed0>, <__main__.Case object at 0x7f34c3fcb790>, <__main__.Case object at 0x7f34c3fcb610>, <__main__.Case object at 0x7f34c3fc8fa0>, <__main__.Case object at 0x7f34c3fcbf70>, <__main__.Case object at 0x7f34c3fcb370>, <__main__.Case object at 0x7f34c3fcbc40>, <__main__.Case object at 0x7f34c3fcb910>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fcad70>, <__main__.Case object at 0x7f34c3fc9570>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 3, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 3, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (5, 1), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 1), solution: 4, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.7000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 6), solution: 4, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (3, 5), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (3, 4), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (2, 4), solution: 4, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 4), solution: 4, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 3), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (0, 2), solution: 4, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 2, tv: 0.6, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.6, time steps: 14\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 0.7999999999999999)\n",
      "Integrated case process. comm case (3, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 6), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (3, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 5), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (3, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 4), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (2, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 4), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (1, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 4), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (1, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 3), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (1, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 2), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (0, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 2), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 0.9999999999999999)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 1, time steps: 3\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 1), solution: 4, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.7000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.6, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.6, time steps: 14\n",
      "Episode: 53, Total Steps: 17, Total Rewards: [89, 84], Status Episode: True\n",
      "------------------------------------------End of episode 53 loop--------------------\n",
      "----- starting point of Episode 54 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 1), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 2), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 3, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 3), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 0), 2, 0.6, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 4), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 1), 4, 0.6, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 4), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 1), 2, 0.6, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 4), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 2), 4, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 5), 2, 0.8999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 6), 4, 0.8999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 0.8999999999999999, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 3) with action 1 to next state (6, 2): pull reward: 0\n",
      "----- starting point of Episode 54 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 5) with action 1 to next state (4, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 54 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fc9570>, <__main__.Case object at 0x7f34c3fc8eb0>, <__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fc94e0>, <__main__.Case object at 0x7f34c3fc9270>, <__main__.Case object at 0x7f34c3fcb130>, <__main__.Case object at 0x7f34c3fcb580>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fcbd30>, <__main__.Case object at 0x7f34c3fc9930>, <__main__.Case object at 0x7f34c3fca770>, <__main__.Case object at 0x7f34c3fc8250>, <__main__.Case object at 0x7f34c3fc8f10>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fc8130>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3fa8820>, <__main__.Case object at 0x7f34c3fc9120>, <__main__.Case object at 0x7f34c3fc9990>, <__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c3fcaec0>, <__main__.Case object at 0x7f34c3fcb3a0>, <__main__.Case object at 0x7f34c3fca2c0>, <__main__.Case object at 0x7f34c3fcadd0>, <__main__.Case object at 0x7f34c3fc8850>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fcbbb0>, <__main__.Case object at 0x7f34c3fcaad0>, <__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fc9b40>, <__main__.Case object at 0x7f34c3fca9e0>, <__main__.Case object at 0x7f34c3fca530>, <__main__.Case object at 0x7f34c3fc8460>]\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 0.9999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 6), solution: 4, tv: 0.9999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 5), solution: 2, tv: 0.9999999999999999, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 4), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (2, 4), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 4), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 3), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.7000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.7000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 3, tv: 0.5, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 2, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (4, 1), solution: 4, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 2, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 3, tv: 0.9, time steps: 17\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (5, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 1)\n",
      "Integrated case process. comm case (5, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 1), 2, 0.6)\n",
      "Integrated case process. comm case (4, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 1), 4, 0.6)\n",
      "Integrated case process. comm case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 2, 0.6)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 3, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 0.9999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.9999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.9999999999999999, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.7000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.7000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.5, time steps: 3\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.7999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.9, time steps: 17\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fca650>, <__main__.Case object at 0x7f34c3fc8a00>, <__main__.Case object at 0x7f34c3fcbb20>, <__main__.Case object at 0x7f34c3fc95d0>, <__main__.Case object at 0x7f34c3fca080>, <__main__.Case object at 0x7f34c3fc9db0>, <__main__.Case object at 0x7f34c3fcab00>, <__main__.Case object at 0x7f34c3fcb9a0>, <__main__.Case object at 0x7f34c3fca110>, <__main__.Case object at 0x7f34c3fca140>, <__main__.Case object at 0x7f34c3fca980>, <__main__.Case object at 0x7f34c3fcacb0>, <__main__.Case object at 0x7f34c3fc9bd0>, <__main__.Case object at 0x7f34c3fc8370>, <__main__.Case object at 0x7f34c3fcb640>, <__main__.Case object at 0x7f34c3fcb190>, <__main__.Case object at 0x7f34c3fcb4f0>, <__main__.Case object at 0x7f34c3fc8c70>, <__main__.Case object at 0x7f34c0f14ac0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fcb940>, <__main__.Case object at 0x7f34c3fc8f70>, <__main__.Case object at 0x7f34c3fc9c00>, <__main__.Case object at 0x7f34c3fc8280>, <__main__.Case object at 0x7f34c3fc94b0>, <__main__.Case object at 0x7f34c3fc8b80>, <__main__.Case object at 0x7f34c3fcb280>, <__main__.Case object at 0x7f34c3fc8070>, <__main__.Case object at 0x7f34c3fcb8e0>, <__main__.Case object at 0x7f34c3fcbf40>, <__main__.Case object at 0x7f34c3fc9420>, <__main__.Case object at 0x7f34c3fcbeb0>, <__main__.Case object at 0x7f34c3fcb700>, <__main__.Case object at 0x7f34c3fc9540>, <__main__.Case object at 0x7f34c3fc98d0>, <__main__.Case object at 0x7f34c3fcb250>, <__main__.Case object at 0x7f34c3fca6b0>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 3, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 3, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (5, 1), solution: 2, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 1), solution: 4, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 2, tv: 0.7, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.6000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 6), solution: 4, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (3, 5), solution: 2, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (3, 4), solution: 2, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (2, 4), solution: 4, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 4), solution: 4, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 3), solution: 2, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 2, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (0, 2), solution: 4, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 2, tv: 0.5, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.5, time steps: 14\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 0.8999999999999999)\n",
      "Integrated case process. comm case (3, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 6), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (3, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 5), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (3, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 4), 2, 1)\n",
      "Integrated case process. comm case (2, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 4), 4, 1)\n",
      "Integrated case process. comm case (1, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 4), 4, 1)\n",
      "Integrated case process. comm case (1, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 3), 2, 1)\n",
      "Integrated case process. comm case (1, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 2), 2, 1)\n",
      "Integrated case process. comm case (0, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 2), 4, 1)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 2, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 1, time steps: 3\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 1), solution: 4, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.6000000000000001, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.5, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.5, time steps: 14\n",
      "Episode: 54, Total Steps: 19, Total Rewards: [89, 82], Status Episode: True\n",
      "------------------------------------------End of episode 54 loop--------------------\n",
      "----- starting point of Episode 55 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 1) with action 0 to next state (0, 1): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 1), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 3, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 2), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 0), 2, 0.7, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 3), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 1), 4, 0.7, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 4), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 1), 2, 0.7, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 4), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 2), 4, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 4), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 5), 2, 0.9999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 6), 4, 0.9999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 0.9999999999999999, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 5), 1, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 55 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fca6b0>, <__main__.Case object at 0x7f34c3fcb3a0>, <__main__.Case object at 0x7f34c3fcbf10>, <__main__.Case object at 0x7f34c3fcad10>, <__main__.Case object at 0x7f34c3fcaa10>, <__main__.Case object at 0x7f34c3fc8ee0>, <__main__.Case object at 0x7f34c3fc85e0>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fc9570>, <__main__.Case object at 0x7f34c3fcb6a0>, <__main__.Case object at 0x7f34c3fcb910>, <__main__.Case object at 0x7f34c3fc8bb0>, <__main__.Case object at 0x7f34c3fca260>, <__main__.Case object at 0x7f34c3fca650>, <__main__.Case object at 0x7f34c3fca080>, <__main__.Case object at 0x7f34c3fca110>, <__main__.Case object at 0x7f34c3fcacb0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3faafe0>, <__main__.Case object at 0x7f34c3fcaec0>, <__main__.Case object at 0x7f34c3fc9a50>, <__main__.Case object at 0x7f34c3fcba00>, <__main__.Case object at 0x7f34c3fcbac0>, <__main__.Case object at 0x7f34c3fc9b10>, <__main__.Case object at 0x7f34c3fcbf70>, <__main__.Case object at 0x7f34c3fca890>, <__main__.Case object at 0x7f34c3fcb670>, <__main__.Case object at 0x7f34c3fcaef0>, <__main__.Case object at 0x7f34c3fc8fa0>, <__main__.Case object at 0x7f34c3fca230>, <__main__.Case object at 0x7f34c3fc8340>, <__main__.Case object at 0x7f34c3fcbc10>, <__main__.Case object at 0x7f34c3fc95d0>, <__main__.Case object at 0x7f34c3fcb9a0>, <__main__.Case object at 0x7f34c3fc9bd0>]\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 6), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 5), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 4), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (2, 4), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 4), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 3), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 3, tv: 0.4, time steps: 3\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 3, tv: 0.8, time steps: 17\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 5) is empty. Temporary case base stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (5, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 1)\n",
      "Integrated case process. comm case (5, 1) is empty. Temporary case base stored to the case base: ((5, 1), 2, 0.7)\n",
      "Integrated case process. comm case (4, 1) is empty. Temporary case base stored to the case base: ((4, 1), 4, 0.7)\n",
      "Integrated case process. comm case (4, 0) is empty. Temporary case base stored to the case base: ((4, 0), 2, 0.7)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 3, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.6999999999999998, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.8, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 1), solution: 4, tv: 0.7, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.7, time steps: 17\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fc9660>, <__main__.Case object at 0x7f34c3fcadd0>, <__main__.Case object at 0x7f34c3fc9ff0>, <__main__.Case object at 0x7f34c3fc80d0>, <__main__.Case object at 0x7f34c3fc9120>, <__main__.Case object at 0x7f34c3fc9ed0>, <__main__.Case object at 0x7f34c3fcb970>, <__main__.Case object at 0x7f34c3fc99f0>, <__main__.Case object at 0x7f34c3fc8d60>, <__main__.Case object at 0x7f34c3fc8e80>, <__main__.Case object at 0x7f34c3fc8640>, <__main__.Case object at 0x7f34c3fc9330>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fcbb20>, <__main__.Case object at 0x7f34c3fcab00>, <__main__.Case object at 0x7f34c3fca980>, <__main__.Case object at 0x7f34c3fc9090>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fc83d0>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fcb940>, <__main__.Case object at 0x7f34c3fc86d0>, <__main__.Case object at 0x7f34c3fcbdf0>, <__main__.Case object at 0x7f34c3fc9390>, <__main__.Case object at 0x7f34c3fc87c0>, <__main__.Case object at 0x7f34c3fc8d00>, <__main__.Case object at 0x7f34c3fc9ab0>, <__main__.Case object at 0x7f34c3fc9c90>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fc8a00>, <__main__.Case object at 0x7f34c3fc9db0>, <__main__.Case object at 0x7f34c3fca140>, <__main__.Case object at 0x7f34c3fcac80>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 3, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 3, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (5, 1), solution: 2, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 1), solution: 4, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 2, tv: 0.7999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.5000000000000001, time steps: 16\n",
      "case content after REVISE for agent 1, problem: (3, 6), solution: 4, tv: 0.4, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (3, 5), solution: 2, tv: 0.4, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (3, 4), solution: 2, tv: 0.4, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (2, 4), solution: 4, tv: 0.4, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 4), solution: 4, tv: 0.4, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 3), solution: 2, tv: 0.4, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 2, tv: 0.4, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (0, 2), solution: 4, tv: 0.4, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 2, tv: 0.4, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.4, time steps: 14\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 0.9999999999999999)\n",
      "Integrated case process. comm case (3, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 6), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (3, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 5), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (3, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 4), 2, 1)\n",
      "Integrated case process. comm case (2, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 4), 4, 1)\n",
      "Integrated case process. comm case (1, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 4), 4, 1)\n",
      "Integrated case process. comm case (1, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 3), 2, 1)\n",
      "Integrated case process. comm case (1, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 2), 2, 1)\n",
      "Integrated case process. comm case (0, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 2), 4, 1)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 2, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 1, time steps: 3\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.7999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 1), solution: 4, tv: 0.7999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.7999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.5000000000000001, time steps: 16\n",
      "Episode: 55, Total Steps: 17, Total Rewards: [88, 84], Status Episode: True\n",
      "------------------------------------------End of episode 55 loop--------------------\n",
      "----- starting point of Episode 56 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 1), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (0, 2) with action 0 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 3, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 2), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 0), 2, 0.7999999999999999, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 3), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 1), 4, 0.7999999999999999, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 4), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 1), 2, 0.7999999999999999, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 4), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 2), 4, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 4), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 0\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 5), 2, 1, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 2) with action 0 to next state (6, 2): pull reward: 0\n",
      "----- starting point of Episode 56 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 6), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 5) with action 1 to next state (4, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 56 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 4) with action 0 to next state (4, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fcac80>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fcb580>, <__main__.Case object at 0x7f34c3fc95d0>, <__main__.Case object at 0x7f34c3fcaec0>, <__main__.Case object at 0x7f34c3fc9540>, <__main__.Case object at 0x7f34c3fca770>, <__main__.Case object at 0x7f34c3fc8310>, <__main__.Case object at 0x7f34c3fcbbb0>, <__main__.Case object at 0x7f34c3fcb8e0>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fcb370>, <__main__.Case object at 0x7f34c3fc9810>, <__main__.Case object at 0x7f34c3fc9c00>, <__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c3fc8130>, <__main__.Case object at 0x7f34c3fc84f0>, <__main__.Case object at 0x7f34c3fc9c30>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3fa8820>, <__main__.Case object at 0x7f34c3fc9a50>, <__main__.Case object at 0x7f34c3fc8400>, <__main__.Case object at 0x7f34c3fcbc10>, <__main__.Case object at 0x7f34c3fca9e0>, <__main__.Case object at 0x7f34c3fcbf40>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fcad70>, <__main__.Case object at 0x7f34c3fc9b40>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fcbb80>, <__main__.Case object at 0x7f34c3fc8850>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fc98d0>, <__main__.Case object at 0x7f34c3fc8250>, <__main__.Case object at 0x7f34c3fca1d0>, <__main__.Case object at 0x7f34c3fc8670>]\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 6), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 5), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 4), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (2, 4), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 4), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 3), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.5000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.5000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.7000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 3, tv: 0.7000000000000001, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.9, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 2, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (4, 1), solution: 4, tv: 0.6, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 2, tv: 0.6, time steps: 17\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (5, 2) is empty. Temporary case base stored to the case base: ((5, 2), 4, 1)\n",
      "Integrated case process. comm case (5, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 1), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 1), 4, 0.7999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 2, 0.7999999999999999)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 1)\n",
      "Integrated case process. comm case (6, 0) is empty. Temporary case base stored to the case base: ((6, 0), 3, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 0.5000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 0.5000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.5999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.7000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.7000000000000001, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.9, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 1), solution: 4, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.6, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 1, time steps: 3\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fc96c0>, <__main__.Case object at 0x7f34c3fca050>, <__main__.Case object at 0x7f34c3fc9450>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fc8b80>, <__main__.Case object at 0x7f34c3fc9270>, <__main__.Case object at 0x7f34c3fcb2e0>, <__main__.Case object at 0x7f34c3fcbc40>, <__main__.Case object at 0x7f34c3fcbe80>, <__main__.Case object at 0x7f34c3fc8a30>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fcb640>, <__main__.Case object at 0x7f34c3fc9d50>, <__main__.Case object at 0x7f34c3fc9420>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fc96f0>, <__main__.Case object at 0x7f34c3fcb250>, <__main__.Case object at 0x7f34c3fc80d0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fc9090>, <__main__.Case object at 0x7f34c3fcbaf0>, <__main__.Case object at 0x7f34c3fcb9a0>, <__main__.Case object at 0x7f34c3fc83d0>, <__main__.Case object at 0x7f34c3fc8eb0>, <__main__.Case object at 0x7f34c3fcb220>, <__main__.Case object at 0x7f34c3fca6b0>, <__main__.Case object at 0x7f34c3fc8ee0>, <__main__.Case object at 0x7f34c3fcb700>, <__main__.Case object at 0x7f34c3fc9930>, <__main__.Case object at 0x7f34c3fc9cc0>, <__main__.Case object at 0x7f34c3fcb280>, <__main__.Case object at 0x7f34c3fcb130>, <__main__.Case object at 0x7f34c3fcbee0>, <__main__.Case object at 0x7f34c3fcae90>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 3, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 3, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (5, 1), solution: 2, tv: 0.8999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 1), solution: 4, tv: 0.8999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 2, tv: 0.8999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.40000000000000013, time steps: 16\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (3, 6) is empty. Temporary case base stored to the case base: ((3, 6), 4, 1)\n",
      "Integrated case process. comm case (3, 5) is empty. Temporary case base stored to the case base: ((3, 5), 2, 1)\n",
      "Integrated case process. comm case (3, 4) is empty. Temporary case base stored to the case base: ((3, 4), 2, 1)\n",
      "Integrated case process. comm case (2, 4) is empty. Temporary case base stored to the case base: ((2, 4), 4, 1)\n",
      "Integrated case process. comm case (1, 4) is empty. Temporary case base stored to the case base: ((1, 4), 4, 1)\n",
      "Integrated case process. comm case (1, 3) is empty. Temporary case base stored to the case base: ((1, 3), 2, 1)\n",
      "Integrated case process. comm case (1, 2) is empty. Temporary case base stored to the case base: ((1, 2), 2, 1)\n",
      "Integrated case process. comm case (0, 2) is empty. Temporary case base stored to the case base: ((0, 2), 4, 1)\n",
      "Integrated case process. comm case (0, 1) is empty. Temporary case base stored to the case base: ((0, 1), 2, 1)\n",
      "Integrated case process. comm case (0, 0) is empty. Temporary case base stored to the case base: ((0, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 1, time steps: 3\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.8999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 1), solution: 4, tv: 0.8999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.8999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 1, time steps: 14\n",
      "Episode: 56, Total Steps: 18, Total Rewards: [88, 83], Status Episode: True\n",
      "------------------------------------------End of episode 56 loop--------------------\n",
      "----- starting point of Episode 57 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 2, 1, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (9, 0) with action 4 to next state (9, 0): pull reward: 0\n",
      "----- starting point of Episode 57 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 1), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 2), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 2) with action 3 to next state (0, 2): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 0), 3, 1, 3)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 2), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 0), 2, 0.8999999999999999, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 3), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 1), 4, 0.8999999999999999, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 4), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 1), 2, 0.8999999999999999, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 4), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 3\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 2), 4, 1, 17)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (3, 4) with action 3 to next state (2, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 4), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 4), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 1\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 5), 2, 1, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 4) with action 1 to next state (6, 3): pull reward: 0\n",
      "----- starting point of Episode 57 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 6), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 6) with action 1 to next state (4, 5): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 5), 1, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 17 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 18 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 57 in steps 19 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3faafe0>, <__main__.Case object at 0x7f34c3fca9e0>, <__main__.Case object at 0x7f34c3fc88e0>, <__main__.Case object at 0x7f34c3fc98d0>, <__main__.Case object at 0x7f34c3fcb130>, <__main__.Case object at 0x7f34c3fcae90>, <__main__.Case object at 0x7f34c3fc95d0>, <__main__.Case object at 0x7f34c3fc8310>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fc8130>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fc9390>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fc9d50>, <__main__.Case object at 0x7f34c3fcb250>, <__main__.Case object at 0x7f34c3fcba90>, <__main__.Case object at 0x7f34c3fc8940>, <__main__.Case object at 0x7f34c3fca2f0>, <__main__.Case object at 0x7f34c0f15240>, <__main__.Case object at 0x7f34c0f151e0>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3fca230>, <__main__.Case object at 0x7f34c3fcad70>, <__main__.Case object at 0x7f34c3fcae30>, <__main__.Case object at 0x7f34c3fc9cc0>, <__main__.Case object at 0x7f34c3fcbee0>, <__main__.Case object at 0x7f34c3fcb580>, <__main__.Case object at 0x7f34c3fca770>, <__main__.Case object at 0x7f34c3fcb370>, <__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c3fcb340>, <__main__.Case object at 0x7f34c3fcaad0>, <__main__.Case object at 0x7f34c3fcadd0>, <__main__.Case object at 0x7f34c3fc96f0>, <__main__.Case object at 0x7f34c3fc91e0>, <__main__.Case object at 0x7f34c3fc9e70>, <__main__.Case object at 0x7f34c3fc99c0>, <__main__.Case object at 0x7f34c3fcb3d0>, <__main__.Case object at 0x7f34c3fcb730>]\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 6), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 5), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 4), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (2, 4), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 4), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 3), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (6, 3), solution: 2, tv: 0.40000000000000013, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 2), solution: 2, tv: 0.40000000000000013, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.4999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.4999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.4999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 3, tv: 0.6000000000000001, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.8, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 2, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (4, 1), solution: 4, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 2, tv: 0.5, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.9, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 3, tv: 0.9, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 3) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (5, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 1)\n",
      "Integrated case process. comm case (5, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 1), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 1), 4, 0.8999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 2, 0.8999999999999999)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 1)\n",
      "Integrated case process. comm case (6, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 0), 3, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 0.4999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 0.4999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 0.4999999999999999, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.6000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.6000000000000001, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.8, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 1), solution: 4, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.5, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.9, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.9, time steps: 3\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fc8670>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fc8850>, <__main__.Case object at 0x7f34c3fcbd30>, <__main__.Case object at 0x7f34c3fc9a50>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fc9540>, <__main__.Case object at 0x7f34c3fcb8e0>, <__main__.Case object at 0x7f34c3fcb940>, <__main__.Case object at 0x7f34c3fc9c30>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fca140>, <__main__.Case object at 0x7f34c3fca680>, <__main__.Case object at 0x7f34c3fc9630>, <__main__.Case object at 0x7f34c3fc86a0>, <__main__.Case object at 0x7f34c3fc9720>, <__main__.Case object at 0x7f34c3fc9ff0>, <__main__.Case object at 0x7f34c3fc8f10>, <__main__.Case object at 0x7f34c0f14ee0>, <__main__.Case object at 0x7f34c0f14790>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fc80d0>, <__main__.Case object at 0x7f34c3fcbf40>, <__main__.Case object at 0x7f34c3fcbb80>, <__main__.Case object at 0x7f34c3fca1d0>, <__main__.Case object at 0x7f34c3fc8220>, <__main__.Case object at 0x7f34c3fcaec0>, <__main__.Case object at 0x7f34c3fcbbb0>, <__main__.Case object at 0x7f34c3fca530>, <__main__.Case object at 0x7f34c3fc8fa0>, <__main__.Case object at 0x7f34c3fc9c90>, <__main__.Case object at 0x7f34c3fcad10>, <__main__.Case object at 0x7f34c3fc9420>, <__main__.Case object at 0x7f34c3fca4d0>, <__main__.Case object at 0x7f34c3fcb4c0>, <__main__.Case object at 0x7f34c3fcb6d0>, <__main__.Case object at 0x7f34c0f14670>, <__main__.Case object at 0x7f34c0f15ed0>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 3, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 3, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (5, 1), solution: 2, tv: 0.9999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 1), solution: 4, tv: 0.9999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 2, tv: 0.9999999999999999, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 6), solution: 4, tv: 0.9, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (3, 5), solution: 2, tv: 0.9, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (3, 4), solution: 2, tv: 0.9, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (2, 4), solution: 4, tv: 0.9, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 4), solution: 4, tv: 0.9, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 3), solution: 2, tv: 0.9, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 2, tv: 0.9, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (0, 2), solution: 4, tv: 0.9, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 2, tv: 0.9, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.9, time steps: 14\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) is empty. Temporary case base stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (3, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 6), 4, 1)\n",
      "Integrated case process. comm case (3, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 5), 2, 1)\n",
      "Integrated case process. comm case (3, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 4), 2, 1)\n",
      "Integrated case process. comm case (2, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 4), 4, 1)\n",
      "Integrated case process. comm case (2, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 4), 4, 1)\n",
      "Integrated case process. comm case (1, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 4), 4, 1)\n",
      "Integrated case process. comm case (1, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 3), 2, 1)\n",
      "Integrated case process. comm case (1, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 2), 2, 1)\n",
      "Integrated case process. comm case (0, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 2), 4, 1)\n",
      "Integrated case process. comm case (0, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 2), 4, 1)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 2, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 1, time steps: 3\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 0.9999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 1), solution: 4, tv: 0.9999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 0.9999999999999999, time steps: 17\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.9, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.9, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.9, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.9, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.9, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.9, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.9, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.9, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.9, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.9, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "Episode: 57, Total Steps: 20, Total Rewards: [85, 81], Status Episode: True\n",
      "------------------------------------------End of episode 57 loop--------------------\n",
      "----- starting point of Episode 58 in steps 0 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((9, 0), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 0), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 1 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((8, 0), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 1), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 2 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((7, 0), 3, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((0, 2), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 3 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from problem solver: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: 0\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 2), 2, 1, 14)\n",
      "action type of agent: 1: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 1\n",
      "Calculating pull reward agent 1: from state (6, 0) with action 3 to next state (5, 0): pull reward: 0\n",
      "----- starting point of Episode 58 in steps 4 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 0\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 0), 3, 1, 17)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (1, 3) with action 0 to next state (1, 3): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 5 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 0), 2, 0.9999999999999999, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 3), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 6 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((4, 1), 4, 0.9999999999999999, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((1, 4), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 7 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 1), 2, 0.9999999999999999, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((2, 4), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 8 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 4\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((5, 2), 4, 1, 17)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 4), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 9 loop -----\n",
      "Physical Action for Agent 0 from case base: 2\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 2), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 5), 2, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 10 loop -----\n",
      "Physical Action for Agent 0 from case base: 4\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 3), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((3, 6), 4, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 11 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 is ongoing!\n",
      "win status agent 0 = False\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [False, False]\n",
      "comm next state for agent 0: ((6, 4), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 6), 1, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 12 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 2\n",
      "agent 0 reach the target!\n",
      "win status agent 0 = True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 5), 1, 1, 14)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 13 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 14 loop -----\n",
      "Physical Action for Agent 0 from problem solver: 1\n",
      "Physical Action for Agent 1 from case base: 3\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 13)\n",
      "action type of agent: 0: problem solver, agent learned\n",
      "Calculating weighted distance-based pull reward for agent 0\n",
      "Calculating pull reward agent 0: from state (4, 4) with action 1 to next state (4, 4): pull reward: 0\n",
      "comm next state for agent 1: 0\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 15 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 is ongoing!\n",
      "win status agent 1 = False\n",
      "wins all agent situation in the environment: [True, False]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "----- starting point of Episode 58 in steps 16 loop -----\n",
      "Physical Action for Agent 0 from case base: 1\n",
      "Physical Action for Agent 1 from case base: 1\n",
      "agent 0 is locked. Done status: True, win status: True\n",
      "agent 1 reach the target!\n",
      "win status agent 1 = True\n",
      "wins all agent situation in the environment: [True, True]\n",
      "comm next state for agent 0: ((6, 5), 2, 1, 13)\n",
      "action type of agent: 0: using solution from case base, no learning\n",
      "comm next state for agent 1: ((4, 4), 1, 1, 16)\n",
      "action type of agent: 1: using solution from case base, no learning\n",
      "win status of agent 0  before update the case base: True\n",
      "agent0 own temp case base: [<__main__.Case object at 0x7f34c3fca230>, <__main__.Case object at 0x7f34c3fcb580>, <__main__.Case object at 0x7f34c3fcb340>, <__main__.Case object at 0x7f34c3fc96f0>, <__main__.Case object at 0x7f34c3fc8970>, <__main__.Case object at 0x7f34c3fcac20>, <__main__.Case object at 0x7f34c3fc9570>, <__main__.Case object at 0x7f34c3fca080>, <__main__.Case object at 0x7f34c3fc8100>, <__main__.Case object at 0x7f34c3fcb130>, <__main__.Case object at 0x7f34c3fca320>, <__main__.Case object at 0x7f34c3fc9d80>, <__main__.Case object at 0x7f34c3fcba90>, <__main__.Case object at 0x7f34c3fc8850>, <__main__.Case object at 0x7f34c3fc9540>, <__main__.Case object at 0x7f34c3fc9c30>, <__main__.Case object at 0x7f34c3fca680>]\n",
      "agent0 comm temp case base: [<__main__.Case object at 0x7f34c3fa8820>, <__main__.Case object at 0x7f34c3fcbee0>, <__main__.Case object at 0x7f34c3fca860>, <__main__.Case object at 0x7f34c3fcb3d0>, <__main__.Case object at 0x7f34c3fcbdc0>, <__main__.Case object at 0x7f34c3fc8d00>, <__main__.Case object at 0x7f34c3fcbc40>, <__main__.Case object at 0x7f34c3fc9780>, <__main__.Case object at 0x7f34c3fc98d0>, <__main__.Case object at 0x7f34c3fc8310>, <__main__.Case object at 0x7f34c3fc9390>, <__main__.Case object at 0x7f34c3fc8940>, <__main__.Case object at 0x7f34c3fc8520>, <__main__.Case object at 0x7f34c3fcaf80>, <__main__.Case object at 0x7f34c3fcb940>, <__main__.Case object at 0x7f34c3fc9630>]\n",
      "case content after REVISE for agent 0, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 6), solution: 1, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "case content after REVISE for agent 0, problem: (3, 6), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 5), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (3, 4), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (2, 4), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 4), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 3), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (1, 2), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 2), solution: 4, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 1), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (0, 0), solution: 2, tv: 1, time steps: 14\n",
      "case content after REVISE for agent 0, problem: (7, 0), solution: 3, tv: 0.3999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (8, 0), solution: 3, tv: 0.3999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (9, 0), solution: 3, tv: 0.3999999999999999, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (6, 4), solution: 2, tv: 0.5000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 0), solution: 3, tv: 0.5000000000000001, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (6, 5), solution: 2, tv: 0.7000000000000001, time steps: 13\n",
      "case content after REVISE for agent 0, problem: (5, 1), solution: 2, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (4, 1), solution: 4, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (4, 0), solution: 2, tv: 0.4, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (5, 2), solution: 4, tv: 0.8, time steps: 17\n",
      "case content after REVISE for agent 0, problem: (6, 0), solution: 3, tv: 0.8, time steps: 3\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 6) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 5) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (3, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (2, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 4) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 3) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (1, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 2) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 1) for agent 0 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (0, 0) for agent 0 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 5) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 5), 2, 1)\n",
      "Integrated case process. comm case (6, 4) for agent 0 is not empty. Temporary case base that not stored to the case base: ((6, 4), 2, 1)\n",
      "Integrated case process. comm case (6, 3) is empty. Temporary case base stored to the case base: ((6, 3), 2, 1)\n",
      "Integrated case process. comm case (6, 2) is empty. Temporary case base stored to the case base: ((6, 2), 2, 1)\n",
      "Integrated case process. comm case (5, 2) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 2), 4, 1)\n",
      "Integrated case process. comm case (5, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 1), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 1) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 1), 4, 0.9999999999999999)\n",
      "Integrated case process. comm case (4, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((4, 0), 2, 0.9999999999999999)\n",
      "Integrated case process. comm case (5, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((5, 0), 3, 1)\n",
      "Integrated case process. comm case (7, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((7, 0), 3, 1)\n",
      "Integrated case process. comm case (8, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((8, 0), 3, 1)\n",
      "Integrated case process. comm case (9, 0) for agent 0 is not empty. Temporary case base that not stored to the case base: ((9, 0), 3, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 1, time steps: 16\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 1, time steps: 14\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 0.5000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 0.5000000000000001, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 0.7000000000000001, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 0.8, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 0.8, time steps: 3\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "win status of agent 1  before update the case base: True\n",
      "agent1 own temp case base: [<__main__.Case object at 0x7f34c3fc9cc0>, <__main__.Case object at 0x7f34c3fcb370>, <__main__.Case object at 0x7f34c3fcadd0>, <__main__.Case object at 0x7f34c3fc9e70>, <__main__.Case object at 0x7f34c3fc8f70>, <__main__.Case object at 0x7f34c3fcb670>, <__main__.Case object at 0x7f34c3fcafe0>, <__main__.Case object at 0x7f34c3fc9f00>, <__main__.Case object at 0x7f34c3fcacb0>, <__main__.Case object at 0x7f34c3fc95d0>, <__main__.Case object at 0x7f34c3fc82b0>, <__main__.Case object at 0x7f34c3fcb250>, <__main__.Case object at 0x7f34c3fc8a30>, <__main__.Case object at 0x7f34c3fc9a50>, <__main__.Case object at 0x7f34c3fc9ae0>, <__main__.Case object at 0x7f34c3fca140>, <__main__.Case object at 0x7f34c3fca5f0>]\n",
      "agent1 comm temp case base: [<__main__.Case object at 0x7f34c3fc8e80>, <__main__.Case object at 0x7f34c3fca770>, <__main__.Case object at 0x7f34c3fcaad0>, <__main__.Case object at 0x7f34c3fc91e0>, <__main__.Case object at 0x7f34c3fc9330>, <__main__.Case object at 0x7f34c3fcb970>, <__main__.Case object at 0x7f34c3fcb8b0>, <__main__.Case object at 0x7f34c3fc87c0>, <__main__.Case object at 0x7f34c3fcae90>, <__main__.Case object at 0x7f34c3fc8130>, <__main__.Case object at 0x7f34c3fc9d50>, <__main__.Case object at 0x7f34c3fcb2b0>, <__main__.Case object at 0x7f34c3fcbd30>, <__main__.Case object at 0x7f34c3fcabf0>, <__main__.Case object at 0x7f34c3fcb4f0>]\n",
      "case content after REVISE for agent 1, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (7, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (8, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "case content after REVISE for agent 1, problem: (5, 2), solution: 4, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (5, 0), solution: 3, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (6, 0), solution: 3, tv: 1, time steps: 3\n",
      "case content after REVISE for agent 1, problem: (5, 1), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 1), solution: 4, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (4, 0), solution: 2, tv: 1, time steps: 17\n",
      "case content after REVISE for agent 1, problem: (3, 6), solution: 4, tv: 0.8, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (3, 5), solution: 2, tv: 0.8, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (3, 4), solution: 2, tv: 0.8, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (2, 4), solution: 4, tv: 0.8, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 4), solution: 4, tv: 0.8, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 3), solution: 2, tv: 0.8, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (1, 2), solution: 2, tv: 0.8, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (0, 2), solution: 4, tv: 0.8, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (0, 1), solution: 2, tv: 0.8, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (0, 0), solution: 2, tv: 0.8, time steps: 14\n",
      "case content after REVISE for agent 1, problem: (4, 4), solution: 1, tv: 0.9, time steps: 16\n",
      "Episode succeeded, case (4, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 6) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 5) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 4) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 3) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 2) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 1) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (4, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (5, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (6, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (7, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (8, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Episode succeeded, case (9, 0) for agent 1 is not updated as it has more or equal steps.\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 4), 1, 1)\n",
      "Integrated case process. comm case (4, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 5), 1, 1)\n",
      "Integrated case process. comm case (4, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((4, 6), 1, 1)\n",
      "Integrated case process. comm case (3, 6) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 6), 4, 1)\n",
      "Integrated case process. comm case (3, 5) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 5), 2, 1)\n",
      "Integrated case process. comm case (3, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((3, 4), 2, 1)\n",
      "Integrated case process. comm case (2, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((2, 4), 4, 1)\n",
      "Integrated case process. comm case (1, 4) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 4), 4, 1)\n",
      "Integrated case process. comm case (1, 3) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 3), 2, 1)\n",
      "Integrated case process. comm case (1, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((1, 2), 2, 1)\n",
      "Integrated case process. comm case (0, 2) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 2), 4, 1)\n",
      "Integrated case process. comm case (0, 1) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 1), 2, 1)\n",
      "Integrated case process. comm case (0, 0) for agent 1 is not empty. Temporary case base that not stored to the case base: ((0, 0), 2, 1)\n",
      "cases content after RETAIN, problem: (4, 5), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (4, 6), solution: 1, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 6), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 5), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 4), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 3), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (6, 2), solution: 2, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (7, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (8, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (9, 0), solution: 3, tv: 1, time steps: 13\n",
      "cases content after RETAIN, problem: (5, 2), solution: 4, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (5, 0), solution: 3, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (6, 0), solution: 3, tv: 1, time steps: 3\n",
      "cases content after RETAIN, problem: (5, 1), solution: 2, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 1), solution: 4, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (4, 0), solution: 2, tv: 1, time steps: 17\n",
      "cases content after RETAIN, problem: (3, 6), solution: 4, tv: 0.8, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 5), solution: 2, tv: 0.8, time steps: 14\n",
      "cases content after RETAIN, problem: (3, 4), solution: 2, tv: 0.8, time steps: 14\n",
      "cases content after RETAIN, problem: (2, 4), solution: 4, tv: 0.8, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 4), solution: 4, tv: 0.8, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 3), solution: 2, tv: 0.8, time steps: 14\n",
      "cases content after RETAIN, problem: (1, 2), solution: 2, tv: 0.8, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 2), solution: 4, tv: 0.8, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 1), solution: 2, tv: 0.8, time steps: 14\n",
      "cases content after RETAIN, problem: (0, 0), solution: 2, tv: 0.8, time steps: 14\n",
      "cases content after RETAIN, problem: (4, 4), solution: 1, tv: 0.9, time steps: 16\n",
      "Episode: 58, Total Steps: 17, Total Rewards: [88, 84], Status Episode: True\n",
      "------------------------------------------End of episode 58 loop--------------------\n",
      "Success rate: 72.88135593220339%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import json\n",
    "import psutil\n",
    "import pynvml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from environment_ma_reward_distance_dynamic_notrandom import Env\n",
    "\n",
    "class ProblemSolver:\n",
    "    def __init__(self, num_actions, env, alpha, gamma, epsilon, lambda_trace=0.8):\n",
    "        self.env = env\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = alpha\n",
    "        self.discount_factor = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lambda_trace = lambda_trace\n",
    "        self.q_tables = [defaultdict(lambda: [0.0] * num_actions) for _ in range(env.num_agents)]\n",
    "        self.e_trace_tables = [defaultdict(lambda: [0.0] * num_actions) for _ in range(env.num_agents)]  # Eligibility traces\n",
    "\n",
    "    @staticmethod\n",
    "    def arg_max(state_action):\n",
    "        max_index_list = []\n",
    "        max_value = state_action[0]\n",
    "        for index, value in enumerate(state_action):\n",
    "            if value > max_value:\n",
    "                max_index_list.clear()\n",
    "                max_value = value\n",
    "                max_index_list.append(index)\n",
    "            elif value == max_value:\n",
    "                max_index_list.append(index)\n",
    "        return random.choice(max_index_list)\n",
    "\n",
    "    def choose_action(self, agent_idx, state):\n",
    "        state = tuple(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            state_action = self.q_tables[agent_idx][state]\n",
    "            action = self.arg_max(state_action)\n",
    "        return action\n",
    "\n",
    "    def learn(self, agent_idx, state, action, reward, next_state, case_base=None):\n",
    "        state = tuple(state)\n",
    "        next_state = tuple(next_state)\n",
    "        current_q = self.q_tables[agent_idx][state][action]\n",
    "        max_next_q = max(self.q_tables[agent_idx][next_state])\n",
    "\n",
    "        # Calculate the Q-update with weighted distance-based pull reward if case_base exists\n",
    "        if case_base:\n",
    "            print(f\"Calculating weighted distance-based pull reward for agent {agent_idx}\")\n",
    "            pull_reward = 0  # Initialize pull reward\n",
    "            \n",
    "            for case in case_base:\n",
    "                problems = case.problem if isinstance(case.problem, list) else [case.problem]\n",
    "                \n",
    "                weight = case.trust_value\n",
    "                \n",
    "                # Calculate the pull reward based on the weighted difference in distances\n",
    "                for p in problems:\n",
    "                    distance_current = np.linalg.norm(np.array(state) - np.array(p))\n",
    "                    distance_next = np.linalg.norm(np.array(next_state) - np.array(p))\n",
    "\n",
    "                    distance_diff = distance_current - distance_next\n",
    "                    distance_diff = weight * distance_diff\n",
    "                    pull_reward += np.log1p(abs(distance_diff)) * np.sign(distance_diff)\n",
    "                    pull_reward = np.clip(pull_reward, -1 * self.epsilon, self.epsilon)\n",
    "            \n",
    "            pull_reward = 0\n",
    "            \n",
    "            print(f\"Calculating pull reward agent {agent_idx}: from state {state} with action {action} to next state {next_state}: pull reward: {pull_reward}\")\n",
    "            delta = reward + pull_reward + self.discount_factor * max_next_q - current_q\n",
    "        else:\n",
    "            # Standard Q-learning update without pull reward\n",
    "            print(f\"No communication. Standard Q-learning update for agent {agent_idx}\")\n",
    "            delta = reward + self.discount_factor * max_next_q - current_q\n",
    "\n",
    "        # Update eligibility traces for the current state-action pair\n",
    "        self.e_trace_tables[agent_idx][state][action] += 1\n",
    "\n",
    "        # Update Q-values and eligibility traces for all state-action pairs\n",
    "        for s, actions in self.q_tables[agent_idx].items():\n",
    "            for a in range(self.num_actions):\n",
    "                self.q_tables[agent_idx][s][a] += self.learning_rate * delta * self.e_trace_tables[agent_idx][s][a]\n",
    "                self.e_trace_tables[agent_idx][s][a] *= self.discount_factor * self.lambda_trace\n",
    "        \n",
    "        # Reset the trace for the next state-action pair after the update\n",
    "        self.e_trace_tables[agent_idx][next_state] = [0.0] * self.num_actions\n",
    "\n",
    "\n",
    "class Case:\n",
    "    def __init__(self, problem, solution, trust_value=0.5, total_time_steps=0):\n",
    "        self.problem = ast.literal_eval(problem) if isinstance(problem, str) else problem\n",
    "        self.solution = solution\n",
    "        self.trust_value = trust_value\n",
    "        self.total_time_steps = total_time_steps\n",
    "\n",
    "    @staticmethod\n",
    "    def sim_q(state1, state2):\n",
    "        state1 = np.atleast_1d(state1)\n",
    "        state2 = np.atleast_1d(state2)\n",
    "        CNDMaxDist = 6\n",
    "        v = state1.size\n",
    "        DistQ = np.sum([Case.dist_q(Objic, Objip) for Objic, Objip in zip(state1, state2)])\n",
    "        similarity = (CNDMaxDist * v - DistQ) / (CNDMaxDist * v)\n",
    "        return similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def dist_q(X1, X2):\n",
    "        return np.min(np.abs(X1 - X2))\n",
    "\n",
    "    @staticmethod\n",
    "    def retrieve(state, case_base, threshold=0.1):\n",
    "        state = ast.literal_eval(state) if isinstance(state, str) else state\n",
    "        for case in case_base:\n",
    "            if state == case.problem: \n",
    "                return case\n",
    "\n",
    "    @staticmethod\n",
    "    def reuse(agent_idx, c, own_temp_case_base, comm_temp_case_base, source='own'):\n",
    "        \"\"\"Reuse step for adding cases to temporary case bases.\"\"\"\n",
    "        if source == 'own':\n",
    "            own_temp_case_base.append(c)\n",
    "        elif source == 'comm':\n",
    "            comm_temp_case_base.append(c)\n",
    "\n",
    "    @staticmethod\n",
    "    def revise(agent_idx, case_base, temporary_case_base, successful_episodes, total_steps):\n",
    "        for case in case_base:\n",
    "            if any((case.problem, case.solution) == (temp_case.problem, temp_case.solution) for temp_case in temporary_case_base):\n",
    "                if successful_episodes:\n",
    "                    case.trust_value += 0.1\n",
    "                else:\n",
    "                    case.trust_value -= 0.2\n",
    "            else:\n",
    "                if successful_episodes:\n",
    "                    case.trust_value -= 0.1\n",
    "            \n",
    "            case.trust_value = max(0, min(case.trust_value, 1))\n",
    "            print(f\"case content after REVISE for agent {agent_idx}, problem: {case.problem}, solution: {case.solution}, tv: {case.trust_value}, time steps: {case.total_time_steps}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def retain(agent_idx, case_base, own_temp_case_base, comm_temp_case_base, successful_episodes, total_steps, threshold=0.49):\n",
    "        if successful_episodes:\n",
    "            for temp_case in reversed(own_temp_case_base):\n",
    "                state = tuple(np.atleast_1d(temp_case.problem))\n",
    "\n",
    "                existing_case = next((case for case in case_base if tuple(np.atleast_1d(case.problem)) == state), None)\n",
    "                \n",
    "                if existing_case is None:\n",
    "                    case_base.append(temp_case)\n",
    "                    print(f\"Episode succeeded, case {temp_case.problem} is empty. Temporary case base stored to the case base: {temp_case.problem, temp_case.solution, temp_case.trust_value}\")\n",
    "                else:\n",
    "                    if total_steps < existing_case.total_time_steps:\n",
    "                        # Update the case in the case base if the new case has fewer total steps\n",
    "                        existing_case.solution = temp_case.solution\n",
    "                        existing_case.trust_value = max(0, temp_case.trust_value)\n",
    "                        existing_case.total_time_steps = total_steps\n",
    "                        print(f\"Episode succeeded, updated case base with fewer steps: {temp_case.problem, temp_case.solution, temp_case.trust_value, total_steps}\")\n",
    "                    else:\n",
    "                        print(f\"Episode succeeded, case {temp_case.problem} for agent {agent_idx} is not updated as it has more or equal steps.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Episode not succeeded, temporary case base from own experience is not stored to the case base\")\n",
    "\n",
    "        case_base_dict = {tuple(np.atleast_1d(case.problem)): case for case in case_base}\n",
    "\n",
    "        for temp_comm_case in reversed(comm_temp_case_base):\n",
    "            state_comm = tuple(np.atleast_1d(temp_comm_case.problem))\n",
    "            existing_case = case_base_dict.get(state_comm)\n",
    "\n",
    "            if existing_case is None:\n",
    "                case_base.append(temp_comm_case)\n",
    "                case_base_dict[state_comm] = temp_comm_case\n",
    "                print(f\"Integrated case process. comm case {temp_comm_case.problem} is empty. Temporary case base stored to the case base: {temp_comm_case.problem, temp_comm_case.solution, temp_comm_case.trust_value}\")\n",
    "            else:\n",
    "                print(f\"Integrated case process. comm case {temp_comm_case.problem} for agent {agent_idx} is not empty. Temporary case base that not stored to the case base: {temp_comm_case.problem, temp_comm_case.solution, temp_comm_case.trust_value}\")\n",
    "\n",
    "        # Remove cases with trust values below the threshold\n",
    "        case_base[:] = [case for case in case_base if case.trust_value >= threshold]\n",
    "\n",
    "        for case in case_base:\n",
    "            print(f\"cases content after RETAIN, problem: {case.problem}, solution: {case.solution}, tv: {case.trust_value}, time steps: {case.total_time_steps}\")\n",
    "\n",
    "        return case_base\n",
    "\n",
    "\n",
    "class QCBRL:\n",
    "    def __init__(self, num_actions, env, episodes, max_steps, alpha, gamma, epsilon, epsilon_decay, epsilon_min, render):\n",
    "        self.num_actions = num_actions\n",
    "        self.env = env\n",
    "        self.episodes = episodes\n",
    "        self.max_steps = max_steps\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.render = render\n",
    "        self.epsilon_decay = epsilon_decay  \n",
    "        self.epsilon_min = epsilon_min  \n",
    "\n",
    "        self.problem_solvers = [ProblemSolver(num_actions, self.env, alpha, gamma, epsilon) for _ in range(self.env.num_agents)]\n",
    "        self.case_bases = [[] for _ in range(self.env.num_agents)]  # Individual case bases for each agent\n",
    "        self.own_temp_case_bases = [[] for _ in range(self.env.num_agents)]  # Temporary case bases for own experiences\n",
    "        self.comm_temp_case_bases = [[] for _ in range(self.env.num_agents)]  # Temporary case bases for communication experiences\n",
    "        self.successful_episodes = [0] * self.env.num_agents\n",
    "        self.rewards_per_episode = [[] for _ in range(self.env.num_agents)]  \n",
    "        self.total_successful_episodes = 0 \n",
    "        self.action_type = [0] * self.env.num_agents\n",
    "\n",
    "    def run(self):\n",
    "        rewards = []\n",
    "        memory_usage = []\n",
    "        gpu_memory_usage = []\n",
    "        num_successful_episodes = 0\n",
    "        total_steps_list = []\n",
    "        success_steps = []\n",
    "\n",
    "        pynvml.nvmlInit()\n",
    "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "\n",
    "        for episode in range(self.episodes):\n",
    "            states = self.env.reset()\n",
    "            episode_reward = [0] * self.env.num_agents\n",
    "            total_steps = 0 \n",
    "            self.own_temp_case_bases = [[] for _ in range(self.env.num_agents)]\n",
    "            self.comm_temp_case_bases = [[] for _ in range(self.env.num_agents)]\n",
    "            success_count = [0] * self.env.num_agents\n",
    "            dones = [False] * self.env.num_agents\n",
    "            win_states = [False] * self.env.num_agents\n",
    "            successful_episodes = False\n",
    "\n",
    "            while not all(dones):\n",
    "                print(f\"----- starting point of Episode {episode} in steps {total_steps} loop -----\")\n",
    "                \n",
    "                actions = []\n",
    "                for agent_idx in range(self.env.num_agents):\n",
    "                    state = states[agent_idx]\n",
    "                    action = self.take_action(agent_idx, state)\n",
    "                    actions.append(action)\n",
    "\n",
    "                next_states, rewards, dones = self.env.step(actions)\n",
    "\n",
    "                win_states = []\n",
    "                for agent_idx in range(self.env.num_agents):\n",
    "                    state = states[agent_idx]\n",
    "                    action = actions[agent_idx]\n",
    "                    reward = rewards[agent_idx]\n",
    "                    next_state = next_states[agent_idx]\n",
    "\n",
    "                    physical_state = tuple(state[0])\n",
    "                    win_state = state[1]\n",
    "                    comm_state = state[2]  # Communication state containing messages from other agents\n",
    "\n",
    "                    physical_next_state = tuple(next_state[0])\n",
    "                    win_next_state = next_state[1]\n",
    "                    comm_next_state = tuple(next_state[2]) if next_state[2] != 0 else next_state[2]\n",
    "\n",
    "                    physical_action = action[0]\n",
    "                    comm_action = action[1]\n",
    "\n",
    "                    # Process messages received from other agents\n",
    "                    print(f\"comm next state for agent {agent_idx}: {comm_next_state}\")\n",
    "                    \n",
    "                    if comm_next_state == 0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        comm_case = Case(problem=comm_next_state[0], solution=comm_next_state[1], trust_value=comm_next_state[2], total_time_steps=comm_next_state[3])\n",
    "                        Case.reuse(agent_idx, comm_case, self.own_temp_case_bases[agent_idx], self.comm_temp_case_bases[agent_idx], source='comm')\n",
    "\n",
    "                    c = Case(physical_state, physical_action, total_time_steps=total_steps)\n",
    "                    Case.reuse(agent_idx, c, self.own_temp_case_bases[agent_idx], self.comm_temp_case_bases[agent_idx], source='own')\n",
    "\n",
    "                    if self.action_type[agent_idx] == 0:\n",
    "                        print(f\"action type of agent: {agent_idx}: problem solver, agent learned\")\n",
    "                        self.problem_solvers[agent_idx].learn(agent_idx, physical_state, physical_action, reward, physical_next_state, self.case_bases[agent_idx])\n",
    "                    else:\n",
    "                        print(f\"action type of agent: {agent_idx}: using solution from case base, no learning\")\n",
    "\n",
    "                    if win_next_state: \n",
    "                        success_count[agent_idx] += 1\n",
    "\n",
    "                    episode_reward[agent_idx] += reward\n",
    "                    win_states.append(win_next_state)  \n",
    "\n",
    "                states = next_states\n",
    "                total_steps += 1\n",
    "\n",
    "                self.env.render()\n",
    "                \n",
    "            if self.env.win_flag:\n",
    "                self.total_successful_episodes += 1\n",
    "                success_steps.append(total_steps)\n",
    "                successful_episodes = True\n",
    "            \n",
    "            for agent_idx in range(self.env.num_agents):\n",
    "                print(f\"win status of agent {agent_idx}  before update the case base: {win_states[agent_idx]}\")\n",
    "                self.rewards_per_episode[agent_idx].append(episode_reward[agent_idx])\n",
    "\n",
    "                print(f\"agent{agent_idx} own temp case base: {self.own_temp_case_bases[agent_idx]}\")\n",
    "                print(f\"agent{agent_idx} comm temp case base: {self.comm_temp_case_bases[agent_idx]}\")\n",
    "                \n",
    "                Case.revise(agent_idx, self.case_bases[agent_idx], self.own_temp_case_bases[agent_idx], win_states[agent_idx], total_steps=total_steps)\n",
    "                self.case_bases[agent_idx] = Case.retain(agent_idx, self.case_bases[agent_idx], self.own_temp_case_bases[agent_idx], self.comm_temp_case_bases[agent_idx], win_states[agent_idx], total_steps=total_steps)\n",
    "               \n",
    "                \n",
    "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "            \n",
    "            memory_usage.append(psutil.virtual_memory().percent)\n",
    "            gpu_memory_usage.append(pynvml.nvmlDeviceGetMemoryInfo(handle).used / 1024**2)\n",
    "\n",
    "            print(f\"Episode: {episode}, Total Steps: {total_steps}, Total Rewards: {episode_reward}, Status Episode: {successful_episodes}\")\n",
    "            print(f\"------------------------------------------End of episode {episode} loop--------------------\")\n",
    "\n",
    "        success_rate = self.total_successful_episodes / self.episodes * 100\n",
    "\n",
    "        return self.rewards_per_episode, success_rate, memory_usage, gpu_memory_usage, success_steps\n",
    "\n",
    "    def take_action(self, agent_idx, state):\n",
    "        physical_state = tuple(state[0])\n",
    "        win_state = state[1]\n",
    "        comm_state = state[2]\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            physical_action = self.problem_solvers[agent_idx].choose_action(agent_idx, physical_state)\n",
    "            comm_action = 0  # No communication action if using problem solver action\n",
    "            self.action_type[agent_idx] = 0\n",
    "            print(f\"Physical Action for Agent {agent_idx} from problem solver: {physical_action}\")\n",
    "        else:\n",
    "            similar_solution = Case.retrieve(physical_state, self.case_bases[agent_idx])\n",
    "            if similar_solution is not None:\n",
    "                physical_action = similar_solution.solution\n",
    "                comm_action = (similar_solution.problem, similar_solution.solution, similar_solution.trust_value, similar_solution.total_time_steps)\n",
    "                self.action_type[agent_idx] = 1\n",
    "                print(f\"Physical Action for Agent {agent_idx} from case base: {physical_action}\")\n",
    "            else:\n",
    "                physical_action = self.problem_solvers[agent_idx].choose_action(agent_idx, physical_state)\n",
    "                comm_action = 0  # No communication action if using problem solver action\n",
    "                self.action_type[agent_idx] = 0\n",
    "                print(f\"Physical Action for Agent {agent_idx} from problem solver: {physical_action}\")\n",
    "\n",
    "        return (physical_action, comm_action)\n",
    "\n",
    "    def save_case_base_temporary(self):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            filename = f\"cases/case_base_temporary_agent_{agent_idx}.json\"\n",
    "            case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem, \n",
    "                            \"solution\": int(case.solution), \n",
    "                            \"trust_value\": int(case.trust_value),\n",
    "                            \"total_time_steps\": int(case.total_time_steps)} for case in self.own_temp_case_bases[agent_idx] + self.comm_temp_case_bases[agent_idx]]\n",
    "            with open(filename, 'w') as file:\n",
    "                json.dump(case_base_data, file)\n",
    "            print(f\"Temporary case base for Agent {agent_idx} saved successfully.\")\n",
    "\n",
    "    def save_case_base(self):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            filename = f\"cases/case_base_agent_{agent_idx}.json\"\n",
    "            case_base_data = [{\"problem\": case.problem.tolist() if isinstance(case.problem, np.ndarray) else case.problem, \n",
    "                            \"solution\": int(case.solution), \n",
    "                            \"trust_value\": int(case.trust_value),\n",
    "                            \"total_time_steps\": int(case.total_time_steps)} for case in self.case_bases[agent_idx]]\n",
    "            with open(filename, 'w') as file:\n",
    "                json.dump(case_base_data, file)\n",
    "            print(f\"Case base for Agent {agent_idx} saved successfully.\")\n",
    "        \n",
    "    def load_case_base(self):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            filename = f\"cases/case_base_agent_{agent_idx}.json\"\n",
    "            try:\n",
    "                with open(filename, 'r') as file:\n",
    "                    case_base_data = json.load(file)\n",
    "                    self.case_bases[agent_idx] = [Case(np.array(case[\"problem\"]), case[\"solution\"], case[\"trust_value\"], case[\"total_time_steps\"]) for case in case_base_data]\n",
    "                    print(f\"Case base for Agent {agent_idx} loaded successfully.\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Case base file for Agent {agent_idx} not found. Starting with an empty case base.\")\n",
    "\n",
    "    def display_success_rate(self, success_rate):\n",
    "        print(f\"Success rate: {success_rate}%\")\n",
    "\n",
    "    def plot_rewards(self, rewards, window=1):\n",
    "        for agent_idx in range(self.env.num_agents):\n",
    "            moving_avg_rewards = [np.mean(rewards[agent_idx][i:i + window]) for i in range(0, len(rewards[agent_idx]), window)]\n",
    "            \n",
    "            plt.plot(moving_avg_rewards, label=f'Agent {agent_idx}')\n",
    "        \n",
    "        plt.xlabel(f'Episode (Averaged over every {window} episodes)')\n",
    "        plt.ylabel('Average Total Reward')\n",
    "        plt.title('Average Rewards over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_total_steps(self, total_steps_list):\n",
    "        plt.plot(total_steps_list)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Steps')\n",
    "        plt.title('Total Steps for Successful Episodes over Episodes')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_resources(self, memory_usage, gpu_memory_usage):\n",
    "        plt.plot(memory_usage, label='Memory (%)')\n",
    "        plt.plot(gpu_memory_usage, label='GPU Memory (MB)')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Resource Usage')\n",
    "        plt.title('Resource Usage over Episodes')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_agents = 2\n",
    "    num_obstacles = 5\n",
    "    obstacles_random_steps = 20\n",
    "    is_agent_silent = False\n",
    "    episodes = 59\n",
    "    max_steps = 1000\n",
    "    alpha = 0.1\n",
    "    gamma = 0.9\n",
    "    epsilon = 0.1\n",
    "    epsilon_decay = 0.995  \n",
    "    epsilon_min = 0.01  \n",
    "    render = True\n",
    "\n",
    "    env = Env(num_agents=num_agents, num_obstacles=num_obstacles, obstacles_random_steps=obstacles_random_steps, is_agent_silent=is_agent_silent)\n",
    "    \n",
    "    num_actions = len(env.action_space)\n",
    "    \n",
    "    agent = QCBRL(num_actions, env, episodes, max_steps, alpha, gamma, epsilon, epsilon_decay, epsilon_min, render)\n",
    "    rewards, success_rate, memory_usage, gpu_memory_usage, total_step_list = agent.run()\n",
    "\n",
    "    agent.display_success_rate(success_rate)\n",
    "    agent.plot_rewards(rewards)\n",
    "    agent.plot_total_steps(total_step_list)\n",
    "    agent.plot_resources(memory_usage, gpu_memory_usage)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
